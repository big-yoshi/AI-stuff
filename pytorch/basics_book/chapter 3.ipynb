{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function _VariableFunctions.tensor>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = torch.tensor([0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0,6.0,13.0, 21.0])\n",
    "t_u = torch.tensor([35.7,55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u +b\n",
    "\n",
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c) ** 2\n",
    "    return squared_diffs.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,\n",
       "        48.4000, 60.4000, 68.4000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.ones(1)\n",
    "b = torch.zeros(1)\n",
    "\n",
    "t_p = model(t_u, w, b)\n",
    "t_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8846)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(t_p, t_c)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.1\n",
    "loss_rate_of_change_w = (loss_fn(model(t_u,w+delta,b),t_c) -\n",
    "                        loss_fn(model(t_u,w-delta,b),t_c)) / (2.0 *delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w - lr * loss_rate_of_change_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_rate_of_change_b = (loss_fn(model(t_u,b+delta,b),t_c) -\n",
    "                        loss_fn(model(t_u,b-delta,b),t_c)) / (2.0 *delta)\n",
    "b = b-lr * loss_rate_of_change_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13.6430])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "loss = nn.MSELoss()\n",
    "input = torch.randn(10,5,requires_grad = True)\n",
    "target = torch.randn(10,5)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2445, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dloss_fn(t_p, t_c):\n",
    "    dsg_diffs = 2* (t_p - t_c)\n",
    "    return dsg_diffs\n",
    "\n",
    "def model(t_u, w, b):\n",
    "    return w * t_u +b\n",
    "\n",
    "def dmodel_dw(t_u, w, b):\n",
    "    return t_u\n",
    "\n",
    "def dmodel_db(t_u, w, b):\n",
    "    return 1.0\n",
    "def grad_fn(t_u, t_c, t_p,w, b):\n",
    "    dloss_dw = dloss_fn(t_p, t_c) * dmodel_dw(t_u,w,b)\n",
    "    dloss_db = dloss_fn(t_p,t_c) * dmodel_db(t_u, w, b)\n",
    "    return torch.stack([dloss_dw.mean(), dloss_db.mean()])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 1763.884644\n",
      "Params: tensor([1., 0.])\n",
      "Grad:  tensor([4517.2964,   82.6000])\n",
      "Epoch 1 Loss 5802484.500000\n",
      "Params: tensor([-44.1730,  -0.8260])\n",
      "Grad:  tensor([-261257.4062,   -4598.9707])\n",
      "Epoch 2 Loss 19408031744.000000\n",
      "Params: tensor([2568.4011,   45.1637])\n",
      "Grad:  tensor([15109615.0000,   266155.7188])\n",
      "Epoch 3 Loss 64915909902336.000000\n",
      "Params: tensor([-148527.7344,   -2616.3933])\n",
      "Grad:  tensor([-8.7385e+08, -1.5393e+07])\n",
      "Epoch 4 Loss 217130439561707520.000000\n",
      "Params: tensor([8589997.0000,  151310.8750])\n",
      "Grad:  tensor([5.0539e+10, 8.9023e+08])\n",
      "Epoch 5 Loss 726257020202974707712.000000\n",
      "Params: tensor([-4.9680e+08, -8.7510e+06])\n",
      "Grad:  tensor([-2.9229e+12, -5.1486e+10])\n",
      "Epoch 6 Loss 2429181687085405986357248.000000\n",
      "Params: tensor([2.8732e+10, 5.0610e+08])\n",
      "Grad:  tensor([1.6904e+14, 2.9776e+12])\n",
      "Epoch 7 Loss 8125117236949438203699396608.000000\n",
      "Params: tensor([-1.6617e+12, -2.9270e+10])\n",
      "Grad:  tensor([-9.7764e+15, -1.7221e+14])\n",
      "Epoch 8 Loss 27176865195881116022129584766976.000000\n",
      "Params: tensor([9.6102e+13, 1.6928e+12])\n",
      "Grad:  tensor([5.6541e+17, 9.9596e+15])\n",
      "Epoch 9 Loss 90901075478458130961171361977860096.000000\n",
      "Params: tensor([-5.5580e+15, -9.7903e+13])\n",
      "Grad:  tensor([-3.2700e+19, -5.7600e+17])\n",
      "Epoch 10 Loss inf\n",
      "Params: tensor([3.2144e+17, 5.6621e+15])\n",
      "Grad:  tensor([1.8912e+21, 3.3313e+19])\n",
      "Epoch 11 Loss inf\n",
      "Params: tensor([-1.8590e+19, -3.2746e+17])\n",
      "Grad:  tensor([-1.0937e+23, -1.9266e+21])\n",
      "Epoch 12 Loss inf\n",
      "Params: tensor([1.0752e+21, 1.8939e+19])\n",
      "Grad:  tensor([6.3256e+24, 1.1142e+23])\n",
      "Epoch 13 Loss inf\n",
      "Params: tensor([-6.2181e+22, -1.0953e+21])\n",
      "Grad:  tensor([-3.6584e+26, -6.4441e+24])\n",
      "Epoch 14 Loss inf\n",
      "Params: tensor([3.5962e+24, 6.3346e+22])\n",
      "Grad:  tensor([2.1158e+28, 3.7269e+26])\n",
      "Epoch 15 Loss inf\n",
      "Params: tensor([-2.0798e+26, -3.6636e+24])\n",
      "Grad:  tensor([-1.2236e+30, -2.1554e+28])\n",
      "Epoch 16 Loss inf\n",
      "Params: tensor([1.2028e+28, 2.1188e+26])\n",
      "Grad:  tensor([7.0769e+31, 1.2466e+30])\n",
      "Epoch 17 Loss inf\n",
      "Params: tensor([-6.9566e+29, -1.2254e+28])\n",
      "Grad:  tensor([-4.0928e+33, -7.2095e+31])\n",
      "Epoch 18 Loss inf\n",
      "Params: tensor([4.0233e+31, 7.0869e+29])\n",
      "Grad:  tensor([2.3671e+35, 4.1695e+33])\n",
      "Epoch 19 Loss inf\n",
      "Params: tensor([-2.3268e+33, -4.0987e+31])\n",
      "Grad:  tensor([-1.3690e+37, -2.4114e+35])\n",
      "Epoch 20 Loss inf\n",
      "Params: tensor([1.3457e+35, 2.3704e+33])\n",
      "Grad:  tensor([       inf, 1.3946e+37])\n",
      "Epoch 21 Loss inf\n",
      "Params: tensor([       -inf, -1.3709e+35])\n",
      "Grad:  tensor([-inf, -inf])\n",
      "Epoch 22 Loss nan\n",
      "Params: tensor([nan, inf])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 23 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 24 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 25 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 26 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 27 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 28 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 29 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 30 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 31 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 32 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 33 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 34 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 35 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 36 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 37 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 38 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 39 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 40 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 41 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 42 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 43 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 44 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 45 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 46 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 47 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 48 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 49 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 50 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 51 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 52 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 53 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 54 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 55 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 56 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 57 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 58 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 59 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 60 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 61 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 62 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 63 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 64 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 65 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 66 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 67 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 68 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 69 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 70 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 71 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 72 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 73 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 74 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 75 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 76 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 77 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 78 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 79 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 80 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 81 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 82 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 83 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 84 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 85 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 86 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 87 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 88 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 89 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 90 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 91 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 92 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 93 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 94 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 95 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 96 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 97 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 98 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n",
      "Epoch 99 Loss nan\n",
      "Params: tensor([nan, nan])\n",
      "Grad:  tensor([nan, nan])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([nan, nan])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0,0.0])\n",
    "epochs = 100\n",
    "lr = 1e-2\n",
    "for epoch in range(epochs):\n",
    "    w,b = params\n",
    "    t_p = model(t_u, w, b)\n",
    "    loss = loss_fn(t_p, t_c)\n",
    "    print(\"Epoch %d Loss %f\"%(epoch, float(loss)))\n",
    "    grad = grad_fn(t_u, t_c, t_p, w, b)\n",
    "    print('Params:',params)\n",
    "    print('Grad: ',grad)\n",
    "    params = params - lr * grad\n",
    "params\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 1763.884644\n",
      "Params: tensor([1., 0.])\n",
      "Grad:  tensor([4517.2964,   82.6000])\n",
      "Epoch 1 Loss 323.090546\n",
      "Params: tensor([ 0.5483, -0.0083])\n",
      "Grad:  tensor([1859.5493,   35.7843])\n",
      "Epoch 2 Loss 78.929634\n",
      "Params: tensor([ 0.3623, -0.0118])\n",
      "Grad:  tensor([765.4666,  16.5122])\n",
      "Epoch 3 Loss 37.552845\n",
      "Params: tensor([ 0.2858, -0.0135])\n",
      "Grad:  tensor([315.0790,   8.5787])\n",
      "Epoch 4 Loss 30.540285\n",
      "Params: tensor([ 0.2543, -0.0143])\n",
      "Grad:  tensor([129.6733,   5.3127])\n",
      "Epoch 5 Loss 29.351152\n",
      "Params: tensor([ 0.2413, -0.0149])\n",
      "Grad:  tensor([53.3496,  3.9682])\n",
      "Epoch 6 Loss 29.148882\n",
      "Params: tensor([ 0.2360, -0.0153])\n",
      "Grad:  tensor([21.9304,  3.4148])\n",
      "Epoch 7 Loss 29.113848\n",
      "Params: tensor([ 0.2338, -0.0156])\n",
      "Grad:  tensor([8.9964, 3.1869])\n",
      "Epoch 8 Loss 29.107145\n",
      "Params: tensor([ 0.2329, -0.0159])\n",
      "Grad:  tensor([3.6721, 3.0930])\n",
      "Epoch 9 Loss 29.105242\n",
      "Params: tensor([ 0.2325, -0.0162])\n",
      "Grad:  tensor([1.4803, 3.0544])\n",
      "Epoch 10 Loss 29.104168\n",
      "Params: tensor([ 0.2324, -0.0166])\n",
      "Grad:  tensor([0.5780, 3.0384])\n",
      "Epoch 11 Loss 29.103222\n",
      "Params: tensor([ 0.2323, -0.0169])\n",
      "Grad:  tensor([0.2066, 3.0318])\n",
      "Epoch 12 Loss 29.102297\n",
      "Params: tensor([ 0.2323, -0.0172])\n",
      "Grad:  tensor([0.0537, 3.0291])\n",
      "Epoch 13 Loss 29.101379\n",
      "Params: tensor([ 0.2323, -0.0175])\n",
      "Grad:  tensor([-0.0093,  3.0279])\n",
      "Epoch 14 Loss 29.100470\n",
      "Params: tensor([ 0.2323, -0.0178])\n",
      "Grad:  tensor([-0.0353,  3.0274])\n",
      "Epoch 15 Loss 29.099548\n",
      "Params: tensor([ 0.2323, -0.0181])\n",
      "Grad:  tensor([-0.0459,  3.0272])\n",
      "Epoch 16 Loss 29.098631\n",
      "Params: tensor([ 0.2323, -0.0184])\n",
      "Grad:  tensor([-0.0502,  3.0270])\n",
      "Epoch 17 Loss 29.097715\n",
      "Params: tensor([ 0.2323, -0.0187])\n",
      "Grad:  tensor([-0.0520,  3.0270])\n",
      "Epoch 18 Loss 29.096796\n",
      "Params: tensor([ 0.2323, -0.0190])\n",
      "Grad:  tensor([-0.0528,  3.0269])\n",
      "Epoch 19 Loss 29.095884\n",
      "Params: tensor([ 0.2323, -0.0193])\n",
      "Grad:  tensor([-0.0531,  3.0268])\n",
      "Epoch 20 Loss 29.094959\n",
      "Params: tensor([ 0.2323, -0.0196])\n",
      "Grad:  tensor([-0.0533,  3.0268])\n",
      "Epoch 21 Loss 29.094049\n",
      "Params: tensor([ 0.2323, -0.0199])\n",
      "Grad:  tensor([-0.0533,  3.0267])\n",
      "Epoch 22 Loss 29.093134\n",
      "Params: tensor([ 0.2323, -0.0202])\n",
      "Grad:  tensor([-0.0533,  3.0267])\n",
      "Epoch 23 Loss 29.092213\n",
      "Params: tensor([ 0.2323, -0.0205])\n",
      "Grad:  tensor([-0.0533,  3.0266])\n",
      "Epoch 24 Loss 29.091297\n",
      "Params: tensor([ 0.2323, -0.0208])\n",
      "Grad:  tensor([-0.0533,  3.0266])\n",
      "Epoch 25 Loss 29.090382\n",
      "Params: tensor([ 0.2323, -0.0211])\n",
      "Grad:  tensor([-0.0533,  3.0265])\n",
      "Epoch 26 Loss 29.089460\n",
      "Params: tensor([ 0.2323, -0.0214])\n",
      "Grad:  tensor([-0.0533,  3.0265])\n",
      "Epoch 27 Loss 29.088549\n",
      "Params: tensor([ 0.2323, -0.0217])\n",
      "Grad:  tensor([-0.0532,  3.0264])\n",
      "Epoch 28 Loss 29.087635\n",
      "Params: tensor([ 0.2323, -0.0220])\n",
      "Grad:  tensor([-0.0533,  3.0264])\n",
      "Epoch 29 Loss 29.086718\n",
      "Params: tensor([ 0.2323, -0.0223])\n",
      "Grad:  tensor([-0.0533,  3.0263])\n",
      "Epoch 30 Loss 29.085808\n",
      "Params: tensor([ 0.2323, -0.0226])\n",
      "Grad:  tensor([-0.0532,  3.0262])\n",
      "Epoch 31 Loss 29.084888\n",
      "Params: tensor([ 0.2324, -0.0229])\n",
      "Grad:  tensor([-0.0533,  3.0262])\n",
      "Epoch 32 Loss 29.083965\n",
      "Params: tensor([ 0.2324, -0.0232])\n",
      "Grad:  tensor([-0.0533,  3.0261])\n",
      "Epoch 33 Loss 29.083057\n",
      "Params: tensor([ 0.2324, -0.0235])\n",
      "Grad:  tensor([-0.0533,  3.0261])\n",
      "Epoch 34 Loss 29.082142\n",
      "Params: tensor([ 0.2324, -0.0238])\n",
      "Grad:  tensor([-0.0532,  3.0260])\n",
      "Epoch 35 Loss 29.081219\n",
      "Params: tensor([ 0.2324, -0.0241])\n",
      "Grad:  tensor([-0.0533,  3.0260])\n",
      "Epoch 36 Loss 29.080309\n",
      "Params: tensor([ 0.2324, -0.0244])\n",
      "Grad:  tensor([-0.0533,  3.0259])\n",
      "Epoch 37 Loss 29.079393\n",
      "Params: tensor([ 0.2324, -0.0247])\n",
      "Grad:  tensor([-0.0532,  3.0259])\n",
      "Epoch 38 Loss 29.078474\n",
      "Params: tensor([ 0.2324, -0.0250])\n",
      "Grad:  tensor([-0.0533,  3.0258])\n",
      "Epoch 39 Loss 29.077559\n",
      "Params: tensor([ 0.2324, -0.0253])\n",
      "Grad:  tensor([-0.0533,  3.0258])\n",
      "Epoch 40 Loss 29.076653\n",
      "Params: tensor([ 0.2324, -0.0256])\n",
      "Grad:  tensor([-0.0533,  3.0257])\n",
      "Epoch 41 Loss 29.075731\n",
      "Params: tensor([ 0.2324, -0.0259])\n",
      "Grad:  tensor([-0.0532,  3.0257])\n",
      "Epoch 42 Loss 29.074812\n",
      "Params: tensor([ 0.2324, -0.0262])\n",
      "Grad:  tensor([-0.0533,  3.0256])\n",
      "Epoch 43 Loss 29.073896\n",
      "Params: tensor([ 0.2324, -0.0265])\n",
      "Grad:  tensor([-0.0533,  3.0256])\n",
      "Epoch 44 Loss 29.072985\n",
      "Params: tensor([ 0.2324, -0.0268])\n",
      "Grad:  tensor([-0.0533,  3.0255])\n",
      "Epoch 45 Loss 29.072069\n",
      "Params: tensor([ 0.2324, -0.0271])\n",
      "Grad:  tensor([-0.0533,  3.0254])\n",
      "Epoch 46 Loss 29.071148\n",
      "Params: tensor([ 0.2324, -0.0274])\n",
      "Grad:  tensor([-0.0533,  3.0254])\n",
      "Epoch 47 Loss 29.070234\n",
      "Params: tensor([ 0.2324, -0.0277])\n",
      "Grad:  tensor([-0.0533,  3.0253])\n",
      "Epoch 48 Loss 29.069323\n",
      "Params: tensor([ 0.2324, -0.0281])\n",
      "Grad:  tensor([-0.0533,  3.0253])\n",
      "Epoch 49 Loss 29.068401\n",
      "Params: tensor([ 0.2325, -0.0284])\n",
      "Grad:  tensor([-0.0532,  3.0252])\n",
      "Epoch 50 Loss 29.067486\n",
      "Params: tensor([ 0.2325, -0.0287])\n",
      "Grad:  tensor([-0.0533,  3.0252])\n",
      "Epoch 51 Loss 29.066570\n",
      "Params: tensor([ 0.2325, -0.0290])\n",
      "Grad:  tensor([-0.0533,  3.0251])\n",
      "Epoch 52 Loss 29.065655\n",
      "Params: tensor([ 0.2325, -0.0293])\n",
      "Grad:  tensor([-0.0533,  3.0251])\n",
      "Epoch 53 Loss 29.064739\n",
      "Params: tensor([ 0.2325, -0.0296])\n",
      "Grad:  tensor([-0.0533,  3.0250])\n",
      "Epoch 54 Loss 29.063829\n",
      "Params: tensor([ 0.2325, -0.0299])\n",
      "Grad:  tensor([-0.0532,  3.0250])\n",
      "Epoch 55 Loss 29.062910\n",
      "Params: tensor([ 0.2325, -0.0302])\n",
      "Grad:  tensor([-0.0533,  3.0249])\n",
      "Epoch 56 Loss 29.061989\n",
      "Params: tensor([ 0.2325, -0.0305])\n",
      "Grad:  tensor([-0.0532,  3.0249])\n",
      "Epoch 57 Loss 29.061079\n",
      "Params: tensor([ 0.2325, -0.0308])\n",
      "Grad:  tensor([-0.0533,  3.0248])\n",
      "Epoch 58 Loss 29.060169\n",
      "Params: tensor([ 0.2325, -0.0311])\n",
      "Grad:  tensor([-0.0533,  3.0248])\n",
      "Epoch 59 Loss 29.059252\n",
      "Params: tensor([ 0.2325, -0.0314])\n",
      "Grad:  tensor([-0.0533,  3.0247])\n",
      "Epoch 60 Loss 29.058331\n",
      "Params: tensor([ 0.2325, -0.0317])\n",
      "Grad:  tensor([-0.0532,  3.0247])\n",
      "Epoch 61 Loss 29.057417\n",
      "Params: tensor([ 0.2325, -0.0320])\n",
      "Grad:  tensor([-0.0533,  3.0246])\n",
      "Epoch 62 Loss 29.056507\n",
      "Params: tensor([ 0.2325, -0.0323])\n",
      "Grad:  tensor([-0.0533,  3.0245])\n",
      "Epoch 63 Loss 29.055586\n",
      "Params: tensor([ 0.2325, -0.0326])\n",
      "Grad:  tensor([-0.0532,  3.0245])\n",
      "Epoch 64 Loss 29.054670\n",
      "Params: tensor([ 0.2325, -0.0329])\n",
      "Grad:  tensor([-0.0533,  3.0244])\n",
      "Epoch 65 Loss 29.053761\n",
      "Params: tensor([ 0.2325, -0.0332])\n",
      "Grad:  tensor([-0.0533,  3.0244])\n",
      "Epoch 66 Loss 29.052843\n",
      "Params: tensor([ 0.2325, -0.0335])\n",
      "Grad:  tensor([-0.0533,  3.0243])\n",
      "Epoch 67 Loss 29.051929\n",
      "Params: tensor([ 0.2325, -0.0338])\n",
      "Grad:  tensor([-0.0533,  3.0243])\n",
      "Epoch 68 Loss 29.051014\n",
      "Params: tensor([ 0.2326, -0.0341])\n",
      "Grad:  tensor([-0.0533,  3.0242])\n",
      "Epoch 69 Loss 29.050098\n",
      "Params: tensor([ 0.2326, -0.0344])\n",
      "Grad:  tensor([-0.0532,  3.0242])\n",
      "Epoch 70 Loss 29.049183\n",
      "Params: tensor([ 0.2326, -0.0347])\n",
      "Grad:  tensor([-0.0533,  3.0241])\n",
      "Epoch 71 Loss 29.048271\n",
      "Params: tensor([ 0.2326, -0.0350])\n",
      "Grad:  tensor([-0.0533,  3.0241])\n",
      "Epoch 72 Loss 29.047346\n",
      "Params: tensor([ 0.2326, -0.0353])\n",
      "Grad:  tensor([-0.0532,  3.0240])\n",
      "Epoch 73 Loss 29.046442\n",
      "Params: tensor([ 0.2326, -0.0356])\n",
      "Grad:  tensor([-0.0533,  3.0240])\n",
      "Epoch 74 Loss 29.045530\n",
      "Params: tensor([ 0.2326, -0.0359])\n",
      "Grad:  tensor([-0.0533,  3.0239])\n",
      "Epoch 75 Loss 29.044611\n",
      "Params: tensor([ 0.2326, -0.0362])\n",
      "Grad:  tensor([-0.0533,  3.0239])\n",
      "Epoch 76 Loss 29.043699\n",
      "Params: tensor([ 0.2326, -0.0365])\n",
      "Grad:  tensor([-0.0533,  3.0238])\n",
      "Epoch 77 Loss 29.042780\n",
      "Params: tensor([ 0.2326, -0.0368])\n",
      "Grad:  tensor([-0.0533,  3.0238])\n",
      "Epoch 78 Loss 29.041870\n",
      "Params: tensor([ 0.2326, -0.0371])\n",
      "Grad:  tensor([-0.0533,  3.0237])\n",
      "Epoch 79 Loss 29.040955\n",
      "Params: tensor([ 0.2326, -0.0374])\n",
      "Grad:  tensor([-0.0532,  3.0236])\n",
      "Epoch 80 Loss 29.040039\n",
      "Params: tensor([ 0.2326, -0.0377])\n",
      "Grad:  tensor([-0.0534,  3.0236])\n",
      "Epoch 81 Loss 29.039122\n",
      "Params: tensor([ 0.2326, -0.0380])\n",
      "Grad:  tensor([-0.0533,  3.0235])\n",
      "Epoch 82 Loss 29.038214\n",
      "Params: tensor([ 0.2326, -0.0383])\n",
      "Grad:  tensor([-0.0532,  3.0235])\n",
      "Epoch 83 Loss 29.037292\n",
      "Params: tensor([ 0.2326, -0.0386])\n",
      "Grad:  tensor([-0.0533,  3.0234])\n",
      "Epoch 84 Loss 29.036379\n",
      "Params: tensor([ 0.2326, -0.0389])\n",
      "Grad:  tensor([-0.0532,  3.0234])\n",
      "Epoch 85 Loss 29.035463\n",
      "Params: tensor([ 0.2326, -0.0392])\n",
      "Grad:  tensor([-0.0532,  3.0233])\n",
      "Epoch 86 Loss 29.034557\n",
      "Params: tensor([ 0.2326, -0.0395])\n",
      "Grad:  tensor([-0.0533,  3.0233])\n",
      "Epoch 87 Loss 29.033638\n",
      "Params: tensor([ 0.2327, -0.0398])\n",
      "Grad:  tensor([-0.0532,  3.0232])\n",
      "Epoch 88 Loss 29.032721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([ 0.2327, -0.0401])\n",
      "Grad:  tensor([-0.0533,  3.0232])\n",
      "Epoch 89 Loss 29.031805\n",
      "Params: tensor([ 0.2327, -0.0405])\n",
      "Grad:  tensor([-0.0533,  3.0231])\n",
      "Epoch 90 Loss 29.030895\n",
      "Params: tensor([ 0.2327, -0.0408])\n",
      "Grad:  tensor([-0.0532,  3.0231])\n",
      "Epoch 91 Loss 29.029976\n",
      "Params: tensor([ 0.2327, -0.0411])\n",
      "Grad:  tensor([-0.0532,  3.0230])\n",
      "Epoch 92 Loss 29.029066\n",
      "Params: tensor([ 0.2327, -0.0414])\n",
      "Grad:  tensor([-0.0533,  3.0230])\n",
      "Epoch 93 Loss 29.028151\n",
      "Params: tensor([ 0.2327, -0.0417])\n",
      "Grad:  tensor([-0.0532,  3.0229])\n",
      "Epoch 94 Loss 29.027235\n",
      "Params: tensor([ 0.2327, -0.0420])\n",
      "Grad:  tensor([-0.0532,  3.0229])\n",
      "Epoch 95 Loss 29.026323\n",
      "Params: tensor([ 0.2327, -0.0423])\n",
      "Grad:  tensor([-0.0533,  3.0228])\n",
      "Epoch 96 Loss 29.025410\n",
      "Params: tensor([ 0.2327, -0.0426])\n",
      "Grad:  tensor([-0.0533,  3.0227])\n",
      "Epoch 97 Loss 29.024494\n",
      "Params: tensor([ 0.2327, -0.0429])\n",
      "Grad:  tensor([-0.0533,  3.0227])\n",
      "Epoch 98 Loss 29.023582\n",
      "Params: tensor([ 0.2327, -0.0432])\n",
      "Grad:  tensor([-0.0533,  3.0226])\n",
      "Epoch 99 Loss 29.022669\n",
      "Params: tensor([ 0.2327, -0.0435])\n",
      "Grad:  tensor([-0.0532,  3.0226])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2327, -0.0438])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0,0.0])\n",
    "epochs = 100\n",
    "lr = 1e-4\n",
    "for epoch in range(epochs):\n",
    "    w,b = params\n",
    "    t_p = model(t_u, w, b)\n",
    "    loss = loss_fn(t_p, t_c)\n",
    "    print(\"Epoch %d Loss %f\"%(epoch, float(loss)))\n",
    "    grad = grad_fn(t_u, t_c, t_p, w, b)\n",
    "    print('Params:',params)\n",
    "    print('Grad: ',grad)\n",
    "    params = params - lr * grad\n",
    "params\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 80.364342\n",
      "Params: tensor([1., 0.])\n",
      "Grad:  tensor([-77.6140, -10.6400])\n",
      "Epoch 1 Loss 37.574917\n",
      "Params: tensor([1.7761, 0.1064])\n",
      "Grad:  tensor([-30.8623,  -2.3864])\n",
      "Epoch 2 Loss 30.871077\n",
      "Params: tensor([2.0848, 0.1303])\n",
      "Grad:  tensor([-12.4631,   0.8587])\n",
      "Epoch 3 Loss 29.756193\n",
      "Params: tensor([2.2094, 0.1217])\n",
      "Grad:  tensor([-5.2218,  2.1327])\n",
      "Epoch 4 Loss 29.507149\n",
      "Params: tensor([2.2616, 0.1004])\n",
      "Grad:  tensor([-2.3715,  2.6310])\n",
      "Epoch 5 Loss 29.392458\n",
      "Params: tensor([2.2853, 0.0740])\n",
      "Grad:  tensor([-1.2492,  2.8241])\n",
      "Epoch 6 Loss 29.298828\n",
      "Params: tensor([2.2978, 0.0458])\n",
      "Grad:  tensor([-0.8071,  2.8970])\n",
      "Epoch 7 Loss 29.208717\n",
      "Params: tensor([2.3059, 0.0168])\n",
      "Grad:  tensor([-0.6325,  2.9227])\n",
      "Epoch 8 Loss 29.119417\n",
      "Params: tensor([ 2.3122, -0.0124])\n",
      "Grad:  tensor([-0.5633,  2.9298])\n",
      "Epoch 9 Loss 29.030487\n",
      "Params: tensor([ 2.3178, -0.0417])\n",
      "Grad:  tensor([-0.5355,  2.9295])\n",
      "Epoch 10 Loss 28.941875\n",
      "Params: tensor([ 2.3232, -0.0710])\n",
      "Grad:  tensor([-0.5240,  2.9264])\n",
      "Epoch 11 Loss 28.853565\n",
      "Params: tensor([ 2.3284, -0.1003])\n",
      "Grad:  tensor([-0.5190,  2.9222])\n",
      "Epoch 12 Loss 28.765556\n",
      "Params: tensor([ 2.3336, -0.1295])\n",
      "Grad:  tensor([-0.5165,  2.9175])\n",
      "Epoch 13 Loss 28.677851\n",
      "Params: tensor([ 2.3388, -0.1587])\n",
      "Grad:  tensor([-0.5150,  2.9126])\n",
      "Epoch 14 Loss 28.590431\n",
      "Params: tensor([ 2.3439, -0.1878])\n",
      "Grad:  tensor([-0.5138,  2.9077])\n",
      "Epoch 15 Loss 28.503321\n",
      "Params: tensor([ 2.3491, -0.2169])\n",
      "Grad:  tensor([-0.5129,  2.9028])\n",
      "Epoch 16 Loss 28.416496\n",
      "Params: tensor([ 2.3542, -0.2459])\n",
      "Grad:  tensor([-0.5120,  2.8979])\n",
      "Epoch 17 Loss 28.329975\n",
      "Params: tensor([ 2.3593, -0.2749])\n",
      "Grad:  tensor([-0.5111,  2.8930])\n",
      "Epoch 18 Loss 28.243738\n",
      "Params: tensor([ 2.3644, -0.3038])\n",
      "Grad:  tensor([-0.5102,  2.8881])\n",
      "Epoch 19 Loss 28.157801\n",
      "Params: tensor([ 2.3695, -0.3327])\n",
      "Grad:  tensor([-0.5093,  2.8832])\n",
      "Epoch 20 Loss 28.072151\n",
      "Params: tensor([ 2.3746, -0.3615])\n",
      "Grad:  tensor([-0.5084,  2.8783])\n",
      "Epoch 21 Loss 27.986799\n",
      "Params: tensor([ 2.3797, -0.3903])\n",
      "Grad:  tensor([-0.5076,  2.8734])\n",
      "Epoch 22 Loss 27.901731\n",
      "Params: tensor([ 2.3848, -0.4190])\n",
      "Grad:  tensor([-0.5067,  2.8685])\n",
      "Epoch 23 Loss 27.816956\n",
      "Params: tensor([ 2.3899, -0.4477])\n",
      "Grad:  tensor([-0.5059,  2.8636])\n",
      "Epoch 24 Loss 27.732466\n",
      "Params: tensor([ 2.3949, -0.4763])\n",
      "Grad:  tensor([-0.5050,  2.8588])\n",
      "Epoch 25 Loss 27.648256\n",
      "Params: tensor([ 2.4000, -0.5049])\n",
      "Grad:  tensor([-0.5042,  2.8539])\n",
      "Epoch 26 Loss 27.564342\n",
      "Params: tensor([ 2.4050, -0.5335])\n",
      "Grad:  tensor([-0.5033,  2.8490])\n",
      "Epoch 27 Loss 27.480711\n",
      "Params: tensor([ 2.4101, -0.5620])\n",
      "Grad:  tensor([-0.5024,  2.8442])\n",
      "Epoch 28 Loss 27.397358\n",
      "Params: tensor([ 2.4151, -0.5904])\n",
      "Grad:  tensor([-0.5016,  2.8394])\n",
      "Epoch 29 Loss 27.314295\n",
      "Params: tensor([ 2.4201, -0.6188])\n",
      "Grad:  tensor([-0.5007,  2.8346])\n",
      "Epoch 30 Loss 27.231512\n",
      "Params: tensor([ 2.4251, -0.6471])\n",
      "Grad:  tensor([-0.4999,  2.8297])\n",
      "Epoch 31 Loss 27.149006\n",
      "Params: tensor([ 2.4301, -0.6754])\n",
      "Grad:  tensor([-0.4990,  2.8249])\n",
      "Epoch 32 Loss 27.066790\n",
      "Params: tensor([ 2.4351, -0.7037])\n",
      "Grad:  tensor([-0.4982,  2.8201])\n",
      "Epoch 33 Loss 26.984844\n",
      "Params: tensor([ 2.4401, -0.7319])\n",
      "Grad:  tensor([-0.4973,  2.8153])\n",
      "Epoch 34 Loss 26.903173\n",
      "Params: tensor([ 2.4450, -0.7600])\n",
      "Grad:  tensor([-0.4965,  2.8106])\n",
      "Epoch 35 Loss 26.821791\n",
      "Params: tensor([ 2.4500, -0.7881])\n",
      "Grad:  tensor([-0.4957,  2.8058])\n",
      "Epoch 36 Loss 26.740675\n",
      "Params: tensor([ 2.4550, -0.8162])\n",
      "Grad:  tensor([-0.4948,  2.8010])\n",
      "Epoch 37 Loss 26.659838\n",
      "Params: tensor([ 2.4599, -0.8442])\n",
      "Grad:  tensor([-0.4940,  2.7963])\n",
      "Epoch 38 Loss 26.579279\n",
      "Params: tensor([ 2.4649, -0.8722])\n",
      "Grad:  tensor([-0.4931,  2.7915])\n",
      "Epoch 39 Loss 26.498987\n",
      "Params: tensor([ 2.4698, -0.9001])\n",
      "Grad:  tensor([-0.4923,  2.7868])\n",
      "Epoch 40 Loss 26.418974\n",
      "Params: tensor([ 2.4747, -0.9280])\n",
      "Grad:  tensor([-0.4915,  2.7820])\n",
      "Epoch 41 Loss 26.339228\n",
      "Params: tensor([ 2.4796, -0.9558])\n",
      "Grad:  tensor([-0.4906,  2.7773])\n",
      "Epoch 42 Loss 26.259752\n",
      "Params: tensor([ 2.4845, -0.9836])\n",
      "Grad:  tensor([-0.4898,  2.7726])\n",
      "Epoch 43 Loss 26.180548\n",
      "Params: tensor([ 2.4894, -1.0113])\n",
      "Grad:  tensor([-0.4890,  2.7679])\n",
      "Epoch 44 Loss 26.101616\n",
      "Params: tensor([ 2.4943, -1.0390])\n",
      "Grad:  tensor([-0.4881,  2.7632])\n",
      "Epoch 45 Loss 26.022949\n",
      "Params: tensor([ 2.4992, -1.0666])\n",
      "Grad:  tensor([-0.4873,  2.7585])\n",
      "Epoch 46 Loss 25.944542\n",
      "Params: tensor([ 2.5041, -1.0942])\n",
      "Grad:  tensor([-0.4865,  2.7538])\n",
      "Epoch 47 Loss 25.866417\n",
      "Params: tensor([ 2.5089, -1.1217])\n",
      "Grad:  tensor([-0.4856,  2.7491])\n",
      "Epoch 48 Loss 25.788546\n",
      "Params: tensor([ 2.5138, -1.1492])\n",
      "Grad:  tensor([-0.4848,  2.7444])\n",
      "Epoch 49 Loss 25.710936\n",
      "Params: tensor([ 2.5186, -1.1766])\n",
      "Grad:  tensor([-0.4840,  2.7398])\n",
      "Epoch 50 Loss 25.633600\n",
      "Params: tensor([ 2.5235, -1.2040])\n",
      "Grad:  tensor([-0.4832,  2.7351])\n",
      "Epoch 51 Loss 25.556524\n",
      "Params: tensor([ 2.5283, -1.2314])\n",
      "Grad:  tensor([-0.4823,  2.7305])\n",
      "Epoch 52 Loss 25.479700\n",
      "Params: tensor([ 2.5331, -1.2587])\n",
      "Grad:  tensor([-0.4815,  2.7258])\n",
      "Epoch 53 Loss 25.403149\n",
      "Params: tensor([ 2.5379, -1.2860])\n",
      "Grad:  tensor([-0.4807,  2.7212])\n",
      "Epoch 54 Loss 25.326851\n",
      "Params: tensor([ 2.5428, -1.3132])\n",
      "Grad:  tensor([-0.4799,  2.7166])\n",
      "Epoch 55 Loss 25.250811\n",
      "Params: tensor([ 2.5476, -1.3403])\n",
      "Grad:  tensor([-0.4791,  2.7120])\n",
      "Epoch 56 Loss 25.175035\n",
      "Params: tensor([ 2.5523, -1.3675])\n",
      "Grad:  tensor([-0.4783,  2.7074])\n",
      "Epoch 57 Loss 25.099510\n",
      "Params: tensor([ 2.5571, -1.3945])\n",
      "Grad:  tensor([-0.4775,  2.7028])\n",
      "Epoch 58 Loss 25.024248\n",
      "Params: tensor([ 2.5619, -1.4216])\n",
      "Grad:  tensor([-0.4766,  2.6982])\n",
      "Epoch 59 Loss 24.949238\n",
      "Params: tensor([ 2.5667, -1.4485])\n",
      "Grad:  tensor([-0.4758,  2.6936])\n",
      "Epoch 60 Loss 24.874483\n",
      "Params: tensor([ 2.5714, -1.4755])\n",
      "Grad:  tensor([-0.4750,  2.6890])\n",
      "Epoch 61 Loss 24.799980\n",
      "Params: tensor([ 2.5762, -1.5024])\n",
      "Grad:  tensor([-0.4742,  2.6845])\n",
      "Epoch 62 Loss 24.725737\n",
      "Params: tensor([ 2.5809, -1.5292])\n",
      "Grad:  tensor([-0.4734,  2.6799])\n",
      "Epoch 63 Loss 24.651735\n",
      "Params: tensor([ 2.5857, -1.5560])\n",
      "Grad:  tensor([-0.4726,  2.6753])\n",
      "Epoch 64 Loss 24.577986\n",
      "Params: tensor([ 2.5904, -1.5828])\n",
      "Grad:  tensor([-0.4718,  2.6708])\n",
      "Epoch 65 Loss 24.504494\n",
      "Params: tensor([ 2.5951, -1.6095])\n",
      "Grad:  tensor([-0.4710,  2.6663])\n",
      "Epoch 66 Loss 24.431250\n",
      "Params: tensor([ 2.5998, -1.6361])\n",
      "Grad:  tensor([-0.4702,  2.6617])\n",
      "Epoch 67 Loss 24.358257\n",
      "Params: tensor([ 2.6045, -1.6628])\n",
      "Grad:  tensor([-0.4694,  2.6572])\n",
      "Epoch 68 Loss 24.285505\n",
      "Params: tensor([ 2.6092, -1.6893])\n",
      "Grad:  tensor([-0.4686,  2.6527])\n",
      "Epoch 69 Loss 24.212996\n",
      "Params: tensor([ 2.6139, -1.7159])\n",
      "Grad:  tensor([-0.4678,  2.6482])\n",
      "Epoch 70 Loss 24.140741\n",
      "Params: tensor([ 2.6186, -1.7423])\n",
      "Grad:  tensor([-0.4670,  2.6437])\n",
      "Epoch 71 Loss 24.068733\n",
      "Params: tensor([ 2.6232, -1.7688])\n",
      "Grad:  tensor([-0.4662,  2.6392])\n",
      "Epoch 72 Loss 23.996967\n",
      "Params: tensor([ 2.6279, -1.7952])\n",
      "Grad:  tensor([-0.4654,  2.6347])\n",
      "Epoch 73 Loss 23.925446\n",
      "Params: tensor([ 2.6326, -1.8215])\n",
      "Grad:  tensor([-0.4646,  2.6302])\n",
      "Epoch 74 Loss 23.854168\n",
      "Params: tensor([ 2.6372, -1.8478])\n",
      "Grad:  tensor([-0.4638,  2.6258])\n",
      "Epoch 75 Loss 23.783125\n",
      "Params: tensor([ 2.6418, -1.8741])\n",
      "Grad:  tensor([-0.4631,  2.6213])\n",
      "Epoch 76 Loss 23.712328\n",
      "Params: tensor([ 2.6465, -1.9003])\n",
      "Grad:  tensor([-0.4623,  2.6169])\n",
      "Epoch 77 Loss 23.641771\n",
      "Params: tensor([ 2.6511, -1.9265])\n",
      "Grad:  tensor([-0.4615,  2.6124])\n",
      "Epoch 78 Loss 23.571455\n",
      "Params: tensor([ 2.6557, -1.9526])\n",
      "Grad:  tensor([-0.4607,  2.6080])\n",
      "Epoch 79 Loss 23.501379\n",
      "Params: tensor([ 2.6603, -1.9787])\n",
      "Grad:  tensor([-0.4599,  2.6035])\n",
      "Epoch 80 Loss 23.431538\n",
      "Params: tensor([ 2.6649, -2.0047])\n",
      "Grad:  tensor([-0.4591,  2.5991])\n",
      "Epoch 81 Loss 23.361938\n",
      "Params: tensor([ 2.6695, -2.0307])\n",
      "Grad:  tensor([-0.4584,  2.5947])\n",
      "Epoch 82 Loss 23.292570\n",
      "Params: tensor([ 2.6741, -2.0566])\n",
      "Grad:  tensor([-0.4576,  2.5903])\n",
      "Epoch 83 Loss 23.223436\n",
      "Params: tensor([ 2.6787, -2.0825])\n",
      "Grad:  tensor([-0.4568,  2.5859])\n",
      "Epoch 84 Loss 23.154539\n",
      "Params: tensor([ 2.6832, -2.1084])\n",
      "Grad:  tensor([-0.4560,  2.5815])\n",
      "Epoch 85 Loss 23.085882\n",
      "Params: tensor([ 2.6878, -2.1342])\n",
      "Grad:  tensor([-0.4553,  2.5771])\n",
      "Epoch 86 Loss 23.017447\n",
      "Params: tensor([ 2.6923, -2.1600])\n",
      "Grad:  tensor([-0.4545,  2.5727])\n",
      "Epoch 87 Loss 22.949251\n",
      "Params: tensor([ 2.6969, -2.1857])\n",
      "Grad:  tensor([-0.4537,  2.5684])\n",
      "Epoch 88 Loss 22.881283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([ 2.7014, -2.2114])\n",
      "Grad:  tensor([-0.4529,  2.5640])\n",
      "Epoch 89 Loss 22.813547\n",
      "Params: tensor([ 2.7060, -2.2370])\n",
      "Grad:  tensor([-0.4522,  2.5597])\n",
      "Epoch 90 Loss 22.746044\n",
      "Params: tensor([ 2.7105, -2.2626])\n",
      "Grad:  tensor([-0.4514,  2.5553])\n",
      "Epoch 91 Loss 22.678770\n",
      "Params: tensor([ 2.7150, -2.2882])\n",
      "Grad:  tensor([-0.4506,  2.5510])\n",
      "Epoch 92 Loss 22.611717\n",
      "Params: tensor([ 2.7195, -2.3137])\n",
      "Grad:  tensor([-0.4499,  2.5466])\n",
      "Epoch 93 Loss 22.544899\n",
      "Params: tensor([ 2.7240, -2.3392])\n",
      "Grad:  tensor([-0.4491,  2.5423])\n",
      "Epoch 94 Loss 22.478304\n",
      "Params: tensor([ 2.7285, -2.3646])\n",
      "Grad:  tensor([-0.4483,  2.5380])\n",
      "Epoch 95 Loss 22.411938\n",
      "Params: tensor([ 2.7330, -2.3900])\n",
      "Grad:  tensor([-0.4476,  2.5337])\n",
      "Epoch 96 Loss 22.345795\n",
      "Params: tensor([ 2.7374, -2.4153])\n",
      "Grad:  tensor([-0.4468,  2.5294])\n",
      "Epoch 97 Loss 22.279875\n",
      "Params: tensor([ 2.7419, -2.4406])\n",
      "Grad:  tensor([-0.4461,  2.5251])\n",
      "Epoch 98 Loss 22.214186\n",
      "Params: tensor([ 2.7464, -2.4658])\n",
      "Grad:  tensor([-0.4453,  2.5208])\n",
      "Epoch 99 Loss 22.148710\n",
      "Params: tensor([ 2.7508, -2.4910])\n",
      "Grad:  tensor([-0.4445,  2.5165])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 2.7553, -2.5162])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0,0.0])\n",
    "epochs = 100\n",
    "t_u = 0.1 * t_u\n",
    "lr = 1e-2\n",
    "for epoch in range(epochs):\n",
    "    w,b = params\n",
    "    t_p = model(t_u, w, b)\n",
    "    loss = loss_fn(t_p, t_c)\n",
    "    print(\"Epoch %d Loss %f\"%(epoch, float(loss)))\n",
    "    grad = grad_fn(t_u, t_c, t_p, w, b)\n",
    "    print('Params:',params)\n",
    "    print('Grad: ',grad)\n",
    "    params = params - lr * grad\n",
    "params\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 80.364342\n",
      "Params: tensor([1., 0.])\n",
      "Grad:  tensor([-77.6140, -10.6400])\n",
      "Epoch 1 Loss 79.752480\n",
      "Params: tensor([1.0078, 0.0011])\n",
      "Grad:  tensor([-77.1465, -10.5575])\n",
      "Epoch 2 Loss 79.148026\n",
      "Params: tensor([1.0155, 0.0021])\n",
      "Grad:  tensor([-76.6818, -10.4754])\n",
      "Epoch 3 Loss 78.550858\n",
      "Params: tensor([1.0231, 0.0032])\n",
      "Grad:  tensor([-76.2200, -10.3939])\n",
      "Epoch 4 Loss 77.960892\n",
      "Params: tensor([1.0308, 0.0042])\n",
      "Grad:  tensor([-75.7609, -10.3128])\n",
      "Epoch 5 Loss 77.378052\n",
      "Params: tensor([1.0383, 0.0052])\n",
      "Grad:  tensor([-75.3046, -10.2323])\n",
      "Epoch 6 Loss 76.802254\n",
      "Params: tensor([1.0459, 0.0063])\n",
      "Grad:  tensor([-74.8511, -10.1522])\n",
      "Epoch 7 Loss 76.233414\n",
      "Params: tensor([1.0534, 0.0073])\n",
      "Grad:  tensor([-74.4004, -10.0727])\n",
      "Epoch 8 Loss 75.671417\n",
      "Params: tensor([1.0608, 0.0083])\n",
      "Grad:  tensor([-73.9523,  -9.9936])\n",
      "Epoch 9 Loss 75.116219\n",
      "Params: tensor([1.0682, 0.0093])\n",
      "Grad:  tensor([-73.5070,  -9.9150])\n",
      "Epoch 10 Loss 74.567734\n",
      "Params: tensor([1.0755, 0.0103])\n",
      "Grad:  tensor([-73.0644,  -9.8368])\n",
      "Epoch 11 Loss 74.025856\n",
      "Params: tensor([1.0829, 0.0113])\n",
      "Grad:  tensor([-72.6245,  -9.7592])\n",
      "Epoch 12 Loss 73.490524\n",
      "Params: tensor([1.0901, 0.0122])\n",
      "Grad:  tensor([-72.1872,  -9.6820])\n",
      "Epoch 13 Loss 72.961662\n",
      "Params: tensor([1.0973, 0.0132])\n",
      "Grad:  tensor([-71.7526,  -9.6052])\n",
      "Epoch 14 Loss 72.439186\n",
      "Params: tensor([1.1045, 0.0142])\n",
      "Grad:  tensor([-71.3206,  -9.5290])\n",
      "Epoch 15 Loss 71.923004\n",
      "Params: tensor([1.1116, 0.0151])\n",
      "Grad:  tensor([-70.8913,  -9.4532])\n",
      "Epoch 16 Loss 71.413055\n",
      "Params: tensor([1.1187, 0.0161])\n",
      "Grad:  tensor([-70.4645,  -9.3779])\n",
      "Epoch 17 Loss 70.909271\n",
      "Params: tensor([1.1258, 0.0170])\n",
      "Grad:  tensor([-70.0404,  -9.3030])\n",
      "Epoch 18 Loss 70.411568\n",
      "Params: tensor([1.1328, 0.0179])\n",
      "Grad:  tensor([-69.6188,  -9.2286])\n",
      "Epoch 19 Loss 69.919861\n",
      "Params: tensor([1.1397, 0.0189])\n",
      "Grad:  tensor([-69.1998,  -9.1546])\n",
      "Epoch 20 Loss 69.434090\n",
      "Params: tensor([1.1467, 0.0198])\n",
      "Grad:  tensor([-68.7833,  -9.0811])\n",
      "Epoch 21 Loss 68.954193\n",
      "Params: tensor([1.1535, 0.0207])\n",
      "Grad:  tensor([-68.3693,  -9.0080])\n",
      "Epoch 22 Loss 68.480080\n",
      "Params: tensor([1.1604, 0.0216])\n",
      "Grad:  tensor([-67.9579,  -8.9354])\n",
      "Epoch 23 Loss 68.011688\n",
      "Params: tensor([1.1672, 0.0225])\n",
      "Grad:  tensor([-67.5489,  -8.8632])\n",
      "Epoch 24 Loss 67.548958\n",
      "Params: tensor([1.1739, 0.0234])\n",
      "Grad:  tensor([-67.1424,  -8.7914])\n",
      "Epoch 25 Loss 67.091805\n",
      "Params: tensor([1.1806, 0.0242])\n",
      "Grad:  tensor([-66.7384,  -8.7201])\n",
      "Epoch 26 Loss 66.640175\n",
      "Params: tensor([1.1873, 0.0251])\n",
      "Grad:  tensor([-66.3369,  -8.6492])\n",
      "Epoch 27 Loss 66.193993\n",
      "Params: tensor([1.1939, 0.0260])\n",
      "Grad:  tensor([-65.9377,  -8.5788])\n",
      "Epoch 28 Loss 65.753181\n",
      "Params: tensor([1.2005, 0.0268])\n",
      "Grad:  tensor([-65.5410,  -8.5087])\n",
      "Epoch 29 Loss 65.317711\n",
      "Params: tensor([1.2071, 0.0277])\n",
      "Grad:  tensor([-65.1467,  -8.4391])\n",
      "Epoch 30 Loss 64.887482\n",
      "Params: tensor([1.2136, 0.0285])\n",
      "Grad:  tensor([-64.7548,  -8.3699])\n",
      "Epoch 31 Loss 64.462448\n",
      "Params: tensor([1.2201, 0.0294])\n",
      "Grad:  tensor([-64.3653,  -8.3012])\n",
      "Epoch 32 Loss 64.042534\n",
      "Params: tensor([1.2265, 0.0302])\n",
      "Grad:  tensor([-63.9781,  -8.2328])\n",
      "Epoch 33 Loss 63.627697\n",
      "Params: tensor([1.2329, 0.0310])\n",
      "Grad:  tensor([-63.5933,  -8.1649])\n",
      "Epoch 34 Loss 63.217861\n",
      "Params: tensor([1.2393, 0.0318])\n",
      "Grad:  tensor([-63.2108,  -8.0974])\n",
      "Epoch 35 Loss 62.812977\n",
      "Params: tensor([1.2456, 0.0326])\n",
      "Grad:  tensor([-62.8306,  -8.0303])\n",
      "Epoch 36 Loss 62.412975\n",
      "Params: tensor([1.2519, 0.0334])\n",
      "Grad:  tensor([-62.4528,  -7.9636])\n",
      "Epoch 37 Loss 62.017799\n",
      "Params: tensor([1.2581, 0.0342])\n",
      "Grad:  tensor([-62.0772,  -7.8973])\n",
      "Epoch 38 Loss 61.627384\n",
      "Params: tensor([1.2643, 0.0350])\n",
      "Grad:  tensor([-61.7039,  -7.8314])\n",
      "Epoch 39 Loss 61.241676\n",
      "Params: tensor([1.2705, 0.0358])\n",
      "Grad:  tensor([-61.3329,  -7.7659])\n",
      "Epoch 40 Loss 60.860641\n",
      "Params: tensor([1.2766, 0.0366])\n",
      "Grad:  tensor([-60.9641,  -7.7008])\n",
      "Epoch 41 Loss 60.484188\n",
      "Params: tensor([1.2827, 0.0374])\n",
      "Grad:  tensor([-60.5975,  -7.6361])\n",
      "Epoch 42 Loss 60.112278\n",
      "Params: tensor([1.2888, 0.0381])\n",
      "Grad:  tensor([-60.2332,  -7.5718])\n",
      "Epoch 43 Loss 59.744862\n",
      "Params: tensor([1.2948, 0.0389])\n",
      "Grad:  tensor([-59.8711,  -7.5079])\n",
      "Epoch 44 Loss 59.381870\n",
      "Params: tensor([1.3008, 0.0396])\n",
      "Grad:  tensor([-59.5112,  -7.4444])\n",
      "Epoch 45 Loss 59.023258\n",
      "Params: tensor([1.3068, 0.0404])\n",
      "Grad:  tensor([-59.1535,  -7.3812])\n",
      "Epoch 46 Loss 58.668968\n",
      "Params: tensor([1.3127, 0.0411])\n",
      "Grad:  tensor([-58.7979,  -7.3185])\n",
      "Epoch 47 Loss 58.318958\n",
      "Params: tensor([1.3186, 0.0418])\n",
      "Grad:  tensor([-58.4445,  -7.2561])\n",
      "Epoch 48 Loss 57.973156\n",
      "Params: tensor([1.3244, 0.0426])\n",
      "Grad:  tensor([-58.0932,  -7.1941])\n",
      "Epoch 49 Loss 57.631535\n",
      "Params: tensor([1.3302, 0.0433])\n",
      "Grad:  tensor([-57.7441,  -7.1325])\n",
      "Epoch 50 Loss 57.294033\n",
      "Params: tensor([1.3360, 0.0440])\n",
      "Grad:  tensor([-57.3971,  -7.0712])\n",
      "Epoch 51 Loss 56.960606\n",
      "Params: tensor([1.3417, 0.0447])\n",
      "Grad:  tensor([-57.0522,  -7.0103])\n",
      "Epoch 52 Loss 56.631187\n",
      "Params: tensor([1.3474, 0.0454])\n",
      "Grad:  tensor([-56.7093,  -6.9498])\n",
      "Epoch 53 Loss 56.305763\n",
      "Params: tensor([1.3531, 0.0461])\n",
      "Grad:  tensor([-56.3686,  -6.8897])\n",
      "Epoch 54 Loss 55.984241\n",
      "Params: tensor([1.3587, 0.0468])\n",
      "Grad:  tensor([-56.0299,  -6.8299])\n",
      "Epoch 55 Loss 55.666615\n",
      "Params: tensor([1.3643, 0.0475])\n",
      "Grad:  tensor([-55.6933,  -6.7705])\n",
      "Epoch 56 Loss 55.352806\n",
      "Params: tensor([1.3699, 0.0482])\n",
      "Grad:  tensor([-55.3587,  -6.7115])\n",
      "Epoch 57 Loss 55.042782\n",
      "Params: tensor([1.3754, 0.0488])\n",
      "Grad:  tensor([-55.0262,  -6.6528])\n",
      "Epoch 58 Loss 54.736507\n",
      "Params: tensor([1.3809, 0.0495])\n",
      "Grad:  tensor([-54.6956,  -6.5944])\n",
      "Epoch 59 Loss 54.433910\n",
      "Params: tensor([1.3864, 0.0502])\n",
      "Grad:  tensor([-54.3671,  -6.5364])\n",
      "Epoch 60 Loss 54.134975\n",
      "Params: tensor([1.3919, 0.0508])\n",
      "Grad:  tensor([-54.0406,  -6.4788])\n",
      "Epoch 61 Loss 53.839622\n",
      "Params: tensor([1.3973, 0.0515])\n",
      "Grad:  tensor([-53.7160,  -6.4215])\n",
      "Epoch 62 Loss 53.547848\n",
      "Params: tensor([1.4026, 0.0521])\n",
      "Grad:  tensor([-53.3934,  -6.3646])\n",
      "Epoch 63 Loss 53.259583\n",
      "Params: tensor([1.4080, 0.0527])\n",
      "Grad:  tensor([-53.0728,  -6.3080])\n",
      "Epoch 64 Loss 52.974792\n",
      "Params: tensor([1.4133, 0.0534])\n",
      "Grad:  tensor([-52.7541,  -6.2518])\n",
      "Epoch 65 Loss 52.693443\n",
      "Params: tensor([1.4185, 0.0540])\n",
      "Grad:  tensor([-52.4374,  -6.1959])\n",
      "Epoch 66 Loss 52.415478\n",
      "Params: tensor([1.4238, 0.0546])\n",
      "Grad:  tensor([-52.1225,  -6.1403])\n",
      "Epoch 67 Loss 52.140854\n",
      "Params: tensor([1.4290, 0.0552])\n",
      "Grad:  tensor([-51.8096,  -6.0851])\n",
      "Epoch 68 Loss 51.869545\n",
      "Params: tensor([1.4342, 0.0558])\n",
      "Grad:  tensor([-51.4986,  -6.0302])\n",
      "Epoch 69 Loss 51.601524\n",
      "Params: tensor([1.4393, 0.0564])\n",
      "Grad:  tensor([-51.1894,  -5.9756])\n",
      "Epoch 70 Loss 51.336716\n",
      "Params: tensor([1.4445, 0.0570])\n",
      "Grad:  tensor([-50.8822,  -5.9214])\n",
      "Epoch 71 Loss 51.075100\n",
      "Params: tensor([1.4495, 0.0576])\n",
      "Grad:  tensor([-50.5768,  -5.8675])\n",
      "Epoch 72 Loss 50.816647\n",
      "Params: tensor([1.4546, 0.0582])\n",
      "Grad:  tensor([-50.2732,  -5.8139])\n",
      "Epoch 73 Loss 50.561291\n",
      "Params: tensor([1.4596, 0.0588])\n",
      "Grad:  tensor([-49.9715,  -5.7607])\n",
      "Epoch 74 Loss 50.309025\n",
      "Params: tensor([1.4646, 0.0594])\n",
      "Grad:  tensor([-49.6716,  -5.7077])\n",
      "Epoch 75 Loss 50.059799\n",
      "Params: tensor([1.4696, 0.0599])\n",
      "Grad:  tensor([-49.3736,  -5.6551])\n",
      "Epoch 76 Loss 49.813564\n",
      "Params: tensor([1.4745, 0.0605])\n",
      "Grad:  tensor([-49.0773,  -5.6029])\n",
      "Epoch 77 Loss 49.570309\n",
      "Params: tensor([1.4794, 0.0611])\n",
      "Grad:  tensor([-48.7828,  -5.5509])\n",
      "Epoch 78 Loss 49.329971\n",
      "Params: tensor([1.4843, 0.0616])\n",
      "Grad:  tensor([-48.4902,  -5.4993])\n",
      "Epoch 79 Loss 49.092552\n",
      "Params: tensor([1.4892, 0.0622])\n",
      "Grad:  tensor([-48.1993,  -5.4479])\n",
      "Epoch 80 Loss 48.857971\n",
      "Params: tensor([1.4940, 0.0627])\n",
      "Grad:  tensor([-47.9101,  -5.3969])\n",
      "Epoch 81 Loss 48.626221\n",
      "Params: tensor([1.4988, 0.0633])\n",
      "Grad:  tensor([-47.6228,  -5.3462])\n",
      "Epoch 82 Loss 48.397263\n",
      "Params: tensor([1.5035, 0.0638])\n",
      "Grad:  tensor([-47.3371,  -5.2958])\n",
      "Epoch 83 Loss 48.171066\n",
      "Params: tensor([1.5083, 0.0643])\n",
      "Grad:  tensor([-47.0532,  -5.2457])\n",
      "Epoch 84 Loss 47.947594\n",
      "Params: tensor([1.5130, 0.0648])\n",
      "Grad:  tensor([-46.7710,  -5.1959])\n",
      "Epoch 85 Loss 47.726810\n",
      "Params: tensor([1.5177, 0.0654])\n",
      "Grad:  tensor([-46.4906,  -5.1464])\n",
      "Epoch 86 Loss 47.508690\n",
      "Params: tensor([1.5223, 0.0659])\n",
      "Grad:  tensor([-46.2118,  -5.0972])\n",
      "Epoch 87 Loss 47.293186\n",
      "Params: tensor([1.5269, 0.0664])\n",
      "Grad:  tensor([-45.9347,  -5.0483])\n",
      "Epoch 88 Loss 47.080284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([1.5315, 0.0669])\n",
      "Grad:  tensor([-45.6593,  -4.9997])\n",
      "Epoch 89 Loss 46.869946\n",
      "Params: tensor([1.5361, 0.0674])\n",
      "Grad:  tensor([-45.3856,  -4.9514])\n",
      "Epoch 90 Loss 46.662136\n",
      "Params: tensor([1.5406, 0.0679])\n",
      "Grad:  tensor([-45.1135,  -4.9034])\n",
      "Epoch 91 Loss 46.456837\n",
      "Params: tensor([1.5451, 0.0684])\n",
      "Grad:  tensor([-44.8431,  -4.8557])\n",
      "Epoch 92 Loss 46.253998\n",
      "Params: tensor([1.5496, 0.0689])\n",
      "Grad:  tensor([-44.5743,  -4.8082])\n",
      "Epoch 93 Loss 46.053604\n",
      "Params: tensor([1.5541, 0.0693])\n",
      "Grad:  tensor([-44.3072,  -4.7611])\n",
      "Epoch 94 Loss 45.855618\n",
      "Params: tensor([1.5585, 0.0698])\n",
      "Grad:  tensor([-44.0416,  -4.7142])\n",
      "Epoch 95 Loss 45.660030\n",
      "Params: tensor([1.5629, 0.0703])\n",
      "Grad:  tensor([-43.7777,  -4.6677])\n",
      "Epoch 96 Loss 45.466782\n",
      "Params: tensor([1.5673, 0.0708])\n",
      "Grad:  tensor([-43.5154,  -4.6214])\n",
      "Epoch 97 Loss 45.275867\n",
      "Params: tensor([1.5716, 0.0712])\n",
      "Grad:  tensor([-43.2547,  -4.5754])\n",
      "Epoch 98 Loss 45.087246\n",
      "Params: tensor([1.5760, 0.0717])\n",
      "Grad:  tensor([-42.9955,  -4.5296])\n",
      "Epoch 99 Loss 44.900906\n",
      "Params: tensor([1.5803, 0.0721])\n",
      "Grad:  tensor([-42.7380,  -4.4842])\n",
      "Epoch 100 Loss 44.716797\n",
      "Params: tensor([1.5845, 0.0726])\n",
      "Grad:  tensor([-42.4819,  -4.4390])\n",
      "Epoch 101 Loss 44.534901\n",
      "Params: tensor([1.5888, 0.0730])\n",
      "Grad:  tensor([-42.2275,  -4.3941])\n",
      "Epoch 102 Loss 44.355198\n",
      "Params: tensor([1.5930, 0.0735])\n",
      "Grad:  tensor([-41.9746,  -4.3495])\n",
      "Epoch 103 Loss 44.177654\n",
      "Params: tensor([1.5972, 0.0739])\n",
      "Grad:  tensor([-41.7232,  -4.3051])\n",
      "Epoch 104 Loss 44.002254\n",
      "Params: tensor([1.6014, 0.0743])\n",
      "Grad:  tensor([-41.4733,  -4.2611])\n",
      "Epoch 105 Loss 43.828960\n",
      "Params: tensor([1.6055, 0.0748])\n",
      "Grad:  tensor([-41.2250,  -4.2172])\n",
      "Epoch 106 Loss 43.657749\n",
      "Params: tensor([1.6096, 0.0752])\n",
      "Grad:  tensor([-40.9781,  -4.1737])\n",
      "Epoch 107 Loss 43.488598\n",
      "Params: tensor([1.6137, 0.0756])\n",
      "Grad:  tensor([-40.7328,  -4.1304])\n",
      "Epoch 108 Loss 43.321484\n",
      "Params: tensor([1.6178, 0.0760])\n",
      "Grad:  tensor([-40.4890,  -4.0874])\n",
      "Epoch 109 Loss 43.156376\n",
      "Params: tensor([1.6219, 0.0764])\n",
      "Grad:  tensor([-40.2466,  -4.0446])\n",
      "Epoch 110 Loss 42.993252\n",
      "Params: tensor([1.6259, 0.0768])\n",
      "Grad:  tensor([-40.0057,  -4.0021])\n",
      "Epoch 111 Loss 42.832096\n",
      "Params: tensor([1.6299, 0.0772])\n",
      "Grad:  tensor([-39.7662,  -3.9599])\n",
      "Epoch 112 Loss 42.672878\n",
      "Params: tensor([1.6339, 0.0776])\n",
      "Grad:  tensor([-39.5282,  -3.9179])\n",
      "Epoch 113 Loss 42.515568\n",
      "Params: tensor([1.6378, 0.0780])\n",
      "Grad:  tensor([-39.2917,  -3.8761])\n",
      "Epoch 114 Loss 42.360153\n",
      "Params: tensor([1.6418, 0.0784])\n",
      "Grad:  tensor([-39.0566,  -3.8347])\n",
      "Epoch 115 Loss 42.206604\n",
      "Params: tensor([1.6457, 0.0788])\n",
      "Grad:  tensor([-38.8229,  -3.7934])\n",
      "Epoch 116 Loss 42.054901\n",
      "Params: tensor([1.6495, 0.0792])\n",
      "Grad:  tensor([-38.5906,  -3.7524])\n",
      "Epoch 117 Loss 41.905022\n",
      "Params: tensor([1.6534, 0.0795])\n",
      "Grad:  tensor([-38.3598,  -3.7117])\n",
      "Epoch 118 Loss 41.756947\n",
      "Params: tensor([1.6572, 0.0799])\n",
      "Grad:  tensor([-38.1303,  -3.6712])\n",
      "Epoch 119 Loss 41.610653\n",
      "Params: tensor([1.6610, 0.0803])\n",
      "Grad:  tensor([-37.9022,  -3.6310])\n",
      "Epoch 120 Loss 41.466110\n",
      "Params: tensor([1.6648, 0.0806])\n",
      "Grad:  tensor([-37.6755,  -3.5910])\n",
      "Epoch 121 Loss 41.323307\n",
      "Params: tensor([1.6686, 0.0810])\n",
      "Grad:  tensor([-37.4502,  -3.5513])\n",
      "Epoch 122 Loss 41.182213\n",
      "Params: tensor([1.6724, 0.0813])\n",
      "Grad:  tensor([-37.2263,  -3.5117])\n",
      "Epoch 123 Loss 41.042820\n",
      "Params: tensor([1.6761, 0.0817])\n",
      "Grad:  tensor([-37.0037,  -3.4725])\n",
      "Epoch 124 Loss 40.905106\n",
      "Params: tensor([1.6798, 0.0820])\n",
      "Grad:  tensor([-36.7824,  -3.4334])\n",
      "Epoch 125 Loss 40.769043\n",
      "Params: tensor([1.6835, 0.0824])\n",
      "Grad:  tensor([-36.5625,  -3.3947])\n",
      "Epoch 126 Loss 40.634617\n",
      "Params: tensor([1.6871, 0.0827])\n",
      "Grad:  tensor([-36.3440,  -3.3561])\n",
      "Epoch 127 Loss 40.501797\n",
      "Params: tensor([1.6907, 0.0831])\n",
      "Grad:  tensor([-36.1267,  -3.3178])\n",
      "Epoch 128 Loss 40.370579\n",
      "Params: tensor([1.6944, 0.0834])\n",
      "Grad:  tensor([-35.9108,  -3.2797])\n",
      "Epoch 129 Loss 40.240940\n",
      "Params: tensor([1.6979, 0.0837])\n",
      "Grad:  tensor([-35.6962,  -3.2418])\n",
      "Epoch 130 Loss 40.112850\n",
      "Params: tensor([1.7015, 0.0840])\n",
      "Grad:  tensor([-35.4829,  -3.2042])\n",
      "Epoch 131 Loss 39.986301\n",
      "Params: tensor([1.7051, 0.0844])\n",
      "Grad:  tensor([-35.2709,  -3.1668])\n",
      "Epoch 132 Loss 39.861279\n",
      "Params: tensor([1.7086, 0.0847])\n",
      "Grad:  tensor([-35.0602,  -3.1296])\n",
      "Epoch 133 Loss 39.737747\n",
      "Params: tensor([1.7121, 0.0850])\n",
      "Grad:  tensor([-34.8507,  -3.0927])\n",
      "Epoch 134 Loss 39.615700\n",
      "Params: tensor([1.7156, 0.0853])\n",
      "Grad:  tensor([-34.6425,  -3.0559])\n",
      "Epoch 135 Loss 39.495121\n",
      "Params: tensor([1.7190, 0.0856])\n",
      "Grad:  tensor([-34.4356,  -3.0194])\n",
      "Epoch 136 Loss 39.375988\n",
      "Params: tensor([1.7225, 0.0859])\n",
      "Grad:  tensor([-34.2299,  -2.9832])\n",
      "Epoch 137 Loss 39.258289\n",
      "Params: tensor([1.7259, 0.0862])\n",
      "Grad:  tensor([-34.0255,  -2.9471])\n",
      "Epoch 138 Loss 39.141994\n",
      "Params: tensor([1.7293, 0.0865])\n",
      "Grad:  tensor([-33.8224,  -2.9113])\n",
      "Epoch 139 Loss 39.027103\n",
      "Params: tensor([1.7327, 0.0868])\n",
      "Grad:  tensor([-33.6204,  -2.8756])\n",
      "Epoch 140 Loss 38.913578\n",
      "Params: tensor([1.7361, 0.0871])\n",
      "Grad:  tensor([-33.4197,  -2.8402])\n",
      "Epoch 141 Loss 38.801426\n",
      "Params: tensor([1.7394, 0.0874])\n",
      "Grad:  tensor([-33.2202,  -2.8050])\n",
      "Epoch 142 Loss 38.690617\n",
      "Params: tensor([1.7427, 0.0877])\n",
      "Grad:  tensor([-33.0219,  -2.7701])\n",
      "Epoch 143 Loss 38.581135\n",
      "Params: tensor([1.7460, 0.0879])\n",
      "Grad:  tensor([-32.8248,  -2.7353])\n",
      "Epoch 144 Loss 38.472965\n",
      "Params: tensor([1.7493, 0.0882])\n",
      "Grad:  tensor([-32.6289,  -2.7007])\n",
      "Epoch 145 Loss 38.366100\n",
      "Params: tensor([1.7526, 0.0885])\n",
      "Grad:  tensor([-32.4342,  -2.6664])\n",
      "Epoch 146 Loss 38.260502\n",
      "Params: tensor([1.7558, 0.0887])\n",
      "Grad:  tensor([-32.2407,  -2.6323])\n",
      "Epoch 147 Loss 38.156178\n",
      "Params: tensor([1.7590, 0.0890])\n",
      "Grad:  tensor([-32.0483,  -2.5983])\n",
      "Epoch 148 Loss 38.053116\n",
      "Params: tensor([1.7622, 0.0893])\n",
      "Grad:  tensor([-31.8571,  -2.5646])\n",
      "Epoch 149 Loss 37.951267\n",
      "Params: tensor([1.7654, 0.0895])\n",
      "Grad:  tensor([-31.6671,  -2.5311])\n",
      "Epoch 150 Loss 37.850651\n",
      "Params: tensor([1.7686, 0.0898])\n",
      "Grad:  tensor([-31.4782,  -2.4978])\n",
      "Epoch 151 Loss 37.751232\n",
      "Params: tensor([1.7717, 0.0900])\n",
      "Grad:  tensor([-31.2905,  -2.4647])\n",
      "Epoch 152 Loss 37.653019\n",
      "Params: tensor([1.7749, 0.0903])\n",
      "Grad:  tensor([-31.1039,  -2.4318])\n",
      "Epoch 153 Loss 37.555973\n",
      "Params: tensor([1.7780, 0.0905])\n",
      "Grad:  tensor([-30.9185,  -2.3991])\n",
      "Epoch 154 Loss 37.460087\n",
      "Params: tensor([1.7811, 0.0908])\n",
      "Grad:  tensor([-30.7341,  -2.3665])\n",
      "Epoch 155 Loss 37.365360\n",
      "Params: tensor([1.7841, 0.0910])\n",
      "Grad:  tensor([-30.5509,  -2.3342])\n",
      "Epoch 156 Loss 37.271759\n",
      "Params: tensor([1.7872, 0.0912])\n",
      "Grad:  tensor([-30.3688,  -2.3021])\n",
      "Epoch 157 Loss 37.179283\n",
      "Params: tensor([1.7902, 0.0915])\n",
      "Grad:  tensor([-30.1878,  -2.2702])\n",
      "Epoch 158 Loss 37.087914\n",
      "Params: tensor([1.7933, 0.0917])\n",
      "Grad:  tensor([-30.0079,  -2.2385])\n",
      "Epoch 159 Loss 36.997635\n",
      "Params: tensor([1.7963, 0.0919])\n",
      "Grad:  tensor([-29.8291,  -2.2069])\n",
      "Epoch 160 Loss 36.908447\n",
      "Params: tensor([1.7992, 0.0921])\n",
      "Grad:  tensor([-29.6513,  -2.1756])\n",
      "Epoch 161 Loss 36.820320\n",
      "Params: tensor([1.8022, 0.0923])\n",
      "Grad:  tensor([-29.4747,  -2.1444])\n",
      "Epoch 162 Loss 36.733246\n",
      "Params: tensor([1.8052, 0.0926])\n",
      "Grad:  tensor([-29.2991,  -2.1135])\n",
      "Epoch 163 Loss 36.647209\n",
      "Params: tensor([1.8081, 0.0928])\n",
      "Grad:  tensor([-29.1246,  -2.0827])\n",
      "Epoch 164 Loss 36.562210\n",
      "Params: tensor([1.8110, 0.0930])\n",
      "Grad:  tensor([-28.9512,  -2.0521])\n",
      "Epoch 165 Loss 36.478226\n",
      "Params: tensor([1.8139, 0.0932])\n",
      "Grad:  tensor([-28.7788,  -2.0217])\n",
      "Epoch 166 Loss 36.395248\n",
      "Params: tensor([1.8168, 0.0934])\n",
      "Grad:  tensor([-28.6074,  -1.9915])\n",
      "Epoch 167 Loss 36.313259\n",
      "Params: tensor([1.8196, 0.0936])\n",
      "Grad:  tensor([-28.4371,  -1.9614])\n",
      "Epoch 168 Loss 36.232243\n",
      "Params: tensor([1.8225, 0.0938])\n",
      "Grad:  tensor([-28.2678,  -1.9316])\n",
      "Epoch 169 Loss 36.152214\n",
      "Params: tensor([1.8253, 0.0940])\n",
      "Grad:  tensor([-28.0995,  -1.9019])\n",
      "Epoch 170 Loss 36.073124\n",
      "Params: tensor([1.8281, 0.0942])\n",
      "Grad:  tensor([-27.9323,  -1.8724])\n",
      "Epoch 171 Loss 35.994995\n",
      "Params: tensor([1.8309, 0.0943])\n",
      "Grad:  tensor([-27.7661,  -1.8431])\n",
      "Epoch 172 Loss 35.917786\n",
      "Params: tensor([1.8337, 0.0945])\n",
      "Grad:  tensor([-27.6008,  -1.8140])\n",
      "Epoch 173 Loss 35.841507\n",
      "Params: tensor([1.8364, 0.0947])\n",
      "Grad:  tensor([-27.4366,  -1.7850])\n",
      "Epoch 174 Loss 35.766136\n",
      "Params: tensor([1.8392, 0.0949])\n",
      "Grad:  tensor([-27.2734,  -1.7562])\n",
      "Epoch 175 Loss 35.691666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([1.8419, 0.0951])\n",
      "Grad:  tensor([-27.1112,  -1.7276])\n",
      "Epoch 176 Loss 35.618088\n",
      "Params: tensor([1.8446, 0.0952])\n",
      "Grad:  tensor([-26.9499,  -1.6992])\n",
      "Epoch 177 Loss 35.545391\n",
      "Params: tensor([1.8473, 0.0954])\n",
      "Grad:  tensor([-26.7896,  -1.6709])\n",
      "Epoch 178 Loss 35.473557\n",
      "Params: tensor([1.8500, 0.0956])\n",
      "Grad:  tensor([-26.6304,  -1.6429])\n",
      "Epoch 179 Loss 35.402580\n",
      "Params: tensor([1.8527, 0.0957])\n",
      "Grad:  tensor([-26.4720,  -1.6149])\n",
      "Epoch 180 Loss 35.332458\n",
      "Params: tensor([1.8553, 0.0959])\n",
      "Grad:  tensor([-26.3146,  -1.5872])\n",
      "Epoch 181 Loss 35.263168\n",
      "Params: tensor([1.8579, 0.0961])\n",
      "Grad:  tensor([-26.1582,  -1.5596])\n",
      "Epoch 182 Loss 35.194706\n",
      "Params: tensor([1.8606, 0.0962])\n",
      "Grad:  tensor([-26.0028,  -1.5322])\n",
      "Epoch 183 Loss 35.127052\n",
      "Params: tensor([1.8632, 0.0964])\n",
      "Grad:  tensor([-25.8482,  -1.5050])\n",
      "Epoch 184 Loss 35.060215\n",
      "Params: tensor([1.8657, 0.0965])\n",
      "Grad:  tensor([-25.6947,  -1.4779])\n",
      "Epoch 185 Loss 34.994179\n",
      "Params: tensor([1.8683, 0.0967])\n",
      "Grad:  tensor([-25.5420,  -1.4510])\n",
      "Epoch 186 Loss 34.928921\n",
      "Params: tensor([1.8709, 0.0968])\n",
      "Grad:  tensor([-25.3903,  -1.4242])\n",
      "Epoch 187 Loss 34.864445\n",
      "Params: tensor([1.8734, 0.0970])\n",
      "Grad:  tensor([-25.2395,  -1.3976])\n",
      "Epoch 188 Loss 34.800739\n",
      "Params: tensor([1.8759, 0.0971])\n",
      "Grad:  tensor([-25.0896,  -1.3712])\n",
      "Epoch 189 Loss 34.737793\n",
      "Params: tensor([1.8784, 0.0972])\n",
      "Grad:  tensor([-24.9406,  -1.3449])\n",
      "Epoch 190 Loss 34.675594\n",
      "Params: tensor([1.8809, 0.0974])\n",
      "Grad:  tensor([-24.7925,  -1.3188])\n",
      "Epoch 191 Loss 34.614136\n",
      "Params: tensor([1.8834, 0.0975])\n",
      "Grad:  tensor([-24.6453,  -1.2929])\n",
      "Epoch 192 Loss 34.553410\n",
      "Params: tensor([1.8859, 0.0976])\n",
      "Grad:  tensor([-24.4990,  -1.2671])\n",
      "Epoch 193 Loss 34.493412\n",
      "Params: tensor([1.8883, 0.0978])\n",
      "Grad:  tensor([-24.3536,  -1.2414])\n",
      "Epoch 194 Loss 34.434128\n",
      "Params: tensor([1.8908, 0.0979])\n",
      "Grad:  tensor([-24.2091,  -1.2160])\n",
      "Epoch 195 Loss 34.375546\n",
      "Params: tensor([1.8932, 0.0980])\n",
      "Grad:  tensor([-24.0654,  -1.1906])\n",
      "Epoch 196 Loss 34.317661\n",
      "Params: tensor([1.8956, 0.0981])\n",
      "Grad:  tensor([-23.9227,  -1.1655])\n",
      "Epoch 197 Loss 34.260468\n",
      "Params: tensor([1.8980, 0.0982])\n",
      "Grad:  tensor([-23.7807,  -1.1405])\n",
      "Epoch 198 Loss 34.203957\n",
      "Params: tensor([1.9004, 0.0984])\n",
      "Grad:  tensor([-23.6397,  -1.1156])\n",
      "Epoch 199 Loss 34.148117\n",
      "Params: tensor([1.9027, 0.0985])\n",
      "Grad:  tensor([-23.4995,  -1.0909])\n",
      "Epoch 200 Loss 34.092937\n",
      "Params: tensor([1.9051, 0.0986])\n",
      "Grad:  tensor([-23.3602,  -1.0663])\n",
      "Epoch 201 Loss 34.038418\n",
      "Params: tensor([1.9074, 0.0987])\n",
      "Grad:  tensor([-23.2217,  -1.0419])\n",
      "Epoch 202 Loss 33.984547\n",
      "Params: tensor([1.9097, 0.0988])\n",
      "Grad:  tensor([-23.0840,  -1.0176])\n",
      "Epoch 203 Loss 33.931316\n",
      "Params: tensor([1.9120, 0.0989])\n",
      "Grad:  tensor([-22.9472,  -0.9935])\n",
      "Epoch 204 Loss 33.878716\n",
      "Params: tensor([1.9143, 0.0990])\n",
      "Grad:  tensor([-22.8112,  -0.9695])\n",
      "Epoch 205 Loss 33.826744\n",
      "Params: tensor([1.9166, 0.0991])\n",
      "Grad:  tensor([-22.6760,  -0.9457])\n",
      "Epoch 206 Loss 33.775391\n",
      "Params: tensor([1.9189, 0.0992])\n",
      "Grad:  tensor([-22.5417,  -0.9220])\n",
      "Epoch 207 Loss 33.724644\n",
      "Params: tensor([1.9211, 0.0993])\n",
      "Grad:  tensor([-22.4081,  -0.8985])\n",
      "Epoch 208 Loss 33.674503\n",
      "Params: tensor([1.9234, 0.0994])\n",
      "Grad:  tensor([-22.2754,  -0.8751])\n",
      "Epoch 209 Loss 33.624950\n",
      "Params: tensor([1.9256, 0.0994])\n",
      "Grad:  tensor([-22.1435,  -0.8519])\n",
      "Epoch 210 Loss 33.575996\n",
      "Params: tensor([1.9278, 0.0995])\n",
      "Grad:  tensor([-22.0124,  -0.8287])\n",
      "Epoch 211 Loss 33.527618\n",
      "Params: tensor([1.9300, 0.0996])\n",
      "Grad:  tensor([-21.8820,  -0.8058])\n",
      "Epoch 212 Loss 33.479805\n",
      "Params: tensor([1.9322, 0.0997])\n",
      "Grad:  tensor([-21.7525,  -0.7829])\n",
      "Epoch 213 Loss 33.432575\n",
      "Params: tensor([1.9344, 0.0998])\n",
      "Grad:  tensor([-21.6238,  -0.7602])\n",
      "Epoch 214 Loss 33.385895\n",
      "Params: tensor([1.9365, 0.0998])\n",
      "Grad:  tensor([-21.4958,  -0.7377])\n",
      "Epoch 215 Loss 33.339771\n",
      "Params: tensor([1.9387, 0.0999])\n",
      "Grad:  tensor([-21.3686,  -0.7153])\n",
      "Epoch 216 Loss 33.294201\n",
      "Params: tensor([1.9408, 0.1000])\n",
      "Grad:  tensor([-21.2422,  -0.6930])\n",
      "Epoch 217 Loss 33.249161\n",
      "Params: tensor([1.9430, 0.1001])\n",
      "Grad:  tensor([-21.1165,  -0.6708])\n",
      "Epoch 218 Loss 33.204655\n",
      "Params: tensor([1.9451, 0.1001])\n",
      "Grad:  tensor([-20.9916,  -0.6488])\n",
      "Epoch 219 Loss 33.160679\n",
      "Params: tensor([1.9472, 0.1002])\n",
      "Grad:  tensor([-20.8675,  -0.6270])\n",
      "Epoch 220 Loss 33.117222\n",
      "Params: tensor([1.9493, 0.1003])\n",
      "Grad:  tensor([-20.7441,  -0.6052])\n",
      "Epoch 221 Loss 33.074284\n",
      "Params: tensor([1.9513, 0.1003])\n",
      "Grad:  tensor([-20.6215,  -0.5836])\n",
      "Epoch 222 Loss 33.031857\n",
      "Params: tensor([1.9534, 0.1004])\n",
      "Grad:  tensor([-20.4996,  -0.5621])\n",
      "Epoch 223 Loss 32.989925\n",
      "Params: tensor([1.9554, 0.1004])\n",
      "Grad:  tensor([-20.3784,  -0.5408])\n",
      "Epoch 224 Loss 32.948486\n",
      "Params: tensor([1.9575, 0.1005])\n",
      "Grad:  tensor([-20.2580,  -0.5196])\n",
      "Epoch 225 Loss 32.907547\n",
      "Params: tensor([1.9595, 0.1005])\n",
      "Grad:  tensor([-20.1383,  -0.4985])\n",
      "Epoch 226 Loss 32.867088\n",
      "Params: tensor([1.9615, 0.1006])\n",
      "Grad:  tensor([-20.0194,  -0.4775])\n",
      "Epoch 227 Loss 32.827110\n",
      "Params: tensor([1.9635, 0.1006])\n",
      "Grad:  tensor([-19.9011,  -0.4567])\n",
      "Epoch 228 Loss 32.787601\n",
      "Params: tensor([1.9655, 0.1007])\n",
      "Grad:  tensor([-19.7836,  -0.4360])\n",
      "Epoch 229 Loss 32.748554\n",
      "Params: tensor([1.9675, 0.1007])\n",
      "Grad:  tensor([-19.6668,  -0.4154])\n",
      "Epoch 230 Loss 32.709972\n",
      "Params: tensor([1.9695, 0.1008])\n",
      "Grad:  tensor([-19.5507,  -0.3949])\n",
      "Epoch 231 Loss 32.671852\n",
      "Params: tensor([1.9714, 0.1008])\n",
      "Grad:  tensor([-19.4353,  -0.3746])\n",
      "Epoch 232 Loss 32.634171\n",
      "Params: tensor([1.9734, 0.1008])\n",
      "Grad:  tensor([-19.3206,  -0.3544])\n",
      "Epoch 233 Loss 32.596939\n",
      "Params: tensor([1.9753, 0.1009])\n",
      "Grad:  tensor([-19.2066,  -0.3343])\n",
      "Epoch 234 Loss 32.560154\n",
      "Params: tensor([1.9772, 0.1009])\n",
      "Grad:  tensor([-19.0933,  -0.3143])\n",
      "Epoch 235 Loss 32.523788\n",
      "Params: tensor([1.9791, 0.1009])\n",
      "Grad:  tensor([-18.9806,  -0.2945])\n",
      "Epoch 236 Loss 32.487865\n",
      "Params: tensor([1.9810, 0.1010])\n",
      "Grad:  tensor([-18.8687,  -0.2748])\n",
      "Epoch 237 Loss 32.452358\n",
      "Params: tensor([1.9829, 0.1010])\n",
      "Grad:  tensor([-18.7574,  -0.2552])\n",
      "Epoch 238 Loss 32.417274\n",
      "Params: tensor([1.9848, 0.1010])\n",
      "Grad:  tensor([-18.6469,  -0.2357])\n",
      "Epoch 239 Loss 32.382595\n",
      "Params: tensor([1.9866, 0.1011])\n",
      "Grad:  tensor([-18.5369,  -0.2163])\n",
      "Epoch 240 Loss 32.348331\n",
      "Params: tensor([1.9885, 0.1011])\n",
      "Grad:  tensor([-18.4277,  -0.1971])\n",
      "Epoch 241 Loss 32.314472\n",
      "Params: tensor([1.9903, 0.1011])\n",
      "Grad:  tensor([-18.3191,  -0.1779])\n",
      "Epoch 242 Loss 32.281010\n",
      "Params: tensor([1.9922, 0.1011])\n",
      "Grad:  tensor([-18.2112,  -0.1589])\n",
      "Epoch 243 Loss 32.247940\n",
      "Params: tensor([1.9940, 0.1011])\n",
      "Grad:  tensor([-18.1039,  -0.1400])\n",
      "Epoch 244 Loss 32.215260\n",
      "Params: tensor([1.9958, 0.1011])\n",
      "Grad:  tensor([-17.9973,  -0.1212])\n",
      "Epoch 245 Loss 32.182964\n",
      "Params: tensor([1.9976, 0.1012])\n",
      "Grad:  tensor([-17.8913,  -0.1026])\n",
      "Epoch 246 Loss 32.151043\n",
      "Params: tensor([1.9994, 0.1012])\n",
      "Grad:  tensor([-17.7859,  -0.0840])\n",
      "Epoch 247 Loss 32.119507\n",
      "Params: tensor([2.0012, 0.1012])\n",
      "Grad:  tensor([-17.6813,  -0.0656])\n",
      "Epoch 248 Loss 32.088341\n",
      "Params: tensor([2.0029, 0.1012])\n",
      "Grad:  tensor([-17.5772,  -0.0472])\n",
      "Epoch 249 Loss 32.057533\n",
      "Params: tensor([2.0047, 0.1012])\n",
      "Grad:  tensor([-17.4738,  -0.0290])\n",
      "Epoch 250 Loss 32.027092\n",
      "Params: tensor([2.0064, 0.1012])\n",
      "Grad:  tensor([-1.7371e+01, -1.0902e-02])\n",
      "Epoch 251 Loss 31.997004\n",
      "Params: tensor([2.0082, 0.1012])\n",
      "Grad:  tensor([-1.7269e+01,  7.0964e-03])\n",
      "Epoch 252 Loss 31.967268\n",
      "Params: tensor([2.0099, 0.1012])\n",
      "Grad:  tensor([-17.1672,   0.0250])\n",
      "Epoch 253 Loss 31.937885\n",
      "Params: tensor([2.0116, 0.1012])\n",
      "Grad:  tensor([-17.0663,   0.0428])\n",
      "Epoch 254 Loss 31.908846\n",
      "Params: tensor([2.0133, 0.1012])\n",
      "Grad:  tensor([-16.9660,   0.0604])\n",
      "Epoch 255 Loss 31.880146\n",
      "Params: tensor([2.0150, 0.1012])\n",
      "Grad:  tensor([-16.8662,   0.0780])\n",
      "Epoch 256 Loss 31.851784\n",
      "Params: tensor([2.0167, 0.1012])\n",
      "Grad:  tensor([-16.7671,   0.0955])\n",
      "Epoch 257 Loss 31.823750\n",
      "Params: tensor([2.0184, 0.1012])\n",
      "Grad:  tensor([-16.6686,   0.1128])\n",
      "Epoch 258 Loss 31.796049\n",
      "Params: tensor([2.0201, 0.1011])\n",
      "Grad:  tensor([-16.5707,   0.1301])\n",
      "Epoch 259 Loss 31.768671\n",
      "Params: tensor([2.0217, 0.1011])\n",
      "Grad:  tensor([-16.4734,   0.1472])\n",
      "Epoch 260 Loss 31.741613\n",
      "Params: tensor([2.0234, 0.1011])\n",
      "Grad:  tensor([-16.3766,   0.1642])\n",
      "Epoch 261 Loss 31.714872\n",
      "Params: tensor([2.0250, 0.1011])\n",
      "Grad:  tensor([-16.2805,   0.1812])\n",
      "Epoch 262 Loss 31.688433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([2.0266, 0.1011])\n",
      "Grad:  tensor([-16.1849,   0.1980])\n",
      "Epoch 263 Loss 31.662321\n",
      "Params: tensor([2.0282, 0.1011])\n",
      "Grad:  tensor([-16.0899,   0.2147])\n",
      "Epoch 264 Loss 31.636497\n",
      "Params: tensor([2.0299, 0.1010])\n",
      "Grad:  tensor([-15.9955,   0.2314])\n",
      "Epoch 265 Loss 31.610979\n",
      "Params: tensor([2.0315, 0.1010])\n",
      "Grad:  tensor([-15.9017,   0.2479])\n",
      "Epoch 266 Loss 31.585760\n",
      "Params: tensor([2.0330, 0.1010])\n",
      "Grad:  tensor([-15.8084,   0.2643])\n",
      "Epoch 267 Loss 31.560829\n",
      "Params: tensor([2.0346, 0.1010])\n",
      "Grad:  tensor([-15.7157,   0.2806])\n",
      "Epoch 268 Loss 31.536200\n",
      "Params: tensor([2.0362, 0.1009])\n",
      "Grad:  tensor([-15.6235,   0.2969])\n",
      "Epoch 269 Loss 31.511854\n",
      "Params: tensor([2.0378, 0.1009])\n",
      "Grad:  tensor([-15.5319,   0.3130])\n",
      "Epoch 270 Loss 31.487785\n",
      "Params: tensor([2.0393, 0.1009])\n",
      "Grad:  tensor([-15.4409,   0.3290])\n",
      "Epoch 271 Loss 31.464003\n",
      "Params: tensor([2.0409, 0.1008])\n",
      "Grad:  tensor([-15.3504,   0.3449])\n",
      "Epoch 272 Loss 31.440498\n",
      "Params: tensor([2.0424, 0.1008])\n",
      "Grad:  tensor([-15.2605,   0.3608])\n",
      "Epoch 273 Loss 31.417265\n",
      "Params: tensor([2.0439, 0.1008])\n",
      "Grad:  tensor([-15.1711,   0.3765])\n",
      "Epoch 274 Loss 31.394304\n",
      "Params: tensor([2.0454, 0.1007])\n",
      "Grad:  tensor([-15.0823,   0.3922])\n",
      "Epoch 275 Loss 31.371605\n",
      "Params: tensor([2.0469, 0.1007])\n",
      "Grad:  tensor([-14.9940,   0.4077])\n",
      "Epoch 276 Loss 31.349176\n",
      "Params: tensor([2.0484, 0.1007])\n",
      "Grad:  tensor([-14.9062,   0.4232])\n",
      "Epoch 277 Loss 31.326998\n",
      "Params: tensor([2.0499, 0.1006])\n",
      "Grad:  tensor([-14.8190,   0.4385])\n",
      "Epoch 278 Loss 31.305084\n",
      "Params: tensor([2.0514, 0.1006])\n",
      "Grad:  tensor([-14.7323,   0.4538])\n",
      "Epoch 279 Loss 31.283424\n",
      "Params: tensor([2.0529, 0.1005])\n",
      "Grad:  tensor([-14.6461,   0.4689])\n",
      "Epoch 280 Loss 31.262011\n",
      "Params: tensor([2.0544, 0.1005])\n",
      "Grad:  tensor([-14.5604,   0.4840])\n",
      "Epoch 281 Loss 31.240850\n",
      "Params: tensor([2.0558, 0.1004])\n",
      "Grad:  tensor([-14.4753,   0.4990])\n",
      "Epoch 282 Loss 31.219934\n",
      "Params: tensor([2.0573, 0.1004])\n",
      "Grad:  tensor([-14.3907,   0.5139])\n",
      "Epoch 283 Loss 31.199257\n",
      "Params: tensor([2.0587, 0.1003])\n",
      "Grad:  tensor([-14.3066,   0.5287])\n",
      "Epoch 284 Loss 31.178816\n",
      "Params: tensor([2.0601, 0.1003])\n",
      "Grad:  tensor([-14.2230,   0.5434])\n",
      "Epoch 285 Loss 31.158617\n",
      "Params: tensor([2.0615, 0.1002])\n",
      "Grad:  tensor([-14.1399,   0.5581])\n",
      "Epoch 286 Loss 31.138647\n",
      "Params: tensor([2.0630, 0.1002])\n",
      "Grad:  tensor([-14.0573,   0.5726])\n",
      "Epoch 287 Loss 31.118910\n",
      "Params: tensor([2.0644, 0.1001])\n",
      "Grad:  tensor([-13.9752,   0.5870])\n",
      "Epoch 288 Loss 31.099401\n",
      "Params: tensor([2.0658, 0.1000])\n",
      "Grad:  tensor([-13.8936,   0.6014])\n",
      "Epoch 289 Loss 31.080120\n",
      "Params: tensor([2.0672, 0.1000])\n",
      "Grad:  tensor([-13.8125,   0.6157])\n",
      "Epoch 290 Loss 31.061066\n",
      "Params: tensor([2.0685, 0.0999])\n",
      "Grad:  tensor([-13.7319,   0.6299])\n",
      "Epoch 291 Loss 31.042219\n",
      "Params: tensor([2.0699, 0.0999])\n",
      "Grad:  tensor([-13.6518,   0.6440])\n",
      "Epoch 292 Loss 31.023598\n",
      "Params: tensor([2.0713, 0.0998])\n",
      "Grad:  tensor([-13.5722,   0.6580])\n",
      "Epoch 293 Loss 31.005180\n",
      "Params: tensor([2.0726, 0.0997])\n",
      "Grad:  tensor([-13.4930,   0.6719])\n",
      "Epoch 294 Loss 30.986982\n",
      "Params: tensor([2.0740, 0.0997])\n",
      "Grad:  tensor([-13.4144,   0.6858])\n",
      "Epoch 295 Loss 30.969000\n",
      "Params: tensor([2.0753, 0.0996])\n",
      "Grad:  tensor([-13.3362,   0.6995])\n",
      "Epoch 296 Loss 30.951210\n",
      "Params: tensor([2.0767, 0.0995])\n",
      "Grad:  tensor([-13.2585,   0.7132])\n",
      "Epoch 297 Loss 30.933632\n",
      "Params: tensor([2.0780, 0.0995])\n",
      "Grad:  tensor([-13.1812,   0.7268])\n",
      "Epoch 298 Loss 30.916254\n",
      "Params: tensor([2.0793, 0.0994])\n",
      "Grad:  tensor([-13.1045,   0.7403])\n",
      "Epoch 299 Loss 30.899076\n",
      "Params: tensor([2.0806, 0.0993])\n",
      "Grad:  tensor([-13.0281,   0.7537])\n",
      "Epoch 300 Loss 30.882097\n",
      "Params: tensor([2.0819, 0.0992])\n",
      "Grad:  tensor([-12.9523,   0.7671])\n",
      "Epoch 301 Loss 30.865309\n",
      "Params: tensor([2.0832, 0.0992])\n",
      "Grad:  tensor([-12.8769,   0.7803])\n",
      "Epoch 302 Loss 30.848719\n",
      "Params: tensor([2.0845, 0.0991])\n",
      "Grad:  tensor([-12.8020,   0.7935])\n",
      "Epoch 303 Loss 30.832308\n",
      "Params: tensor([2.0858, 0.0990])\n",
      "Grad:  tensor([-12.7275,   0.8066])\n",
      "Epoch 304 Loss 30.816099\n",
      "Params: tensor([2.0870, 0.0989])\n",
      "Grad:  tensor([-12.6535,   0.8197])\n",
      "Epoch 305 Loss 30.800066\n",
      "Params: tensor([2.0883, 0.0988])\n",
      "Grad:  tensor([-12.5799,   0.8326])\n",
      "Epoch 306 Loss 30.784210\n",
      "Params: tensor([2.0896, 0.0988])\n",
      "Grad:  tensor([-12.5068,   0.8455])\n",
      "Epoch 307 Loss 30.768543\n",
      "Params: tensor([2.0908, 0.0987])\n",
      "Grad:  tensor([-12.4341,   0.8582])\n",
      "Epoch 308 Loss 30.753054\n",
      "Params: tensor([2.0921, 0.0986])\n",
      "Grad:  tensor([-12.3619,   0.8710])\n",
      "Epoch 309 Loss 30.737743\n",
      "Params: tensor([2.0933, 0.0985])\n",
      "Grad:  tensor([-12.2901,   0.8836])\n",
      "Epoch 310 Loss 30.722595\n",
      "Params: tensor([2.0945, 0.0984])\n",
      "Grad:  tensor([-12.2187,   0.8961])\n",
      "Epoch 311 Loss 30.707636\n",
      "Params: tensor([2.0958, 0.0983])\n",
      "Grad:  tensor([-12.1478,   0.9086])\n",
      "Epoch 312 Loss 30.692835\n",
      "Params: tensor([2.0970, 0.0982])\n",
      "Grad:  tensor([-12.0773,   0.9210])\n",
      "Epoch 313 Loss 30.678205\n",
      "Params: tensor([2.0982, 0.0981])\n",
      "Grad:  tensor([-12.0072,   0.9334])\n",
      "Epoch 314 Loss 30.663746\n",
      "Params: tensor([2.0994, 0.0980])\n",
      "Grad:  tensor([-11.9375,   0.9456])\n",
      "Epoch 315 Loss 30.649443\n",
      "Params: tensor([2.1006, 0.0979])\n",
      "Grad:  tensor([-11.8683,   0.9578])\n",
      "Epoch 316 Loss 30.635303\n",
      "Params: tensor([2.1018, 0.0979])\n",
      "Grad:  tensor([-11.7995,   0.9699])\n",
      "Epoch 317 Loss 30.621326\n",
      "Params: tensor([2.1029, 0.0978])\n",
      "Grad:  tensor([-11.7311,   0.9819])\n",
      "Epoch 318 Loss 30.607512\n",
      "Params: tensor([2.1041, 0.0977])\n",
      "Grad:  tensor([-11.6631,   0.9939])\n",
      "Epoch 319 Loss 30.593853\n",
      "Params: tensor([2.1053, 0.0976])\n",
      "Grad:  tensor([-11.5955,   1.0058])\n",
      "Epoch 320 Loss 30.580339\n",
      "Params: tensor([2.1064, 0.0975])\n",
      "Grad:  tensor([-11.5284,   1.0176])\n",
      "Epoch 321 Loss 30.566988\n",
      "Params: tensor([2.1076, 0.0974])\n",
      "Grad:  tensor([-11.4616,   1.0293])\n",
      "Epoch 322 Loss 30.553783\n",
      "Params: tensor([2.1087, 0.0973])\n",
      "Grad:  tensor([-11.3953,   1.0410])\n",
      "Epoch 323 Loss 30.540728\n",
      "Params: tensor([2.1099, 0.0971])\n",
      "Grad:  tensor([-11.3293,   1.0526])\n",
      "Epoch 324 Loss 30.527815\n",
      "Params: tensor([2.1110, 0.0970])\n",
      "Grad:  tensor([-11.2638,   1.0641])\n",
      "Epoch 325 Loss 30.515045\n",
      "Params: tensor([2.1121, 0.0969])\n",
      "Grad:  tensor([-11.1986,   1.0756])\n",
      "Epoch 326 Loss 30.502424\n",
      "Params: tensor([2.1133, 0.0968])\n",
      "Grad:  tensor([-11.1339,   1.0870])\n",
      "Epoch 327 Loss 30.489948\n",
      "Params: tensor([2.1144, 0.0967])\n",
      "Grad:  tensor([-11.0695,   1.0983])\n",
      "Epoch 328 Loss 30.477608\n",
      "Params: tensor([2.1155, 0.0966])\n",
      "Grad:  tensor([-11.0056,   1.1095])\n",
      "Epoch 329 Loss 30.465405\n",
      "Params: tensor([2.1166, 0.0965])\n",
      "Grad:  tensor([-10.9420,   1.1207])\n",
      "Epoch 330 Loss 30.453350\n",
      "Params: tensor([2.1177, 0.0964])\n",
      "Grad:  tensor([-10.8788,   1.1318])\n",
      "Epoch 331 Loss 30.441418\n",
      "Params: tensor([2.1188, 0.0963])\n",
      "Grad:  tensor([-10.8160,   1.1429])\n",
      "Epoch 332 Loss 30.429615\n",
      "Params: tensor([2.1198, 0.0962])\n",
      "Grad:  tensor([-10.7535,   1.1538])\n",
      "Epoch 333 Loss 30.417950\n",
      "Params: tensor([2.1209, 0.0960])\n",
      "Grad:  tensor([-10.6915,   1.1647])\n",
      "Epoch 334 Loss 30.406422\n",
      "Params: tensor([2.1220, 0.0959])\n",
      "Grad:  tensor([-10.6298,   1.1756])\n",
      "Epoch 335 Loss 30.395014\n",
      "Params: tensor([2.1230, 0.0958])\n",
      "Grad:  tensor([-10.5685,   1.1864])\n",
      "Epoch 336 Loss 30.383739\n",
      "Params: tensor([2.1241, 0.0957])\n",
      "Grad:  tensor([-10.5076,   1.1971])\n",
      "Epoch 337 Loss 30.372589\n",
      "Params: tensor([2.1252, 0.0956])\n",
      "Grad:  tensor([-10.4470,   1.2077])\n",
      "Epoch 338 Loss 30.361555\n",
      "Params: tensor([2.1262, 0.0955])\n",
      "Grad:  tensor([-10.3868,   1.2183])\n",
      "Epoch 339 Loss 30.350645\n",
      "Params: tensor([2.1272, 0.0953])\n",
      "Grad:  tensor([-10.3270,   1.2288])\n",
      "Epoch 340 Loss 30.339863\n",
      "Params: tensor([2.1283, 0.0952])\n",
      "Grad:  tensor([-10.2675,   1.2393])\n",
      "Epoch 341 Loss 30.329191\n",
      "Params: tensor([2.1293, 0.0951])\n",
      "Grad:  tensor([-10.2084,   1.2497])\n",
      "Epoch 342 Loss 30.318647\n",
      "Params: tensor([2.1303, 0.0950])\n",
      "Grad:  tensor([-10.1497,   1.2600])\n",
      "Epoch 343 Loss 30.308216\n",
      "Params: tensor([2.1313, 0.0948])\n",
      "Grad:  tensor([-10.0913,   1.2702])\n",
      "Epoch 344 Loss 30.297894\n",
      "Params: tensor([2.1323, 0.0947])\n",
      "Grad:  tensor([-10.0332,   1.2804])\n",
      "Epoch 345 Loss 30.287697\n",
      "Params: tensor([2.1333, 0.0946])\n",
      "Grad:  tensor([-9.9756,  1.2906])\n",
      "Epoch 346 Loss 30.277607\n",
      "Params: tensor([2.1343, 0.0944])\n",
      "Grad:  tensor([-9.9182,  1.3007])\n",
      "Epoch 347 Loss 30.267628\n",
      "Params: tensor([2.1353, 0.0943])\n",
      "Grad:  tensor([-9.8612,  1.3107])\n",
      "Epoch 348 Loss 30.257759\n",
      "Params: tensor([2.1363, 0.0942])\n",
      "Grad:  tensor([-9.8046,  1.3206])\n",
      "Epoch 349 Loss 30.247997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([2.1373, 0.0941])\n",
      "Grad:  tensor([-9.7483,  1.3305])\n",
      "Epoch 350 Loss 30.238348\n",
      "Params: tensor([2.1383, 0.0939])\n",
      "Grad:  tensor([-9.6923,  1.3404])\n",
      "Epoch 351 Loss 30.228798\n",
      "Params: tensor([2.1392, 0.0938])\n",
      "Grad:  tensor([-9.6367,  1.3501])\n",
      "Epoch 352 Loss 30.219355\n",
      "Params: tensor([2.1402, 0.0937])\n",
      "Grad:  tensor([-9.5814,  1.3598])\n",
      "Epoch 353 Loss 30.210014\n",
      "Params: tensor([2.1412, 0.0935])\n",
      "Grad:  tensor([-9.5265,  1.3695])\n",
      "Epoch 354 Loss 30.200777\n",
      "Params: tensor([2.1421, 0.0934])\n",
      "Grad:  tensor([-9.4719,  1.3791])\n",
      "Epoch 355 Loss 30.191648\n",
      "Params: tensor([2.1431, 0.0932])\n",
      "Grad:  tensor([-9.4176,  1.3886])\n",
      "Epoch 356 Loss 30.182606\n",
      "Params: tensor([2.1440, 0.0931])\n",
      "Grad:  tensor([-9.3636,  1.3981])\n",
      "Epoch 357 Loss 30.173668\n",
      "Params: tensor([2.1449, 0.0930])\n",
      "Grad:  tensor([-9.3100,  1.4075])\n",
      "Epoch 358 Loss 30.164824\n",
      "Params: tensor([2.1459, 0.0928])\n",
      "Grad:  tensor([-9.2567,  1.4169])\n",
      "Epoch 359 Loss 30.156078\n",
      "Params: tensor([2.1468, 0.0927])\n",
      "Grad:  tensor([-9.2037,  1.4262])\n",
      "Epoch 360 Loss 30.147429\n",
      "Params: tensor([2.1477, 0.0925])\n",
      "Grad:  tensor([-9.1511,  1.4355])\n",
      "Epoch 361 Loss 30.138872\n",
      "Params: tensor([2.1486, 0.0924])\n",
      "Grad:  tensor([-9.0988,  1.4446])\n",
      "Epoch 362 Loss 30.130409\n",
      "Params: tensor([2.1495, 0.0922])\n",
      "Grad:  tensor([-9.0467,  1.4538])\n",
      "Epoch 363 Loss 30.122040\n",
      "Params: tensor([2.1504, 0.0921])\n",
      "Grad:  tensor([-8.9951,  1.4629])\n",
      "Epoch 364 Loss 30.113747\n",
      "Params: tensor([2.1513, 0.0920])\n",
      "Grad:  tensor([-8.9437,  1.4719])\n",
      "Epoch 365 Loss 30.105562\n",
      "Params: tensor([2.1522, 0.0918])\n",
      "Grad:  tensor([-8.8926,  1.4809])\n",
      "Epoch 366 Loss 30.097454\n",
      "Params: tensor([2.1531, 0.0917])\n",
      "Grad:  tensor([-8.8418,  1.4898])\n",
      "Epoch 367 Loss 30.089436\n",
      "Params: tensor([2.1540, 0.0915])\n",
      "Grad:  tensor([-8.7913,  1.4986])\n",
      "Epoch 368 Loss 30.081501\n",
      "Params: tensor([2.1549, 0.0914])\n",
      "Grad:  tensor([-8.7412,  1.5074])\n",
      "Epoch 369 Loss 30.073662\n",
      "Params: tensor([2.1558, 0.0912])\n",
      "Grad:  tensor([-8.6913,  1.5162])\n",
      "Epoch 370 Loss 30.065895\n",
      "Params: tensor([2.1566, 0.0911])\n",
      "Grad:  tensor([-8.6418,  1.5249])\n",
      "Epoch 371 Loss 30.058210\n",
      "Params: tensor([2.1575, 0.0909])\n",
      "Grad:  tensor([-8.5926,  1.5336])\n",
      "Epoch 372 Loss 30.050621\n",
      "Params: tensor([2.1584, 0.0908])\n",
      "Grad:  tensor([-8.5436,  1.5421])\n",
      "Epoch 373 Loss 30.043102\n",
      "Params: tensor([2.1592, 0.0906])\n",
      "Grad:  tensor([-8.4950,  1.5507])\n",
      "Epoch 374 Loss 30.035658\n",
      "Params: tensor([2.1601, 0.0904])\n",
      "Grad:  tensor([-8.4466,  1.5592])\n",
      "Epoch 375 Loss 30.028309\n",
      "Params: tensor([2.1609, 0.0903])\n",
      "Grad:  tensor([-8.3985,  1.5676])\n",
      "Epoch 376 Loss 30.021025\n",
      "Params: tensor([2.1618, 0.0901])\n",
      "Grad:  tensor([-8.3508,  1.5760])\n",
      "Epoch 377 Loss 30.013823\n",
      "Params: tensor([2.1626, 0.0900])\n",
      "Grad:  tensor([-8.3033,  1.5843])\n",
      "Epoch 378 Loss 30.006697\n",
      "Params: tensor([2.1634, 0.0898])\n",
      "Grad:  tensor([-8.2561,  1.5926])\n",
      "Epoch 379 Loss 29.999641\n",
      "Params: tensor([2.1642, 0.0897])\n",
      "Grad:  tensor([-8.2091,  1.6009])\n",
      "Epoch 380 Loss 29.992662\n",
      "Params: tensor([2.1651, 0.0895])\n",
      "Grad:  tensor([-8.1625,  1.6090])\n",
      "Epoch 381 Loss 29.985760\n",
      "Params: tensor([2.1659, 0.0893])\n",
      "Grad:  tensor([-8.1162,  1.6172])\n",
      "Epoch 382 Loss 29.978935\n",
      "Params: tensor([2.1667, 0.0892])\n",
      "Grad:  tensor([-8.0701,  1.6253])\n",
      "Epoch 383 Loss 29.972174\n",
      "Params: tensor([2.1675, 0.0890])\n",
      "Grad:  tensor([-8.0243,  1.6333])\n",
      "Epoch 384 Loss 29.965483\n",
      "Params: tensor([2.1683, 0.0888])\n",
      "Grad:  tensor([-7.9788,  1.6413])\n",
      "Epoch 385 Loss 29.958860\n",
      "Params: tensor([2.1691, 0.0887])\n",
      "Grad:  tensor([-7.9336,  1.6492])\n",
      "Epoch 386 Loss 29.952314\n",
      "Params: tensor([2.1699, 0.0885])\n",
      "Grad:  tensor([-7.8886,  1.6571])\n",
      "Epoch 387 Loss 29.945831\n",
      "Params: tensor([2.1707, 0.0884])\n",
      "Grad:  tensor([-7.8440,  1.6650])\n",
      "Epoch 388 Loss 29.939417\n",
      "Params: tensor([2.1715, 0.0882])\n",
      "Grad:  tensor([-7.7995,  1.6728])\n",
      "Epoch 389 Loss 29.933075\n",
      "Params: tensor([2.1722, 0.0880])\n",
      "Grad:  tensor([-7.7554,  1.6805])\n",
      "Epoch 390 Loss 29.926790\n",
      "Params: tensor([2.1730, 0.0879])\n",
      "Grad:  tensor([-7.7115,  1.6882])\n",
      "Epoch 391 Loss 29.920580\n",
      "Params: tensor([2.1738, 0.0877])\n",
      "Grad:  tensor([-7.6679,  1.6959])\n",
      "Epoch 392 Loss 29.914427\n",
      "Params: tensor([2.1746, 0.0875])\n",
      "Grad:  tensor([-7.6246,  1.7035])\n",
      "Epoch 393 Loss 29.908337\n",
      "Params: tensor([2.1753, 0.0873])\n",
      "Grad:  tensor([-7.5815,  1.7110])\n",
      "Epoch 394 Loss 29.902315\n",
      "Params: tensor([2.1761, 0.0872])\n",
      "Grad:  tensor([-7.5387,  1.7185])\n",
      "Epoch 395 Loss 29.896351\n",
      "Params: tensor([2.1768, 0.0870])\n",
      "Grad:  tensor([-7.4961,  1.7260])\n",
      "Epoch 396 Loss 29.890450\n",
      "Params: tensor([2.1776, 0.0868])\n",
      "Grad:  tensor([-7.4538,  1.7334])\n",
      "Epoch 397 Loss 29.884607\n",
      "Params: tensor([2.1783, 0.0867])\n",
      "Grad:  tensor([-7.4118,  1.7408])\n",
      "Epoch 398 Loss 29.878828\n",
      "Params: tensor([2.1791, 0.0865])\n",
      "Grad:  tensor([-7.3700,  1.7481])\n",
      "Epoch 399 Loss 29.873108\n",
      "Params: tensor([2.1798, 0.0863])\n",
      "Grad:  tensor([-7.3285,  1.7554])\n",
      "Epoch 400 Loss 29.867439\n",
      "Params: tensor([2.1805, 0.0861])\n",
      "Grad:  tensor([-7.2872,  1.7626])\n",
      "Epoch 401 Loss 29.861835\n",
      "Params: tensor([2.1813, 0.0860])\n",
      "Grad:  tensor([-7.2462,  1.7698])\n",
      "Epoch 402 Loss 29.856285\n",
      "Params: tensor([2.1820, 0.0858])\n",
      "Grad:  tensor([-7.2054,  1.7770])\n",
      "Epoch 403 Loss 29.850788\n",
      "Params: tensor([2.1827, 0.0856])\n",
      "Grad:  tensor([-7.1648,  1.7841])\n",
      "Epoch 404 Loss 29.845350\n",
      "Params: tensor([2.1834, 0.0854])\n",
      "Grad:  tensor([-7.1245,  1.7912])\n",
      "Epoch 405 Loss 29.839972\n",
      "Params: tensor([2.1841, 0.0852])\n",
      "Grad:  tensor([-7.0845,  1.7982])\n",
      "Epoch 406 Loss 29.834639\n",
      "Params: tensor([2.1848, 0.0851])\n",
      "Grad:  tensor([-7.0447,  1.8052])\n",
      "Epoch 407 Loss 29.829369\n",
      "Params: tensor([2.1856, 0.0849])\n",
      "Grad:  tensor([-7.0051,  1.8121])\n",
      "Epoch 408 Loss 29.824146\n",
      "Params: tensor([2.1863, 0.0847])\n",
      "Grad:  tensor([-6.9658,  1.8190])\n",
      "Epoch 409 Loss 29.818972\n",
      "Params: tensor([2.1870, 0.0845])\n",
      "Grad:  tensor([-6.9267,  1.8259])\n",
      "Epoch 410 Loss 29.813854\n",
      "Params: tensor([2.1876, 0.0843])\n",
      "Grad:  tensor([-6.8879,  1.8327])\n",
      "Epoch 411 Loss 29.808788\n",
      "Params: tensor([2.1883, 0.0842])\n",
      "Grad:  tensor([-6.8493,  1.8394])\n",
      "Epoch 412 Loss 29.803774\n",
      "Params: tensor([2.1890, 0.0840])\n",
      "Grad:  tensor([-6.8109,  1.8462])\n",
      "Epoch 413 Loss 29.798803\n",
      "Params: tensor([2.1897, 0.0838])\n",
      "Grad:  tensor([-6.7727,  1.8529])\n",
      "Epoch 414 Loss 29.793884\n",
      "Params: tensor([2.1904, 0.0836])\n",
      "Grad:  tensor([-6.7348,  1.8595])\n",
      "Epoch 415 Loss 29.789015\n",
      "Params: tensor([2.1910, 0.0834])\n",
      "Grad:  tensor([-6.6971,  1.8661])\n",
      "Epoch 416 Loss 29.784197\n",
      "Params: tensor([2.1917, 0.0832])\n",
      "Grad:  tensor([-6.6597,  1.8727])\n",
      "Epoch 417 Loss 29.779421\n",
      "Params: tensor([2.1924, 0.0830])\n",
      "Grad:  tensor([-6.6224,  1.8792])\n",
      "Epoch 418 Loss 29.774694\n",
      "Params: tensor([2.1930, 0.0829])\n",
      "Grad:  tensor([-6.5854,  1.8857])\n",
      "Epoch 419 Loss 29.770014\n",
      "Params: tensor([2.1937, 0.0827])\n",
      "Grad:  tensor([-6.5487,  1.8921])\n",
      "Epoch 420 Loss 29.765381\n",
      "Params: tensor([2.1944, 0.0825])\n",
      "Grad:  tensor([-6.5121,  1.8985])\n",
      "Epoch 421 Loss 29.760786\n",
      "Params: tensor([2.1950, 0.0823])\n",
      "Grad:  tensor([-6.4758,  1.9049])\n",
      "Epoch 422 Loss 29.756237\n",
      "Params: tensor([2.1957, 0.0821])\n",
      "Grad:  tensor([-6.4396,  1.9112])\n",
      "Epoch 423 Loss 29.751738\n",
      "Params: tensor([2.1963, 0.0819])\n",
      "Grad:  tensor([-6.4038,  1.9175])\n",
      "Epoch 424 Loss 29.747282\n",
      "Params: tensor([2.1969, 0.0817])\n",
      "Grad:  tensor([-6.3681,  1.9238])\n",
      "Epoch 425 Loss 29.742870\n",
      "Params: tensor([2.1976, 0.0815])\n",
      "Grad:  tensor([-6.3326,  1.9300])\n",
      "Epoch 426 Loss 29.738493\n",
      "Params: tensor([2.1982, 0.0813])\n",
      "Grad:  tensor([-6.2974,  1.9362])\n",
      "Epoch 427 Loss 29.734163\n",
      "Params: tensor([2.1988, 0.0811])\n",
      "Grad:  tensor([-6.2623,  1.9423])\n",
      "Epoch 428 Loss 29.729876\n",
      "Params: tensor([2.1995, 0.0809])\n",
      "Grad:  tensor([-6.2275,  1.9484])\n",
      "Epoch 429 Loss 29.725630\n",
      "Params: tensor([2.2001, 0.0807])\n",
      "Grad:  tensor([-6.1929,  1.9545])\n",
      "Epoch 430 Loss 29.721418\n",
      "Params: tensor([2.2007, 0.0805])\n",
      "Grad:  tensor([-6.1585,  1.9605])\n",
      "Epoch 431 Loss 29.717260\n",
      "Params: tensor([2.2013, 0.0804])\n",
      "Grad:  tensor([-6.1243,  1.9665])\n",
      "Epoch 432 Loss 29.713133\n",
      "Params: tensor([2.2019, 0.0802])\n",
      "Grad:  tensor([-6.0903,  1.9724])\n",
      "Epoch 433 Loss 29.709040\n",
      "Params: tensor([2.2025, 0.0800])\n",
      "Grad:  tensor([-6.0566,  1.9783])\n",
      "Epoch 434 Loss 29.704988\n",
      "Params: tensor([2.2032, 0.0798])\n",
      "Grad:  tensor([-6.0230,  1.9842])\n",
      "Epoch 435 Loss 29.700983\n",
      "Params: tensor([2.2038, 0.0796])\n",
      "Grad:  tensor([-5.9896,  1.9900])\n",
      "Epoch 436 Loss 29.697004\n",
      "Params: tensor([2.2044, 0.0794])\n",
      "Grad:  tensor([-5.9565,  1.9959])\n",
      "Epoch 437 Loss 29.693068\n",
      "Params: tensor([2.2050, 0.0792])\n",
      "Grad:  tensor([-5.9235,  2.0016])\n",
      "Epoch 438 Loss 29.689171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([2.2055, 0.0790])\n",
      "Grad:  tensor([-5.8908,  2.0074])\n",
      "Epoch 439 Loss 29.685301\n",
      "Params: tensor([2.2061, 0.0788])\n",
      "Grad:  tensor([-5.8582,  2.0131])\n",
      "Epoch 440 Loss 29.681477\n",
      "Params: tensor([2.2067, 0.0786])\n",
      "Grad:  tensor([-5.8258,  2.0187])\n",
      "Epoch 441 Loss 29.677687\n",
      "Params: tensor([2.2073, 0.0784])\n",
      "Grad:  tensor([-5.7936,  2.0244])\n",
      "Epoch 442 Loss 29.673925\n",
      "Params: tensor([2.2079, 0.0782])\n",
      "Grad:  tensor([-5.7617,  2.0300])\n",
      "Epoch 443 Loss 29.670204\n",
      "Params: tensor([2.2085, 0.0780])\n",
      "Grad:  tensor([-5.7299,  2.0355])\n",
      "Epoch 444 Loss 29.666512\n",
      "Params: tensor([2.2090, 0.0777])\n",
      "Grad:  tensor([-5.6983,  2.0411])\n",
      "Epoch 445 Loss 29.662861\n",
      "Params: tensor([2.2096, 0.0775])\n",
      "Grad:  tensor([-5.6669,  2.0465])\n",
      "Epoch 446 Loss 29.659235\n",
      "Params: tensor([2.2102, 0.0773])\n",
      "Grad:  tensor([-5.6357,  2.0520])\n",
      "Epoch 447 Loss 29.655645\n",
      "Params: tensor([2.2107, 0.0771])\n",
      "Grad:  tensor([-5.6046,  2.0574])\n",
      "Epoch 448 Loss 29.652092\n",
      "Params: tensor([2.2113, 0.0769])\n",
      "Grad:  tensor([-5.5738,  2.0628])\n",
      "Epoch 449 Loss 29.648565\n",
      "Params: tensor([2.2118, 0.0767])\n",
      "Grad:  tensor([-5.5431,  2.0682])\n",
      "Epoch 450 Loss 29.645073\n",
      "Params: tensor([2.2124, 0.0765])\n",
      "Grad:  tensor([-5.5127,  2.0735])\n",
      "Epoch 451 Loss 29.641619\n",
      "Params: tensor([2.2130, 0.0763])\n",
      "Grad:  tensor([-5.4824,  2.0788])\n",
      "Epoch 452 Loss 29.638184\n",
      "Params: tensor([2.2135, 0.0761])\n",
      "Grad:  tensor([-5.4523,  2.0841])\n",
      "Epoch 453 Loss 29.634783\n",
      "Params: tensor([2.2140, 0.0759])\n",
      "Grad:  tensor([-5.4224,  2.0893])\n",
      "Epoch 454 Loss 29.631414\n",
      "Params: tensor([2.2146, 0.0757])\n",
      "Grad:  tensor([-5.3927,  2.0945])\n",
      "Epoch 455 Loss 29.628069\n",
      "Params: tensor([2.2151, 0.0755])\n",
      "Grad:  tensor([-5.3631,  2.0997])\n",
      "Epoch 456 Loss 29.624767\n",
      "Params: tensor([2.2157, 0.0753])\n",
      "Grad:  tensor([-5.3338,  2.1048])\n",
      "Epoch 457 Loss 29.621485\n",
      "Params: tensor([2.2162, 0.0751])\n",
      "Grad:  tensor([-5.3046,  2.1099])\n",
      "Epoch 458 Loss 29.618233\n",
      "Params: tensor([2.2167, 0.0748])\n",
      "Grad:  tensor([-5.2756,  2.1150])\n",
      "Epoch 459 Loss 29.615004\n",
      "Params: tensor([2.2173, 0.0746])\n",
      "Grad:  tensor([-5.2467,  2.1200])\n",
      "Epoch 460 Loss 29.611816\n",
      "Params: tensor([2.2178, 0.0744])\n",
      "Grad:  tensor([-5.2181,  2.1251])\n",
      "Epoch 461 Loss 29.608648\n",
      "Params: tensor([2.2183, 0.0742])\n",
      "Grad:  tensor([-5.1896,  2.1300])\n",
      "Epoch 462 Loss 29.605501\n",
      "Params: tensor([2.2188, 0.0740])\n",
      "Grad:  tensor([-5.1613,  2.1350])\n",
      "Epoch 463 Loss 29.602392\n",
      "Params: tensor([2.2193, 0.0738])\n",
      "Grad:  tensor([-5.1331,  2.1399])\n",
      "Epoch 464 Loss 29.599304\n",
      "Params: tensor([2.2199, 0.0736])\n",
      "Grad:  tensor([-5.1051,  2.1448])\n",
      "Epoch 465 Loss 29.596247\n",
      "Params: tensor([2.2204, 0.0734])\n",
      "Grad:  tensor([-5.0773,  2.1497])\n",
      "Epoch 466 Loss 29.593212\n",
      "Params: tensor([2.2209, 0.0731])\n",
      "Grad:  tensor([-5.0497,  2.1545])\n",
      "Epoch 467 Loss 29.590202\n",
      "Params: tensor([2.2214, 0.0729])\n",
      "Grad:  tensor([-5.0222,  2.1593])\n",
      "Epoch 468 Loss 29.587221\n",
      "Params: tensor([2.2219, 0.0727])\n",
      "Grad:  tensor([-4.9949,  2.1641])\n",
      "Epoch 469 Loss 29.584270\n",
      "Params: tensor([2.2224, 0.0725])\n",
      "Grad:  tensor([-4.9678,  2.1688])\n",
      "Epoch 470 Loss 29.581329\n",
      "Params: tensor([2.2229, 0.0723])\n",
      "Grad:  tensor([-4.9408,  2.1735])\n",
      "Epoch 471 Loss 29.578424\n",
      "Params: tensor([2.2234, 0.0721])\n",
      "Grad:  tensor([-4.9140,  2.1782])\n",
      "Epoch 472 Loss 29.575539\n",
      "Params: tensor([2.2239, 0.0718])\n",
      "Grad:  tensor([-4.8874,  2.1829])\n",
      "Epoch 473 Loss 29.572681\n",
      "Params: tensor([2.2243, 0.0716])\n",
      "Grad:  tensor([-4.8609,  2.1875])\n",
      "Epoch 474 Loss 29.569847\n",
      "Params: tensor([2.2248, 0.0714])\n",
      "Grad:  tensor([-4.8346,  2.1921])\n",
      "Epoch 475 Loss 29.567034\n",
      "Params: tensor([2.2253, 0.0712])\n",
      "Grad:  tensor([-4.8084,  2.1966])\n",
      "Epoch 476 Loss 29.564247\n",
      "Params: tensor([2.2258, 0.0710])\n",
      "Grad:  tensor([-4.7824,  2.2012])\n",
      "Epoch 477 Loss 29.561481\n",
      "Params: tensor([2.2263, 0.0707])\n",
      "Grad:  tensor([-4.7565,  2.2057])\n",
      "Epoch 478 Loss 29.558739\n",
      "Params: tensor([2.2268, 0.0705])\n",
      "Grad:  tensor([-4.7308,  2.2102])\n",
      "Epoch 479 Loss 29.556017\n",
      "Params: tensor([2.2272, 0.0703])\n",
      "Grad:  tensor([-4.7053,  2.2147])\n",
      "Epoch 480 Loss 29.553316\n",
      "Params: tensor([2.2277, 0.0701])\n",
      "Grad:  tensor([-4.6799,  2.2191])\n",
      "Epoch 481 Loss 29.550640\n",
      "Params: tensor([2.2282, 0.0699])\n",
      "Grad:  tensor([-4.6547,  2.2235])\n",
      "Epoch 482 Loss 29.547985\n",
      "Params: tensor([2.2286, 0.0696])\n",
      "Grad:  tensor([-4.6296,  2.2279])\n",
      "Epoch 483 Loss 29.545347\n",
      "Params: tensor([2.2291, 0.0694])\n",
      "Grad:  tensor([-4.6047,  2.2322])\n",
      "Epoch 484 Loss 29.542736\n",
      "Params: tensor([2.2296, 0.0692])\n",
      "Grad:  tensor([-4.5799,  2.2365])\n",
      "Epoch 485 Loss 29.540150\n",
      "Params: tensor([2.2300, 0.0690])\n",
      "Grad:  tensor([-4.5553,  2.2408])\n",
      "Epoch 486 Loss 29.537569\n",
      "Params: tensor([2.2305, 0.0687])\n",
      "Grad:  tensor([-4.5308,  2.2451])\n",
      "Epoch 487 Loss 29.535017\n",
      "Params: tensor([2.2309, 0.0685])\n",
      "Grad:  tensor([-4.5065,  2.2494])\n",
      "Epoch 488 Loss 29.532484\n",
      "Params: tensor([2.2314, 0.0683])\n",
      "Grad:  tensor([-4.4824,  2.2536])\n",
      "Epoch 489 Loss 29.529974\n",
      "Params: tensor([2.2318, 0.0681])\n",
      "Grad:  tensor([-4.4583,  2.2578])\n",
      "Epoch 490 Loss 29.527485\n",
      "Params: tensor([2.2323, 0.0678])\n",
      "Grad:  tensor([-4.4344,  2.2619])\n",
      "Epoch 491 Loss 29.525013\n",
      "Params: tensor([2.2327, 0.0676])\n",
      "Grad:  tensor([-4.4107,  2.2661])\n",
      "Epoch 492 Loss 29.522554\n",
      "Params: tensor([2.2331, 0.0674])\n",
      "Grad:  tensor([-4.3871,  2.2702])\n",
      "Epoch 493 Loss 29.520119\n",
      "Params: tensor([2.2336, 0.0672])\n",
      "Grad:  tensor([-4.3637,  2.2743])\n",
      "Epoch 494 Loss 29.517708\n",
      "Params: tensor([2.2340, 0.0669])\n",
      "Grad:  tensor([-4.3404,  2.2783])\n",
      "Epoch 495 Loss 29.515308\n",
      "Params: tensor([2.2345, 0.0667])\n",
      "Grad:  tensor([-4.3172,  2.2824])\n",
      "Epoch 496 Loss 29.512926\n",
      "Params: tensor([2.2349, 0.0665])\n",
      "Grad:  tensor([-4.2942,  2.2864])\n",
      "Epoch 497 Loss 29.510561\n",
      "Params: tensor([2.2353, 0.0662])\n",
      "Grad:  tensor([-4.2713,  2.2904])\n",
      "Epoch 498 Loss 29.508215\n",
      "Params: tensor([2.2357, 0.0660])\n",
      "Grad:  tensor([-4.2485,  2.2944])\n",
      "Epoch 499 Loss 29.505892\n",
      "Params: tensor([2.2362, 0.0658])\n",
      "Grad:  tensor([-4.2259,  2.2983])\n",
      "Epoch 500 Loss 29.503582\n",
      "Params: tensor([2.2366, 0.0656])\n",
      "Grad:  tensor([-4.2034,  2.3022])\n",
      "Epoch 501 Loss 29.501289\n",
      "Params: tensor([2.2370, 0.0653])\n",
      "Grad:  tensor([-4.1811,  2.3061])\n",
      "Epoch 502 Loss 29.499006\n",
      "Params: tensor([2.2374, 0.0651])\n",
      "Grad:  tensor([-4.1589,  2.3100])\n",
      "Epoch 503 Loss 29.496758\n",
      "Params: tensor([2.2378, 0.0649])\n",
      "Grad:  tensor([-4.1368,  2.3138])\n",
      "Epoch 504 Loss 29.494509\n",
      "Params: tensor([2.2383, 0.0646])\n",
      "Grad:  tensor([-4.1149,  2.3177])\n",
      "Epoch 505 Loss 29.492285\n",
      "Params: tensor([2.2387, 0.0644])\n",
      "Grad:  tensor([-4.0931,  2.3215])\n",
      "Epoch 506 Loss 29.490076\n",
      "Params: tensor([2.2391, 0.0642])\n",
      "Grad:  tensor([-4.0714,  2.3252])\n",
      "Epoch 507 Loss 29.487879\n",
      "Params: tensor([2.2395, 0.0639])\n",
      "Grad:  tensor([-4.0499,  2.3290])\n",
      "Epoch 508 Loss 29.485701\n",
      "Params: tensor([2.2399, 0.0637])\n",
      "Grad:  tensor([-4.0285,  2.3327])\n",
      "Epoch 509 Loss 29.483538\n",
      "Params: tensor([2.2403, 0.0635])\n",
      "Grad:  tensor([-4.0072,  2.3364])\n",
      "Epoch 510 Loss 29.481386\n",
      "Params: tensor([2.2407, 0.0632])\n",
      "Grad:  tensor([-3.9860,  2.3401])\n",
      "Epoch 511 Loss 29.479259\n",
      "Params: tensor([2.2411, 0.0630])\n",
      "Grad:  tensor([-3.9650,  2.3438])\n",
      "Epoch 512 Loss 29.477140\n",
      "Params: tensor([2.2415, 0.0628])\n",
      "Grad:  tensor([-3.9441,  2.3474])\n",
      "Epoch 513 Loss 29.475035\n",
      "Params: tensor([2.2419, 0.0625])\n",
      "Grad:  tensor([-3.9234,  2.3510])\n",
      "Epoch 514 Loss 29.472950\n",
      "Params: tensor([2.2423, 0.0623])\n",
      "Grad:  tensor([-3.9027,  2.3546])\n",
      "Epoch 515 Loss 29.470869\n",
      "Params: tensor([2.2427, 0.0621])\n",
      "Grad:  tensor([-3.8822,  2.3582])\n",
      "Epoch 516 Loss 29.468813\n",
      "Params: tensor([2.2431, 0.0618])\n",
      "Grad:  tensor([-3.8618,  2.3617])\n",
      "Epoch 517 Loss 29.466768\n",
      "Params: tensor([2.2434, 0.0616])\n",
      "Grad:  tensor([-3.8415,  2.3653])\n",
      "Epoch 518 Loss 29.464739\n",
      "Params: tensor([2.2438, 0.0614])\n",
      "Grad:  tensor([-3.8214,  2.3688])\n",
      "Epoch 519 Loss 29.462719\n",
      "Params: tensor([2.2442, 0.0611])\n",
      "Grad:  tensor([-3.8014,  2.3723])\n",
      "Epoch 520 Loss 29.460712\n",
      "Params: tensor([2.2446, 0.0609])\n",
      "Grad:  tensor([-3.7815,  2.3757])\n",
      "Epoch 521 Loss 29.458723\n",
      "Params: tensor([2.2450, 0.0606])\n",
      "Grad:  tensor([-3.7617,  2.3792])\n",
      "Epoch 522 Loss 29.456749\n",
      "Params: tensor([2.2453, 0.0604])\n",
      "Grad:  tensor([-3.7420,  2.3826])\n",
      "Epoch 523 Loss 29.454784\n",
      "Params: tensor([2.2457, 0.0602])\n",
      "Grad:  tensor([-3.7225,  2.3860])\n",
      "Epoch 524 Loss 29.452833\n",
      "Params: tensor([2.2461, 0.0599])\n",
      "Grad:  tensor([-3.7031,  2.3894])\n",
      "Epoch 525 Loss 29.450890\n",
      "Params: tensor([2.2465, 0.0597])\n",
      "Grad:  tensor([-3.6838,  2.3927])\n",
      "Epoch 526 Loss 29.448963\n",
      "Params: tensor([2.2468, 0.0595])\n",
      "Grad:  tensor([-3.6646,  2.3961])\n",
      "Epoch 527 Loss 29.447052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([2.2472, 0.0592])\n",
      "Grad:  tensor([-3.6455,  2.3994])\n",
      "Epoch 528 Loss 29.445147\n",
      "Params: tensor([2.2476, 0.0590])\n",
      "Grad:  tensor([-3.6266,  2.4027])\n",
      "Epoch 529 Loss 29.443262\n",
      "Params: tensor([2.2479, 0.0587])\n",
      "Grad:  tensor([-3.6077,  2.4060])\n",
      "Epoch 530 Loss 29.441378\n",
      "Params: tensor([2.2483, 0.0585])\n",
      "Grad:  tensor([-3.5890,  2.4092])\n",
      "Epoch 531 Loss 29.439516\n",
      "Params: tensor([2.2486, 0.0583])\n",
      "Grad:  tensor([-3.5704,  2.4124])\n",
      "Epoch 532 Loss 29.437666\n",
      "Params: tensor([2.2490, 0.0580])\n",
      "Grad:  tensor([-3.5519,  2.4157])\n",
      "Epoch 533 Loss 29.435820\n",
      "Params: tensor([2.2494, 0.0578])\n",
      "Grad:  tensor([-3.5335,  2.4189])\n",
      "Epoch 534 Loss 29.433985\n",
      "Params: tensor([2.2497, 0.0575])\n",
      "Grad:  tensor([-3.5152,  2.4220])\n",
      "Epoch 535 Loss 29.432173\n",
      "Params: tensor([2.2501, 0.0573])\n",
      "Grad:  tensor([-3.4971,  2.4252])\n",
      "Epoch 536 Loss 29.430357\n",
      "Params: tensor([2.2504, 0.0570])\n",
      "Grad:  tensor([-3.4790,  2.4283])\n",
      "Epoch 537 Loss 29.428566\n",
      "Params: tensor([2.2508, 0.0568])\n",
      "Grad:  tensor([-3.4611,  2.4315])\n",
      "Epoch 538 Loss 29.426777\n",
      "Params: tensor([2.2511, 0.0566])\n",
      "Grad:  tensor([-3.4432,  2.4346])\n",
      "Epoch 539 Loss 29.425005\n",
      "Params: tensor([2.2514, 0.0563])\n",
      "Grad:  tensor([-3.4255,  2.4376])\n",
      "Epoch 540 Loss 29.423237\n",
      "Params: tensor([2.2518, 0.0561])\n",
      "Grad:  tensor([-3.4079,  2.4407])\n",
      "Epoch 541 Loss 29.421484\n",
      "Params: tensor([2.2521, 0.0558])\n",
      "Grad:  tensor([-3.3903,  2.4437])\n",
      "Epoch 542 Loss 29.419741\n",
      "Params: tensor([2.2525, 0.0556])\n",
      "Grad:  tensor([-3.3729,  2.4468])\n",
      "Epoch 543 Loss 29.418007\n",
      "Params: tensor([2.2528, 0.0553])\n",
      "Grad:  tensor([-3.3556,  2.4498])\n",
      "Epoch 544 Loss 29.416283\n",
      "Params: tensor([2.2531, 0.0551])\n",
      "Grad:  tensor([-3.3384,  2.4528])\n",
      "Epoch 545 Loss 29.414564\n",
      "Params: tensor([2.2535, 0.0548])\n",
      "Grad:  tensor([-3.3214,  2.4557])\n",
      "Epoch 546 Loss 29.412867\n",
      "Params: tensor([2.2538, 0.0546])\n",
      "Grad:  tensor([-3.3044,  2.4587])\n",
      "Epoch 547 Loss 29.411171\n",
      "Params: tensor([2.2541, 0.0544])\n",
      "Grad:  tensor([-3.2875,  2.4616])\n",
      "Epoch 548 Loss 29.409487\n",
      "Params: tensor([2.2545, 0.0541])\n",
      "Grad:  tensor([-3.2707,  2.4645])\n",
      "Epoch 549 Loss 29.407812\n",
      "Params: tensor([2.2548, 0.0539])\n",
      "Grad:  tensor([-3.2540,  2.4674])\n",
      "Epoch 550 Loss 29.406147\n",
      "Params: tensor([2.2551, 0.0536])\n",
      "Grad:  tensor([-3.2374,  2.4703])\n",
      "Epoch 551 Loss 29.404493\n",
      "Params: tensor([2.2554, 0.0534])\n",
      "Grad:  tensor([-3.2209,  2.4731])\n",
      "Epoch 552 Loss 29.402843\n",
      "Params: tensor([2.2558, 0.0531])\n",
      "Grad:  tensor([-3.2045,  2.4760])\n",
      "Epoch 553 Loss 29.401203\n",
      "Params: tensor([2.2561, 0.0529])\n",
      "Grad:  tensor([-3.1883,  2.4788])\n",
      "Epoch 554 Loss 29.399580\n",
      "Params: tensor([2.2564, 0.0526])\n",
      "Grad:  tensor([-3.1721,  2.4816])\n",
      "Epoch 555 Loss 29.397955\n",
      "Params: tensor([2.2567, 0.0524])\n",
      "Grad:  tensor([-3.1560,  2.4844])\n",
      "Epoch 556 Loss 29.396343\n",
      "Params: tensor([2.2570, 0.0521])\n",
      "Grad:  tensor([-3.1400,  2.4872])\n",
      "Epoch 557 Loss 29.394747\n",
      "Params: tensor([2.2574, 0.0519])\n",
      "Grad:  tensor([-3.1241,  2.4899])\n",
      "Epoch 558 Loss 29.393147\n",
      "Params: tensor([2.2577, 0.0516])\n",
      "Grad:  tensor([-3.1083,  2.4927])\n",
      "Epoch 559 Loss 29.391565\n",
      "Params: tensor([2.2580, 0.0514])\n",
      "Grad:  tensor([-3.0926,  2.4954])\n",
      "Epoch 560 Loss 29.389990\n",
      "Params: tensor([2.2583, 0.0511])\n",
      "Grad:  tensor([-3.0770,  2.4981])\n",
      "Epoch 561 Loss 29.388416\n",
      "Params: tensor([2.2586, 0.0509])\n",
      "Grad:  tensor([-3.0615,  2.5008])\n",
      "Epoch 562 Loss 29.386860\n",
      "Params: tensor([2.2589, 0.0506])\n",
      "Grad:  tensor([-3.0461,  2.5035])\n",
      "Epoch 563 Loss 29.385298\n",
      "Params: tensor([2.2592, 0.0504])\n",
      "Grad:  tensor([-3.0308,  2.5061])\n",
      "Epoch 564 Loss 29.383760\n",
      "Params: tensor([2.2595, 0.0501])\n",
      "Grad:  tensor([-3.0155,  2.5088])\n",
      "Epoch 565 Loss 29.382225\n",
      "Params: tensor([2.2598, 0.0499])\n",
      "Grad:  tensor([-3.0004,  2.5114])\n",
      "Epoch 566 Loss 29.380693\n",
      "Params: tensor([2.2601, 0.0496])\n",
      "Grad:  tensor([-2.9854,  2.5140])\n",
      "Epoch 567 Loss 29.379179\n",
      "Params: tensor([2.2604, 0.0494])\n",
      "Grad:  tensor([-2.9704,  2.5166])\n",
      "Epoch 568 Loss 29.377663\n",
      "Params: tensor([2.2607, 0.0491])\n",
      "Grad:  tensor([-2.9555,  2.5192])\n",
      "Epoch 569 Loss 29.376152\n",
      "Params: tensor([2.2610, 0.0489])\n",
      "Grad:  tensor([-2.9408,  2.5217])\n",
      "Epoch 570 Loss 29.374657\n",
      "Params: tensor([2.2613, 0.0486])\n",
      "Grad:  tensor([-2.9261,  2.5243])\n",
      "Epoch 571 Loss 29.373163\n",
      "Params: tensor([2.2616, 0.0484])\n",
      "Grad:  tensor([-2.9115,  2.5268])\n",
      "Epoch 572 Loss 29.371681\n",
      "Params: tensor([2.2619, 0.0481])\n",
      "Grad:  tensor([-2.8970,  2.5293])\n",
      "Epoch 573 Loss 29.370203\n",
      "Params: tensor([2.2622, 0.0479])\n",
      "Grad:  tensor([-2.8826,  2.5318])\n",
      "Epoch 574 Loss 29.368731\n",
      "Params: tensor([2.2625, 0.0476])\n",
      "Grad:  tensor([-2.8682,  2.5343])\n",
      "Epoch 575 Loss 29.367268\n",
      "Params: tensor([2.2627, 0.0474])\n",
      "Grad:  tensor([-2.8540,  2.5367])\n",
      "Epoch 576 Loss 29.365812\n",
      "Params: tensor([2.2630, 0.0471])\n",
      "Grad:  tensor([-2.8398,  2.5392])\n",
      "Epoch 577 Loss 29.364361\n",
      "Params: tensor([2.2633, 0.0469])\n",
      "Grad:  tensor([-2.8258,  2.5416])\n",
      "Epoch 578 Loss 29.362921\n",
      "Params: tensor([2.2636, 0.0466])\n",
      "Grad:  tensor([-2.8118,  2.5440])\n",
      "Epoch 579 Loss 29.361486\n",
      "Params: tensor([2.2639, 0.0463])\n",
      "Grad:  tensor([-2.7979,  2.5464])\n",
      "Epoch 580 Loss 29.360058\n",
      "Params: tensor([2.2642, 0.0461])\n",
      "Grad:  tensor([-2.7841,  2.5488])\n",
      "Epoch 581 Loss 29.358631\n",
      "Params: tensor([2.2644, 0.0458])\n",
      "Grad:  tensor([-2.7703,  2.5512])\n",
      "Epoch 582 Loss 29.357216\n",
      "Params: tensor([2.2647, 0.0456])\n",
      "Grad:  tensor([-2.7567,  2.5536])\n",
      "Epoch 583 Loss 29.355803\n",
      "Params: tensor([2.2650, 0.0453])\n",
      "Grad:  tensor([-2.7431,  2.5559])\n",
      "Epoch 584 Loss 29.354395\n",
      "Params: tensor([2.2653, 0.0451])\n",
      "Grad:  tensor([-2.7296,  2.5582])\n",
      "Epoch 585 Loss 29.353003\n",
      "Params: tensor([2.2655, 0.0448])\n",
      "Grad:  tensor([-2.7162,  2.5606])\n",
      "Epoch 586 Loss 29.351612\n",
      "Params: tensor([2.2658, 0.0446])\n",
      "Grad:  tensor([-2.7029,  2.5629])\n",
      "Epoch 587 Loss 29.350225\n",
      "Params: tensor([2.2661, 0.0443])\n",
      "Grad:  tensor([-2.6896,  2.5651])\n",
      "Epoch 588 Loss 29.348848\n",
      "Params: tensor([2.2663, 0.0440])\n",
      "Grad:  tensor([-2.6765,  2.5674])\n",
      "Epoch 589 Loss 29.347467\n",
      "Params: tensor([2.2666, 0.0438])\n",
      "Grad:  tensor([-2.6634,  2.5697])\n",
      "Epoch 590 Loss 29.346102\n",
      "Params: tensor([2.2669, 0.0435])\n",
      "Grad:  tensor([-2.6504,  2.5719])\n",
      "Epoch 591 Loss 29.344738\n",
      "Params: tensor([2.2671, 0.0433])\n",
      "Grad:  tensor([-2.6375,  2.5742])\n",
      "Epoch 592 Loss 29.343382\n",
      "Params: tensor([2.2674, 0.0430])\n",
      "Grad:  tensor([-2.6246,  2.5764])\n",
      "Epoch 593 Loss 29.342028\n",
      "Params: tensor([2.2677, 0.0428])\n",
      "Grad:  tensor([-2.6119,  2.5786])\n",
      "Epoch 594 Loss 29.340685\n",
      "Params: tensor([2.2679, 0.0425])\n",
      "Grad:  tensor([-2.5992,  2.5808])\n",
      "Epoch 595 Loss 29.339338\n",
      "Params: tensor([2.2682, 0.0422])\n",
      "Grad:  tensor([-2.5866,  2.5829])\n",
      "Epoch 596 Loss 29.338007\n",
      "Params: tensor([2.2684, 0.0420])\n",
      "Grad:  tensor([-2.5740,  2.5851])\n",
      "Epoch 597 Loss 29.336676\n",
      "Params: tensor([2.2687, 0.0417])\n",
      "Grad:  tensor([-2.5616,  2.5872])\n",
      "Epoch 598 Loss 29.335356\n",
      "Params: tensor([2.2690, 0.0415])\n",
      "Grad:  tensor([-2.5492,  2.5894])\n",
      "Epoch 599 Loss 29.334040\n",
      "Params: tensor([2.2692, 0.0412])\n",
      "Grad:  tensor([-2.5369,  2.5915])\n",
      "Epoch 600 Loss 29.332726\n",
      "Params: tensor([2.2695, 0.0409])\n",
      "Grad:  tensor([-2.5246,  2.5936])\n",
      "Epoch 601 Loss 29.331415\n",
      "Params: tensor([2.2697, 0.0407])\n",
      "Grad:  tensor([-2.5125,  2.5957])\n",
      "Epoch 602 Loss 29.330109\n",
      "Params: tensor([2.2700, 0.0404])\n",
      "Grad:  tensor([-2.5004,  2.5978])\n",
      "Epoch 603 Loss 29.328808\n",
      "Params: tensor([2.2702, 0.0402])\n",
      "Grad:  tensor([-2.4884,  2.5999])\n",
      "Epoch 604 Loss 29.327520\n",
      "Params: tensor([2.2705, 0.0399])\n",
      "Grad:  tensor([-2.4764,  2.6019])\n",
      "Epoch 605 Loss 29.326227\n",
      "Params: tensor([2.2707, 0.0396])\n",
      "Grad:  tensor([-2.4645,  2.6040])\n",
      "Epoch 606 Loss 29.324940\n",
      "Params: tensor([2.2710, 0.0394])\n",
      "Grad:  tensor([-2.4527,  2.6060])\n",
      "Epoch 607 Loss 29.323669\n",
      "Params: tensor([2.2712, 0.0391])\n",
      "Grad:  tensor([-2.4410,  2.6080])\n",
      "Epoch 608 Loss 29.322395\n",
      "Params: tensor([2.2715, 0.0389])\n",
      "Grad:  tensor([-2.4294,  2.6100])\n",
      "Epoch 609 Loss 29.321114\n",
      "Params: tensor([2.2717, 0.0386])\n",
      "Grad:  tensor([-2.4178,  2.6120])\n",
      "Epoch 610 Loss 29.319853\n",
      "Params: tensor([2.2719, 0.0383])\n",
      "Grad:  tensor([-2.4063,  2.6140])\n",
      "Epoch 611 Loss 29.318590\n",
      "Params: tensor([2.2722, 0.0381])\n",
      "Grad:  tensor([-2.3948,  2.6160])\n",
      "Epoch 612 Loss 29.317339\n",
      "Params: tensor([2.2724, 0.0378])\n",
      "Grad:  tensor([-2.3834,  2.6179])\n",
      "Epoch 613 Loss 29.316084\n",
      "Params: tensor([2.2727, 0.0376])\n",
      "Grad:  tensor([-2.3721,  2.6199])\n",
      "Epoch 614 Loss 29.314833\n",
      "Params: tensor([2.2729, 0.0373])\n",
      "Grad:  tensor([-2.3609,  2.6218])\n",
      "Epoch 615 Loss 29.313593\n",
      "Params: tensor([2.2731, 0.0370])\n",
      "Grad:  tensor([-2.3497,  2.6237])\n",
      "Epoch 616 Loss 29.312349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([2.2734, 0.0368])\n",
      "Grad:  tensor([-2.3386,  2.6257])\n",
      "Epoch 617 Loss 29.311115\n",
      "Params: tensor([2.2736, 0.0365])\n",
      "Grad:  tensor([-2.3276,  2.6275])\n",
      "Epoch 618 Loss 29.309893\n",
      "Params: tensor([2.2738, 0.0362])\n",
      "Grad:  tensor([-2.3166,  2.6294])\n",
      "Epoch 619 Loss 29.308657\n",
      "Params: tensor([2.2741, 0.0360])\n",
      "Grad:  tensor([-2.3057,  2.6313])\n",
      "Epoch 620 Loss 29.307436\n",
      "Params: tensor([2.2743, 0.0357])\n",
      "Grad:  tensor([-2.2949,  2.6332])\n",
      "Epoch 621 Loss 29.306215\n",
      "Params: tensor([2.2745, 0.0355])\n",
      "Grad:  tensor([-2.2841,  2.6350])\n",
      "Epoch 622 Loss 29.305000\n",
      "Params: tensor([2.2748, 0.0352])\n",
      "Grad:  tensor([-2.2734,  2.6369])\n",
      "Epoch 623 Loss 29.303789\n",
      "Params: tensor([2.2750, 0.0349])\n",
      "Grad:  tensor([-2.2628,  2.6387])\n",
      "Epoch 624 Loss 29.302582\n",
      "Params: tensor([2.2752, 0.0347])\n",
      "Grad:  tensor([-2.2522,  2.6405])\n",
      "Epoch 625 Loss 29.301378\n",
      "Params: tensor([2.2754, 0.0344])\n",
      "Grad:  tensor([-2.2417,  2.6423])\n",
      "Epoch 626 Loss 29.300177\n",
      "Params: tensor([2.2757, 0.0341])\n",
      "Grad:  tensor([-2.2312,  2.6441])\n",
      "Epoch 627 Loss 29.298983\n",
      "Params: tensor([2.2759, 0.0339])\n",
      "Grad:  tensor([-2.2208,  2.6459])\n",
      "Epoch 628 Loss 29.297792\n",
      "Params: tensor([2.2761, 0.0336])\n",
      "Grad:  tensor([-2.2105,  2.6477])\n",
      "Epoch 629 Loss 29.296600\n",
      "Params: tensor([2.2763, 0.0333])\n",
      "Grad:  tensor([-2.2003,  2.6494])\n",
      "Epoch 630 Loss 29.295418\n",
      "Params: tensor([2.2765, 0.0331])\n",
      "Grad:  tensor([-2.1901,  2.6512])\n",
      "Epoch 631 Loss 29.294233\n",
      "Params: tensor([2.2768, 0.0328])\n",
      "Grad:  tensor([-2.1799,  2.6529])\n",
      "Epoch 632 Loss 29.293053\n",
      "Params: tensor([2.2770, 0.0325])\n",
      "Grad:  tensor([-2.1699,  2.6546])\n",
      "Epoch 633 Loss 29.291878\n",
      "Params: tensor([2.2772, 0.0323])\n",
      "Grad:  tensor([-2.1598,  2.6564])\n",
      "Epoch 634 Loss 29.290709\n",
      "Params: tensor([2.2774, 0.0320])\n",
      "Grad:  tensor([-2.1499,  2.6581])\n",
      "Epoch 635 Loss 29.289543\n",
      "Params: tensor([2.2776, 0.0318])\n",
      "Grad:  tensor([-2.1400,  2.6598])\n",
      "Epoch 636 Loss 29.288374\n",
      "Params: tensor([2.2778, 0.0315])\n",
      "Grad:  tensor([-2.1302,  2.6614])\n",
      "Epoch 637 Loss 29.287214\n",
      "Params: tensor([2.2781, 0.0312])\n",
      "Grad:  tensor([-2.1204,  2.6631])\n",
      "Epoch 638 Loss 29.286064\n",
      "Params: tensor([2.2783, 0.0310])\n",
      "Grad:  tensor([-2.1107,  2.6648])\n",
      "Epoch 639 Loss 29.284901\n",
      "Params: tensor([2.2785, 0.0307])\n",
      "Grad:  tensor([-2.1010,  2.6664])\n",
      "Epoch 640 Loss 29.283751\n",
      "Params: tensor([2.2787, 0.0304])\n",
      "Grad:  tensor([-2.0914,  2.6681])\n",
      "Epoch 641 Loss 29.282600\n",
      "Params: tensor([2.2789, 0.0302])\n",
      "Grad:  tensor([-2.0819,  2.6697])\n",
      "Epoch 642 Loss 29.281464\n",
      "Params: tensor([2.2791, 0.0299])\n",
      "Grad:  tensor([-2.0724,  2.6713])\n",
      "Epoch 643 Loss 29.280312\n",
      "Params: tensor([2.2793, 0.0296])\n",
      "Grad:  tensor([-2.0630,  2.6729])\n",
      "Epoch 644 Loss 29.279175\n",
      "Params: tensor([2.2795, 0.0294])\n",
      "Grad:  tensor([-2.0537,  2.6745])\n",
      "Epoch 645 Loss 29.278044\n",
      "Params: tensor([2.2797, 0.0291])\n",
      "Grad:  tensor([-2.0444,  2.6761])\n",
      "Epoch 646 Loss 29.276911\n",
      "Params: tensor([2.2799, 0.0288])\n",
      "Grad:  tensor([-2.0351,  2.6777])\n",
      "Epoch 647 Loss 29.275780\n",
      "Params: tensor([2.2801, 0.0285])\n",
      "Grad:  tensor([-2.0259,  2.6793])\n",
      "Epoch 648 Loss 29.274651\n",
      "Params: tensor([2.2803, 0.0283])\n",
      "Grad:  tensor([-2.0168,  2.6809])\n",
      "Epoch 649 Loss 29.273527\n",
      "Params: tensor([2.2805, 0.0280])\n",
      "Grad:  tensor([-2.0077,  2.6824])\n",
      "Epoch 650 Loss 29.272406\n",
      "Params: tensor([2.2807, 0.0277])\n",
      "Grad:  tensor([-1.9987,  2.6840])\n",
      "Epoch 651 Loss 29.271284\n",
      "Params: tensor([2.2809, 0.0275])\n",
      "Grad:  tensor([-1.9897,  2.6855])\n",
      "Epoch 652 Loss 29.270164\n",
      "Params: tensor([2.2811, 0.0272])\n",
      "Grad:  tensor([-1.9808,  2.6870])\n",
      "Epoch 653 Loss 29.269056\n",
      "Params: tensor([2.2813, 0.0269])\n",
      "Grad:  tensor([-1.9719,  2.6885])\n",
      "Epoch 654 Loss 29.267942\n",
      "Params: tensor([2.2815, 0.0267])\n",
      "Grad:  tensor([-1.9631,  2.6900])\n",
      "Epoch 655 Loss 29.266832\n",
      "Params: tensor([2.2817, 0.0264])\n",
      "Grad:  tensor([-1.9543,  2.6915])\n",
      "Epoch 656 Loss 29.265724\n",
      "Params: tensor([2.2819, 0.0261])\n",
      "Grad:  tensor([-1.9456,  2.6930])\n",
      "Epoch 657 Loss 29.264629\n",
      "Params: tensor([2.2821, 0.0259])\n",
      "Grad:  tensor([-1.9370,  2.6945])\n",
      "Epoch 658 Loss 29.263525\n",
      "Params: tensor([2.2823, 0.0256])\n",
      "Grad:  tensor([-1.9284,  2.6960])\n",
      "Epoch 659 Loss 29.262426\n",
      "Params: tensor([2.2825, 0.0253])\n",
      "Grad:  tensor([-1.9198,  2.6974])\n",
      "Epoch 660 Loss 29.261335\n",
      "Params: tensor([2.2827, 0.0251])\n",
      "Grad:  tensor([-1.9113,  2.6989])\n",
      "Epoch 661 Loss 29.260237\n",
      "Params: tensor([2.2829, 0.0248])\n",
      "Grad:  tensor([-1.9029,  2.7003])\n",
      "Epoch 662 Loss 29.259153\n",
      "Params: tensor([2.2831, 0.0245])\n",
      "Grad:  tensor([-1.8945,  2.7017])\n",
      "Epoch 663 Loss 29.258059\n",
      "Params: tensor([2.2833, 0.0242])\n",
      "Grad:  tensor([-1.8861,  2.7032])\n",
      "Epoch 664 Loss 29.256975\n",
      "Params: tensor([2.2835, 0.0240])\n",
      "Grad:  tensor([-1.8778,  2.7046])\n",
      "Epoch 665 Loss 29.255892\n",
      "Params: tensor([2.2836, 0.0237])\n",
      "Grad:  tensor([-1.8696,  2.7060])\n",
      "Epoch 666 Loss 29.254808\n",
      "Params: tensor([2.2838, 0.0234])\n",
      "Grad:  tensor([-1.8614,  2.7074])\n",
      "Epoch 667 Loss 29.253729\n",
      "Params: tensor([2.2840, 0.0232])\n",
      "Grad:  tensor([-1.8532,  2.7088])\n",
      "Epoch 668 Loss 29.252655\n",
      "Params: tensor([2.2842, 0.0229])\n",
      "Grad:  tensor([-1.8452,  2.7101])\n",
      "Epoch 669 Loss 29.251581\n",
      "Params: tensor([2.2844, 0.0226])\n",
      "Grad:  tensor([-1.8371,  2.7115])\n",
      "Epoch 670 Loss 29.250511\n",
      "Params: tensor([2.2846, 0.0223])\n",
      "Grad:  tensor([-1.8291,  2.7129])\n",
      "Epoch 671 Loss 29.249437\n",
      "Params: tensor([2.2848, 0.0221])\n",
      "Grad:  tensor([-1.8212,  2.7142])\n",
      "Epoch 672 Loss 29.248365\n",
      "Params: tensor([2.2849, 0.0218])\n",
      "Grad:  tensor([-1.8133,  2.7156])\n",
      "Epoch 673 Loss 29.247303\n",
      "Params: tensor([2.2851, 0.0215])\n",
      "Grad:  tensor([-1.8054,  2.7169])\n",
      "Epoch 674 Loss 29.246235\n",
      "Params: tensor([2.2853, 0.0213])\n",
      "Grad:  tensor([-1.7976,  2.7182])\n",
      "Epoch 675 Loss 29.245178\n",
      "Params: tensor([2.2855, 0.0210])\n",
      "Grad:  tensor([-1.7898,  2.7196])\n",
      "Epoch 676 Loss 29.244118\n",
      "Params: tensor([2.2857, 0.0207])\n",
      "Grad:  tensor([-1.7821,  2.7209])\n",
      "Epoch 677 Loss 29.243061\n",
      "Params: tensor([2.2858, 0.0204])\n",
      "Grad:  tensor([-1.7745,  2.7222])\n",
      "Epoch 678 Loss 29.242010\n",
      "Params: tensor([2.2860, 0.0202])\n",
      "Grad:  tensor([-1.7669,  2.7235])\n",
      "Epoch 679 Loss 29.240953\n",
      "Params: tensor([2.2862, 0.0199])\n",
      "Grad:  tensor([-1.7593,  2.7247])\n",
      "Epoch 680 Loss 29.239899\n",
      "Params: tensor([2.2864, 0.0196])\n",
      "Grad:  tensor([-1.7518,  2.7260])\n",
      "Epoch 681 Loss 29.238853\n",
      "Params: tensor([2.2865, 0.0194])\n",
      "Grad:  tensor([-1.7443,  2.7273])\n",
      "Epoch 682 Loss 29.237804\n",
      "Params: tensor([2.2867, 0.0191])\n",
      "Grad:  tensor([-1.7369,  2.7286])\n",
      "Epoch 683 Loss 29.236757\n",
      "Params: tensor([2.2869, 0.0188])\n",
      "Grad:  tensor([-1.7295,  2.7298])\n",
      "Epoch 684 Loss 29.235716\n",
      "Params: tensor([2.2871, 0.0185])\n",
      "Grad:  tensor([-1.7221,  2.7310])\n",
      "Epoch 685 Loss 29.234678\n",
      "Params: tensor([2.2872, 0.0183])\n",
      "Grad:  tensor([-1.7148,  2.7323])\n",
      "Epoch 686 Loss 29.233629\n",
      "Params: tensor([2.2874, 0.0180])\n",
      "Grad:  tensor([-1.7076,  2.7335])\n",
      "Epoch 687 Loss 29.232599\n",
      "Params: tensor([2.2876, 0.0177])\n",
      "Grad:  tensor([-1.7004,  2.7347])\n",
      "Epoch 688 Loss 29.231560\n",
      "Params: tensor([2.2877, 0.0174])\n",
      "Grad:  tensor([-1.6932,  2.7360])\n",
      "Epoch 689 Loss 29.230524\n",
      "Params: tensor([2.2879, 0.0172])\n",
      "Grad:  tensor([-1.6861,  2.7372])\n",
      "Epoch 690 Loss 29.229490\n",
      "Params: tensor([2.2881, 0.0169])\n",
      "Grad:  tensor([-1.6790,  2.7384])\n",
      "Epoch 691 Loss 29.228462\n",
      "Params: tensor([2.2883, 0.0166])\n",
      "Grad:  tensor([-1.6720,  2.7395])\n",
      "Epoch 692 Loss 29.227430\n",
      "Params: tensor([2.2884, 0.0164])\n",
      "Grad:  tensor([-1.6650,  2.7407])\n",
      "Epoch 693 Loss 29.226398\n",
      "Params: tensor([2.2886, 0.0161])\n",
      "Grad:  tensor([-1.6580,  2.7419])\n",
      "Epoch 694 Loss 29.225378\n",
      "Params: tensor([2.2888, 0.0158])\n",
      "Grad:  tensor([-1.6511,  2.7431])\n",
      "Epoch 695 Loss 29.224352\n",
      "Params: tensor([2.2889, 0.0155])\n",
      "Grad:  tensor([-1.6443,  2.7442])\n",
      "Epoch 696 Loss 29.223326\n",
      "Params: tensor([2.2891, 0.0153])\n",
      "Grad:  tensor([-1.6374,  2.7454])\n",
      "Epoch 697 Loss 29.222307\n",
      "Params: tensor([2.2892, 0.0150])\n",
      "Grad:  tensor([-1.6306,  2.7465])\n",
      "Epoch 698 Loss 29.221289\n",
      "Params: tensor([2.2894, 0.0147])\n",
      "Grad:  tensor([-1.6239,  2.7477])\n",
      "Epoch 699 Loss 29.220268\n",
      "Params: tensor([2.2896, 0.0144])\n",
      "Grad:  tensor([-1.6172,  2.7488])\n",
      "Epoch 700 Loss 29.219250\n",
      "Params: tensor([2.2897, 0.0142])\n",
      "Grad:  tensor([-1.6105,  2.7499])\n",
      "Epoch 701 Loss 29.218233\n",
      "Params: tensor([2.2899, 0.0139])\n",
      "Grad:  tensor([-1.6039,  2.7511])\n",
      "Epoch 702 Loss 29.217218\n",
      "Params: tensor([2.2901, 0.0136])\n",
      "Grad:  tensor([-1.5973,  2.7522])\n",
      "Epoch 703 Loss 29.216209\n",
      "Params: tensor([2.2902, 0.0133])\n",
      "Grad:  tensor([-1.5908,  2.7533])\n",
      "Epoch 704 Loss 29.215199\n",
      "Params: tensor([2.2904, 0.0131])\n",
      "Grad:  tensor([-1.5843,  2.7544])\n",
      "Epoch 705 Loss 29.214190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([2.2905, 0.0128])\n",
      "Grad:  tensor([-1.5778,  2.7555])\n",
      "Epoch 706 Loss 29.213182\n",
      "Params: tensor([2.2907, 0.0125])\n",
      "Grad:  tensor([-1.5714,  2.7565])\n",
      "Epoch 707 Loss 29.212172\n",
      "Params: tensor([2.2908, 0.0122])\n",
      "Grad:  tensor([-1.5650,  2.7576])\n",
      "Epoch 708 Loss 29.211170\n",
      "Params: tensor([2.2910, 0.0120])\n",
      "Grad:  tensor([-1.5586,  2.7587])\n",
      "Epoch 709 Loss 29.210167\n",
      "Params: tensor([2.2912, 0.0117])\n",
      "Grad:  tensor([-1.5523,  2.7598])\n",
      "Epoch 710 Loss 29.209167\n",
      "Params: tensor([2.2913, 0.0114])\n",
      "Grad:  tensor([-1.5461,  2.7608])\n",
      "Epoch 711 Loss 29.208168\n",
      "Params: tensor([2.2915, 0.0111])\n",
      "Grad:  tensor([-1.5398,  2.7619])\n",
      "Epoch 712 Loss 29.207161\n",
      "Params: tensor([2.2916, 0.0108])\n",
      "Grad:  tensor([-1.5336,  2.7629])\n",
      "Epoch 713 Loss 29.206169\n",
      "Params: tensor([2.2918, 0.0106])\n",
      "Grad:  tensor([-1.5275,  2.7639])\n",
      "Epoch 714 Loss 29.205172\n",
      "Params: tensor([2.2919, 0.0103])\n",
      "Grad:  tensor([-1.5214,  2.7650])\n",
      "Epoch 715 Loss 29.204174\n",
      "Params: tensor([2.2921, 0.0100])\n",
      "Grad:  tensor([-1.5153,  2.7660])\n",
      "Epoch 716 Loss 29.203178\n",
      "Params: tensor([2.2922, 0.0097])\n",
      "Grad:  tensor([-1.5092,  2.7670])\n",
      "Epoch 717 Loss 29.202187\n",
      "Params: tensor([2.2924, 0.0095])\n",
      "Grad:  tensor([-1.5032,  2.7680])\n",
      "Epoch 718 Loss 29.201191\n",
      "Params: tensor([2.2925, 0.0092])\n",
      "Grad:  tensor([-1.4972,  2.7690])\n",
      "Epoch 719 Loss 29.200203\n",
      "Params: tensor([2.2927, 0.0089])\n",
      "Grad:  tensor([-1.4913,  2.7700])\n",
      "Epoch 720 Loss 29.199219\n",
      "Params: tensor([2.2928, 0.0086])\n",
      "Grad:  tensor([-1.4854,  2.7710])\n",
      "Epoch 721 Loss 29.198229\n",
      "Params: tensor([2.2930, 0.0084])\n",
      "Grad:  tensor([-1.4795,  2.7720])\n",
      "Epoch 722 Loss 29.197241\n",
      "Params: tensor([2.2931, 0.0081])\n",
      "Grad:  tensor([-1.4737,  2.7730])\n",
      "Epoch 723 Loss 29.196253\n",
      "Params: tensor([2.2933, 0.0078])\n",
      "Grad:  tensor([-1.4679,  2.7739])\n",
      "Epoch 724 Loss 29.195271\n",
      "Params: tensor([2.2934, 0.0075])\n",
      "Grad:  tensor([-1.4621,  2.7749])\n",
      "Epoch 725 Loss 29.194283\n",
      "Params: tensor([2.2936, 0.0072])\n",
      "Grad:  tensor([-1.4564,  2.7759])\n",
      "Epoch 726 Loss 29.193310\n",
      "Params: tensor([2.2937, 0.0070])\n",
      "Grad:  tensor([-1.4507,  2.7768])\n",
      "Epoch 727 Loss 29.192322\n",
      "Params: tensor([2.2939, 0.0067])\n",
      "Grad:  tensor([-1.4451,  2.7778])\n",
      "Epoch 728 Loss 29.191345\n",
      "Params: tensor([2.2940, 0.0064])\n",
      "Grad:  tensor([-1.4395,  2.7787])\n",
      "Epoch 729 Loss 29.190363\n",
      "Params: tensor([2.2941, 0.0061])\n",
      "Grad:  tensor([-1.4339,  2.7797])\n",
      "Epoch 730 Loss 29.189381\n",
      "Params: tensor([2.2943, 0.0059])\n",
      "Grad:  tensor([-1.4283,  2.7806])\n",
      "Epoch 731 Loss 29.188408\n",
      "Params: tensor([2.2944, 0.0056])\n",
      "Grad:  tensor([-1.4228,  2.7815])\n",
      "Epoch 732 Loss 29.187433\n",
      "Params: tensor([2.2946, 0.0053])\n",
      "Grad:  tensor([-1.4173,  2.7824])\n",
      "Epoch 733 Loss 29.186460\n",
      "Params: tensor([2.2947, 0.0050])\n",
      "Grad:  tensor([-1.4119,  2.7833])\n",
      "Epoch 734 Loss 29.185484\n",
      "Params: tensor([2.2949, 0.0047])\n",
      "Grad:  tensor([-1.4064,  2.7842])\n",
      "Epoch 735 Loss 29.184515\n",
      "Params: tensor([2.2950, 0.0045])\n",
      "Grad:  tensor([-1.4011,  2.7851])\n",
      "Epoch 736 Loss 29.183538\n",
      "Params: tensor([2.2951, 0.0042])\n",
      "Grad:  tensor([-1.3957,  2.7860])\n",
      "Epoch 737 Loss 29.182568\n",
      "Params: tensor([2.2953, 0.0039])\n",
      "Grad:  tensor([-1.3904,  2.7869])\n",
      "Epoch 738 Loss 29.181599\n",
      "Params: tensor([2.2954, 0.0036])\n",
      "Grad:  tensor([-1.3851,  2.7878])\n",
      "Epoch 739 Loss 29.180634\n",
      "Params: tensor([2.2956, 0.0034])\n",
      "Grad:  tensor([-1.3798,  2.7887])\n",
      "Epoch 740 Loss 29.179659\n",
      "Params: tensor([2.2957, 0.0031])\n",
      "Grad:  tensor([-1.3746,  2.7896])\n",
      "Epoch 741 Loss 29.178698\n",
      "Params: tensor([2.2958, 0.0028])\n",
      "Grad:  tensor([-1.3694,  2.7904])\n",
      "Epoch 742 Loss 29.177727\n",
      "Params: tensor([2.2960, 0.0025])\n",
      "Grad:  tensor([-1.3642,  2.7913])\n",
      "Epoch 743 Loss 29.176760\n",
      "Params: tensor([2.2961e+00, 2.2369e-03])\n",
      "Grad:  tensor([-1.3591,  2.7921])\n",
      "Epoch 744 Loss 29.175804\n",
      "Params: tensor([2.2962e+00, 1.9577e-03])\n",
      "Grad:  tensor([-1.3540,  2.7930])\n",
      "Epoch 745 Loss 29.174835\n",
      "Params: tensor([2.2964e+00, 1.6784e-03])\n",
      "Grad:  tensor([-1.3489,  2.7938])\n",
      "Epoch 746 Loss 29.173878\n",
      "Params: tensor([2.2965e+00, 1.3990e-03])\n",
      "Grad:  tensor([-1.3439,  2.7947])\n",
      "Epoch 747 Loss 29.172918\n",
      "Params: tensor([2.2966e+00, 1.1196e-03])\n",
      "Grad:  tensor([-1.3389,  2.7955])\n",
      "Epoch 748 Loss 29.171953\n",
      "Params: tensor([2.2968e+00, 8.4001e-04])\n",
      "Grad:  tensor([-1.3339,  2.7963])\n",
      "Epoch 749 Loss 29.170992\n",
      "Params: tensor([2.2969e+00, 5.6038e-04])\n",
      "Grad:  tensor([-1.3290,  2.7972])\n",
      "Epoch 750 Loss 29.170036\n",
      "Params: tensor([2.2970e+00, 2.8066e-04])\n",
      "Grad:  tensor([-1.3240,  2.7980])\n",
      "Epoch 751 Loss 29.169075\n",
      "Params: tensor([2.2972e+00, 8.6310e-07])\n",
      "Grad:  tensor([-1.3192,  2.7988])\n",
      "Epoch 752 Loss 29.168119\n",
      "Params: tensor([ 2.2973e+00, -2.7902e-04])\n",
      "Grad:  tensor([-1.3143,  2.7996])\n",
      "Epoch 753 Loss 29.167162\n",
      "Params: tensor([ 2.2974e+00, -5.5897e-04])\n",
      "Grad:  tensor([-1.3095,  2.8004])\n",
      "Epoch 754 Loss 29.166204\n",
      "Params: tensor([ 2.2976e+00, -8.3901e-04])\n",
      "Grad:  tensor([-1.3047,  2.8012])\n",
      "Epoch 755 Loss 29.165253\n",
      "Params: tensor([ 2.2977e+00, -1.1191e-03])\n",
      "Grad:  tensor([-1.2999,  2.8020])\n",
      "Epoch 756 Loss 29.164301\n",
      "Params: tensor([ 2.2978e+00, -1.3993e-03])\n",
      "Grad:  tensor([-1.2952,  2.8028])\n",
      "Epoch 757 Loss 29.163347\n",
      "Params: tensor([ 2.2980e+00, -1.6796e-03])\n",
      "Grad:  tensor([-1.2905,  2.8035])\n",
      "Epoch 758 Loss 29.162392\n",
      "Params: tensor([ 2.2981e+00, -1.9600e-03])\n",
      "Grad:  tensor([-1.2858,  2.8043])\n",
      "Epoch 759 Loss 29.161446\n",
      "Params: tensor([ 2.2982e+00, -2.2404e-03])\n",
      "Grad:  tensor([-1.2811,  2.8051])\n",
      "Epoch 760 Loss 29.160492\n",
      "Params: tensor([ 2.2983, -0.0025])\n",
      "Grad:  tensor([-1.2765,  2.8059])\n",
      "Epoch 761 Loss 29.159540\n",
      "Params: tensor([ 2.2985, -0.0028])\n",
      "Grad:  tensor([-1.2719,  2.8066])\n",
      "Epoch 762 Loss 29.158592\n",
      "Params: tensor([ 2.2986, -0.0031])\n",
      "Grad:  tensor([-1.2673,  2.8074])\n",
      "Epoch 763 Loss 29.157642\n",
      "Params: tensor([ 2.2987, -0.0034])\n",
      "Grad:  tensor([-1.2628,  2.8081])\n",
      "Epoch 764 Loss 29.156694\n",
      "Params: tensor([ 2.2989, -0.0036])\n",
      "Grad:  tensor([-1.2583,  2.8089])\n",
      "Epoch 765 Loss 29.155750\n",
      "Params: tensor([ 2.2990, -0.0039])\n",
      "Grad:  tensor([-1.2538,  2.8096])\n",
      "Epoch 766 Loss 29.154799\n",
      "Params: tensor([ 2.2991, -0.0042])\n",
      "Grad:  tensor([-1.2493,  2.8103])\n",
      "Epoch 767 Loss 29.153852\n",
      "Params: tensor([ 2.2992, -0.0045])\n",
      "Grad:  tensor([-1.2449,  2.8111])\n",
      "Epoch 768 Loss 29.152912\n",
      "Params: tensor([ 2.2994, -0.0048])\n",
      "Grad:  tensor([-1.2405,  2.8118])\n",
      "Epoch 769 Loss 29.151966\n",
      "Params: tensor([ 2.2995, -0.0050])\n",
      "Grad:  tensor([-1.2361,  2.8125])\n",
      "Epoch 770 Loss 29.151026\n",
      "Params: tensor([ 2.2996, -0.0053])\n",
      "Grad:  tensor([-1.2317,  2.8132])\n",
      "Epoch 771 Loss 29.150084\n",
      "Params: tensor([ 2.2997, -0.0056])\n",
      "Grad:  tensor([-1.2274,  2.8140])\n",
      "Epoch 772 Loss 29.149139\n",
      "Params: tensor([ 2.2999, -0.0059])\n",
      "Grad:  tensor([-1.2231,  2.8147])\n",
      "Epoch 773 Loss 29.148195\n",
      "Params: tensor([ 2.3000, -0.0062])\n",
      "Grad:  tensor([-1.2188,  2.8154])\n",
      "Epoch 774 Loss 29.147259\n",
      "Params: tensor([ 2.3001, -0.0065])\n",
      "Grad:  tensor([-1.2146,  2.8161])\n",
      "Epoch 775 Loss 29.146313\n",
      "Params: tensor([ 2.3002, -0.0067])\n",
      "Grad:  tensor([-1.2103,  2.8168])\n",
      "Epoch 776 Loss 29.145378\n",
      "Params: tensor([ 2.3003, -0.0070])\n",
      "Grad:  tensor([-1.2061,  2.8175])\n",
      "Epoch 777 Loss 29.144434\n",
      "Params: tensor([ 2.3005, -0.0073])\n",
      "Grad:  tensor([-1.2020,  2.8181])\n",
      "Epoch 778 Loss 29.143501\n",
      "Params: tensor([ 2.3006, -0.0076])\n",
      "Grad:  tensor([-1.1978,  2.8188])\n",
      "Epoch 779 Loss 29.142565\n",
      "Params: tensor([ 2.3007, -0.0079])\n",
      "Grad:  tensor([-1.1937,  2.8195])\n",
      "Epoch 780 Loss 29.141621\n",
      "Params: tensor([ 2.3008, -0.0081])\n",
      "Grad:  tensor([-1.1896,  2.8202])\n",
      "Epoch 781 Loss 29.140686\n",
      "Params: tensor([ 2.3009, -0.0084])\n",
      "Grad:  tensor([-1.1855,  2.8208])\n",
      "Epoch 782 Loss 29.139751\n",
      "Params: tensor([ 2.3011, -0.0087])\n",
      "Grad:  tensor([-1.1815,  2.8215])\n",
      "Epoch 783 Loss 29.138817\n",
      "Params: tensor([ 2.3012, -0.0090])\n",
      "Grad:  tensor([-1.1774,  2.8222])\n",
      "Epoch 784 Loss 29.137880\n",
      "Params: tensor([ 2.3013, -0.0093])\n",
      "Grad:  tensor([-1.1734,  2.8228])\n",
      "Epoch 785 Loss 29.136946\n",
      "Params: tensor([ 2.3014, -0.0096])\n",
      "Grad:  tensor([-1.1695,  2.8235])\n",
      "Epoch 786 Loss 29.136015\n",
      "Params: tensor([ 2.3015, -0.0098])\n",
      "Grad:  tensor([-1.1655,  2.8241])\n",
      "Epoch 787 Loss 29.135077\n",
      "Params: tensor([ 2.3016, -0.0101])\n",
      "Grad:  tensor([-1.1616,  2.8248])\n",
      "Epoch 788 Loss 29.134142\n",
      "Params: tensor([ 2.3018, -0.0104])\n",
      "Grad:  tensor([-1.1577,  2.8254])\n",
      "Epoch 789 Loss 29.133215\n",
      "Params: tensor([ 2.3019, -0.0107])\n",
      "Grad:  tensor([-1.1538,  2.8260])\n",
      "Epoch 790 Loss 29.132275\n",
      "Params: tensor([ 2.3020, -0.0110])\n",
      "Grad:  tensor([-1.1499,  2.8267])\n",
      "Epoch 791 Loss 29.131353\n",
      "Params: tensor([ 2.3021, -0.0113])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor([-1.1461,  2.8273])\n",
      "Epoch 792 Loss 29.130421\n",
      "Params: tensor([ 2.3022, -0.0115])\n",
      "Grad:  tensor([-1.1423,  2.8279])\n",
      "Epoch 793 Loss 29.129488\n",
      "Params: tensor([ 2.3023, -0.0118])\n",
      "Grad:  tensor([-1.1385,  2.8285])\n",
      "Epoch 794 Loss 29.128559\n",
      "Params: tensor([ 2.3024, -0.0121])\n",
      "Grad:  tensor([-1.1347,  2.8292])\n",
      "Epoch 795 Loss 29.127630\n",
      "Params: tensor([ 2.3026, -0.0124])\n",
      "Grad:  tensor([-1.1310,  2.8298])\n",
      "Epoch 796 Loss 29.126701\n",
      "Params: tensor([ 2.3027, -0.0127])\n",
      "Grad:  tensor([-1.1273,  2.8304])\n",
      "Epoch 797 Loss 29.125774\n",
      "Params: tensor([ 2.3028, -0.0129])\n",
      "Grad:  tensor([-1.1236,  2.8310])\n",
      "Epoch 798 Loss 29.124849\n",
      "Params: tensor([ 2.3029, -0.0132])\n",
      "Grad:  tensor([-1.1199,  2.8316])\n",
      "Epoch 799 Loss 29.123924\n",
      "Params: tensor([ 2.3030, -0.0135])\n",
      "Grad:  tensor([-1.1162,  2.8322])\n",
      "Epoch 800 Loss 29.122992\n",
      "Params: tensor([ 2.3031, -0.0138])\n",
      "Grad:  tensor([-1.1126,  2.8328])\n",
      "Epoch 801 Loss 29.122065\n",
      "Params: tensor([ 2.3032, -0.0141])\n",
      "Grad:  tensor([-1.1090,  2.8333])\n",
      "Epoch 802 Loss 29.121143\n",
      "Params: tensor([ 2.3033, -0.0144])\n",
      "Grad:  tensor([-1.1054,  2.8339])\n",
      "Epoch 803 Loss 29.120214\n",
      "Params: tensor([ 2.3035, -0.0146])\n",
      "Grad:  tensor([-1.1018,  2.8345])\n",
      "Epoch 804 Loss 29.119291\n",
      "Params: tensor([ 2.3036, -0.0149])\n",
      "Grad:  tensor([-1.0983,  2.8351])\n",
      "Epoch 805 Loss 29.118366\n",
      "Params: tensor([ 2.3037, -0.0152])\n",
      "Grad:  tensor([-1.0948,  2.8356])\n",
      "Epoch 806 Loss 29.117437\n",
      "Params: tensor([ 2.3038, -0.0155])\n",
      "Grad:  tensor([-1.0913,  2.8362])\n",
      "Epoch 807 Loss 29.116522\n",
      "Params: tensor([ 2.3039, -0.0158])\n",
      "Grad:  tensor([-1.0878,  2.8368])\n",
      "Epoch 808 Loss 29.115601\n",
      "Params: tensor([ 2.3040, -0.0161])\n",
      "Grad:  tensor([-1.0843,  2.8373])\n",
      "Epoch 809 Loss 29.114672\n",
      "Params: tensor([ 2.3041, -0.0164])\n",
      "Grad:  tensor([-1.0809,  2.8379])\n",
      "Epoch 810 Loss 29.113750\n",
      "Params: tensor([ 2.3042, -0.0166])\n",
      "Grad:  tensor([-1.0775,  2.8384])\n",
      "Epoch 811 Loss 29.112831\n",
      "Params: tensor([ 2.3043, -0.0169])\n",
      "Grad:  tensor([-1.0741,  2.8390])\n",
      "Epoch 812 Loss 29.111910\n",
      "Params: tensor([ 2.3044, -0.0172])\n",
      "Grad:  tensor([-1.0707,  2.8395])\n",
      "Epoch 813 Loss 29.110987\n",
      "Params: tensor([ 2.3045, -0.0175])\n",
      "Grad:  tensor([-1.0673,  2.8401])\n",
      "Epoch 814 Loss 29.110069\n",
      "Params: tensor([ 2.3046, -0.0178])\n",
      "Grad:  tensor([-1.0640,  2.8406])\n",
      "Epoch 815 Loss 29.109150\n",
      "Params: tensor([ 2.3048, -0.0181])\n",
      "Grad:  tensor([-1.0607,  2.8411])\n",
      "Epoch 816 Loss 29.108229\n",
      "Params: tensor([ 2.3049, -0.0183])\n",
      "Grad:  tensor([-1.0574,  2.8417])\n",
      "Epoch 817 Loss 29.107313\n",
      "Params: tensor([ 2.3050, -0.0186])\n",
      "Grad:  tensor([-1.0541,  2.8422])\n",
      "Epoch 818 Loss 29.106392\n",
      "Params: tensor([ 2.3051, -0.0189])\n",
      "Grad:  tensor([-1.0509,  2.8427])\n",
      "Epoch 819 Loss 29.105471\n",
      "Params: tensor([ 2.3052, -0.0192])\n",
      "Grad:  tensor([-1.0476,  2.8433])\n",
      "Epoch 820 Loss 29.104555\n",
      "Params: tensor([ 2.3053, -0.0195])\n",
      "Grad:  tensor([-1.0444,  2.8438])\n",
      "Epoch 821 Loss 29.103636\n",
      "Params: tensor([ 2.3054, -0.0198])\n",
      "Grad:  tensor([-1.0412,  2.8443])\n",
      "Epoch 822 Loss 29.102720\n",
      "Params: tensor([ 2.3055, -0.0200])\n",
      "Grad:  tensor([-1.0380,  2.8448])\n",
      "Epoch 823 Loss 29.101801\n",
      "Params: tensor([ 2.3056, -0.0203])\n",
      "Grad:  tensor([-1.0349,  2.8453])\n",
      "Epoch 824 Loss 29.100885\n",
      "Params: tensor([ 2.3057, -0.0206])\n",
      "Grad:  tensor([-1.0317,  2.8458])\n",
      "Epoch 825 Loss 29.099968\n",
      "Params: tensor([ 2.3058, -0.0209])\n",
      "Grad:  tensor([-1.0286,  2.8463])\n",
      "Epoch 826 Loss 29.099054\n",
      "Params: tensor([ 2.3059, -0.0212])\n",
      "Grad:  tensor([-1.0255,  2.8468])\n",
      "Epoch 827 Loss 29.098133\n",
      "Params: tensor([ 2.3060, -0.0215])\n",
      "Grad:  tensor([-1.0224,  2.8473])\n",
      "Epoch 828 Loss 29.097227\n",
      "Params: tensor([ 2.3061, -0.0218])\n",
      "Grad:  tensor([-1.0194,  2.8478])\n",
      "Epoch 829 Loss 29.096308\n",
      "Params: tensor([ 2.3062, -0.0220])\n",
      "Grad:  tensor([-1.0163,  2.8483])\n",
      "Epoch 830 Loss 29.095396\n",
      "Params: tensor([ 2.3063, -0.0223])\n",
      "Grad:  tensor([-1.0133,  2.8487])\n",
      "Epoch 831 Loss 29.094482\n",
      "Params: tensor([ 2.3064, -0.0226])\n",
      "Grad:  tensor([-1.0103,  2.8492])\n",
      "Epoch 832 Loss 29.093567\n",
      "Params: tensor([ 2.3065, -0.0229])\n",
      "Grad:  tensor([-1.0073,  2.8497])\n",
      "Epoch 833 Loss 29.092649\n",
      "Params: tensor([ 2.3066, -0.0232])\n",
      "Grad:  tensor([-1.0043,  2.8502])\n",
      "Epoch 834 Loss 29.091738\n",
      "Params: tensor([ 2.3067, -0.0235])\n",
      "Grad:  tensor([-1.0014,  2.8506])\n",
      "Epoch 835 Loss 29.090826\n",
      "Params: tensor([ 2.3068, -0.0237])\n",
      "Grad:  tensor([-0.9985,  2.8511])\n",
      "Epoch 836 Loss 29.089916\n",
      "Params: tensor([ 2.3069, -0.0240])\n",
      "Grad:  tensor([-0.9955,  2.8516])\n",
      "Epoch 837 Loss 29.089001\n",
      "Params: tensor([ 2.3070, -0.0243])\n",
      "Grad:  tensor([-0.9926,  2.8520])\n",
      "Epoch 838 Loss 29.088091\n",
      "Params: tensor([ 2.3071, -0.0246])\n",
      "Grad:  tensor([-0.9897,  2.8525])\n",
      "Epoch 839 Loss 29.087181\n",
      "Params: tensor([ 2.3072, -0.0249])\n",
      "Grad:  tensor([-0.9869,  2.8530])\n",
      "Epoch 840 Loss 29.086262\n",
      "Params: tensor([ 2.3073, -0.0252])\n",
      "Grad:  tensor([-0.9840,  2.8534])\n",
      "Epoch 841 Loss 29.085358\n",
      "Params: tensor([ 2.3074, -0.0255])\n",
      "Grad:  tensor([-0.9812,  2.8539])\n",
      "Epoch 842 Loss 29.084448\n",
      "Params: tensor([ 2.3075, -0.0257])\n",
      "Grad:  tensor([-0.9784,  2.8543])\n",
      "Epoch 843 Loss 29.083534\n",
      "Params: tensor([ 2.3076, -0.0260])\n",
      "Grad:  tensor([-0.9756,  2.8547])\n",
      "Epoch 844 Loss 29.082628\n",
      "Params: tensor([ 2.3077, -0.0263])\n",
      "Grad:  tensor([-0.9728,  2.8552])\n",
      "Epoch 845 Loss 29.081715\n",
      "Params: tensor([ 2.3078, -0.0266])\n",
      "Grad:  tensor([-0.9700,  2.8556])\n",
      "Epoch 846 Loss 29.080805\n",
      "Params: tensor([ 2.3079, -0.0269])\n",
      "Grad:  tensor([-0.9673,  2.8561])\n",
      "Epoch 847 Loss 29.079893\n",
      "Params: tensor([ 2.3080, -0.0272])\n",
      "Grad:  tensor([-0.9646,  2.8565])\n",
      "Epoch 848 Loss 29.078985\n",
      "Params: tensor([ 2.3081, -0.0275])\n",
      "Grad:  tensor([-0.9618,  2.8569])\n",
      "Epoch 849 Loss 29.078081\n",
      "Params: tensor([ 2.3082, -0.0277])\n",
      "Grad:  tensor([-0.9591,  2.8573])\n",
      "Epoch 850 Loss 29.077173\n",
      "Params: tensor([ 2.3083, -0.0280])\n",
      "Grad:  tensor([-0.9565,  2.8578])\n",
      "Epoch 851 Loss 29.076262\n",
      "Params: tensor([ 2.3084, -0.0283])\n",
      "Grad:  tensor([-0.9538,  2.8582])\n",
      "Epoch 852 Loss 29.075350\n",
      "Params: tensor([ 2.3085, -0.0286])\n",
      "Grad:  tensor([-0.9512,  2.8586])\n",
      "Epoch 853 Loss 29.074446\n",
      "Params: tensor([ 2.3086, -0.0289])\n",
      "Grad:  tensor([-0.9485,  2.8590])\n",
      "Epoch 854 Loss 29.073542\n",
      "Params: tensor([ 2.3087, -0.0292])\n",
      "Grad:  tensor([-0.9459,  2.8594])\n",
      "Epoch 855 Loss 29.072632\n",
      "Params: tensor([ 2.3088, -0.0295])\n",
      "Grad:  tensor([-0.9433,  2.8598])\n",
      "Epoch 856 Loss 29.071724\n",
      "Params: tensor([ 2.3089, -0.0297])\n",
      "Grad:  tensor([-0.9407,  2.8602])\n",
      "Epoch 857 Loss 29.070820\n",
      "Params: tensor([ 2.3089, -0.0300])\n",
      "Grad:  tensor([-0.9381,  2.8606])\n",
      "Epoch 858 Loss 29.069914\n",
      "Params: tensor([ 2.3090, -0.0303])\n",
      "Grad:  tensor([-0.9356,  2.8610])\n",
      "Epoch 859 Loss 29.069008\n",
      "Params: tensor([ 2.3091, -0.0306])\n",
      "Grad:  tensor([-0.9331,  2.8614])\n",
      "Epoch 860 Loss 29.068098\n",
      "Params: tensor([ 2.3092, -0.0309])\n",
      "Grad:  tensor([-0.9305,  2.8618])\n",
      "Epoch 861 Loss 29.067200\n",
      "Params: tensor([ 2.3093, -0.0312])\n",
      "Grad:  tensor([-0.9280,  2.8622])\n",
      "Epoch 862 Loss 29.066292\n",
      "Params: tensor([ 2.3094, -0.0315])\n",
      "Grad:  tensor([-0.9255,  2.8626])\n",
      "Epoch 863 Loss 29.065388\n",
      "Params: tensor([ 2.3095, -0.0317])\n",
      "Grad:  tensor([-0.9231,  2.8630])\n",
      "Epoch 864 Loss 29.064482\n",
      "Params: tensor([ 2.3096, -0.0320])\n",
      "Grad:  tensor([-0.9206,  2.8634])\n",
      "Epoch 865 Loss 29.063574\n",
      "Params: tensor([ 2.3097, -0.0323])\n",
      "Grad:  tensor([-0.9182,  2.8638])\n",
      "Epoch 866 Loss 29.062670\n",
      "Params: tensor([ 2.3098, -0.0326])\n",
      "Grad:  tensor([-0.9157,  2.8641])\n",
      "Epoch 867 Loss 29.061766\n",
      "Params: tensor([ 2.3099, -0.0329])\n",
      "Grad:  tensor([-0.9133,  2.8645])\n",
      "Epoch 868 Loss 29.060863\n",
      "Params: tensor([ 2.3100, -0.0332])\n",
      "Grad:  tensor([-0.9109,  2.8649])\n",
      "Epoch 869 Loss 29.059961\n",
      "Params: tensor([ 2.3101, -0.0335])\n",
      "Grad:  tensor([-0.9085,  2.8653])\n",
      "Epoch 870 Loss 29.059059\n",
      "Params: tensor([ 2.3101, -0.0338])\n",
      "Grad:  tensor([-0.9061,  2.8656])\n",
      "Epoch 871 Loss 29.058155\n",
      "Params: tensor([ 2.3102, -0.0340])\n",
      "Grad:  tensor([-0.9038,  2.8660])\n",
      "Epoch 872 Loss 29.057253\n",
      "Params: tensor([ 2.3103, -0.0343])\n",
      "Grad:  tensor([-0.9014,  2.8663])\n",
      "Epoch 873 Loss 29.056347\n",
      "Params: tensor([ 2.3104, -0.0346])\n",
      "Grad:  tensor([-0.8991,  2.8667])\n",
      "Epoch 874 Loss 29.055445\n",
      "Params: tensor([ 2.3105, -0.0349])\n",
      "Grad:  tensor([-0.8968,  2.8671])\n",
      "Epoch 875 Loss 29.054548\n",
      "Params: tensor([ 2.3106, -0.0352])\n",
      "Grad:  tensor([-0.8945,  2.8674])\n",
      "Epoch 876 Loss 29.053638\n",
      "Params: tensor([ 2.3107, -0.0355])\n",
      "Grad:  tensor([-0.8922,  2.8678])\n",
      "Epoch 877 Loss 29.052736\n",
      "Params: tensor([ 2.3108, -0.0358])\n",
      "Grad:  tensor([-0.8899,  2.8681])\n",
      "Epoch 878 Loss 29.051836\n",
      "Params: tensor([ 2.3109, -0.0360])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor([-0.8877,  2.8685])\n",
      "Epoch 879 Loss 29.050934\n",
      "Params: tensor([ 2.3110, -0.0363])\n",
      "Grad:  tensor([-0.8854,  2.8688])\n",
      "Epoch 880 Loss 29.050035\n",
      "Params: tensor([ 2.3110, -0.0366])\n",
      "Grad:  tensor([-0.8832,  2.8692])\n",
      "Epoch 881 Loss 29.049135\n",
      "Params: tensor([ 2.3111, -0.0369])\n",
      "Grad:  tensor([-0.8810,  2.8695])\n",
      "Epoch 882 Loss 29.048231\n",
      "Params: tensor([ 2.3112, -0.0372])\n",
      "Grad:  tensor([-0.8788,  2.8698])\n",
      "Epoch 883 Loss 29.047333\n",
      "Params: tensor([ 2.3113, -0.0375])\n",
      "Grad:  tensor([-0.8766,  2.8702])\n",
      "Epoch 884 Loss 29.046429\n",
      "Params: tensor([ 2.3114, -0.0378])\n",
      "Grad:  tensor([-0.8744,  2.8705])\n",
      "Epoch 885 Loss 29.045532\n",
      "Params: tensor([ 2.3115, -0.0381])\n",
      "Grad:  tensor([-0.8722,  2.8708])\n",
      "Epoch 886 Loss 29.044630\n",
      "Params: tensor([ 2.3116, -0.0383])\n",
      "Grad:  tensor([-0.8700,  2.8712])\n",
      "Epoch 887 Loss 29.043730\n",
      "Params: tensor([ 2.3117, -0.0386])\n",
      "Grad:  tensor([-0.8679,  2.8715])\n",
      "Epoch 888 Loss 29.042824\n",
      "Params: tensor([ 2.3117, -0.0389])\n",
      "Grad:  tensor([-0.8658,  2.8718])\n",
      "Epoch 889 Loss 29.041931\n",
      "Params: tensor([ 2.3118, -0.0392])\n",
      "Grad:  tensor([-0.8637,  2.8721])\n",
      "Epoch 890 Loss 29.041029\n",
      "Params: tensor([ 2.3119, -0.0395])\n",
      "Grad:  tensor([-0.8616,  2.8725])\n",
      "Epoch 891 Loss 29.040133\n",
      "Params: tensor([ 2.3120, -0.0398])\n",
      "Grad:  tensor([-0.8595,  2.8728])\n",
      "Epoch 892 Loss 29.039232\n",
      "Params: tensor([ 2.3121, -0.0401])\n",
      "Grad:  tensor([-0.8574,  2.8731])\n",
      "Epoch 893 Loss 29.038332\n",
      "Params: tensor([ 2.3122, -0.0404])\n",
      "Grad:  tensor([-0.8553,  2.8734])\n",
      "Epoch 894 Loss 29.037434\n",
      "Params: tensor([ 2.3123, -0.0406])\n",
      "Grad:  tensor([-0.8533,  2.8737])\n",
      "Epoch 895 Loss 29.036541\n",
      "Params: tensor([ 2.3123, -0.0409])\n",
      "Grad:  tensor([-0.8512,  2.8740])\n",
      "Epoch 896 Loss 29.035633\n",
      "Params: tensor([ 2.3124, -0.0412])\n",
      "Grad:  tensor([-0.8492,  2.8743])\n",
      "Epoch 897 Loss 29.034737\n",
      "Params: tensor([ 2.3125, -0.0415])\n",
      "Grad:  tensor([-0.8472,  2.8746])\n",
      "Epoch 898 Loss 29.033844\n",
      "Params: tensor([ 2.3126, -0.0418])\n",
      "Grad:  tensor([-0.8452,  2.8750])\n",
      "Epoch 899 Loss 29.032942\n",
      "Params: tensor([ 2.3127, -0.0421])\n",
      "Grad:  tensor([-0.8432,  2.8753])\n",
      "Epoch 900 Loss 29.032045\n",
      "Params: tensor([ 2.3128, -0.0424])\n",
      "Grad:  tensor([-0.8412,  2.8756])\n",
      "Epoch 901 Loss 29.031147\n",
      "Params: tensor([ 2.3129, -0.0427])\n",
      "Grad:  tensor([-0.8392,  2.8758])\n",
      "Epoch 902 Loss 29.030251\n",
      "Params: tensor([ 2.3129, -0.0429])\n",
      "Grad:  tensor([-0.8373,  2.8761])\n",
      "Epoch 903 Loss 29.029356\n",
      "Params: tensor([ 2.3130, -0.0432])\n",
      "Grad:  tensor([-0.8353,  2.8764])\n",
      "Epoch 904 Loss 29.028456\n",
      "Params: tensor([ 2.3131, -0.0435])\n",
      "Grad:  tensor([-0.8334,  2.8767])\n",
      "Epoch 905 Loss 29.027559\n",
      "Params: tensor([ 2.3132, -0.0438])\n",
      "Grad:  tensor([-0.8315,  2.8770])\n",
      "Epoch 906 Loss 29.026667\n",
      "Params: tensor([ 2.3133, -0.0441])\n",
      "Grad:  tensor([-0.8296,  2.8773])\n",
      "Epoch 907 Loss 29.025764\n",
      "Params: tensor([ 2.3134, -0.0444])\n",
      "Grad:  tensor([-0.8277,  2.8776])\n",
      "Epoch 908 Loss 29.024866\n",
      "Params: tensor([ 2.3134, -0.0447])\n",
      "Grad:  tensor([-0.8258,  2.8779])\n",
      "Epoch 909 Loss 29.023975\n",
      "Params: tensor([ 2.3135, -0.0450])\n",
      "Grad:  tensor([-0.8239,  2.8781])\n",
      "Epoch 910 Loss 29.023071\n",
      "Params: tensor([ 2.3136, -0.0452])\n",
      "Grad:  tensor([-0.8220,  2.8784])\n",
      "Epoch 911 Loss 29.022179\n",
      "Params: tensor([ 2.3137, -0.0455])\n",
      "Grad:  tensor([-0.8202,  2.8787])\n",
      "Epoch 912 Loss 29.021278\n",
      "Params: tensor([ 2.3138, -0.0458])\n",
      "Grad:  tensor([-0.8183,  2.8790])\n",
      "Epoch 913 Loss 29.020388\n",
      "Params: tensor([ 2.3138, -0.0461])\n",
      "Grad:  tensor([-0.8165,  2.8792])\n",
      "Epoch 914 Loss 29.019493\n",
      "Params: tensor([ 2.3139, -0.0464])\n",
      "Grad:  tensor([-0.8147,  2.8795])\n",
      "Epoch 915 Loss 29.018599\n",
      "Params: tensor([ 2.3140, -0.0467])\n",
      "Grad:  tensor([-0.8129,  2.8798])\n",
      "Epoch 916 Loss 29.017706\n",
      "Params: tensor([ 2.3141, -0.0470])\n",
      "Grad:  tensor([-0.8111,  2.8800])\n",
      "Epoch 917 Loss 29.016809\n",
      "Params: tensor([ 2.3142, -0.0473])\n",
      "Grad:  tensor([-0.8093,  2.8803])\n",
      "Epoch 918 Loss 29.015913\n",
      "Params: tensor([ 2.3143, -0.0475])\n",
      "Grad:  tensor([-0.8075,  2.8806])\n",
      "Epoch 919 Loss 29.015017\n",
      "Params: tensor([ 2.3143, -0.0478])\n",
      "Grad:  tensor([-0.8058,  2.8808])\n",
      "Epoch 920 Loss 29.014124\n",
      "Params: tensor([ 2.3144, -0.0481])\n",
      "Grad:  tensor([-0.8040,  2.8811])\n",
      "Epoch 921 Loss 29.013231\n",
      "Params: tensor([ 2.3145, -0.0484])\n",
      "Grad:  tensor([-0.8023,  2.8813])\n",
      "Epoch 922 Loss 29.012331\n",
      "Params: tensor([ 2.3146, -0.0487])\n",
      "Grad:  tensor([-0.8005,  2.8816])\n",
      "Epoch 923 Loss 29.011438\n",
      "Params: tensor([ 2.3147, -0.0490])\n",
      "Grad:  tensor([-0.7988,  2.8819])\n",
      "Epoch 924 Loss 29.010551\n",
      "Params: tensor([ 2.3147, -0.0493])\n",
      "Grad:  tensor([-0.7971,  2.8821])\n",
      "Epoch 925 Loss 29.009649\n",
      "Params: tensor([ 2.3148, -0.0496])\n",
      "Grad:  tensor([-0.7954,  2.8824])\n",
      "Epoch 926 Loss 29.008759\n",
      "Params: tensor([ 2.3149, -0.0498])\n",
      "Grad:  tensor([-0.7937,  2.8826])\n",
      "Epoch 927 Loss 29.007862\n",
      "Params: tensor([ 2.3150, -0.0501])\n",
      "Grad:  tensor([-0.7920,  2.8829])\n",
      "Epoch 928 Loss 29.006966\n",
      "Params: tensor([ 2.3151, -0.0504])\n",
      "Grad:  tensor([-0.7903,  2.8831])\n",
      "Epoch 929 Loss 29.006075\n",
      "Params: tensor([ 2.3151, -0.0507])\n",
      "Grad:  tensor([-0.7887,  2.8833])\n",
      "Epoch 930 Loss 29.005180\n",
      "Params: tensor([ 2.3152, -0.0510])\n",
      "Grad:  tensor([-0.7870,  2.8836])\n",
      "Epoch 931 Loss 29.004290\n",
      "Params: tensor([ 2.3153, -0.0513])\n",
      "Grad:  tensor([-0.7854,  2.8838])\n",
      "Epoch 932 Loss 29.003399\n",
      "Params: tensor([ 2.3154, -0.0516])\n",
      "Grad:  tensor([-0.7838,  2.8841])\n",
      "Epoch 933 Loss 29.002501\n",
      "Params: tensor([ 2.3154, -0.0519])\n",
      "Grad:  tensor([-0.7821,  2.8843])\n",
      "Epoch 934 Loss 29.001610\n",
      "Params: tensor([ 2.3155, -0.0522])\n",
      "Grad:  tensor([-0.7805,  2.8845])\n",
      "Epoch 935 Loss 29.000713\n",
      "Params: tensor([ 2.3156, -0.0524])\n",
      "Grad:  tensor([-0.7789,  2.8848])\n",
      "Epoch 936 Loss 28.999828\n",
      "Params: tensor([ 2.3157, -0.0527])\n",
      "Grad:  tensor([-0.7773,  2.8850])\n",
      "Epoch 937 Loss 28.998930\n",
      "Params: tensor([ 2.3158, -0.0530])\n",
      "Grad:  tensor([-0.7758,  2.8852])\n",
      "Epoch 938 Loss 28.998041\n",
      "Params: tensor([ 2.3158, -0.0533])\n",
      "Grad:  tensor([-0.7742,  2.8854])\n",
      "Epoch 939 Loss 28.997143\n",
      "Params: tensor([ 2.3159, -0.0536])\n",
      "Grad:  tensor([-0.7726,  2.8857])\n",
      "Epoch 940 Loss 28.996254\n",
      "Params: tensor([ 2.3160, -0.0539])\n",
      "Grad:  tensor([-0.7711,  2.8859])\n",
      "Epoch 941 Loss 28.995361\n",
      "Params: tensor([ 2.3161, -0.0542])\n",
      "Grad:  tensor([-0.7695,  2.8861])\n",
      "Epoch 942 Loss 28.994465\n",
      "Params: tensor([ 2.3161, -0.0545])\n",
      "Grad:  tensor([-0.7680,  2.8863])\n",
      "Epoch 943 Loss 28.993584\n",
      "Params: tensor([ 2.3162, -0.0548])\n",
      "Grad:  tensor([-0.7665,  2.8865])\n",
      "Epoch 944 Loss 28.992683\n",
      "Params: tensor([ 2.3163, -0.0550])\n",
      "Grad:  tensor([-0.7650,  2.8868])\n",
      "Epoch 945 Loss 28.991791\n",
      "Params: tensor([ 2.3164, -0.0553])\n",
      "Grad:  tensor([-0.7634,  2.8870])\n",
      "Epoch 946 Loss 28.990904\n",
      "Params: tensor([ 2.3165, -0.0556])\n",
      "Grad:  tensor([-0.7619,  2.8872])\n",
      "Epoch 947 Loss 28.990009\n",
      "Params: tensor([ 2.3165, -0.0559])\n",
      "Grad:  tensor([-0.7605,  2.8874])\n",
      "Epoch 948 Loss 28.989113\n",
      "Params: tensor([ 2.3166, -0.0562])\n",
      "Grad:  tensor([-0.7590,  2.8876])\n",
      "Epoch 949 Loss 28.988226\n",
      "Params: tensor([ 2.3167, -0.0565])\n",
      "Grad:  tensor([-0.7575,  2.8878])\n",
      "Epoch 950 Loss 28.987337\n",
      "Params: tensor([ 2.3168, -0.0568])\n",
      "Grad:  tensor([-0.7560,  2.8880])\n",
      "Epoch 951 Loss 28.986444\n",
      "Params: tensor([ 2.3168, -0.0571])\n",
      "Grad:  tensor([-0.7546,  2.8882])\n",
      "Epoch 952 Loss 28.985554\n",
      "Params: tensor([ 2.3169, -0.0574])\n",
      "Grad:  tensor([-0.7531,  2.8884])\n",
      "Epoch 953 Loss 28.984663\n",
      "Params: tensor([ 2.3170, -0.0576])\n",
      "Grad:  tensor([-0.7517,  2.8886])\n",
      "Epoch 954 Loss 28.983772\n",
      "Params: tensor([ 2.3171, -0.0579])\n",
      "Grad:  tensor([-0.7503,  2.8888])\n",
      "Epoch 955 Loss 28.982878\n",
      "Params: tensor([ 2.3171, -0.0582])\n",
      "Grad:  tensor([-0.7489,  2.8890])\n",
      "Epoch 956 Loss 28.981989\n",
      "Params: tensor([ 2.3172, -0.0585])\n",
      "Grad:  tensor([-0.7474,  2.8892])\n",
      "Epoch 957 Loss 28.981096\n",
      "Params: tensor([ 2.3173, -0.0588])\n",
      "Grad:  tensor([-0.7460,  2.8894])\n",
      "Epoch 958 Loss 28.980207\n",
      "Params: tensor([ 2.3174, -0.0591])\n",
      "Grad:  tensor([-0.7446,  2.8896])\n",
      "Epoch 959 Loss 28.979317\n",
      "Params: tensor([ 2.3174, -0.0594])\n",
      "Grad:  tensor([-0.7433,  2.8898])\n",
      "Epoch 960 Loss 28.978430\n",
      "Params: tensor([ 2.3175, -0.0597])\n",
      "Grad:  tensor([-0.7419,  2.8900])\n",
      "Epoch 961 Loss 28.977533\n",
      "Params: tensor([ 2.3176, -0.0600])\n",
      "Grad:  tensor([-0.7405,  2.8902])\n",
      "Epoch 962 Loss 28.976646\n",
      "Params: tensor([ 2.3177, -0.0602])\n",
      "Grad:  tensor([-0.7391,  2.8904])\n",
      "Epoch 963 Loss 28.975756\n",
      "Params: tensor([ 2.3177, -0.0605])\n",
      "Grad:  tensor([-0.7378,  2.8906])\n",
      "Epoch 964 Loss 28.974865\n",
      "Params: tensor([ 2.3178, -0.0608])\n",
      "Grad:  tensor([-0.7364,  2.8908])\n",
      "Epoch 965 Loss 28.973982\n",
      "Params: tensor([ 2.3179, -0.0611])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor([-0.7351,  2.8910])\n",
      "Epoch 966 Loss 28.973089\n",
      "Params: tensor([ 2.3179, -0.0614])\n",
      "Grad:  tensor([-0.7338,  2.8911])\n",
      "Epoch 967 Loss 28.972197\n",
      "Params: tensor([ 2.3180, -0.0617])\n",
      "Grad:  tensor([-0.7325,  2.8913])\n",
      "Epoch 968 Loss 28.971306\n",
      "Params: tensor([ 2.3181, -0.0620])\n",
      "Grad:  tensor([-0.7312,  2.8915])\n",
      "Epoch 969 Loss 28.970421\n",
      "Params: tensor([ 2.3182, -0.0623])\n",
      "Grad:  tensor([-0.7298,  2.8917])\n",
      "Epoch 970 Loss 28.969530\n",
      "Params: tensor([ 2.3182, -0.0626])\n",
      "Grad:  tensor([-0.7286,  2.8919])\n",
      "Epoch 971 Loss 28.968641\n",
      "Params: tensor([ 2.3183, -0.0628])\n",
      "Grad:  tensor([-0.7273,  2.8920])\n",
      "Epoch 972 Loss 28.967754\n",
      "Params: tensor([ 2.3184, -0.0631])\n",
      "Grad:  tensor([-0.7260,  2.8922])\n",
      "Epoch 973 Loss 28.966864\n",
      "Params: tensor([ 2.3185, -0.0634])\n",
      "Grad:  tensor([-0.7247,  2.8924])\n",
      "Epoch 974 Loss 28.965975\n",
      "Params: tensor([ 2.3185, -0.0637])\n",
      "Grad:  tensor([-0.7234,  2.8926])\n",
      "Epoch 975 Loss 28.965086\n",
      "Params: tensor([ 2.3186, -0.0640])\n",
      "Grad:  tensor([-0.7222,  2.8927])\n",
      "Epoch 976 Loss 28.964191\n",
      "Params: tensor([ 2.3187, -0.0643])\n",
      "Grad:  tensor([-0.7209,  2.8929])\n",
      "Epoch 977 Loss 28.963305\n",
      "Params: tensor([ 2.3187, -0.0646])\n",
      "Grad:  tensor([-0.7197,  2.8931])\n",
      "Epoch 978 Loss 28.962421\n",
      "Params: tensor([ 2.3188, -0.0649])\n",
      "Grad:  tensor([-0.7185,  2.8932])\n",
      "Epoch 979 Loss 28.961525\n",
      "Params: tensor([ 2.3189, -0.0652])\n",
      "Grad:  tensor([-0.7172,  2.8934])\n",
      "Epoch 980 Loss 28.960644\n",
      "Params: tensor([ 2.3190, -0.0654])\n",
      "Grad:  tensor([-0.7160,  2.8936])\n",
      "Epoch 981 Loss 28.959753\n",
      "Params: tensor([ 2.3190, -0.0657])\n",
      "Grad:  tensor([-0.7148,  2.8937])\n",
      "Epoch 982 Loss 28.958862\n",
      "Params: tensor([ 2.3191, -0.0660])\n",
      "Grad:  tensor([-0.7136,  2.8939])\n",
      "Epoch 983 Loss 28.957975\n",
      "Params: tensor([ 2.3192, -0.0663])\n",
      "Grad:  tensor([-0.7124,  2.8940])\n",
      "Epoch 984 Loss 28.957087\n",
      "Params: tensor([ 2.3192, -0.0666])\n",
      "Grad:  tensor([-0.7112,  2.8942])\n",
      "Epoch 985 Loss 28.956194\n",
      "Params: tensor([ 2.3193, -0.0669])\n",
      "Grad:  tensor([-0.7100,  2.8944])\n",
      "Epoch 986 Loss 28.955309\n",
      "Params: tensor([ 2.3194, -0.0672])\n",
      "Grad:  tensor([-0.7088,  2.8945])\n",
      "Epoch 987 Loss 28.954424\n",
      "Params: tensor([ 2.3195, -0.0675])\n",
      "Grad:  tensor([-0.7077,  2.8947])\n",
      "Epoch 988 Loss 28.953535\n",
      "Params: tensor([ 2.3195, -0.0678])\n",
      "Grad:  tensor([-0.7065,  2.8948])\n",
      "Epoch 989 Loss 28.952648\n",
      "Params: tensor([ 2.3196, -0.0680])\n",
      "Grad:  tensor([-0.7054,  2.8950])\n",
      "Epoch 990 Loss 28.951763\n",
      "Params: tensor([ 2.3197, -0.0683])\n",
      "Grad:  tensor([-0.7042,  2.8951])\n",
      "Epoch 991 Loss 28.950872\n",
      "Params: tensor([ 2.3197, -0.0686])\n",
      "Grad:  tensor([-0.7031,  2.8953])\n",
      "Epoch 992 Loss 28.949984\n",
      "Params: tensor([ 2.3198, -0.0689])\n",
      "Grad:  tensor([-0.7019,  2.8954])\n",
      "Epoch 993 Loss 28.949097\n",
      "Params: tensor([ 2.3199, -0.0692])\n",
      "Grad:  tensor([-0.7008,  2.8956])\n",
      "Epoch 994 Loss 28.948204\n",
      "Params: tensor([ 2.3200, -0.0695])\n",
      "Grad:  tensor([-0.6997,  2.8957])\n",
      "Epoch 995 Loss 28.947319\n",
      "Params: tensor([ 2.3200, -0.0698])\n",
      "Grad:  tensor([-0.6986,  2.8959])\n",
      "Epoch 996 Loss 28.946428\n",
      "Params: tensor([ 2.3201, -0.0701])\n",
      "Grad:  tensor([-0.6975,  2.8960])\n",
      "Epoch 997 Loss 28.945545\n",
      "Params: tensor([ 2.3202, -0.0704])\n",
      "Grad:  tensor([-0.6964,  2.8962])\n",
      "Epoch 998 Loss 28.944660\n",
      "Params: tensor([ 2.3202, -0.0707])\n",
      "Grad:  tensor([-0.6953,  2.8963])\n",
      "Epoch 999 Loss 28.943769\n",
      "Params: tensor([ 2.3203, -0.0709])\n",
      "Grad:  tensor([-0.6942,  2.8964])\n",
      "Epoch 1000 Loss 28.942879\n",
      "Params: tensor([ 2.3204, -0.0712])\n",
      "Grad:  tensor([-0.6931,  2.8966])\n",
      "Epoch 1001 Loss 28.941994\n",
      "Params: tensor([ 2.3204, -0.0715])\n",
      "Grad:  tensor([-0.6920,  2.8967])\n",
      "Epoch 1002 Loss 28.941113\n",
      "Params: tensor([ 2.3205, -0.0718])\n",
      "Grad:  tensor([-0.6910,  2.8969])\n",
      "Epoch 1003 Loss 28.940222\n",
      "Params: tensor([ 2.3206, -0.0721])\n",
      "Grad:  tensor([-0.6899,  2.8970])\n",
      "Epoch 1004 Loss 28.939339\n",
      "Params: tensor([ 2.3206, -0.0724])\n",
      "Grad:  tensor([-0.6888,  2.8971])\n",
      "Epoch 1005 Loss 28.938452\n",
      "Params: tensor([ 2.3207, -0.0727])\n",
      "Grad:  tensor([-0.6878,  2.8973])\n",
      "Epoch 1006 Loss 28.937559\n",
      "Params: tensor([ 2.3208, -0.0730])\n",
      "Grad:  tensor([-0.6867,  2.8974])\n",
      "Epoch 1007 Loss 28.936678\n",
      "Params: tensor([ 2.3209, -0.0733])\n",
      "Grad:  tensor([-0.6857,  2.8975])\n",
      "Epoch 1008 Loss 28.935789\n",
      "Params: tensor([ 2.3209, -0.0736])\n",
      "Grad:  tensor([-0.6847,  2.8977])\n",
      "Epoch 1009 Loss 28.934900\n",
      "Params: tensor([ 2.3210, -0.0738])\n",
      "Grad:  tensor([-0.6837,  2.8978])\n",
      "Epoch 1010 Loss 28.934019\n",
      "Params: tensor([ 2.3211, -0.0741])\n",
      "Grad:  tensor([-0.6826,  2.8979])\n",
      "Epoch 1011 Loss 28.933134\n",
      "Params: tensor([ 2.3211, -0.0744])\n",
      "Grad:  tensor([-0.6816,  2.8980])\n",
      "Epoch 1012 Loss 28.932249\n",
      "Params: tensor([ 2.3212, -0.0747])\n",
      "Grad:  tensor([-0.6806,  2.8982])\n",
      "Epoch 1013 Loss 28.931360\n",
      "Params: tensor([ 2.3213, -0.0750])\n",
      "Grad:  tensor([-0.6796,  2.8983])\n",
      "Epoch 1014 Loss 28.930473\n",
      "Params: tensor([ 2.3213, -0.0753])\n",
      "Grad:  tensor([-0.6786,  2.8984])\n",
      "Epoch 1015 Loss 28.929585\n",
      "Params: tensor([ 2.3214, -0.0756])\n",
      "Grad:  tensor([-0.6776,  2.8985])\n",
      "Epoch 1016 Loss 28.928698\n",
      "Params: tensor([ 2.3215, -0.0759])\n",
      "Grad:  tensor([-0.6767,  2.8987])\n",
      "Epoch 1017 Loss 28.927813\n",
      "Params: tensor([ 2.3215, -0.0762])\n",
      "Grad:  tensor([-0.6757,  2.8988])\n",
      "Epoch 1018 Loss 28.926929\n",
      "Params: tensor([ 2.3216, -0.0765])\n",
      "Grad:  tensor([-0.6747,  2.8989])\n",
      "Epoch 1019 Loss 28.926043\n",
      "Params: tensor([ 2.3217, -0.0767])\n",
      "Grad:  tensor([-0.6737,  2.8990])\n",
      "Epoch 1020 Loss 28.925152\n",
      "Params: tensor([ 2.3217, -0.0770])\n",
      "Grad:  tensor([-0.6728,  2.8991])\n",
      "Epoch 1021 Loss 28.924267\n",
      "Params: tensor([ 2.3218, -0.0773])\n",
      "Grad:  tensor([-0.6718,  2.8993])\n",
      "Epoch 1022 Loss 28.923384\n",
      "Params: tensor([ 2.3219, -0.0776])\n",
      "Grad:  tensor([-0.6709,  2.8994])\n",
      "Epoch 1023 Loss 28.922499\n",
      "Params: tensor([ 2.3219, -0.0779])\n",
      "Grad:  tensor([-0.6699,  2.8995])\n",
      "Epoch 1024 Loss 28.921608\n",
      "Params: tensor([ 2.3220, -0.0782])\n",
      "Grad:  tensor([-0.6690,  2.8996])\n",
      "Epoch 1025 Loss 28.920727\n",
      "Params: tensor([ 2.3221, -0.0785])\n",
      "Grad:  tensor([-0.6681,  2.8997])\n",
      "Epoch 1026 Loss 28.919844\n",
      "Params: tensor([ 2.3221, -0.0788])\n",
      "Grad:  tensor([-0.6671,  2.8998])\n",
      "Epoch 1027 Loss 28.918959\n",
      "Params: tensor([ 2.3222, -0.0791])\n",
      "Grad:  tensor([-0.6662,  2.8999])\n",
      "Epoch 1028 Loss 28.918074\n",
      "Params: tensor([ 2.3223, -0.0794])\n",
      "Grad:  tensor([-0.6653,  2.9001])\n",
      "Epoch 1029 Loss 28.917189\n",
      "Params: tensor([ 2.3223, -0.0796])\n",
      "Grad:  tensor([-0.6644,  2.9002])\n",
      "Epoch 1030 Loss 28.916302\n",
      "Params: tensor([ 2.3224, -0.0799])\n",
      "Grad:  tensor([-0.6635,  2.9003])\n",
      "Epoch 1031 Loss 28.915417\n",
      "Params: tensor([ 2.3225, -0.0802])\n",
      "Grad:  tensor([-0.6626,  2.9004])\n",
      "Epoch 1032 Loss 28.914528\n",
      "Params: tensor([ 2.3225, -0.0805])\n",
      "Grad:  tensor([-0.6617,  2.9005])\n",
      "Epoch 1033 Loss 28.913647\n",
      "Params: tensor([ 2.3226, -0.0808])\n",
      "Grad:  tensor([-0.6608,  2.9006])\n",
      "Epoch 1034 Loss 28.912758\n",
      "Params: tensor([ 2.3227, -0.0811])\n",
      "Grad:  tensor([-0.6599,  2.9007])\n",
      "Epoch 1035 Loss 28.911877\n",
      "Params: tensor([ 2.3227, -0.0814])\n",
      "Grad:  tensor([-0.6591,  2.9008])\n",
      "Epoch 1036 Loss 28.910994\n",
      "Params: tensor([ 2.3228, -0.0817])\n",
      "Grad:  tensor([-0.6582,  2.9009])\n",
      "Epoch 1037 Loss 28.910103\n",
      "Params: tensor([ 2.3229, -0.0820])\n",
      "Grad:  tensor([-0.6573,  2.9010])\n",
      "Epoch 1038 Loss 28.909222\n",
      "Params: tensor([ 2.3229, -0.0823])\n",
      "Grad:  tensor([-0.6565,  2.9011])\n",
      "Epoch 1039 Loss 28.908333\n",
      "Params: tensor([ 2.3230, -0.0825])\n",
      "Grad:  tensor([-0.6556,  2.9012])\n",
      "Epoch 1040 Loss 28.907452\n",
      "Params: tensor([ 2.3231, -0.0828])\n",
      "Grad:  tensor([-0.6548,  2.9013])\n",
      "Epoch 1041 Loss 28.906569\n",
      "Params: tensor([ 2.3231, -0.0831])\n",
      "Grad:  tensor([-0.6539,  2.9014])\n",
      "Epoch 1042 Loss 28.905684\n",
      "Params: tensor([ 2.3232, -0.0834])\n",
      "Grad:  tensor([-0.6531,  2.9015])\n",
      "Epoch 1043 Loss 28.904799\n",
      "Params: tensor([ 2.3233, -0.0837])\n",
      "Grad:  tensor([-0.6522,  2.9016])\n",
      "Epoch 1044 Loss 28.903913\n",
      "Params: tensor([ 2.3233, -0.0840])\n",
      "Grad:  tensor([-0.6514,  2.9017])\n",
      "Epoch 1045 Loss 28.903028\n",
      "Params: tensor([ 2.3234, -0.0843])\n",
      "Grad:  tensor([-0.6506,  2.9018])\n",
      "Epoch 1046 Loss 28.902149\n",
      "Params: tensor([ 2.3235, -0.0846])\n",
      "Grad:  tensor([-0.6498,  2.9019])\n",
      "Epoch 1047 Loss 28.901258\n",
      "Params: tensor([ 2.3235, -0.0849])\n",
      "Grad:  tensor([-0.6489,  2.9020])\n",
      "Epoch 1048 Loss 28.900379\n",
      "Params: tensor([ 2.3236, -0.0852])\n",
      "Grad:  tensor([-0.6481,  2.9021])\n",
      "Epoch 1049 Loss 28.899492\n",
      "Params: tensor([ 2.3237, -0.0854])\n",
      "Grad:  tensor([-0.6473,  2.9022])\n",
      "Epoch 1050 Loss 28.898609\n",
      "Params: tensor([ 2.3237, -0.0857])\n",
      "Grad:  tensor([-0.6465,  2.9022])\n",
      "Epoch 1051 Loss 28.897724\n",
      "Params: tensor([ 2.3238, -0.0860])\n",
      "Grad:  tensor([-0.6457,  2.9023])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1052 Loss 28.896837\n",
      "Params: tensor([ 2.3238, -0.0863])\n",
      "Grad:  tensor([-0.6449,  2.9024])\n",
      "Epoch 1053 Loss 28.895958\n",
      "Params: tensor([ 2.3239, -0.0866])\n",
      "Grad:  tensor([-0.6441,  2.9025])\n",
      "Epoch 1054 Loss 28.895069\n",
      "Params: tensor([ 2.3240, -0.0869])\n",
      "Grad:  tensor([-0.6434,  2.9026])\n",
      "Epoch 1055 Loss 28.894188\n",
      "Params: tensor([ 2.3240, -0.0872])\n",
      "Grad:  tensor([-0.6426,  2.9027])\n",
      "Epoch 1056 Loss 28.893303\n",
      "Params: tensor([ 2.3241, -0.0875])\n",
      "Grad:  tensor([-0.6418,  2.9028])\n",
      "Epoch 1057 Loss 28.892420\n",
      "Params: tensor([ 2.3242, -0.0878])\n",
      "Grad:  tensor([-0.6410,  2.9029])\n",
      "Epoch 1058 Loss 28.891535\n",
      "Params: tensor([ 2.3242, -0.0881])\n",
      "Grad:  tensor([-0.6403,  2.9029])\n",
      "Epoch 1059 Loss 28.890650\n",
      "Params: tensor([ 2.3243, -0.0883])\n",
      "Grad:  tensor([-0.6395,  2.9030])\n",
      "Epoch 1060 Loss 28.889769\n",
      "Params: tensor([ 2.3244, -0.0886])\n",
      "Grad:  tensor([-0.6388,  2.9031])\n",
      "Epoch 1061 Loss 28.888884\n",
      "Params: tensor([ 2.3244, -0.0889])\n",
      "Grad:  tensor([-0.6380,  2.9032])\n",
      "Epoch 1062 Loss 28.888000\n",
      "Params: tensor([ 2.3245, -0.0892])\n",
      "Grad:  tensor([-0.6372,  2.9033])\n",
      "Epoch 1063 Loss 28.887117\n",
      "Params: tensor([ 2.3246, -0.0895])\n",
      "Grad:  tensor([-0.6365,  2.9033])\n",
      "Epoch 1064 Loss 28.886232\n",
      "Params: tensor([ 2.3246, -0.0898])\n",
      "Grad:  tensor([-0.6358,  2.9034])\n",
      "Epoch 1065 Loss 28.885353\n",
      "Params: tensor([ 2.3247, -0.0901])\n",
      "Grad:  tensor([-0.6350,  2.9035])\n",
      "Epoch 1066 Loss 28.884472\n",
      "Params: tensor([ 2.3247, -0.0904])\n",
      "Grad:  tensor([-0.6343,  2.9036])\n",
      "Epoch 1067 Loss 28.883583\n",
      "Params: tensor([ 2.3248, -0.0907])\n",
      "Grad:  tensor([-0.6336,  2.9037])\n",
      "Epoch 1068 Loss 28.882698\n",
      "Params: tensor([ 2.3249, -0.0910])\n",
      "Grad:  tensor([-0.6329,  2.9037])\n",
      "Epoch 1069 Loss 28.881819\n",
      "Params: tensor([ 2.3249, -0.0912])\n",
      "Grad:  tensor([-0.6322,  2.9038])\n",
      "Epoch 1070 Loss 28.880938\n",
      "Params: tensor([ 2.3250, -0.0915])\n",
      "Grad:  tensor([-0.6315,  2.9039])\n",
      "Epoch 1071 Loss 28.880049\n",
      "Params: tensor([ 2.3251, -0.0918])\n",
      "Grad:  tensor([-0.6307,  2.9040])\n",
      "Epoch 1072 Loss 28.879168\n",
      "Params: tensor([ 2.3251, -0.0921])\n",
      "Grad:  tensor([-0.6300,  2.9040])\n",
      "Epoch 1073 Loss 28.878290\n",
      "Params: tensor([ 2.3252, -0.0924])\n",
      "Grad:  tensor([-0.6294,  2.9041])\n",
      "Epoch 1074 Loss 28.877399\n",
      "Params: tensor([ 2.3252, -0.0927])\n",
      "Grad:  tensor([-0.6287,  2.9042])\n",
      "Epoch 1075 Loss 28.876520\n",
      "Params: tensor([ 2.3253, -0.0930])\n",
      "Grad:  tensor([-0.6280,  2.9042])\n",
      "Epoch 1076 Loss 28.875639\n",
      "Params: tensor([ 2.3254, -0.0933])\n",
      "Grad:  tensor([-0.6273,  2.9043])\n",
      "Epoch 1077 Loss 28.874758\n",
      "Params: tensor([ 2.3254, -0.0936])\n",
      "Grad:  tensor([-0.6266,  2.9044])\n",
      "Epoch 1078 Loss 28.873869\n",
      "Params: tensor([ 2.3255, -0.0939])\n",
      "Grad:  tensor([-0.6259,  2.9044])\n",
      "Epoch 1079 Loss 28.872992\n",
      "Params: tensor([ 2.3256, -0.0942])\n",
      "Grad:  tensor([-0.6252,  2.9045])\n",
      "Epoch 1080 Loss 28.872108\n",
      "Params: tensor([ 2.3256, -0.0944])\n",
      "Grad:  tensor([-0.6246,  2.9046])\n",
      "Epoch 1081 Loss 28.871227\n",
      "Params: tensor([ 2.3257, -0.0947])\n",
      "Grad:  tensor([-0.6239,  2.9046])\n",
      "Epoch 1082 Loss 28.870338\n",
      "Params: tensor([ 2.3257, -0.0950])\n",
      "Grad:  tensor([-0.6233,  2.9047])\n",
      "Epoch 1083 Loss 28.869452\n",
      "Params: tensor([ 2.3258, -0.0953])\n",
      "Grad:  tensor([-0.6226,  2.9048])\n",
      "Epoch 1084 Loss 28.868578\n",
      "Params: tensor([ 2.3259, -0.0956])\n",
      "Grad:  tensor([-0.6220,  2.9048])\n",
      "Epoch 1085 Loss 28.867699\n",
      "Params: tensor([ 2.3259, -0.0959])\n",
      "Grad:  tensor([-0.6213,  2.9049])\n",
      "Epoch 1086 Loss 28.866808\n",
      "Params: tensor([ 2.3260, -0.0962])\n",
      "Grad:  tensor([-0.6207,  2.9050])\n",
      "Epoch 1087 Loss 28.865934\n",
      "Params: tensor([ 2.3261, -0.0965])\n",
      "Grad:  tensor([-0.6200,  2.9050])\n",
      "Epoch 1088 Loss 28.865046\n",
      "Params: tensor([ 2.3261, -0.0968])\n",
      "Grad:  tensor([-0.6194,  2.9051])\n",
      "Epoch 1089 Loss 28.864164\n",
      "Params: tensor([ 2.3262, -0.0971])\n",
      "Grad:  tensor([-0.6187,  2.9052])\n",
      "Epoch 1090 Loss 28.863283\n",
      "Params: tensor([ 2.3262, -0.0973])\n",
      "Grad:  tensor([-0.6181,  2.9052])\n",
      "Epoch 1091 Loss 28.862402\n",
      "Params: tensor([ 2.3263, -0.0976])\n",
      "Grad:  tensor([-0.6175,  2.9053])\n",
      "Epoch 1092 Loss 28.861517\n",
      "Params: tensor([ 2.3264, -0.0979])\n",
      "Grad:  tensor([-0.6169,  2.9053])\n",
      "Epoch 1093 Loss 28.860638\n",
      "Params: tensor([ 2.3264, -0.0982])\n",
      "Grad:  tensor([-0.6162,  2.9054])\n",
      "Epoch 1094 Loss 28.859753\n",
      "Params: tensor([ 2.3265, -0.0985])\n",
      "Grad:  tensor([-0.6156,  2.9054])\n",
      "Epoch 1095 Loss 28.858873\n",
      "Params: tensor([ 2.3266, -0.0988])\n",
      "Grad:  tensor([-0.6150,  2.9055])\n",
      "Epoch 1096 Loss 28.857983\n",
      "Params: tensor([ 2.3266, -0.0991])\n",
      "Grad:  tensor([-0.6144,  2.9056])\n",
      "Epoch 1097 Loss 28.857109\n",
      "Params: tensor([ 2.3267, -0.0994])\n",
      "Grad:  tensor([-0.6138,  2.9056])\n",
      "Epoch 1098 Loss 28.856226\n",
      "Params: tensor([ 2.3267, -0.0997])\n",
      "Grad:  tensor([-0.6132,  2.9057])\n",
      "Epoch 1099 Loss 28.855341\n",
      "Params: tensor([ 2.3268, -0.1000])\n",
      "Grad:  tensor([-0.6126,  2.9057])\n",
      "Epoch 1100 Loss 28.854462\n",
      "Params: tensor([ 2.3269, -0.1003])\n",
      "Grad:  tensor([-0.6120,  2.9058])\n",
      "Epoch 1101 Loss 28.853582\n",
      "Params: tensor([ 2.3269, -0.1005])\n",
      "Grad:  tensor([-0.6114,  2.9058])\n",
      "Epoch 1102 Loss 28.852699\n",
      "Params: tensor([ 2.3270, -0.1008])\n",
      "Grad:  tensor([-0.6109,  2.9059])\n",
      "Epoch 1103 Loss 28.851818\n",
      "Params: tensor([ 2.3270, -0.1011])\n",
      "Grad:  tensor([-0.6103,  2.9059])\n",
      "Epoch 1104 Loss 28.850929\n",
      "Params: tensor([ 2.3271, -0.1014])\n",
      "Grad:  tensor([-0.6097,  2.9060])\n",
      "Epoch 1105 Loss 28.850060\n",
      "Params: tensor([ 2.3272, -0.1017])\n",
      "Grad:  tensor([-0.6091,  2.9060])\n",
      "Epoch 1106 Loss 28.849169\n",
      "Params: tensor([ 2.3272, -0.1020])\n",
      "Grad:  tensor([-0.6086,  2.9061])\n",
      "Epoch 1107 Loss 28.848286\n",
      "Params: tensor([ 2.3273, -0.1023])\n",
      "Grad:  tensor([-0.6080,  2.9061])\n",
      "Epoch 1108 Loss 28.847410\n",
      "Params: tensor([ 2.3273, -0.1026])\n",
      "Grad:  tensor([-0.6074,  2.9062])\n",
      "Epoch 1109 Loss 28.846527\n",
      "Params: tensor([ 2.3274, -0.1029])\n",
      "Grad:  tensor([-0.6069,  2.9062])\n",
      "Epoch 1110 Loss 28.845640\n",
      "Params: tensor([ 2.3275, -0.1032])\n",
      "Grad:  tensor([-0.6063,  2.9063])\n",
      "Epoch 1111 Loss 28.844763\n",
      "Params: tensor([ 2.3275, -0.1035])\n",
      "Grad:  tensor([-0.6057,  2.9063])\n",
      "Epoch 1112 Loss 28.843884\n",
      "Params: tensor([ 2.3276, -0.1037])\n",
      "Grad:  tensor([-0.6052,  2.9064])\n",
      "Epoch 1113 Loss 28.843006\n",
      "Params: tensor([ 2.3277, -0.1040])\n",
      "Grad:  tensor([-0.6046,  2.9064])\n",
      "Epoch 1114 Loss 28.842119\n",
      "Params: tensor([ 2.3277, -0.1043])\n",
      "Grad:  tensor([-0.6041,  2.9065])\n",
      "Epoch 1115 Loss 28.841236\n",
      "Params: tensor([ 2.3278, -0.1046])\n",
      "Grad:  tensor([-0.6036,  2.9065])\n",
      "Epoch 1116 Loss 28.840361\n",
      "Params: tensor([ 2.3278, -0.1049])\n",
      "Grad:  tensor([-0.6030,  2.9066])\n",
      "Epoch 1117 Loss 28.839479\n",
      "Params: tensor([ 2.3279, -0.1052])\n",
      "Grad:  tensor([-0.6025,  2.9066])\n",
      "Epoch 1118 Loss 28.838594\n",
      "Params: tensor([ 2.3280, -0.1055])\n",
      "Grad:  tensor([-0.6019,  2.9066])\n",
      "Epoch 1119 Loss 28.837715\n",
      "Params: tensor([ 2.3280, -0.1058])\n",
      "Grad:  tensor([-0.6014,  2.9067])\n",
      "Epoch 1120 Loss 28.836836\n",
      "Params: tensor([ 2.3281, -0.1061])\n",
      "Grad:  tensor([-0.6009,  2.9067])\n",
      "Epoch 1121 Loss 28.835951\n",
      "Params: tensor([ 2.3281, -0.1064])\n",
      "Grad:  tensor([-0.6004,  2.9068])\n",
      "Epoch 1122 Loss 28.835072\n",
      "Params: tensor([ 2.3282, -0.1066])\n",
      "Grad:  tensor([-0.5999,  2.9068])\n",
      "Epoch 1123 Loss 28.834185\n",
      "Params: tensor([ 2.3283, -0.1069])\n",
      "Grad:  tensor([-0.5993,  2.9068])\n",
      "Epoch 1124 Loss 28.833311\n",
      "Params: tensor([ 2.3283, -0.1072])\n",
      "Grad:  tensor([-0.5988,  2.9069])\n",
      "Epoch 1125 Loss 28.832426\n",
      "Params: tensor([ 2.3284, -0.1075])\n",
      "Grad:  tensor([-0.5983,  2.9069])\n",
      "Epoch 1126 Loss 28.831549\n",
      "Params: tensor([ 2.3284, -0.1078])\n",
      "Grad:  tensor([-0.5978,  2.9070])\n",
      "Epoch 1127 Loss 28.830666\n",
      "Params: tensor([ 2.3285, -0.1081])\n",
      "Grad:  tensor([-0.5973,  2.9070])\n",
      "Epoch 1128 Loss 28.829786\n",
      "Params: tensor([ 2.3286, -0.1084])\n",
      "Grad:  tensor([-0.5968,  2.9070])\n",
      "Epoch 1129 Loss 28.828901\n",
      "Params: tensor([ 2.3286, -0.1087])\n",
      "Grad:  tensor([-0.5963,  2.9071])\n",
      "Epoch 1130 Loss 28.828022\n",
      "Params: tensor([ 2.3287, -0.1090])\n",
      "Grad:  tensor([-0.5958,  2.9071])\n",
      "Epoch 1131 Loss 28.827147\n",
      "Params: tensor([ 2.3287, -0.1093])\n",
      "Grad:  tensor([-0.5953,  2.9071])\n",
      "Epoch 1132 Loss 28.826265\n",
      "Params: tensor([ 2.3288, -0.1096])\n",
      "Grad:  tensor([-0.5948,  2.9072])\n",
      "Epoch 1133 Loss 28.825384\n",
      "Params: tensor([ 2.3289, -0.1098])\n",
      "Grad:  tensor([-0.5943,  2.9072])\n",
      "Epoch 1134 Loss 28.824499\n",
      "Params: tensor([ 2.3289, -0.1101])\n",
      "Grad:  tensor([-0.5939,  2.9072])\n",
      "Epoch 1135 Loss 28.823620\n",
      "Params: tensor([ 2.3290, -0.1104])\n",
      "Grad:  tensor([-0.5934,  2.9073])\n",
      "Epoch 1136 Loss 28.822742\n",
      "Params: tensor([ 2.3290, -0.1107])\n",
      "Grad:  tensor([-0.5929,  2.9073])\n",
      "Epoch 1137 Loss 28.821861\n",
      "Params: tensor([ 2.3291, -0.1110])\n",
      "Grad:  tensor([-0.5924,  2.9073])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1138 Loss 28.820982\n",
      "Params: tensor([ 2.3291, -0.1113])\n",
      "Grad:  tensor([-0.5919,  2.9074])\n",
      "Epoch 1139 Loss 28.820107\n",
      "Params: tensor([ 2.3292, -0.1116])\n",
      "Grad:  tensor([-0.5915,  2.9074])\n",
      "Epoch 1140 Loss 28.819221\n",
      "Params: tensor([ 2.3293, -0.1119])\n",
      "Grad:  tensor([-0.5910,  2.9074])\n",
      "Epoch 1141 Loss 28.818342\n",
      "Params: tensor([ 2.3293, -0.1122])\n",
      "Grad:  tensor([-0.5906,  2.9075])\n",
      "Epoch 1142 Loss 28.817457\n",
      "Params: tensor([ 2.3294, -0.1125])\n",
      "Grad:  tensor([-0.5901,  2.9075])\n",
      "Epoch 1143 Loss 28.816582\n",
      "Params: tensor([ 2.3294, -0.1128])\n",
      "Grad:  tensor([-0.5896,  2.9075])\n",
      "Epoch 1144 Loss 28.815699\n",
      "Params: tensor([ 2.3295, -0.1130])\n",
      "Grad:  tensor([-0.5892,  2.9076])\n",
      "Epoch 1145 Loss 28.814821\n",
      "Params: tensor([ 2.3296, -0.1133])\n",
      "Grad:  tensor([-0.5887,  2.9076])\n",
      "Epoch 1146 Loss 28.813942\n",
      "Params: tensor([ 2.3296, -0.1136])\n",
      "Grad:  tensor([-0.5883,  2.9076])\n",
      "Epoch 1147 Loss 28.813063\n",
      "Params: tensor([ 2.3297, -0.1139])\n",
      "Grad:  tensor([-0.5878,  2.9076])\n",
      "Epoch 1148 Loss 28.812178\n",
      "Params: tensor([ 2.3297, -0.1142])\n",
      "Grad:  tensor([-0.5874,  2.9077])\n",
      "Epoch 1149 Loss 28.811296\n",
      "Params: tensor([ 2.3298, -0.1145])\n",
      "Grad:  tensor([-0.5869,  2.9077])\n",
      "Epoch 1150 Loss 28.810417\n",
      "Params: tensor([ 2.3299, -0.1148])\n",
      "Grad:  tensor([-0.5865,  2.9077])\n",
      "Epoch 1151 Loss 28.809540\n",
      "Params: tensor([ 2.3299, -0.1151])\n",
      "Grad:  tensor([-0.5861,  2.9078])\n",
      "Epoch 1152 Loss 28.808661\n",
      "Params: tensor([ 2.3300, -0.1154])\n",
      "Grad:  tensor([-0.5856,  2.9078])\n",
      "Epoch 1153 Loss 28.807781\n",
      "Params: tensor([ 2.3300, -0.1157])\n",
      "Grad:  tensor([-0.5852,  2.9078])\n",
      "Epoch 1154 Loss 28.806898\n",
      "Params: tensor([ 2.3301, -0.1160])\n",
      "Grad:  tensor([-0.5848,  2.9078])\n",
      "Epoch 1155 Loss 28.806019\n",
      "Params: tensor([ 2.3301, -0.1162])\n",
      "Grad:  tensor([-0.5843,  2.9079])\n",
      "Epoch 1156 Loss 28.805143\n",
      "Params: tensor([ 2.3302, -0.1165])\n",
      "Grad:  tensor([-0.5839,  2.9079])\n",
      "Epoch 1157 Loss 28.804262\n",
      "Params: tensor([ 2.3303, -0.1168])\n",
      "Grad:  tensor([-0.5835,  2.9079])\n",
      "Epoch 1158 Loss 28.803383\n",
      "Params: tensor([ 2.3303, -0.1171])\n",
      "Grad:  tensor([-0.5831,  2.9079])\n",
      "Epoch 1159 Loss 28.802504\n",
      "Params: tensor([ 2.3304, -0.1174])\n",
      "Grad:  tensor([-0.5827,  2.9079])\n",
      "Epoch 1160 Loss 28.801619\n",
      "Params: tensor([ 2.3304, -0.1177])\n",
      "Grad:  tensor([-0.5822,  2.9080])\n",
      "Epoch 1161 Loss 28.800737\n",
      "Params: tensor([ 2.3305, -0.1180])\n",
      "Grad:  tensor([-0.5818,  2.9080])\n",
      "Epoch 1162 Loss 28.799864\n",
      "Params: tensor([ 2.3306, -0.1183])\n",
      "Grad:  tensor([-0.5814,  2.9080])\n",
      "Epoch 1163 Loss 28.798983\n",
      "Params: tensor([ 2.3306, -0.1186])\n",
      "Grad:  tensor([-0.5810,  2.9080])\n",
      "Epoch 1164 Loss 28.798101\n",
      "Params: tensor([ 2.3307, -0.1189])\n",
      "Grad:  tensor([-0.5806,  2.9081])\n",
      "Epoch 1165 Loss 28.797228\n",
      "Params: tensor([ 2.3307, -0.1191])\n",
      "Grad:  tensor([-0.5802,  2.9081])\n",
      "Epoch 1166 Loss 28.796339\n",
      "Params: tensor([ 2.3308, -0.1194])\n",
      "Grad:  tensor([-0.5798,  2.9081])\n",
      "Epoch 1167 Loss 28.795469\n",
      "Params: tensor([ 2.3308, -0.1197])\n",
      "Grad:  tensor([-0.5794,  2.9081])\n",
      "Epoch 1168 Loss 28.794586\n",
      "Params: tensor([ 2.3309, -0.1200])\n",
      "Grad:  tensor([-0.5790,  2.9081])\n",
      "Epoch 1169 Loss 28.793707\n",
      "Params: tensor([ 2.3310, -0.1203])\n",
      "Grad:  tensor([-0.5786,  2.9081])\n",
      "Epoch 1170 Loss 28.792828\n",
      "Params: tensor([ 2.3310, -0.1206])\n",
      "Grad:  tensor([-0.5782,  2.9082])\n",
      "Epoch 1171 Loss 28.791954\n",
      "Params: tensor([ 2.3311, -0.1209])\n",
      "Grad:  tensor([-0.5778,  2.9082])\n",
      "Epoch 1172 Loss 28.791069\n",
      "Params: tensor([ 2.3311, -0.1212])\n",
      "Grad:  tensor([-0.5775,  2.9082])\n",
      "Epoch 1173 Loss 28.790194\n",
      "Params: tensor([ 2.3312, -0.1215])\n",
      "Grad:  tensor([-0.5771,  2.9082])\n",
      "Epoch 1174 Loss 28.789312\n",
      "Params: tensor([ 2.3313, -0.1218])\n",
      "Grad:  tensor([-0.5767,  2.9082])\n",
      "Epoch 1175 Loss 28.788435\n",
      "Params: tensor([ 2.3313, -0.1221])\n",
      "Grad:  tensor([-0.5763,  2.9082])\n",
      "Epoch 1176 Loss 28.787556\n",
      "Params: tensor([ 2.3314, -0.1223])\n",
      "Grad:  tensor([-0.5759,  2.9083])\n",
      "Epoch 1177 Loss 28.786676\n",
      "Params: tensor([ 2.3314, -0.1226])\n",
      "Grad:  tensor([-0.5755,  2.9083])\n",
      "Epoch 1178 Loss 28.785795\n",
      "Params: tensor([ 2.3315, -0.1229])\n",
      "Grad:  tensor([-0.5752,  2.9083])\n",
      "Epoch 1179 Loss 28.784918\n",
      "Params: tensor([ 2.3315, -0.1232])\n",
      "Grad:  tensor([-0.5748,  2.9083])\n",
      "Epoch 1180 Loss 28.784035\n",
      "Params: tensor([ 2.3316, -0.1235])\n",
      "Grad:  tensor([-0.5744,  2.9083])\n",
      "Epoch 1181 Loss 28.783155\n",
      "Params: tensor([ 2.3317, -0.1238])\n",
      "Grad:  tensor([-0.5741,  2.9083])\n",
      "Epoch 1182 Loss 28.782282\n",
      "Params: tensor([ 2.3317, -0.1241])\n",
      "Grad:  tensor([-0.5737,  2.9083])\n",
      "Epoch 1183 Loss 28.781403\n",
      "Params: tensor([ 2.3318, -0.1244])\n",
      "Grad:  tensor([-0.5734,  2.9084])\n",
      "Epoch 1184 Loss 28.780523\n",
      "Params: tensor([ 2.3318, -0.1247])\n",
      "Grad:  tensor([-0.5730,  2.9084])\n",
      "Epoch 1185 Loss 28.779644\n",
      "Params: tensor([ 2.3319, -0.1250])\n",
      "Grad:  tensor([-0.5726,  2.9084])\n",
      "Epoch 1186 Loss 28.778767\n",
      "Params: tensor([ 2.3319, -0.1253])\n",
      "Grad:  tensor([-0.5723,  2.9084])\n",
      "Epoch 1187 Loss 28.777885\n",
      "Params: tensor([ 2.3320, -0.1255])\n",
      "Grad:  tensor([-0.5719,  2.9084])\n",
      "Epoch 1188 Loss 28.777006\n",
      "Params: tensor([ 2.3321, -0.1258])\n",
      "Grad:  tensor([-0.5716,  2.9084])\n",
      "Epoch 1189 Loss 28.776129\n",
      "Params: tensor([ 2.3321, -0.1261])\n",
      "Grad:  tensor([-0.5712,  2.9084])\n",
      "Epoch 1190 Loss 28.775255\n",
      "Params: tensor([ 2.3322, -0.1264])\n",
      "Grad:  tensor([-0.5709,  2.9084])\n",
      "Epoch 1191 Loss 28.774376\n",
      "Params: tensor([ 2.3322, -0.1267])\n",
      "Grad:  tensor([-0.5705,  2.9085])\n",
      "Epoch 1192 Loss 28.773497\n",
      "Params: tensor([ 2.3323, -0.1270])\n",
      "Grad:  tensor([-0.5702,  2.9085])\n",
      "Epoch 1193 Loss 28.772615\n",
      "Params: tensor([ 2.3323, -0.1273])\n",
      "Grad:  tensor([-0.5699,  2.9085])\n",
      "Epoch 1194 Loss 28.771740\n",
      "Params: tensor([ 2.3324, -0.1276])\n",
      "Grad:  tensor([-0.5695,  2.9085])\n",
      "Epoch 1195 Loss 28.770863\n",
      "Params: tensor([ 2.3325, -0.1279])\n",
      "Grad:  tensor([-0.5692,  2.9085])\n",
      "Epoch 1196 Loss 28.769987\n",
      "Params: tensor([ 2.3325, -0.1282])\n",
      "Grad:  tensor([-0.5689,  2.9085])\n",
      "Epoch 1197 Loss 28.769104\n",
      "Params: tensor([ 2.3326, -0.1285])\n",
      "Grad:  tensor([-0.5685,  2.9085])\n",
      "Epoch 1198 Loss 28.768227\n",
      "Params: tensor([ 2.3326, -0.1287])\n",
      "Grad:  tensor([-0.5682,  2.9085])\n",
      "Epoch 1199 Loss 28.767347\n",
      "Params: tensor([ 2.3327, -0.1290])\n",
      "Grad:  tensor([-0.5679,  2.9085])\n",
      "Epoch 1200 Loss 28.766474\n",
      "Params: tensor([ 2.3327, -0.1293])\n",
      "Grad:  tensor([-0.5675,  2.9085])\n",
      "Epoch 1201 Loss 28.765593\n",
      "Params: tensor([ 2.3328, -0.1296])\n",
      "Grad:  tensor([-0.5672,  2.9085])\n",
      "Epoch 1202 Loss 28.764717\n",
      "Params: tensor([ 2.3329, -0.1299])\n",
      "Grad:  tensor([-0.5669,  2.9085])\n",
      "Epoch 1203 Loss 28.763836\n",
      "Params: tensor([ 2.3329, -0.1302])\n",
      "Grad:  tensor([-0.5666,  2.9085])\n",
      "Epoch 1204 Loss 28.762962\n",
      "Params: tensor([ 2.3330, -0.1305])\n",
      "Grad:  tensor([-0.5662,  2.9085])\n",
      "Epoch 1205 Loss 28.762083\n",
      "Params: tensor([ 2.3330, -0.1308])\n",
      "Grad:  tensor([-0.5659,  2.9086])\n",
      "Epoch 1206 Loss 28.761202\n",
      "Params: tensor([ 2.3331, -0.1311])\n",
      "Grad:  tensor([-0.5656,  2.9086])\n",
      "Epoch 1207 Loss 28.760326\n",
      "Params: tensor([ 2.3331, -0.1314])\n",
      "Grad:  tensor([-0.5653,  2.9086])\n",
      "Epoch 1208 Loss 28.759449\n",
      "Params: tensor([ 2.3332, -0.1317])\n",
      "Grad:  tensor([-0.5650,  2.9086])\n",
      "Epoch 1209 Loss 28.758568\n",
      "Params: tensor([ 2.3332, -0.1319])\n",
      "Grad:  tensor([-0.5647,  2.9086])\n",
      "Epoch 1210 Loss 28.757690\n",
      "Params: tensor([ 2.3333, -0.1322])\n",
      "Grad:  tensor([-0.5644,  2.9086])\n",
      "Epoch 1211 Loss 28.756811\n",
      "Params: tensor([ 2.3334, -0.1325])\n",
      "Grad:  tensor([-0.5641,  2.9086])\n",
      "Epoch 1212 Loss 28.755938\n",
      "Params: tensor([ 2.3334, -0.1328])\n",
      "Grad:  tensor([-0.5638,  2.9086])\n",
      "Epoch 1213 Loss 28.755058\n",
      "Params: tensor([ 2.3335, -0.1331])\n",
      "Grad:  tensor([-0.5635,  2.9086])\n",
      "Epoch 1214 Loss 28.754175\n",
      "Params: tensor([ 2.3335, -0.1334])\n",
      "Grad:  tensor([-0.5632,  2.9086])\n",
      "Epoch 1215 Loss 28.753302\n",
      "Params: tensor([ 2.3336, -0.1337])\n",
      "Grad:  tensor([-0.5629,  2.9086])\n",
      "Epoch 1216 Loss 28.752419\n",
      "Params: tensor([ 2.3336, -0.1340])\n",
      "Grad:  tensor([-0.5626,  2.9086])\n",
      "Epoch 1217 Loss 28.751543\n",
      "Params: tensor([ 2.3337, -0.1343])\n",
      "Grad:  tensor([-0.5623,  2.9086])\n",
      "Epoch 1218 Loss 28.750666\n",
      "Params: tensor([ 2.3338, -0.1346])\n",
      "Grad:  tensor([-0.5620,  2.9086])\n",
      "Epoch 1219 Loss 28.749788\n",
      "Params: tensor([ 2.3338, -0.1349])\n",
      "Grad:  tensor([-0.5617,  2.9086])\n",
      "Epoch 1220 Loss 28.748913\n",
      "Params: tensor([ 2.3339, -0.1351])\n",
      "Grad:  tensor([-0.5614,  2.9086])\n",
      "Epoch 1221 Loss 28.748030\n",
      "Params: tensor([ 2.3339, -0.1354])\n",
      "Grad:  tensor([-0.5611,  2.9086])\n",
      "Epoch 1222 Loss 28.747154\n",
      "Params: tensor([ 2.3340, -0.1357])\n",
      "Grad:  tensor([-0.5608,  2.9086])\n",
      "Epoch 1223 Loss 28.746277\n",
      "Params: tensor([ 2.3340, -0.1360])\n",
      "Grad:  tensor([-0.5605,  2.9086])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1224 Loss 28.745399\n",
      "Params: tensor([ 2.3341, -0.1363])\n",
      "Grad:  tensor([-0.5603,  2.9086])\n",
      "Epoch 1225 Loss 28.744520\n",
      "Params: tensor([ 2.3341, -0.1366])\n",
      "Grad:  tensor([-0.5600,  2.9086])\n",
      "Epoch 1226 Loss 28.743645\n",
      "Params: tensor([ 2.3342, -0.1369])\n",
      "Grad:  tensor([-0.5597,  2.9086])\n",
      "Epoch 1227 Loss 28.742769\n",
      "Params: tensor([ 2.3343, -0.1372])\n",
      "Grad:  tensor([-0.5594,  2.9086])\n",
      "Epoch 1228 Loss 28.741890\n",
      "Params: tensor([ 2.3343, -0.1375])\n",
      "Grad:  tensor([-0.5591,  2.9086])\n",
      "Epoch 1229 Loss 28.741014\n",
      "Params: tensor([ 2.3344, -0.1378])\n",
      "Grad:  tensor([-0.5588,  2.9086])\n",
      "Epoch 1230 Loss 28.740137\n",
      "Params: tensor([ 2.3344, -0.1381])\n",
      "Grad:  tensor([-0.5586,  2.9086])\n",
      "Epoch 1231 Loss 28.739260\n",
      "Params: tensor([ 2.3345, -0.1383])\n",
      "Grad:  tensor([-0.5583,  2.9086])\n",
      "Epoch 1232 Loss 28.738380\n",
      "Params: tensor([ 2.3345, -0.1386])\n",
      "Grad:  tensor([-0.5580,  2.9086])\n",
      "Epoch 1233 Loss 28.737505\n",
      "Params: tensor([ 2.3346, -0.1389])\n",
      "Grad:  tensor([-0.5578,  2.9086])\n",
      "Epoch 1234 Loss 28.736633\n",
      "Params: tensor([ 2.3347, -0.1392])\n",
      "Grad:  tensor([-0.5575,  2.9086])\n",
      "Epoch 1235 Loss 28.735756\n",
      "Params: tensor([ 2.3347, -0.1395])\n",
      "Grad:  tensor([-0.5572,  2.9086])\n",
      "Epoch 1236 Loss 28.734875\n",
      "Params: tensor([ 2.3348, -0.1398])\n",
      "Grad:  tensor([-0.5570,  2.9085])\n",
      "Epoch 1237 Loss 28.734001\n",
      "Params: tensor([ 2.3348, -0.1401])\n",
      "Grad:  tensor([-0.5567,  2.9085])\n",
      "Epoch 1238 Loss 28.733118\n",
      "Params: tensor([ 2.3349, -0.1404])\n",
      "Grad:  tensor([-0.5564,  2.9085])\n",
      "Epoch 1239 Loss 28.732244\n",
      "Params: tensor([ 2.3349, -0.1407])\n",
      "Grad:  tensor([-0.5562,  2.9085])\n",
      "Epoch 1240 Loss 28.731367\n",
      "Params: tensor([ 2.3350, -0.1410])\n",
      "Grad:  tensor([-0.5559,  2.9085])\n",
      "Epoch 1241 Loss 28.730499\n",
      "Params: tensor([ 2.3350, -0.1413])\n",
      "Grad:  tensor([-0.5557,  2.9085])\n",
      "Epoch 1242 Loss 28.729612\n",
      "Params: tensor([ 2.3351, -0.1415])\n",
      "Grad:  tensor([-0.5554,  2.9085])\n",
      "Epoch 1243 Loss 28.728741\n",
      "Params: tensor([ 2.3352, -0.1418])\n",
      "Grad:  tensor([-0.5552,  2.9085])\n",
      "Epoch 1244 Loss 28.727858\n",
      "Params: tensor([ 2.3352, -0.1421])\n",
      "Grad:  tensor([-0.5549,  2.9085])\n",
      "Epoch 1245 Loss 28.726984\n",
      "Params: tensor([ 2.3353, -0.1424])\n",
      "Grad:  tensor([-0.5547,  2.9085])\n",
      "Epoch 1246 Loss 28.726110\n",
      "Params: tensor([ 2.3353, -0.1427])\n",
      "Grad:  tensor([-0.5544,  2.9085])\n",
      "Epoch 1247 Loss 28.725237\n",
      "Params: tensor([ 2.3354, -0.1430])\n",
      "Grad:  tensor([-0.5542,  2.9085])\n",
      "Epoch 1248 Loss 28.724354\n",
      "Params: tensor([ 2.3354, -0.1433])\n",
      "Grad:  tensor([-0.5539,  2.9085])\n",
      "Epoch 1249 Loss 28.723480\n",
      "Params: tensor([ 2.3355, -0.1436])\n",
      "Grad:  tensor([-0.5537,  2.9085])\n",
      "Epoch 1250 Loss 28.722601\n",
      "Params: tensor([ 2.3355, -0.1439])\n",
      "Grad:  tensor([-0.5534,  2.9085])\n",
      "Epoch 1251 Loss 28.721727\n",
      "Params: tensor([ 2.3356, -0.1442])\n",
      "Grad:  tensor([-0.5532,  2.9084])\n",
      "Epoch 1252 Loss 28.720848\n",
      "Params: tensor([ 2.3357, -0.1445])\n",
      "Grad:  tensor([-0.5530,  2.9084])\n",
      "Epoch 1253 Loss 28.719971\n",
      "Params: tensor([ 2.3357, -0.1447])\n",
      "Grad:  tensor([-0.5527,  2.9084])\n",
      "Epoch 1254 Loss 28.719093\n",
      "Params: tensor([ 2.3358, -0.1450])\n",
      "Grad:  tensor([-0.5525,  2.9084])\n",
      "Epoch 1255 Loss 28.718229\n",
      "Params: tensor([ 2.3358, -0.1453])\n",
      "Grad:  tensor([-0.5522,  2.9084])\n",
      "Epoch 1256 Loss 28.717340\n",
      "Params: tensor([ 2.3359, -0.1456])\n",
      "Grad:  tensor([-0.5520,  2.9084])\n",
      "Epoch 1257 Loss 28.716469\n",
      "Params: tensor([ 2.3359, -0.1459])\n",
      "Grad:  tensor([-0.5518,  2.9084])\n",
      "Epoch 1258 Loss 28.715593\n",
      "Params: tensor([ 2.3360, -0.1462])\n",
      "Grad:  tensor([-0.5515,  2.9084])\n",
      "Epoch 1259 Loss 28.714720\n",
      "Params: tensor([ 2.3360, -0.1465])\n",
      "Grad:  tensor([-0.5513,  2.9084])\n",
      "Epoch 1260 Loss 28.713839\n",
      "Params: tensor([ 2.3361, -0.1468])\n",
      "Grad:  tensor([-0.5511,  2.9084])\n",
      "Epoch 1261 Loss 28.712963\n",
      "Params: tensor([ 2.3361, -0.1471])\n",
      "Grad:  tensor([-0.5509,  2.9084])\n",
      "Epoch 1262 Loss 28.712084\n",
      "Params: tensor([ 2.3362, -0.1474])\n",
      "Grad:  tensor([-0.5506,  2.9083])\n",
      "Epoch 1263 Loss 28.711206\n",
      "Params: tensor([ 2.3363, -0.1477])\n",
      "Grad:  tensor([-0.5504,  2.9083])\n",
      "Epoch 1264 Loss 28.710333\n",
      "Params: tensor([ 2.3363, -0.1479])\n",
      "Grad:  tensor([-0.5502,  2.9083])\n",
      "Epoch 1265 Loss 28.709455\n",
      "Params: tensor([ 2.3364, -0.1482])\n",
      "Grad:  tensor([-0.5499,  2.9083])\n",
      "Epoch 1266 Loss 28.708580\n",
      "Params: tensor([ 2.3364, -0.1485])\n",
      "Grad:  tensor([-0.5497,  2.9083])\n",
      "Epoch 1267 Loss 28.707705\n",
      "Params: tensor([ 2.3365, -0.1488])\n",
      "Grad:  tensor([-0.5495,  2.9083])\n",
      "Epoch 1268 Loss 28.706829\n",
      "Params: tensor([ 2.3365, -0.1491])\n",
      "Grad:  tensor([-0.5493,  2.9083])\n",
      "Epoch 1269 Loss 28.705950\n",
      "Params: tensor([ 2.3366, -0.1494])\n",
      "Grad:  tensor([-0.5491,  2.9083])\n",
      "Epoch 1270 Loss 28.705078\n",
      "Params: tensor([ 2.3366, -0.1497])\n",
      "Grad:  tensor([-0.5489,  2.9082])\n",
      "Epoch 1271 Loss 28.704199\n",
      "Params: tensor([ 2.3367, -0.1500])\n",
      "Grad:  tensor([-0.5486,  2.9082])\n",
      "Epoch 1272 Loss 28.703325\n",
      "Params: tensor([ 2.3368, -0.1503])\n",
      "Grad:  tensor([-0.5484,  2.9082])\n",
      "Epoch 1273 Loss 28.702448\n",
      "Params: tensor([ 2.3368, -0.1506])\n",
      "Grad:  tensor([-0.5482,  2.9082])\n",
      "Epoch 1274 Loss 28.701571\n",
      "Params: tensor([ 2.3369, -0.1509])\n",
      "Grad:  tensor([-0.5480,  2.9082])\n",
      "Epoch 1275 Loss 28.700697\n",
      "Params: tensor([ 2.3369, -0.1511])\n",
      "Grad:  tensor([-0.5478,  2.9082])\n",
      "Epoch 1276 Loss 28.699823\n",
      "Params: tensor([ 2.3370, -0.1514])\n",
      "Grad:  tensor([-0.5476,  2.9082])\n",
      "Epoch 1277 Loss 28.698946\n",
      "Params: tensor([ 2.3370, -0.1517])\n",
      "Grad:  tensor([-0.5474,  2.9082])\n",
      "Epoch 1278 Loss 28.698072\n",
      "Params: tensor([ 2.3371, -0.1520])\n",
      "Grad:  tensor([-0.5472,  2.9081])\n",
      "Epoch 1279 Loss 28.697193\n",
      "Params: tensor([ 2.3371, -0.1523])\n",
      "Grad:  tensor([-0.5470,  2.9081])\n",
      "Epoch 1280 Loss 28.696321\n",
      "Params: tensor([ 2.3372, -0.1526])\n",
      "Grad:  tensor([-0.5468,  2.9081])\n",
      "Epoch 1281 Loss 28.695442\n",
      "Params: tensor([ 2.3372, -0.1529])\n",
      "Grad:  tensor([-0.5466,  2.9081])\n",
      "Epoch 1282 Loss 28.694563\n",
      "Params: tensor([ 2.3373, -0.1532])\n",
      "Grad:  tensor([-0.5464,  2.9081])\n",
      "Epoch 1283 Loss 28.693689\n",
      "Params: tensor([ 2.3374, -0.1535])\n",
      "Grad:  tensor([-0.5462,  2.9081])\n",
      "Epoch 1284 Loss 28.692810\n",
      "Params: tensor([ 2.3374, -0.1538])\n",
      "Grad:  tensor([-0.5460,  2.9080])\n",
      "Epoch 1285 Loss 28.691942\n",
      "Params: tensor([ 2.3375, -0.1541])\n",
      "Grad:  tensor([-0.5458,  2.9080])\n",
      "Epoch 1286 Loss 28.691065\n",
      "Params: tensor([ 2.3375, -0.1543])\n",
      "Grad:  tensor([-0.5456,  2.9080])\n",
      "Epoch 1287 Loss 28.690191\n",
      "Params: tensor([ 2.3376, -0.1546])\n",
      "Grad:  tensor([-0.5454,  2.9080])\n",
      "Epoch 1288 Loss 28.689312\n",
      "Params: tensor([ 2.3376, -0.1549])\n",
      "Grad:  tensor([-0.5452,  2.9080])\n",
      "Epoch 1289 Loss 28.688444\n",
      "Params: tensor([ 2.3377, -0.1552])\n",
      "Grad:  tensor([-0.5450,  2.9080])\n",
      "Epoch 1290 Loss 28.687563\n",
      "Params: tensor([ 2.3377, -0.1555])\n",
      "Grad:  tensor([-0.5448,  2.9079])\n",
      "Epoch 1291 Loss 28.686693\n",
      "Params: tensor([ 2.3378, -0.1558])\n",
      "Grad:  tensor([-0.5446,  2.9079])\n",
      "Epoch 1292 Loss 28.685814\n",
      "Params: tensor([ 2.3378, -0.1561])\n",
      "Grad:  tensor([-0.5444,  2.9079])\n",
      "Epoch 1293 Loss 28.684938\n",
      "Params: tensor([ 2.3379, -0.1564])\n",
      "Grad:  tensor([-0.5442,  2.9079])\n",
      "Epoch 1294 Loss 28.684069\n",
      "Params: tensor([ 2.3380, -0.1567])\n",
      "Grad:  tensor([-0.5440,  2.9079])\n",
      "Epoch 1295 Loss 28.683189\n",
      "Params: tensor([ 2.3380, -0.1570])\n",
      "Grad:  tensor([-0.5439,  2.9079])\n",
      "Epoch 1296 Loss 28.682312\n",
      "Params: tensor([ 2.3381, -0.1572])\n",
      "Grad:  tensor([-0.5437,  2.9078])\n",
      "Epoch 1297 Loss 28.681435\n",
      "Params: tensor([ 2.3381, -0.1575])\n",
      "Grad:  tensor([-0.5435,  2.9078])\n",
      "Epoch 1298 Loss 28.680565\n",
      "Params: tensor([ 2.3382, -0.1578])\n",
      "Grad:  tensor([-0.5433,  2.9078])\n",
      "Epoch 1299 Loss 28.679686\n",
      "Params: tensor([ 2.3382, -0.1581])\n",
      "Grad:  tensor([-0.5431,  2.9078])\n",
      "Epoch 1300 Loss 28.678814\n",
      "Params: tensor([ 2.3383, -0.1584])\n",
      "Grad:  tensor([-0.5429,  2.9078])\n",
      "Epoch 1301 Loss 28.677940\n",
      "Params: tensor([ 2.3383, -0.1587])\n",
      "Grad:  tensor([-0.5427,  2.9077])\n",
      "Epoch 1302 Loss 28.677065\n",
      "Params: tensor([ 2.3384, -0.1590])\n",
      "Grad:  tensor([-0.5426,  2.9077])\n",
      "Epoch 1303 Loss 28.676189\n",
      "Params: tensor([ 2.3384, -0.1593])\n",
      "Grad:  tensor([-0.5424,  2.9077])\n",
      "Epoch 1304 Loss 28.675316\n",
      "Params: tensor([ 2.3385, -0.1596])\n",
      "Grad:  tensor([-0.5422,  2.9077])\n",
      "Epoch 1305 Loss 28.674440\n",
      "Params: tensor([ 2.3386, -0.1599])\n",
      "Grad:  tensor([-0.5420,  2.9077])\n",
      "Epoch 1306 Loss 28.673561\n",
      "Params: tensor([ 2.3386, -0.1602])\n",
      "Grad:  tensor([-0.5419,  2.9076])\n",
      "Epoch 1307 Loss 28.672686\n",
      "Params: tensor([ 2.3387, -0.1604])\n",
      "Grad:  tensor([-0.5417,  2.9076])\n",
      "Epoch 1308 Loss 28.671812\n",
      "Params: tensor([ 2.3387, -0.1607])\n",
      "Grad:  tensor([-0.5415,  2.9076])\n",
      "Epoch 1309 Loss 28.670942\n",
      "Params: tensor([ 2.3388, -0.1610])\n",
      "Grad:  tensor([-0.5414,  2.9076])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1310 Loss 28.670067\n",
      "Params: tensor([ 2.3388, -0.1613])\n",
      "Grad:  tensor([-0.5412,  2.9076])\n",
      "Epoch 1311 Loss 28.669191\n",
      "Params: tensor([ 2.3389, -0.1616])\n",
      "Grad:  tensor([-0.5410,  2.9075])\n",
      "Epoch 1312 Loss 28.668316\n",
      "Params: tensor([ 2.3389, -0.1619])\n",
      "Grad:  tensor([-0.5408,  2.9075])\n",
      "Epoch 1313 Loss 28.667442\n",
      "Params: tensor([ 2.3390, -0.1622])\n",
      "Grad:  tensor([-0.5407,  2.9075])\n",
      "Epoch 1314 Loss 28.666571\n",
      "Params: tensor([ 2.3390, -0.1625])\n",
      "Grad:  tensor([-0.5405,  2.9075])\n",
      "Epoch 1315 Loss 28.665688\n",
      "Params: tensor([ 2.3391, -0.1628])\n",
      "Grad:  tensor([-0.5403,  2.9075])\n",
      "Epoch 1316 Loss 28.664818\n",
      "Params: tensor([ 2.3391, -0.1631])\n",
      "Grad:  tensor([-0.5402,  2.9074])\n",
      "Epoch 1317 Loss 28.663939\n",
      "Params: tensor([ 2.3392, -0.1634])\n",
      "Grad:  tensor([-0.5400,  2.9074])\n",
      "Epoch 1318 Loss 28.663067\n",
      "Params: tensor([ 2.3393, -0.1636])\n",
      "Grad:  tensor([-0.5398,  2.9074])\n",
      "Epoch 1319 Loss 28.662193\n",
      "Params: tensor([ 2.3393, -0.1639])\n",
      "Grad:  tensor([-0.5397,  2.9074])\n",
      "Epoch 1320 Loss 28.661322\n",
      "Params: tensor([ 2.3394, -0.1642])\n",
      "Grad:  tensor([-0.5395,  2.9073])\n",
      "Epoch 1321 Loss 28.660444\n",
      "Params: tensor([ 2.3394, -0.1645])\n",
      "Grad:  tensor([-0.5394,  2.9073])\n",
      "Epoch 1322 Loss 28.659571\n",
      "Params: tensor([ 2.3395, -0.1648])\n",
      "Grad:  tensor([-0.5392,  2.9073])\n",
      "Epoch 1323 Loss 28.658697\n",
      "Params: tensor([ 2.3395, -0.1651])\n",
      "Grad:  tensor([-0.5390,  2.9073])\n",
      "Epoch 1324 Loss 28.657820\n",
      "Params: tensor([ 2.3396, -0.1654])\n",
      "Grad:  tensor([-0.5389,  2.9073])\n",
      "Epoch 1325 Loss 28.656952\n",
      "Params: tensor([ 2.3396, -0.1657])\n",
      "Grad:  tensor([-0.5387,  2.9072])\n",
      "Epoch 1326 Loss 28.656073\n",
      "Params: tensor([ 2.3397, -0.1660])\n",
      "Grad:  tensor([-0.5386,  2.9072])\n",
      "Epoch 1327 Loss 28.655201\n",
      "Params: tensor([ 2.3397, -0.1663])\n",
      "Grad:  tensor([-0.5384,  2.9072])\n",
      "Epoch 1328 Loss 28.654325\n",
      "Params: tensor([ 2.3398, -0.1666])\n",
      "Grad:  tensor([-0.5383,  2.9072])\n",
      "Epoch 1329 Loss 28.653448\n",
      "Params: tensor([ 2.3398, -0.1668])\n",
      "Grad:  tensor([-0.5381,  2.9071])\n",
      "Epoch 1330 Loss 28.652580\n",
      "Params: tensor([ 2.3399, -0.1671])\n",
      "Grad:  tensor([-0.5379,  2.9071])\n",
      "Epoch 1331 Loss 28.651703\n",
      "Params: tensor([ 2.3400, -0.1674])\n",
      "Grad:  tensor([-0.5378,  2.9071])\n",
      "Epoch 1332 Loss 28.650831\n",
      "Params: tensor([ 2.3400, -0.1677])\n",
      "Grad:  tensor([-0.5376,  2.9071])\n",
      "Epoch 1333 Loss 28.649958\n",
      "Params: tensor([ 2.3401, -0.1680])\n",
      "Grad:  tensor([-0.5375,  2.9070])\n",
      "Epoch 1334 Loss 28.649078\n",
      "Params: tensor([ 2.3401, -0.1683])\n",
      "Grad:  tensor([-0.5373,  2.9070])\n",
      "Epoch 1335 Loss 28.648205\n",
      "Params: tensor([ 2.3402, -0.1686])\n",
      "Grad:  tensor([-0.5372,  2.9070])\n",
      "Epoch 1336 Loss 28.647331\n",
      "Params: tensor([ 2.3402, -0.1689])\n",
      "Grad:  tensor([-0.5371,  2.9070])\n",
      "Epoch 1337 Loss 28.646460\n",
      "Params: tensor([ 2.3403, -0.1692])\n",
      "Grad:  tensor([-0.5369,  2.9069])\n",
      "Epoch 1338 Loss 28.645588\n",
      "Params: tensor([ 2.3403, -0.1695])\n",
      "Grad:  tensor([-0.5368,  2.9069])\n",
      "Epoch 1339 Loss 28.644714\n",
      "Params: tensor([ 2.3404, -0.1698])\n",
      "Grad:  tensor([-0.5366,  2.9069])\n",
      "Epoch 1340 Loss 28.643837\n",
      "Params: tensor([ 2.3404, -0.1700])\n",
      "Grad:  tensor([-0.5365,  2.9069])\n",
      "Epoch 1341 Loss 28.642963\n",
      "Params: tensor([ 2.3405, -0.1703])\n",
      "Grad:  tensor([-0.5363,  2.9068])\n",
      "Epoch 1342 Loss 28.642090\n",
      "Params: tensor([ 2.3405, -0.1706])\n",
      "Grad:  tensor([-0.5362,  2.9068])\n",
      "Epoch 1343 Loss 28.641216\n",
      "Params: tensor([ 2.3406, -0.1709])\n",
      "Grad:  tensor([-0.5360,  2.9068])\n",
      "Epoch 1344 Loss 28.640345\n",
      "Params: tensor([ 2.3407, -0.1712])\n",
      "Grad:  tensor([-0.5359,  2.9068])\n",
      "Epoch 1345 Loss 28.639465\n",
      "Params: tensor([ 2.3407, -0.1715])\n",
      "Grad:  tensor([-0.5358,  2.9067])\n",
      "Epoch 1346 Loss 28.638594\n",
      "Params: tensor([ 2.3408, -0.1718])\n",
      "Grad:  tensor([-0.5356,  2.9067])\n",
      "Epoch 1347 Loss 28.637724\n",
      "Params: tensor([ 2.3408, -0.1721])\n",
      "Grad:  tensor([-0.5355,  2.9067])\n",
      "Epoch 1348 Loss 28.636848\n",
      "Params: tensor([ 2.3409, -0.1724])\n",
      "Grad:  tensor([-0.5353,  2.9067])\n",
      "Epoch 1349 Loss 28.635975\n",
      "Params: tensor([ 2.3409, -0.1727])\n",
      "Grad:  tensor([-0.5352,  2.9066])\n",
      "Epoch 1350 Loss 28.635103\n",
      "Params: tensor([ 2.3410, -0.1729])\n",
      "Grad:  tensor([-0.5351,  2.9066])\n",
      "Epoch 1351 Loss 28.634222\n",
      "Params: tensor([ 2.3410, -0.1732])\n",
      "Grad:  tensor([-0.5349,  2.9066])\n",
      "Epoch 1352 Loss 28.633356\n",
      "Params: tensor([ 2.3411, -0.1735])\n",
      "Grad:  tensor([-0.5348,  2.9065])\n",
      "Epoch 1353 Loss 28.632479\n",
      "Params: tensor([ 2.3411, -0.1738])\n",
      "Grad:  tensor([-0.5347,  2.9065])\n",
      "Epoch 1354 Loss 28.631609\n",
      "Params: tensor([ 2.3412, -0.1741])\n",
      "Grad:  tensor([-0.5345,  2.9065])\n",
      "Epoch 1355 Loss 28.630735\n",
      "Params: tensor([ 2.3412, -0.1744])\n",
      "Grad:  tensor([-0.5344,  2.9065])\n",
      "Epoch 1356 Loss 28.629858\n",
      "Params: tensor([ 2.3413, -0.1747])\n",
      "Grad:  tensor([-0.5343,  2.9064])\n",
      "Epoch 1357 Loss 28.628992\n",
      "Params: tensor([ 2.3413, -0.1750])\n",
      "Grad:  tensor([-0.5341,  2.9064])\n",
      "Epoch 1358 Loss 28.628113\n",
      "Params: tensor([ 2.3414, -0.1753])\n",
      "Grad:  tensor([-0.5340,  2.9064])\n",
      "Epoch 1359 Loss 28.627235\n",
      "Params: tensor([ 2.3415, -0.1756])\n",
      "Grad:  tensor([-0.5339,  2.9064])\n",
      "Epoch 1360 Loss 28.626371\n",
      "Params: tensor([ 2.3415, -0.1759])\n",
      "Grad:  tensor([-0.5338,  2.9063])\n",
      "Epoch 1361 Loss 28.625494\n",
      "Params: tensor([ 2.3416, -0.1761])\n",
      "Grad:  tensor([-0.5336,  2.9063])\n",
      "Epoch 1362 Loss 28.624620\n",
      "Params: tensor([ 2.3416, -0.1764])\n",
      "Grad:  tensor([-0.5335,  2.9063])\n",
      "Epoch 1363 Loss 28.623751\n",
      "Params: tensor([ 2.3417, -0.1767])\n",
      "Grad:  tensor([-0.5334,  2.9062])\n",
      "Epoch 1364 Loss 28.622877\n",
      "Params: tensor([ 2.3417, -0.1770])\n",
      "Grad:  tensor([-0.5332,  2.9062])\n",
      "Epoch 1365 Loss 28.622007\n",
      "Params: tensor([ 2.3418, -0.1773])\n",
      "Grad:  tensor([-0.5331,  2.9062])\n",
      "Epoch 1366 Loss 28.621132\n",
      "Params: tensor([ 2.3418, -0.1776])\n",
      "Grad:  tensor([-0.5330,  2.9062])\n",
      "Epoch 1367 Loss 28.620256\n",
      "Params: tensor([ 2.3419, -0.1779])\n",
      "Grad:  tensor([-0.5328,  2.9061])\n",
      "Epoch 1368 Loss 28.619383\n",
      "Params: tensor([ 2.3419, -0.1782])\n",
      "Grad:  tensor([-0.5327,  2.9061])\n",
      "Epoch 1369 Loss 28.618511\n",
      "Params: tensor([ 2.3420, -0.1785])\n",
      "Grad:  tensor([-0.5326,  2.9061])\n",
      "Epoch 1370 Loss 28.617638\n",
      "Params: tensor([ 2.3420, -0.1788])\n",
      "Grad:  tensor([-0.5325,  2.9060])\n",
      "Epoch 1371 Loss 28.616766\n",
      "Params: tensor([ 2.3421, -0.1791])\n",
      "Grad:  tensor([-0.5324,  2.9060])\n",
      "Epoch 1372 Loss 28.615894\n",
      "Params: tensor([ 2.3421, -0.1793])\n",
      "Grad:  tensor([-0.5323,  2.9060])\n",
      "Epoch 1373 Loss 28.615019\n",
      "Params: tensor([ 2.3422, -0.1796])\n",
      "Grad:  tensor([-0.5321,  2.9059])\n",
      "Epoch 1374 Loss 28.614149\n",
      "Params: tensor([ 2.3423, -0.1799])\n",
      "Grad:  tensor([-0.5320,  2.9059])\n",
      "Epoch 1375 Loss 28.613279\n",
      "Params: tensor([ 2.3423, -0.1802])\n",
      "Grad:  tensor([-0.5319,  2.9059])\n",
      "Epoch 1376 Loss 28.612404\n",
      "Params: tensor([ 2.3424, -0.1805])\n",
      "Grad:  tensor([-0.5318,  2.9059])\n",
      "Epoch 1377 Loss 28.611528\n",
      "Params: tensor([ 2.3424, -0.1808])\n",
      "Grad:  tensor([-0.5317,  2.9058])\n",
      "Epoch 1378 Loss 28.610659\n",
      "Params: tensor([ 2.3425, -0.1811])\n",
      "Grad:  tensor([-0.5316,  2.9058])\n",
      "Epoch 1379 Loss 28.609789\n",
      "Params: tensor([ 2.3425, -0.1814])\n",
      "Grad:  tensor([-0.5314,  2.9058])\n",
      "Epoch 1380 Loss 28.608910\n",
      "Params: tensor([ 2.3426, -0.1817])\n",
      "Grad:  tensor([-0.5313,  2.9057])\n",
      "Epoch 1381 Loss 28.608034\n",
      "Params: tensor([ 2.3426, -0.1820])\n",
      "Grad:  tensor([-0.5312,  2.9057])\n",
      "Epoch 1382 Loss 28.607166\n",
      "Params: tensor([ 2.3427, -0.1822])\n",
      "Grad:  tensor([-0.5311,  2.9057])\n",
      "Epoch 1383 Loss 28.606298\n",
      "Params: tensor([ 2.3427, -0.1825])\n",
      "Grad:  tensor([-0.5310,  2.9056])\n",
      "Epoch 1384 Loss 28.605421\n",
      "Params: tensor([ 2.3428, -0.1828])\n",
      "Grad:  tensor([-0.5309,  2.9056])\n",
      "Epoch 1385 Loss 28.604553\n",
      "Params: tensor([ 2.3428, -0.1831])\n",
      "Grad:  tensor([-0.5307,  2.9056])\n",
      "Epoch 1386 Loss 28.603676\n",
      "Params: tensor([ 2.3429, -0.1834])\n",
      "Grad:  tensor([-0.5306,  2.9055])\n",
      "Epoch 1387 Loss 28.602806\n",
      "Params: tensor([ 2.3429, -0.1837])\n",
      "Grad:  tensor([-0.5305,  2.9055])\n",
      "Epoch 1388 Loss 28.601934\n",
      "Params: tensor([ 2.3430, -0.1840])\n",
      "Grad:  tensor([-0.5304,  2.9055])\n",
      "Epoch 1389 Loss 28.601057\n",
      "Params: tensor([ 2.3431, -0.1843])\n",
      "Grad:  tensor([-0.5303,  2.9055])\n",
      "Epoch 1390 Loss 28.600189\n",
      "Params: tensor([ 2.3431, -0.1846])\n",
      "Grad:  tensor([-0.5302,  2.9054])\n",
      "Epoch 1391 Loss 28.599316\n",
      "Params: tensor([ 2.3432, -0.1849])\n",
      "Grad:  tensor([-0.5301,  2.9054])\n",
      "Epoch 1392 Loss 28.598438\n",
      "Params: tensor([ 2.3432, -0.1852])\n",
      "Grad:  tensor([-0.5300,  2.9054])\n",
      "Epoch 1393 Loss 28.597567\n",
      "Params: tensor([ 2.3433, -0.1854])\n",
      "Grad:  tensor([-0.5299,  2.9053])\n",
      "Epoch 1394 Loss 28.596703\n",
      "Params: tensor([ 2.3433, -0.1857])\n",
      "Grad:  tensor([-0.5298,  2.9053])\n",
      "Epoch 1395 Loss 28.595827\n",
      "Params: tensor([ 2.3434, -0.1860])\n",
      "Grad:  tensor([-0.5297,  2.9053])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1396 Loss 28.594954\n",
      "Params: tensor([ 2.3434, -0.1863])\n",
      "Grad:  tensor([-0.5296,  2.9052])\n",
      "Epoch 1397 Loss 28.594084\n",
      "Params: tensor([ 2.3435, -0.1866])\n",
      "Grad:  tensor([-0.5295,  2.9052])\n",
      "Epoch 1398 Loss 28.593206\n",
      "Params: tensor([ 2.3435, -0.1869])\n",
      "Grad:  tensor([-0.5294,  2.9052])\n",
      "Epoch 1399 Loss 28.592335\n",
      "Params: tensor([ 2.3436, -0.1872])\n",
      "Grad:  tensor([-0.5293,  2.9051])\n",
      "Epoch 1400 Loss 28.591463\n",
      "Params: tensor([ 2.3436, -0.1875])\n",
      "Grad:  tensor([-0.5291,  2.9051])\n",
      "Epoch 1401 Loss 28.590599\n",
      "Params: tensor([ 2.3437, -0.1878])\n",
      "Grad:  tensor([-0.5291,  2.9051])\n",
      "Epoch 1402 Loss 28.589722\n",
      "Params: tensor([ 2.3437, -0.1881])\n",
      "Grad:  tensor([-0.5289,  2.9050])\n",
      "Epoch 1403 Loss 28.588848\n",
      "Params: tensor([ 2.3438, -0.1883])\n",
      "Grad:  tensor([-0.5288,  2.9050])\n",
      "Epoch 1404 Loss 28.587982\n",
      "Params: tensor([ 2.3438, -0.1886])\n",
      "Grad:  tensor([-0.5287,  2.9050])\n",
      "Epoch 1405 Loss 28.587105\n",
      "Params: tensor([ 2.3439, -0.1889])\n",
      "Grad:  tensor([-0.5286,  2.9049])\n",
      "Epoch 1406 Loss 28.586231\n",
      "Params: tensor([ 2.3440, -0.1892])\n",
      "Grad:  tensor([-0.5285,  2.9049])\n",
      "Epoch 1407 Loss 28.585365\n",
      "Params: tensor([ 2.3440, -0.1895])\n",
      "Grad:  tensor([-0.5284,  2.9049])\n",
      "Epoch 1408 Loss 28.584486\n",
      "Params: tensor([ 2.3441, -0.1898])\n",
      "Grad:  tensor([-0.5283,  2.9048])\n",
      "Epoch 1409 Loss 28.583618\n",
      "Params: tensor([ 2.3441, -0.1901])\n",
      "Grad:  tensor([-0.5282,  2.9048])\n",
      "Epoch 1410 Loss 28.582750\n",
      "Params: tensor([ 2.3442, -0.1904])\n",
      "Grad:  tensor([-0.5281,  2.9048])\n",
      "Epoch 1411 Loss 28.581877\n",
      "Params: tensor([ 2.3442, -0.1907])\n",
      "Grad:  tensor([-0.5280,  2.9047])\n",
      "Epoch 1412 Loss 28.581005\n",
      "Params: tensor([ 2.3443, -0.1910])\n",
      "Grad:  tensor([-0.5279,  2.9047])\n",
      "Epoch 1413 Loss 28.580132\n",
      "Params: tensor([ 2.3443, -0.1913])\n",
      "Grad:  tensor([-0.5278,  2.9047])\n",
      "Epoch 1414 Loss 28.579260\n",
      "Params: tensor([ 2.3444, -0.1915])\n",
      "Grad:  tensor([-0.5277,  2.9046])\n",
      "Epoch 1415 Loss 28.578388\n",
      "Params: tensor([ 2.3444, -0.1918])\n",
      "Grad:  tensor([-0.5277,  2.9046])\n",
      "Epoch 1416 Loss 28.577517\n",
      "Params: tensor([ 2.3445, -0.1921])\n",
      "Grad:  tensor([-0.5276,  2.9046])\n",
      "Epoch 1417 Loss 28.576647\n",
      "Params: tensor([ 2.3445, -0.1924])\n",
      "Grad:  tensor([-0.5275,  2.9045])\n",
      "Epoch 1418 Loss 28.575777\n",
      "Params: tensor([ 2.3446, -0.1927])\n",
      "Grad:  tensor([-0.5274,  2.9045])\n",
      "Epoch 1419 Loss 28.574903\n",
      "Params: tensor([ 2.3446, -0.1930])\n",
      "Grad:  tensor([-0.5273,  2.9045])\n",
      "Epoch 1420 Loss 28.574030\n",
      "Params: tensor([ 2.3447, -0.1933])\n",
      "Grad:  tensor([-0.5272,  2.9044])\n",
      "Epoch 1421 Loss 28.573162\n",
      "Params: tensor([ 2.3447, -0.1936])\n",
      "Grad:  tensor([-0.5271,  2.9044])\n",
      "Epoch 1422 Loss 28.572290\n",
      "Params: tensor([ 2.3448, -0.1939])\n",
      "Grad:  tensor([-0.5270,  2.9044])\n",
      "Epoch 1423 Loss 28.571419\n",
      "Params: tensor([ 2.3448, -0.1942])\n",
      "Grad:  tensor([-0.5269,  2.9043])\n",
      "Epoch 1424 Loss 28.570543\n",
      "Params: tensor([ 2.3449, -0.1944])\n",
      "Grad:  tensor([-0.5268,  2.9043])\n",
      "Epoch 1425 Loss 28.569674\n",
      "Params: tensor([ 2.3450, -0.1947])\n",
      "Grad:  tensor([-0.5267,  2.9042])\n",
      "Epoch 1426 Loss 28.568804\n",
      "Params: tensor([ 2.3450, -0.1950])\n",
      "Grad:  tensor([-0.5267,  2.9042])\n",
      "Epoch 1427 Loss 28.567934\n",
      "Params: tensor([ 2.3451, -0.1953])\n",
      "Grad:  tensor([-0.5266,  2.9042])\n",
      "Epoch 1428 Loss 28.567066\n",
      "Params: tensor([ 2.3451, -0.1956])\n",
      "Grad:  tensor([-0.5265,  2.9041])\n",
      "Epoch 1429 Loss 28.566187\n",
      "Params: tensor([ 2.3452, -0.1959])\n",
      "Grad:  tensor([-0.5264,  2.9041])\n",
      "Epoch 1430 Loss 28.565321\n",
      "Params: tensor([ 2.3452, -0.1962])\n",
      "Grad:  tensor([-0.5263,  2.9041])\n",
      "Epoch 1431 Loss 28.564451\n",
      "Params: tensor([ 2.3453, -0.1965])\n",
      "Grad:  tensor([-0.5262,  2.9040])\n",
      "Epoch 1432 Loss 28.563585\n",
      "Params: tensor([ 2.3453, -0.1968])\n",
      "Grad:  tensor([-0.5261,  2.9040])\n",
      "Epoch 1433 Loss 28.562702\n",
      "Params: tensor([ 2.3454, -0.1971])\n",
      "Grad:  tensor([-0.5260,  2.9040])\n",
      "Epoch 1434 Loss 28.561836\n",
      "Params: tensor([ 2.3454, -0.1974])\n",
      "Grad:  tensor([-0.5259,  2.9039])\n",
      "Epoch 1435 Loss 28.560966\n",
      "Params: tensor([ 2.3455, -0.1976])\n",
      "Grad:  tensor([-0.5259,  2.9039])\n",
      "Epoch 1436 Loss 28.560101\n",
      "Params: tensor([ 2.3455, -0.1979])\n",
      "Grad:  tensor([-0.5258,  2.9039])\n",
      "Epoch 1437 Loss 28.559221\n",
      "Params: tensor([ 2.3456, -0.1982])\n",
      "Grad:  tensor([-0.5257,  2.9038])\n",
      "Epoch 1438 Loss 28.558352\n",
      "Params: tensor([ 2.3456, -0.1985])\n",
      "Grad:  tensor([-0.5256,  2.9038])\n",
      "Epoch 1439 Loss 28.557482\n",
      "Params: tensor([ 2.3457, -0.1988])\n",
      "Grad:  tensor([-0.5255,  2.9038])\n",
      "Epoch 1440 Loss 28.556612\n",
      "Params: tensor([ 2.3457, -0.1991])\n",
      "Grad:  tensor([-0.5254,  2.9037])\n",
      "Epoch 1441 Loss 28.555738\n",
      "Params: tensor([ 2.3458, -0.1994])\n",
      "Grad:  tensor([-0.5254,  2.9037])\n",
      "Epoch 1442 Loss 28.554869\n",
      "Params: tensor([ 2.3458, -0.1997])\n",
      "Grad:  tensor([-0.5253,  2.9036])\n",
      "Epoch 1443 Loss 28.553999\n",
      "Params: tensor([ 2.3459, -0.2000])\n",
      "Grad:  tensor([-0.5252,  2.9036])\n",
      "Epoch 1444 Loss 28.553123\n",
      "Params: tensor([ 2.3460, -0.2003])\n",
      "Grad:  tensor([-0.5251,  2.9036])\n",
      "Epoch 1445 Loss 28.552259\n",
      "Params: tensor([ 2.3460, -0.2005])\n",
      "Grad:  tensor([-0.5251,  2.9035])\n",
      "Epoch 1446 Loss 28.551386\n",
      "Params: tensor([ 2.3461, -0.2008])\n",
      "Grad:  tensor([-0.5250,  2.9035])\n",
      "Epoch 1447 Loss 28.550518\n",
      "Params: tensor([ 2.3461, -0.2011])\n",
      "Grad:  tensor([-0.5249,  2.9035])\n",
      "Epoch 1448 Loss 28.549644\n",
      "Params: tensor([ 2.3462, -0.2014])\n",
      "Grad:  tensor([-0.5248,  2.9034])\n",
      "Epoch 1449 Loss 28.548779\n",
      "Params: tensor([ 2.3462, -0.2017])\n",
      "Grad:  tensor([-0.5247,  2.9034])\n",
      "Epoch 1450 Loss 28.547907\n",
      "Params: tensor([ 2.3463, -0.2020])\n",
      "Grad:  tensor([-0.5247,  2.9033])\n",
      "Epoch 1451 Loss 28.547033\n",
      "Params: tensor([ 2.3463, -0.2023])\n",
      "Grad:  tensor([-0.5246,  2.9033])\n",
      "Epoch 1452 Loss 28.546165\n",
      "Params: tensor([ 2.3464, -0.2026])\n",
      "Grad:  tensor([-0.5245,  2.9033])\n",
      "Epoch 1453 Loss 28.545294\n",
      "Params: tensor([ 2.3464, -0.2029])\n",
      "Grad:  tensor([-0.5244,  2.9032])\n",
      "Epoch 1454 Loss 28.544426\n",
      "Params: tensor([ 2.3465, -0.2032])\n",
      "Grad:  tensor([-0.5244,  2.9032])\n",
      "Epoch 1455 Loss 28.543550\n",
      "Params: tensor([ 2.3465, -0.2035])\n",
      "Grad:  tensor([-0.5243,  2.9032])\n",
      "Epoch 1456 Loss 28.542683\n",
      "Params: tensor([ 2.3466, -0.2037])\n",
      "Grad:  tensor([-0.5242,  2.9031])\n",
      "Epoch 1457 Loss 28.541809\n",
      "Params: tensor([ 2.3466, -0.2040])\n",
      "Grad:  tensor([-0.5241,  2.9031])\n",
      "Epoch 1458 Loss 28.540947\n",
      "Params: tensor([ 2.3467, -0.2043])\n",
      "Grad:  tensor([-0.5241,  2.9030])\n",
      "Epoch 1459 Loss 28.540071\n",
      "Params: tensor([ 2.3467, -0.2046])\n",
      "Grad:  tensor([-0.5240,  2.9030])\n",
      "Epoch 1460 Loss 28.539202\n",
      "Params: tensor([ 2.3468, -0.2049])\n",
      "Grad:  tensor([-0.5239,  2.9030])\n",
      "Epoch 1461 Loss 28.538330\n",
      "Params: tensor([ 2.3468, -0.2052])\n",
      "Grad:  tensor([-0.5238,  2.9029])\n",
      "Epoch 1462 Loss 28.537458\n",
      "Params: tensor([ 2.3469, -0.2055])\n",
      "Grad:  tensor([-0.5237,  2.9029])\n",
      "Epoch 1463 Loss 28.536592\n",
      "Params: tensor([ 2.3470, -0.2058])\n",
      "Grad:  tensor([-0.5237,  2.9029])\n",
      "Epoch 1464 Loss 28.535719\n",
      "Params: tensor([ 2.3470, -0.2061])\n",
      "Grad:  tensor([-0.5236,  2.9028])\n",
      "Epoch 1465 Loss 28.534853\n",
      "Params: tensor([ 2.3471, -0.2064])\n",
      "Grad:  tensor([-0.5235,  2.9028])\n",
      "Epoch 1466 Loss 28.533983\n",
      "Params: tensor([ 2.3471, -0.2066])\n",
      "Grad:  tensor([-0.5234,  2.9027])\n",
      "Epoch 1467 Loss 28.533112\n",
      "Params: tensor([ 2.3472, -0.2069])\n",
      "Grad:  tensor([-0.5234,  2.9027])\n",
      "Epoch 1468 Loss 28.532238\n",
      "Params: tensor([ 2.3472, -0.2072])\n",
      "Grad:  tensor([-0.5233,  2.9027])\n",
      "Epoch 1469 Loss 28.531374\n",
      "Params: tensor([ 2.3473, -0.2075])\n",
      "Grad:  tensor([-0.5232,  2.9026])\n",
      "Epoch 1470 Loss 28.530500\n",
      "Params: tensor([ 2.3473, -0.2078])\n",
      "Grad:  tensor([-0.5231,  2.9026])\n",
      "Epoch 1471 Loss 28.529633\n",
      "Params: tensor([ 2.3474, -0.2081])\n",
      "Grad:  tensor([-0.5231,  2.9026])\n",
      "Epoch 1472 Loss 28.528765\n",
      "Params: tensor([ 2.3474, -0.2084])\n",
      "Grad:  tensor([-0.5230,  2.9025])\n",
      "Epoch 1473 Loss 28.527887\n",
      "Params: tensor([ 2.3475, -0.2087])\n",
      "Grad:  tensor([-0.5230,  2.9025])\n",
      "Epoch 1474 Loss 28.527021\n",
      "Params: tensor([ 2.3475, -0.2090])\n",
      "Grad:  tensor([-0.5229,  2.9024])\n",
      "Epoch 1475 Loss 28.526154\n",
      "Params: tensor([ 2.3476, -0.2093])\n",
      "Grad:  tensor([-0.5228,  2.9024])\n",
      "Epoch 1476 Loss 28.525282\n",
      "Params: tensor([ 2.3476, -0.2095])\n",
      "Grad:  tensor([-0.5228,  2.9024])\n",
      "Epoch 1477 Loss 28.524414\n",
      "Params: tensor([ 2.3477, -0.2098])\n",
      "Grad:  tensor([-0.5227,  2.9023])\n",
      "Epoch 1478 Loss 28.523542\n",
      "Params: tensor([ 2.3477, -0.2101])\n",
      "Grad:  tensor([-0.5226,  2.9023])\n",
      "Epoch 1479 Loss 28.522673\n",
      "Params: tensor([ 2.3478, -0.2104])\n",
      "Grad:  tensor([-0.5226,  2.9022])\n",
      "Epoch 1480 Loss 28.521809\n",
      "Params: tensor([ 2.3478, -0.2107])\n",
      "Grad:  tensor([-0.5225,  2.9022])\n",
      "Epoch 1481 Loss 28.520935\n",
      "Params: tensor([ 2.3479, -0.2110])\n",
      "Grad:  tensor([-0.5224,  2.9022])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1482 Loss 28.520067\n",
      "Params: tensor([ 2.3479, -0.2113])\n",
      "Grad:  tensor([-0.5224,  2.9021])\n",
      "Epoch 1483 Loss 28.519196\n",
      "Params: tensor([ 2.3480, -0.2116])\n",
      "Grad:  tensor([-0.5223,  2.9021])\n",
      "Epoch 1484 Loss 28.518324\n",
      "Params: tensor([ 2.3480, -0.2119])\n",
      "Grad:  tensor([-0.5222,  2.9020])\n",
      "Epoch 1485 Loss 28.517456\n",
      "Params: tensor([ 2.3481, -0.2122])\n",
      "Grad:  tensor([-0.5222,  2.9020])\n",
      "Epoch 1486 Loss 28.516584\n",
      "Params: tensor([ 2.3482, -0.2124])\n",
      "Grad:  tensor([-0.5221,  2.9020])\n",
      "Epoch 1487 Loss 28.515718\n",
      "Params: tensor([ 2.3482, -0.2127])\n",
      "Grad:  tensor([-0.5221,  2.9019])\n",
      "Epoch 1488 Loss 28.514849\n",
      "Params: tensor([ 2.3483, -0.2130])\n",
      "Grad:  tensor([-0.5220,  2.9019])\n",
      "Epoch 1489 Loss 28.513977\n",
      "Params: tensor([ 2.3483, -0.2133])\n",
      "Grad:  tensor([-0.5219,  2.9018])\n",
      "Epoch 1490 Loss 28.513109\n",
      "Params: tensor([ 2.3484, -0.2136])\n",
      "Grad:  tensor([-0.5219,  2.9018])\n",
      "Epoch 1491 Loss 28.512239\n",
      "Params: tensor([ 2.3484, -0.2139])\n",
      "Grad:  tensor([-0.5218,  2.9018])\n",
      "Epoch 1492 Loss 28.511372\n",
      "Params: tensor([ 2.3485, -0.2142])\n",
      "Grad:  tensor([-0.5217,  2.9017])\n",
      "Epoch 1493 Loss 28.510498\n",
      "Params: tensor([ 2.3485, -0.2145])\n",
      "Grad:  tensor([-0.5217,  2.9017])\n",
      "Epoch 1494 Loss 28.509636\n",
      "Params: tensor([ 2.3486, -0.2148])\n",
      "Grad:  tensor([-0.5216,  2.9016])\n",
      "Epoch 1495 Loss 28.508764\n",
      "Params: tensor([ 2.3486, -0.2151])\n",
      "Grad:  tensor([-0.5215,  2.9016])\n",
      "Epoch 1496 Loss 28.507893\n",
      "Params: tensor([ 2.3487, -0.2154])\n",
      "Grad:  tensor([-0.5215,  2.9016])\n",
      "Epoch 1497 Loss 28.507027\n",
      "Params: tensor([ 2.3487, -0.2156])\n",
      "Grad:  tensor([-0.5214,  2.9015])\n",
      "Epoch 1498 Loss 28.506153\n",
      "Params: tensor([ 2.3488, -0.2159])\n",
      "Grad:  tensor([-0.5213,  2.9015])\n",
      "Epoch 1499 Loss 28.505285\n",
      "Params: tensor([ 2.3488, -0.2162])\n",
      "Grad:  tensor([-0.5213,  2.9015])\n",
      "Epoch 1500 Loss 28.504419\n",
      "Params: tensor([ 2.3489, -0.2165])\n",
      "Grad:  tensor([-0.5212,  2.9014])\n",
      "Epoch 1501 Loss 28.503546\n",
      "Params: tensor([ 2.3489, -0.2168])\n",
      "Grad:  tensor([-0.5211,  2.9014])\n",
      "Epoch 1502 Loss 28.502684\n",
      "Params: tensor([ 2.3490, -0.2171])\n",
      "Grad:  tensor([-0.5211,  2.9013])\n",
      "Epoch 1503 Loss 28.501808\n",
      "Params: tensor([ 2.3490, -0.2174])\n",
      "Grad:  tensor([-0.5210,  2.9013])\n",
      "Epoch 1504 Loss 28.500938\n",
      "Params: tensor([ 2.3491, -0.2177])\n",
      "Grad:  tensor([-0.5210,  2.9013])\n",
      "Epoch 1505 Loss 28.500074\n",
      "Params: tensor([ 2.3491, -0.2180])\n",
      "Grad:  tensor([-0.5209,  2.9012])\n",
      "Epoch 1506 Loss 28.499205\n",
      "Params: tensor([ 2.3492, -0.2183])\n",
      "Grad:  tensor([-0.5208,  2.9012])\n",
      "Epoch 1507 Loss 28.498339\n",
      "Params: tensor([ 2.3492, -0.2185])\n",
      "Grad:  tensor([-0.5208,  2.9011])\n",
      "Epoch 1508 Loss 28.497467\n",
      "Params: tensor([ 2.3493, -0.2188])\n",
      "Grad:  tensor([-0.5207,  2.9011])\n",
      "Epoch 1509 Loss 28.496595\n",
      "Params: tensor([ 2.3494, -0.2191])\n",
      "Grad:  tensor([-0.5207,  2.9010])\n",
      "Epoch 1510 Loss 28.495728\n",
      "Params: tensor([ 2.3494, -0.2194])\n",
      "Grad:  tensor([-0.5206,  2.9010])\n",
      "Epoch 1511 Loss 28.494865\n",
      "Params: tensor([ 2.3495, -0.2197])\n",
      "Grad:  tensor([-0.5206,  2.9010])\n",
      "Epoch 1512 Loss 28.493994\n",
      "Params: tensor([ 2.3495, -0.2200])\n",
      "Grad:  tensor([-0.5205,  2.9009])\n",
      "Epoch 1513 Loss 28.493126\n",
      "Params: tensor([ 2.3496, -0.2203])\n",
      "Grad:  tensor([-0.5205,  2.9009])\n",
      "Epoch 1514 Loss 28.492256\n",
      "Params: tensor([ 2.3496, -0.2206])\n",
      "Grad:  tensor([-0.5204,  2.9008])\n",
      "Epoch 1515 Loss 28.491388\n",
      "Params: tensor([ 2.3497, -0.2209])\n",
      "Grad:  tensor([-0.5204,  2.9008])\n",
      "Epoch 1516 Loss 28.490511\n",
      "Params: tensor([ 2.3497, -0.2212])\n",
      "Grad:  tensor([-0.5203,  2.9008])\n",
      "Epoch 1517 Loss 28.489653\n",
      "Params: tensor([ 2.3498, -0.2214])\n",
      "Grad:  tensor([-0.5203,  2.9007])\n",
      "Epoch 1518 Loss 28.488787\n",
      "Params: tensor([ 2.3498, -0.2217])\n",
      "Grad:  tensor([-0.5202,  2.9007])\n",
      "Epoch 1519 Loss 28.487909\n",
      "Params: tensor([ 2.3499, -0.2220])\n",
      "Grad:  tensor([-0.5202,  2.9006])\n",
      "Epoch 1520 Loss 28.487047\n",
      "Params: tensor([ 2.3499, -0.2223])\n",
      "Grad:  tensor([-0.5201,  2.9006])\n",
      "Epoch 1521 Loss 28.486174\n",
      "Params: tensor([ 2.3500, -0.2226])\n",
      "Grad:  tensor([-0.5201,  2.9005])\n",
      "Epoch 1522 Loss 28.485308\n",
      "Params: tensor([ 2.3500, -0.2229])\n",
      "Grad:  tensor([-0.5200,  2.9005])\n",
      "Epoch 1523 Loss 28.484442\n",
      "Params: tensor([ 2.3501, -0.2232])\n",
      "Grad:  tensor([-0.5200,  2.9005])\n",
      "Epoch 1524 Loss 28.483580\n",
      "Params: tensor([ 2.3501, -0.2235])\n",
      "Grad:  tensor([-0.5199,  2.9004])\n",
      "Epoch 1525 Loss 28.482702\n",
      "Params: tensor([ 2.3502, -0.2238])\n",
      "Grad:  tensor([-0.5199,  2.9004])\n",
      "Epoch 1526 Loss 28.481836\n",
      "Params: tensor([ 2.3502, -0.2241])\n",
      "Grad:  tensor([-0.5198,  2.9003])\n",
      "Epoch 1527 Loss 28.480970\n",
      "Params: tensor([ 2.3503, -0.2243])\n",
      "Grad:  tensor([-0.5198,  2.9003])\n",
      "Epoch 1528 Loss 28.480101\n",
      "Params: tensor([ 2.3503, -0.2246])\n",
      "Grad:  tensor([-0.5197,  2.9003])\n",
      "Epoch 1529 Loss 28.479229\n",
      "Params: tensor([ 2.3504, -0.2249])\n",
      "Grad:  tensor([-0.5196,  2.9002])\n",
      "Epoch 1530 Loss 28.478365\n",
      "Params: tensor([ 2.3504, -0.2252])\n",
      "Grad:  tensor([-0.5196,  2.9002])\n",
      "Epoch 1531 Loss 28.477489\n",
      "Params: tensor([ 2.3505, -0.2255])\n",
      "Grad:  tensor([-0.5195,  2.9001])\n",
      "Epoch 1532 Loss 28.476624\n",
      "Params: tensor([ 2.3505, -0.2258])\n",
      "Grad:  tensor([-0.5195,  2.9001])\n",
      "Epoch 1533 Loss 28.475758\n",
      "Params: tensor([ 2.3506, -0.2261])\n",
      "Grad:  tensor([-0.5194,  2.9000])\n",
      "Epoch 1534 Loss 28.474892\n",
      "Params: tensor([ 2.3507, -0.2264])\n",
      "Grad:  tensor([-0.5194,  2.9000])\n",
      "Epoch 1535 Loss 28.474022\n",
      "Params: tensor([ 2.3507, -0.2267])\n",
      "Grad:  tensor([-0.5193,  2.9000])\n",
      "Epoch 1536 Loss 28.473150\n",
      "Params: tensor([ 2.3508, -0.2270])\n",
      "Grad:  tensor([-0.5193,  2.8999])\n",
      "Epoch 1537 Loss 28.472282\n",
      "Params: tensor([ 2.3508, -0.2272])\n",
      "Grad:  tensor([-0.5192,  2.8999])\n",
      "Epoch 1538 Loss 28.471418\n",
      "Params: tensor([ 2.3509, -0.2275])\n",
      "Grad:  tensor([-0.5192,  2.8998])\n",
      "Epoch 1539 Loss 28.470545\n",
      "Params: tensor([ 2.3509, -0.2278])\n",
      "Grad:  tensor([-0.5191,  2.8998])\n",
      "Epoch 1540 Loss 28.469683\n",
      "Params: tensor([ 2.3510, -0.2281])\n",
      "Grad:  tensor([-0.5191,  2.8998])\n",
      "Epoch 1541 Loss 28.468817\n",
      "Params: tensor([ 2.3510, -0.2284])\n",
      "Grad:  tensor([-0.5190,  2.8997])\n",
      "Epoch 1542 Loss 28.467943\n",
      "Params: tensor([ 2.3511, -0.2287])\n",
      "Grad:  tensor([-0.5190,  2.8997])\n",
      "Epoch 1543 Loss 28.467079\n",
      "Params: tensor([ 2.3511, -0.2290])\n",
      "Grad:  tensor([-0.5189,  2.8996])\n",
      "Epoch 1544 Loss 28.466209\n",
      "Params: tensor([ 2.3512, -0.2293])\n",
      "Grad:  tensor([-0.5189,  2.8996])\n",
      "Epoch 1545 Loss 28.465343\n",
      "Params: tensor([ 2.3512, -0.2296])\n",
      "Grad:  tensor([-0.5188,  2.8995])\n",
      "Epoch 1546 Loss 28.464478\n",
      "Params: tensor([ 2.3513, -0.2299])\n",
      "Grad:  tensor([-0.5188,  2.8995])\n",
      "Epoch 1547 Loss 28.463610\n",
      "Params: tensor([ 2.3513, -0.2301])\n",
      "Grad:  tensor([-0.5187,  2.8995])\n",
      "Epoch 1548 Loss 28.462740\n",
      "Params: tensor([ 2.3514, -0.2304])\n",
      "Grad:  tensor([-0.5186,  2.8994])\n",
      "Epoch 1549 Loss 28.461872\n",
      "Params: tensor([ 2.3514, -0.2307])\n",
      "Grad:  tensor([-0.5186,  2.8994])\n",
      "Epoch 1550 Loss 28.461006\n",
      "Params: tensor([ 2.3515, -0.2310])\n",
      "Grad:  tensor([-0.5185,  2.8993])\n",
      "Epoch 1551 Loss 28.460138\n",
      "Params: tensor([ 2.3515, -0.2313])\n",
      "Grad:  tensor([-0.5185,  2.8993])\n",
      "Epoch 1552 Loss 28.459272\n",
      "Params: tensor([ 2.3516, -0.2316])\n",
      "Grad:  tensor([-0.5185,  2.8993])\n",
      "Epoch 1553 Loss 28.458410\n",
      "Params: tensor([ 2.3516, -0.2319])\n",
      "Grad:  tensor([-0.5184,  2.8992])\n",
      "Epoch 1554 Loss 28.457537\n",
      "Params: tensor([ 2.3517, -0.2322])\n",
      "Grad:  tensor([-0.5184,  2.8992])\n",
      "Epoch 1555 Loss 28.456671\n",
      "Params: tensor([ 2.3517, -0.2325])\n",
      "Grad:  tensor([-0.5183,  2.8991])\n",
      "Epoch 1556 Loss 28.455803\n",
      "Params: tensor([ 2.3518, -0.2328])\n",
      "Grad:  tensor([-0.5183,  2.8991])\n",
      "Epoch 1557 Loss 28.454933\n",
      "Params: tensor([ 2.3518, -0.2330])\n",
      "Grad:  tensor([-0.5183,  2.8990])\n",
      "Epoch 1558 Loss 28.454071\n",
      "Params: tensor([ 2.3519, -0.2333])\n",
      "Grad:  tensor([-0.5182,  2.8990])\n",
      "Epoch 1559 Loss 28.453199\n",
      "Params: tensor([ 2.3519, -0.2336])\n",
      "Grad:  tensor([-0.5182,  2.8989])\n",
      "Epoch 1560 Loss 28.452333\n",
      "Params: tensor([ 2.3520, -0.2339])\n",
      "Grad:  tensor([-0.5181,  2.8989])\n",
      "Epoch 1561 Loss 28.451466\n",
      "Params: tensor([ 2.3521, -0.2342])\n",
      "Grad:  tensor([-0.5181,  2.8989])\n",
      "Epoch 1562 Loss 28.450600\n",
      "Params: tensor([ 2.3521, -0.2345])\n",
      "Grad:  tensor([-0.5181,  2.8988])\n",
      "Epoch 1563 Loss 28.449736\n",
      "Params: tensor([ 2.3522, -0.2348])\n",
      "Grad:  tensor([-0.5180,  2.8988])\n",
      "Epoch 1564 Loss 28.448864\n",
      "Params: tensor([ 2.3522, -0.2351])\n",
      "Grad:  tensor([-0.5180,  2.8987])\n",
      "Epoch 1565 Loss 28.447998\n",
      "Params: tensor([ 2.3523, -0.2354])\n",
      "Grad:  tensor([-0.5179,  2.8987])\n",
      "Epoch 1566 Loss 28.447130\n",
      "Params: tensor([ 2.3523, -0.2357])\n",
      "Grad:  tensor([-0.5179,  2.8986])\n",
      "Epoch 1567 Loss 28.446260\n",
      "Params: tensor([ 2.3524, -0.2359])\n",
      "Grad:  tensor([-0.5179,  2.8986])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1568 Loss 28.445391\n",
      "Params: tensor([ 2.3524, -0.2362])\n",
      "Grad:  tensor([-0.5178,  2.8986])\n",
      "Epoch 1569 Loss 28.444525\n",
      "Params: tensor([ 2.3525, -0.2365])\n",
      "Grad:  tensor([-0.5178,  2.8985])\n",
      "Epoch 1570 Loss 28.443659\n",
      "Params: tensor([ 2.3525, -0.2368])\n",
      "Grad:  tensor([-0.5177,  2.8985])\n",
      "Epoch 1571 Loss 28.442793\n",
      "Params: tensor([ 2.3526, -0.2371])\n",
      "Grad:  tensor([-0.5177,  2.8984])\n",
      "Epoch 1572 Loss 28.441927\n",
      "Params: tensor([ 2.3526, -0.2374])\n",
      "Grad:  tensor([-0.5177,  2.8984])\n",
      "Epoch 1573 Loss 28.441059\n",
      "Params: tensor([ 2.3527, -0.2377])\n",
      "Grad:  tensor([-0.5176,  2.8983])\n",
      "Epoch 1574 Loss 28.440197\n",
      "Params: tensor([ 2.3527, -0.2380])\n",
      "Grad:  tensor([-0.5176,  2.8983])\n",
      "Epoch 1575 Loss 28.439325\n",
      "Params: tensor([ 2.3528, -0.2383])\n",
      "Grad:  tensor([-0.5175,  2.8982])\n",
      "Epoch 1576 Loss 28.438459\n",
      "Params: tensor([ 2.3528, -0.2386])\n",
      "Grad:  tensor([-0.5175,  2.8982])\n",
      "Epoch 1577 Loss 28.437597\n",
      "Params: tensor([ 2.3529, -0.2388])\n",
      "Grad:  tensor([-0.5175,  2.8982])\n",
      "Epoch 1578 Loss 28.436726\n",
      "Params: tensor([ 2.3529, -0.2391])\n",
      "Grad:  tensor([-0.5174,  2.8981])\n",
      "Epoch 1579 Loss 28.435865\n",
      "Params: tensor([ 2.3530, -0.2394])\n",
      "Grad:  tensor([-0.5174,  2.8981])\n",
      "Epoch 1580 Loss 28.434998\n",
      "Params: tensor([ 2.3530, -0.2397])\n",
      "Grad:  tensor([-0.5173,  2.8980])\n",
      "Epoch 1581 Loss 28.434126\n",
      "Params: tensor([ 2.3531, -0.2400])\n",
      "Grad:  tensor([-0.5173,  2.8980])\n",
      "Epoch 1582 Loss 28.433260\n",
      "Params: tensor([ 2.3531, -0.2403])\n",
      "Grad:  tensor([-0.5173,  2.8979])\n",
      "Epoch 1583 Loss 28.432392\n",
      "Params: tensor([ 2.3532, -0.2406])\n",
      "Grad:  tensor([-0.5172,  2.8979])\n",
      "Epoch 1584 Loss 28.431530\n",
      "Params: tensor([ 2.3532, -0.2409])\n",
      "Grad:  tensor([-0.5172,  2.8979])\n",
      "Epoch 1585 Loss 28.430662\n",
      "Params: tensor([ 2.3533, -0.2412])\n",
      "Grad:  tensor([-0.5171,  2.8978])\n",
      "Epoch 1586 Loss 28.429790\n",
      "Params: tensor([ 2.3533, -0.2414])\n",
      "Grad:  tensor([-0.5171,  2.8978])\n",
      "Epoch 1587 Loss 28.428926\n",
      "Params: tensor([ 2.3534, -0.2417])\n",
      "Grad:  tensor([-0.5171,  2.8977])\n",
      "Epoch 1588 Loss 28.428064\n",
      "Params: tensor([ 2.3534, -0.2420])\n",
      "Grad:  tensor([-0.5170,  2.8977])\n",
      "Epoch 1589 Loss 28.427197\n",
      "Params: tensor([ 2.3535, -0.2423])\n",
      "Grad:  tensor([-0.5170,  2.8976])\n",
      "Epoch 1590 Loss 28.426331\n",
      "Params: tensor([ 2.3536, -0.2426])\n",
      "Grad:  tensor([-0.5169,  2.8976])\n",
      "tensor([-0.5139,  2.8934])\n",
      "Epoch 1685 Loss 28.344158\n",
      "Params: tensor([ 2.3584, -0.2701])\n",
      "Grad:  tensor([-0.5139,  2.8933])\n",
      "Epoch 1686 Loss 28.343298\n",
      "Params: tensor([ 2.3585, -0.2704])\n",
      "Grad:  tensor([-0.5139,  2.8933])\n",
      "Epoch 1687 Loss 28.342432\n",
      "Params: tensor([ 2.3586, -0.2707])\n",
      "Grad:  tensor([-0.5138,  2.8932])\n",
      "Epoch 1688 Loss 28.341572\n",
      "Params: tensor([ 2.3586, -0.2710])\n",
      "Grad:  tensor([-0.5138,  2.8932])\n",
      "Epoch 1689 Loss 28.340710\n",
      "Params: tensor([ 2.3587, -0.2713])\n",
      "Grad:  tensor([-0.5138,  2.8931])\n",
      "Epoch 1690 Loss 28.339842\n",
      "Params: tensor([ 2.3587, -0.2716])\n",
      "Grad:  tensor([-0.5137,  2.8931])\n",
      "Epoch 1691 Loss 28.338976\n",
      "Params: tensor([ 2.3588, -0.2719])\n",
      "Grad:  tensor([-0.5137,  2.8930])\n",
      "Epoch 1692 Loss 28.338121\n",
      "Params: tensor([ 2.3588, -0.2721])\n",
      "Grad:  tensor([-0.5137,  2.8930])\n",
      "Epoch 1693 Loss 28.337250\n",
      "Params: tensor([ 2.3589, -0.2724])\n",
      "Grad:  tensor([-0.5137,  2.8929])\n",
      "Epoch 1694 Loss 28.336388\n",
      "Params: tensor([ 2.3589, -0.2727])\n",
      "Grad:  tensor([-0.5137,  2.8929])\n",
      "Epoch 1695 Loss 28.335529\n",
      "Params: tensor([ 2.3590, -0.2730])\n",
      "Grad:  tensor([-0.5137,  2.8928])\n",
      "Epoch 1696 Loss 28.334661\n",
      "Params: tensor([ 2.3590, -0.2733])\n",
      "Grad:  tensor([-0.5136,  2.8928])\n",
      "Epoch 1697 Loss 28.333799\n",
      "Params: tensor([ 2.3591, -0.2736])\n",
      "Grad:  tensor([-0.5136,  2.8928])\n",
      "Epoch 1698 Loss 28.332939\n",
      "Params: tensor([ 2.3591, -0.2739])\n",
      "Grad:  tensor([-0.5136,  2.8927])\n",
      "Epoch 1699 Loss 28.332069\n",
      "Params: tensor([ 2.3592, -0.2742])\n",
      "Grad:  tensor([-0.5136,  2.8927])\n",
      "Epoch 1700 Loss 28.331207\n",
      "Params: tensor([ 2.3592, -0.2745])\n",
      "Grad:  tensor([-0.5136,  2.8926])\n",
      "Epoch 1701 Loss 28.330345\n",
      "Params: tensor([ 2.3593, -0.2747])\n",
      "Grad:  tensor([-0.5135,  2.8926])\n",
      "Epoch 1702 Loss 28.329485\n",
      "Params: tensor([ 2.3593, -0.2750])\n",
      "Grad:  tensor([-0.5135,  2.8925])\n",
      "Epoch 1703 Loss 28.328619\n",
      "Params: tensor([ 2.3594, -0.2753])\n",
      "Grad:  tensor([-0.5135,  2.8925])\n",
      "Epoch 1704 Loss 28.327761\n",
      "Params: tensor([ 2.3594, -0.2756])\n",
      "Grad:  tensor([-0.5135,  2.8924])\n",
      "Epoch 1705 Loss 28.326897\n",
      "Params: tensor([ 2.3595, -0.2759])\n",
      "Grad:  tensor([-0.5135,  2.8924])\n",
      "Epoch 1706 Loss 28.326036\n",
      "Params: tensor([ 2.3595, -0.2762])\n",
      "Grad:  tensor([-0.5135,  2.8923])\n",
      "Epoch 1707 Loss 28.325167\n",
      "Params: tensor([ 2.3596, -0.2765])\n",
      "Grad:  tensor([-0.5134,  2.8923])\n",
      "Epoch 1708 Loss 28.324303\n",
      "Params: tensor([ 2.3596, -0.2768])\n",
      "Grad:  tensor([-0.5134,  2.8922])\n",
      "Epoch 1709 Loss 28.323448\n",
      "Params: tensor([ 2.3597, -0.2771])\n",
      "Grad:  tensor([-0.5134,  2.8922])\n",
      "Epoch 1710 Loss 28.322578\n",
      "Params: tensor([ 2.3597, -0.2773])\n",
      "Grad:  tensor([-0.5134,  2.8921])\n",
      "Epoch 1711 Loss 28.321718\n",
      "Params: tensor([ 2.3598, -0.2776])\n",
      "Grad:  tensor([-0.5134,  2.8921])\n",
      "Epoch 1712 Loss 28.320856\n",
      "Params: tensor([ 2.3598, -0.2779])\n",
      "Grad:  tensor([-0.5133,  2.8920])\n",
      "Epoch 1713 Loss 28.319990\n",
      "Params: tensor([ 2.3599, -0.2782])\n",
      "Grad:  tensor([-0.5133,  2.8920])\n",
      "Epoch 1714 Loss 28.319130\n",
      "Params: tensor([ 2.3599, -0.2785])\n",
      "Grad:  tensor([-0.5133,  2.8919])\n",
      "Epoch 1715 Loss 28.318266\n",
      "Params: tensor([ 2.3600, -0.2788])\n",
      "Grad:  tensor([-0.5133,  2.8919])\n",
      "Epoch 1716 Loss 28.317406\n",
      "Params: tensor([ 2.3600, -0.2791])\n",
      "Grad:  tensor([-0.5133,  2.8918])\n",
      "Epoch 1717 Loss 28.316545\n",
      "Params: tensor([ 2.3601, -0.2794])\n",
      "Grad:  tensor([-0.5132,  2.8918])\n",
      "Epoch 1718 Loss 28.315674\n",
      "Params: tensor([ 2.3601, -0.2797])\n",
      "Grad:  tensor([-0.5132,  2.8918])\n",
      "Epoch 1719 Loss 28.314814\n",
      "Params: tensor([ 2.3602, -0.2799])\n",
      "Grad:  tensor([-0.5132,  2.8917])\n",
      "Epoch 1720 Loss 28.313953\n",
      "Params: tensor([ 2.3602, -0.2802])\n",
      "Grad:  tensor([-0.5132,  2.8917])\n",
      "Epoch 1721 Loss 28.313087\n",
      "Params: tensor([ 2.3603, -0.2805])\n",
      "Grad:  tensor([-0.5132,  2.8916])\n",
      "Epoch 1722 Loss 28.312227\n",
      "Params: tensor([ 2.3603, -0.2808])\n",
      "Grad:  tensor([-0.5132,  2.8916])\n",
      "Epoch 1723 Loss 28.311363\n",
      "Params: tensor([ 2.3604, -0.2811])\n",
      "Grad:  tensor([-0.5131,  2.8915])\n",
      "Epoch 1724 Loss 28.310509\n",
      "Params: tensor([ 2.3604, -0.2814])\n",
      "Grad:  tensor([-0.5131,  2.8915])\n",
      "Epoch 1725 Loss 28.309649\n",
      "Params: tensor([ 2.3605, -0.2817])\n",
      "Grad:  tensor([-0.5131,  2.8914])\n",
      "Epoch 1726 Loss 28.308783\n",
      "Params: tensor([ 2.3606, -0.2820])\n",
      "Grad:  tensor([-0.5131,  2.8914])\n",
      "Epoch 1727 Loss 28.307915\n",
      "Params: tensor([ 2.3606, -0.2823])\n",
      "Grad:  tensor([-0.5131,  2.8913])\n",
      "Epoch 1728 Loss 28.307049\n",
      "Params: tensor([ 2.3607, -0.2826])\n",
      "Grad:  tensor([-0.5130,  2.8913])\n",
      "Epoch 1729 Loss 28.306190\n",
      "Params: tensor([ 2.3607, -0.2828])\n",
      "Grad:  tensor([-0.5130,  2.8912])\n",
      "Epoch 1730 Loss 28.305334\n",
      "Params: tensor([ 2.3608, -0.2831])\n",
      "Grad:  tensor([-0.5130,  2.8912])\n",
      "Epoch 1731 Loss 28.304468\n",
      "Params: tensor([ 2.3608, -0.2834])\n",
      "Grad:  tensor([-0.5130,  2.8911])\n",
      "Epoch 1732 Loss 28.303612\n",
      "Params: tensor([ 2.3609, -0.2837])\n",
      "Grad:  tensor([-0.5130,  2.8911])\n",
      "Epoch 1733 Loss 28.302746\n",
      "Params: tensor([ 2.3609, -0.2840])\n",
      "Grad:  tensor([-0.5129,  2.8910])\n",
      "Epoch 1734 Loss 28.301882\n",
      "Params: tensor([ 2.3610, -0.2843])\n",
      "Grad:  tensor([-0.5129,  2.8910])\n",
      "Epoch 1735 Loss 28.301020\n",
      "Params: tensor([ 2.3610, -0.2846])\n",
      "Grad:  tensor([-0.5129,  2.8910])\n",
      "Epoch 1736 Loss 28.300158\n",
      "Params: tensor([ 2.3611, -0.2849])\n",
      "Grad:  tensor([-0.5129,  2.8909])\n",
      "Epoch 1737 Loss 28.299294\n",
      "Params: tensor([ 2.3611, -0.2852])\n",
      "Grad:  tensor([-0.5129,  2.8909])\n",
      "Epoch 1738 Loss 28.298433\n",
      "Params: tensor([ 2.3612, -0.2854])\n",
      "Grad:  tensor([-0.5128,  2.8908])\n",
      "Epoch 1739 Loss 28.297575\n",
      "Params: tensor([ 2.3612, -0.2857])\n",
      "Grad:  tensor([-0.5128,  2.8908])\n",
      "Epoch 1740 Loss 28.296715\n",
      "Params: tensor([ 2.3613, -0.2860])\n",
      "Grad:  tensor([-0.5128,  2.8907])\n",
      "Epoch 1741 Loss 28.295851\n",
      "Params: tensor([ 2.3613, -0.2863])\n",
      "Grad:  tensor([-0.5128,  2.8907])\n",
      "Epoch 1742 Loss 28.294985\n",
      "Params: tensor([ 2.3614, -0.2866])\n",
      "Grad:  tensor([-0.5128,  2.8906])\n",
      "Epoch 1743 Loss 28.294128\n",
      "Params: tensor([ 2.3614, -0.2869])\n",
      "Grad:  tensor([-0.5127,  2.8906])\n",
      "Epoch 1744 Loss 28.293262\n",
      "Params: tensor([ 2.3615, -0.2872])\n",
      "Grad:  tensor([-0.5127,  2.8905])\n",
      "Epoch 1745 Loss 28.292402\n",
      "Params: tensor([ 2.3615, -0.2875])\n",
      "Grad:  tensor([-0.5127,  2.8905])\n",
      "Epoch 1746 Loss 28.291536\n",
      "Params: tensor([ 2.3616, -0.2878])\n",
      "Grad:  tensor([-0.5127,  2.8904])\n",
      "Epoch 1747 Loss 28.290674\n",
      "Params: tensor([ 2.3616, -0.2880])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor([-0.5127,  2.8904])\n",
      "Epoch 1748 Loss 28.289824\n",
      "Params: tensor([ 2.3617, -0.2883])\n",
      "Grad:  tensor([-0.5126,  2.8903])\n",
      "Epoch 1749 Loss 28.288952\n",
      "Params: tensor([ 2.3617, -0.2886])\n",
      "Grad:  tensor([-0.5126,  2.8903])\n",
      "Epoch 1750 Loss 28.288097\n",
      "Params: tensor([ 2.3618, -0.2889])\n",
      "Grad:  tensor([-0.5126,  2.8902])\n",
      "Epoch 1751 Loss 28.287233\n",
      "Params: tensor([ 2.3618, -0.2892])\n",
      "Grad:  tensor([-0.5126,  2.8902])\n",
      "Epoch 1752 Loss 28.286371\n",
      "Params: tensor([ 2.3619, -0.2895])\n",
      "Grad:  tensor([-0.5126,  2.8902])\n",
      "Epoch 1753 Loss 28.285503\n",
      "Params: tensor([ 2.3619, -0.2898])\n",
      "Grad:  tensor([-0.5125,  2.8901])\n",
      "Epoch 1754 Loss 28.284645\n",
      "Params: tensor([ 2.3620, -0.2901])\n",
      "Grad:  tensor([-0.5125,  2.8901])\n",
      "Epoch 1755 Loss 28.283781\n",
      "Params: tensor([ 2.3620, -0.2904])\n",
      "Grad:  tensor([-0.5125,  2.8900])\n",
      "Epoch 1756 Loss 28.282921\n",
      "Params: tensor([ 2.3621, -0.2906])\n",
      "Grad:  tensor([-0.5125,  2.8900])\n",
      "Epoch 1757 Loss 28.282063\n",
      "Params: tensor([ 2.3621, -0.2909])\n",
      "Grad:  tensor([-0.5125,  2.8899])\n",
      "Epoch 1758 Loss 28.281200\n",
      "Params: tensor([ 2.3622, -0.2912])\n",
      "Grad:  tensor([-0.5124,  2.8899])\n",
      "Epoch 1759 Loss 28.280340\n",
      "Params: tensor([ 2.3622, -0.2915])\n",
      "Grad:  tensor([-0.5124,  2.8898])\n",
      "Epoch 1760 Loss 28.279478\n",
      "Params: tensor([ 2.3623, -0.2918])\n",
      "Grad:  tensor([-0.5124,  2.8898])\n",
      "Epoch 1761 Loss 28.278614\n",
      "Params: tensor([ 2.3623, -0.2921])\n",
      "Grad:  tensor([-0.5124,  2.8897])\n",
      "Epoch 1762 Loss 28.277754\n",
      "Params: tensor([ 2.3624, -0.2924])\n",
      "Grad:  tensor([-0.5124,  2.8897])\n",
      "Epoch 1763 Loss 28.276892\n",
      "Params: tensor([ 2.3624, -0.2927])\n",
      "Grad:  tensor([-0.5123,  2.8896])\n",
      "Epoch 1764 Loss 28.276033\n",
      "Params: tensor([ 2.3625, -0.2930])\n",
      "Grad:  tensor([-0.5123,  2.8896])\n",
      "Epoch 1765 Loss 28.275166\n",
      "Params: tensor([ 2.3626, -0.2932])\n",
      "Grad:  tensor([-0.5123,  2.8895])\n",
      "Epoch 1766 Loss 28.274311\n",
      "Params: tensor([ 2.3626, -0.2935])\n",
      "Grad:  tensor([-0.5123,  2.8895])\n",
      "Epoch 1767 Loss 28.273445\n",
      "Params: tensor([ 2.3627, -0.2938])\n",
      "Grad:  tensor([-0.5122,  2.8894])\n",
      "Epoch 1768 Loss 28.272585\n",
      "Params: tensor([ 2.3627, -0.2941])\n",
      "Grad:  tensor([-0.5122,  2.8894])\n",
      "Epoch 1769 Loss 28.271727\n",
      "Params: tensor([ 2.3628, -0.2944])\n",
      "Grad:  tensor([-0.5122,  2.8894])\n",
      "Epoch 1770 Loss 28.270866\n",
      "Params: tensor([ 2.3628, -0.2947])\n",
      "Grad:  tensor([-0.5122,  2.8893])\n",
      "Epoch 1771 Loss 28.270002\n",
      "Params: tensor([ 2.3629, -0.2950])\n",
      "Grad:  tensor([-0.5122,  2.8893])\n",
      "Epoch 1772 Loss 28.269146\n",
      "Params: tensor([ 2.3629, -0.2953])\n",
      "Grad:  tensor([-0.5121,  2.8892])\n",
      "Epoch 1773 Loss 28.268286\n",
      "Params: tensor([ 2.3630, -0.2956])\n",
      "Grad:  tensor([-0.5121,  2.8892])\n",
      "Epoch 1774 Loss 28.267420\n",
      "Params: tensor([ 2.3630, -0.2958])\n",
      "Grad:  tensor([-0.5121,  2.8891])\n",
      "Epoch 1775 Loss 28.266554\n",
      "Params: tensor([ 2.3631, -0.2961])\n",
      "Grad:  tensor([-0.5121,  2.8891])\n",
      "Epoch 1776 Loss 28.265699\n",
      "Params: tensor([ 2.3631, -0.2964])\n",
      "Grad:  tensor([-0.5120,  2.8890])\n",
      "Epoch 1777 Loss 28.264839\n",
      "Params: tensor([ 2.3632, -0.2967])\n",
      "Grad:  tensor([-0.5120,  2.8890])\n",
      "Epoch 1778 Loss 28.263979\n",
      "Params: tensor([ 2.3632, -0.2970])\n",
      "Grad:  tensor([-0.5120,  2.8889])\n",
      "Epoch 1779 Loss 28.263121\n",
      "Params: tensor([ 2.3633, -0.2973])\n",
      "Grad:  tensor([-0.5120,  2.8889])\n",
      "Epoch 1780 Loss 28.262260\n",
      "Params: tensor([ 2.3633, -0.2976])\n",
      "Grad:  tensor([-0.5120,  2.8888])\n",
      "Epoch 1781 Loss 28.261396\n",
      "Params: tensor([ 2.3634, -0.2979])\n",
      "Grad:  tensor([-0.5119,  2.8888])\n",
      "Epoch 1782 Loss 28.260536\n",
      "Params: tensor([ 2.3634, -0.2982])\n",
      "Grad:  tensor([-0.5119,  2.8887])\n",
      "Epoch 1783 Loss 28.259674\n",
      "Params: tensor([ 2.3635, -0.2984])\n",
      "Grad:  tensor([-0.5119,  2.8887])\n",
      "Epoch 1784 Loss 28.258808\n",
      "Params: tensor([ 2.3635, -0.2987])\n",
      "Grad:  tensor([-0.5119,  2.8887])\n",
      "Epoch 1785 Loss 28.257954\n",
      "Params: tensor([ 2.3636, -0.2990])\n",
      "Grad:  tensor([-0.5118,  2.8886])\n",
      "Epoch 1786 Loss 28.257092\n",
      "Params: tensor([ 2.3636, -0.2993])\n",
      "Grad:  tensor([-0.5118,  2.8886])\n",
      "Epoch 1787 Loss 28.256233\n",
      "Params: tensor([ 2.3637, -0.2996])\n",
      "Grad:  tensor([-0.5118,  2.8885])\n",
      "Epoch 1788 Loss 28.255371\n",
      "Params: tensor([ 2.3637, -0.2999])\n",
      "Grad:  tensor([-0.5118,  2.8885])\n",
      "Epoch 1789 Loss 28.254517\n",
      "Params: tensor([ 2.3638, -0.3002])\n",
      "Grad:  tensor([-0.5118,  2.8884])\n",
      "Epoch 1790 Loss 28.253651\n",
      "Params: tensor([ 2.3638, -0.3005])\n",
      "Grad:  tensor([-0.5117,  2.8884])\n",
      "Epoch 1791 Loss 28.252790\n",
      "Params: tensor([ 2.3639, -0.3008])\n",
      "Grad:  tensor([-0.5117,  2.8883])\n",
      "Epoch 1792 Loss 28.251930\n",
      "Params: tensor([ 2.3639, -0.3010])\n",
      "Grad:  tensor([-0.5117,  2.8883])\n",
      "Epoch 1793 Loss 28.251062\n",
      "Params: tensor([ 2.3640, -0.3013])\n",
      "Grad:  tensor([-0.5117,  2.8882])\n",
      "Epoch 1794 Loss 28.250208\n",
      "Params: tensor([ 2.3640, -0.3016])\n",
      "Grad:  tensor([-0.5116,  2.8882])\n",
      "Epoch 1795 Loss 28.249352\n",
      "Params: tensor([ 2.3641, -0.3019])\n",
      "Grad:  tensor([-0.5116,  2.8881])\n",
      "Epoch 1796 Loss 28.248487\n",
      "Params: tensor([ 2.3641, -0.3022])\n",
      "Grad:  tensor([-0.5116,  2.8881])\n",
      "Epoch 1797 Loss 28.247627\n",
      "Params: tensor([ 2.3642, -0.3025])\n",
      "Grad:  tensor([-0.5116,  2.8880])\n",
      "Epoch 1798 Loss 28.246765\n",
      "Params: tensor([ 2.3642, -0.3028])\n",
      "Grad:  tensor([-0.5116,  2.8880])\n",
      "Epoch 1799 Loss 28.245905\n",
      "Params: tensor([ 2.3643, -0.3031])\n",
      "Grad:  tensor([-0.5115,  2.8880])\n",
      "Epoch 1800 Loss 28.245045\n",
      "Params: tensor([ 2.3643, -0.3034])\n",
      "Grad:  tensor([-0.5115,  2.8879])\n",
      "Epoch 1801 Loss 28.244188\n",
      "Params: tensor([ 2.3644, -0.3036])\n",
      "Grad:  tensor([-0.5115,  2.8879])\n",
      "Epoch 1802 Loss 28.243328\n",
      "Params: tensor([ 2.3644, -0.3039])\n",
      "Grad:  tensor([-0.5115,  2.8878])\n",
      "Epoch 1803 Loss 28.242462\n",
      "Params: tensor([ 2.3645, -0.3042])\n",
      "Grad:  tensor([-0.5114,  2.8878])\n",
      "Epoch 1804 Loss 28.241602\n",
      "Params: tensor([ 2.3646, -0.3045])\n",
      "Grad:  tensor([-0.5114,  2.8877])\n",
      "Epoch 1805 Loss 28.240746\n",
      "Params: tensor([ 2.3646, -0.3048])\n",
      "Grad:  tensor([-0.5114,  2.8877])\n",
      "Epoch 1806 Loss 28.239887\n",
      "Params: tensor([ 2.3647, -0.3051])\n",
      "Grad:  tensor([-0.5114,  2.8876])\n",
      "Epoch 1807 Loss 28.239027\n",
      "Params: tensor([ 2.3647, -0.3054])\n",
      "Grad:  tensor([-0.5114,  2.8876])\n",
      "Epoch 1808 Loss 28.238165\n",
      "Params: tensor([ 2.3648, -0.3057])\n",
      "Grad:  tensor([-0.5114,  2.8875])\n",
      "Epoch 1809 Loss 28.237305\n",
      "Params: tensor([ 2.3648, -0.3060])\n",
      "Grad:  tensor([-0.5114,  2.8875])\n",
      "Epoch 1810 Loss 28.236450\n",
      "Params: tensor([ 2.3649, -0.3062])\n",
      "Grad:  tensor([-0.5113,  2.8874])\n",
      "Epoch 1811 Loss 28.235590\n",
      "Params: tensor([ 2.3649, -0.3065])\n",
      "Grad:  tensor([-0.5113,  2.8874])\n",
      "Epoch 1812 Loss 28.234728\n",
      "Params: tensor([ 2.3650, -0.3068])\n",
      "Grad:  tensor([-0.5113,  2.8873])\n",
      "Epoch 1813 Loss 28.233870\n",
      "Params: tensor([ 2.3650, -0.3071])\n",
      "Grad:  tensor([-0.5113,  2.8873])\n",
      "Epoch 1814 Loss 28.233009\n",
      "Params: tensor([ 2.3651, -0.3074])\n",
      "Grad:  tensor([-0.5113,  2.8872])\n",
      "Epoch 1815 Loss 28.232147\n",
      "Params: tensor([ 2.3651, -0.3077])\n",
      "Grad:  tensor([-0.5113,  2.8872])\n",
      "Epoch 1816 Loss 28.231293\n",
      "Params: tensor([ 2.3652, -0.3080])\n",
      "Grad:  tensor([-0.5113,  2.8871])\n",
      "Epoch 1817 Loss 28.230431\n",
      "Params: tensor([ 2.3652, -0.3083])\n",
      "Grad:  tensor([-0.5113,  2.8871])\n",
      "Epoch 1818 Loss 28.229567\n",
      "Params: tensor([ 2.3653, -0.3086])\n",
      "Grad:  tensor([-0.5113,  2.8870])\n",
      "Epoch 1819 Loss 28.228716\n",
      "Params: tensor([ 2.3653, -0.3088])\n",
      "Grad:  tensor([-0.5113,  2.8870])\n",
      "Epoch 1820 Loss 28.227852\n",
      "Params: tensor([ 2.3654, -0.3091])\n",
      "Grad:  tensor([-0.5112,  2.8869])\n",
      "Epoch 1821 Loss 28.226990\n",
      "Params: tensor([ 2.3654, -0.3094])\n",
      "Grad:  tensor([-0.5112,  2.8869])\n",
      "Epoch 1822 Loss 28.226135\n",
      "Params: tensor([ 2.3655, -0.3097])\n",
      "Grad:  tensor([-0.5112,  2.8868])\n",
      "Epoch 1823 Loss 28.225269\n",
      "Params: tensor([ 2.3655, -0.3100])\n",
      "Grad:  tensor([-0.5112,  2.8868])\n",
      "Epoch 1824 Loss 28.224409\n",
      "Params: tensor([ 2.3656, -0.3103])\n",
      "Grad:  tensor([-0.5112,  2.8867])\n",
      "Epoch 1825 Loss 28.223553\n",
      "Params: tensor([ 2.3656, -0.3106])\n",
      "Grad:  tensor([-0.5112,  2.8867])\n",
      "Epoch 1826 Loss 28.222689\n",
      "Params: tensor([ 2.3657, -0.3109])\n",
      "Grad:  tensor([-0.5112,  2.8866])\n",
      "Epoch 1827 Loss 28.221832\n",
      "Params: tensor([ 2.3657, -0.3112])\n",
      "Grad:  tensor([-0.5112,  2.8866])\n",
      "Epoch 1828 Loss 28.220976\n",
      "Params: tensor([ 2.3658, -0.3114])\n",
      "Grad:  tensor([-0.5112,  2.8866])\n",
      "Epoch 1829 Loss 28.220112\n",
      "Params: tensor([ 2.3658, -0.3117])\n",
      "Grad:  tensor([-0.5112,  2.8865])\n",
      "Epoch 1830 Loss 28.219255\n",
      "Params: tensor([ 2.3659, -0.3120])\n",
      "Grad:  tensor([-0.5111,  2.8865])\n",
      "Epoch 1831 Loss 28.218395\n",
      "Params: tensor([ 2.3659, -0.3123])\n",
      "Grad:  tensor([-0.5111,  2.8864])\n",
      "Epoch 1832 Loss 28.217537\n",
      "Params: tensor([ 2.3660, -0.3126])\n",
      "Grad:  tensor([-0.5111,  2.8864])\n",
      "Epoch 1833 Loss 28.216675\n",
      "Params: tensor([ 2.3660, -0.3129])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor([-0.5111,  2.8863])\n",
      "Epoch 1834 Loss 28.215820\n",
      "Params: tensor([ 2.3661, -0.3132])\n",
      "Grad:  tensor([-0.5111,  2.8863])\n",
      "Epoch 1835 Loss 28.214960\n",
      "Params: tensor([ 2.3661, -0.3135])\n",
      "Grad:  tensor([-0.5111,  2.8862])\n",
      "Epoch 1836 Loss 28.214098\n",
      "Params: tensor([ 2.3662, -0.3138])\n",
      "Grad:  tensor([-0.5111,  2.8862])\n",
      "Epoch 1837 Loss 28.213243\n",
      "Params: tensor([ 2.3662, -0.3140])\n",
      "Grad:  tensor([-0.5111,  2.8861])\n",
      "Epoch 1838 Loss 28.212385\n",
      "Params: tensor([ 2.3663, -0.3143])\n",
      "Grad:  tensor([-0.5111,  2.8861])\n",
      "Epoch 1839 Loss 28.211519\n",
      "Params: tensor([ 2.3663, -0.3146])\n",
      "Grad:  tensor([-0.5110,  2.8860])\n",
      "Epoch 1840 Loss 28.210669\n",
      "Params: tensor([ 2.3664, -0.3149])\n",
      "Grad:  tensor([-0.5110,  2.8860])\n",
      "Epoch 1841 Loss 28.209806\n",
      "Params: tensor([ 2.3664, -0.3152])\n",
      "Grad:  tensor([-0.5110,  2.8859])\n",
      "Epoch 1842 Loss 28.208946\n",
      "Params: tensor([ 2.3665, -0.3155])\n",
      "Grad:  tensor([-0.5110,  2.8859])\n",
      "Epoch 1843 Loss 28.208092\n",
      "Params: tensor([ 2.3665, -0.3158])\n",
      "Grad:  tensor([-0.5110,  2.8858])\n",
      "Epoch 1844 Loss 28.207228\n",
      "Params: tensor([ 2.3666, -0.3161])\n",
      "Grad:  tensor([-0.5110,  2.8858])\n",
      "Epoch 1845 Loss 28.206367\n",
      "Params: tensor([ 2.3666, -0.3163])\n",
      "Grad:  tensor([-0.5110,  2.8857])\n",
      "Epoch 1846 Loss 28.205513\n",
      "Params: tensor([ 2.3667, -0.3166])\n",
      "Grad:  tensor([-0.5110,  2.8857])\n",
      "Epoch 1847 Loss 28.204651\n",
      "Params: tensor([ 2.3667, -0.3169])\n",
      "Grad:  tensor([-0.5110,  2.8856])\n",
      "Epoch 1848 Loss 28.203794\n",
      "Params: tensor([ 2.3668, -0.3172])\n",
      "Grad:  tensor([-0.5110,  2.8856])\n",
      "Epoch 1849 Loss 28.202934\n",
      "Params: tensor([ 2.3668, -0.3175])\n",
      "Grad:  tensor([-0.5109,  2.8855])\n",
      "Epoch 1850 Loss 28.202076\n",
      "Params: tensor([ 2.3669, -0.3178])\n",
      "Grad:  tensor([-0.5109,  2.8855])\n",
      "Epoch 1851 Loss 28.201214\n",
      "Params: tensor([ 2.3669, -0.3181])\n",
      "Grad:  tensor([-0.5109,  2.8854])\n",
      "Epoch 1852 Loss 28.200356\n",
      "Params: tensor([ 2.3670, -0.3184])\n",
      "Grad:  tensor([-0.5109,  2.8854])\n",
      "Epoch 1853 Loss 28.199495\n",
      "Params: tensor([ 2.3671, -0.3187])\n",
      "Grad:  tensor([-0.5109,  2.8853])\n",
      "Epoch 1854 Loss 28.198641\n",
      "Params: tensor([ 2.3671, -0.3189])\n",
      "Grad:  tensor([-0.5109,  2.8853])\n",
      "Epoch 1855 Loss 28.197784\n",
      "Params: tensor([ 2.3672, -0.3192])\n",
      "Grad:  tensor([-0.5109,  2.8852])\n",
      "Epoch 1856 Loss 28.196922\n",
      "Params: tensor([ 2.3672, -0.3195])\n",
      "Grad:  tensor([-0.5109,  2.8852])\n",
      "Epoch 1857 Loss 28.196068\n",
      "Params: tensor([ 2.3673, -0.3198])\n",
      "Grad:  tensor([-0.5108,  2.8851])\n",
      "Epoch 1858 Loss 28.195210\n",
      "Params: tensor([ 2.3673, -0.3201])\n",
      "Grad:  tensor([-0.5108,  2.8851])\n",
      "Epoch 1859 Loss 28.194355\n",
      "Params: tensor([ 2.3674, -0.3204])\n",
      "Grad:  tensor([-0.5108,  2.8850])\n",
      "Epoch 1860 Loss 28.193489\n",
      "Params: tensor([ 2.3674, -0.3207])\n",
      "Grad:  tensor([-0.5108,  2.8850])\n",
      "Epoch 1861 Loss 28.192638\n",
      "Params: tensor([ 2.3675, -0.3210])\n",
      "Grad:  tensor([-0.5108,  2.8849])\n",
      "Epoch 1862 Loss 28.191774\n",
      "Params: tensor([ 2.3675, -0.3213])\n",
      "Grad:  tensor([-0.5108,  2.8849])\n",
      "Epoch 1863 Loss 28.190912\n",
      "Params: tensor([ 2.3676, -0.3215])\n",
      "Grad:  tensor([-0.5108,  2.8849])\n",
      "Epoch 1864 Loss 28.190056\n",
      "Params: tensor([ 2.3676, -0.3218])\n",
      "Grad:  tensor([-0.5108,  2.8848])\n",
      "Epoch 1865 Loss 28.189198\n",
      "Params: tensor([ 2.3677, -0.3221])\n",
      "Grad:  tensor([-0.5107,  2.8848])\n",
      "Epoch 1866 Loss 28.188341\n",
      "Params: tensor([ 2.3677, -0.3224])\n",
      "Grad:  tensor([-0.5107,  2.8847])\n",
      "Epoch 1867 Loss 28.187481\n",
      "Params: tensor([ 2.3678, -0.3227])\n",
      "Grad:  tensor([-0.5107,  2.8847])\n",
      "Epoch 1868 Loss 28.186617\n",
      "Params: tensor([ 2.3678, -0.3230])\n",
      "Grad:  tensor([-0.5107,  2.8846])\n",
      "Epoch 1869 Loss 28.185772\n",
      "Params: tensor([ 2.3679, -0.3233])\n",
      "Grad:  tensor([-0.5107,  2.8846])\n",
      "Epoch 1870 Loss 28.184908\n",
      "Params: tensor([ 2.3679, -0.3236])\n",
      "Grad:  tensor([-0.5107,  2.8845])\n",
      "Epoch 1871 Loss 28.184052\n",
      "Params: tensor([ 2.3680, -0.3238])\n",
      "Grad:  tensor([-0.5107,  2.8845])\n",
      "Epoch 1872 Loss 28.183195\n",
      "Params: tensor([ 2.3680, -0.3241])\n",
      "Grad:  tensor([-0.5107,  2.8844])\n",
      "Epoch 1873 Loss 28.182335\n",
      "Params: tensor([ 2.3681, -0.3244])\n",
      "Grad:  tensor([-0.5106,  2.8844])\n",
      "Epoch 1874 Loss 28.181480\n",
      "Params: tensor([ 2.3681, -0.3247])\n",
      "Grad:  tensor([-0.5106,  2.8843])\n",
      "Epoch 1875 Loss 28.180620\n",
      "Params: tensor([ 2.3682, -0.3250])\n",
      "Grad:  tensor([-0.5106,  2.8843])\n",
      "Epoch 1876 Loss 28.179756\n",
      "Params: tensor([ 2.3682, -0.3253])\n",
      "Grad:  tensor([-0.5106,  2.8842])\n",
      "Epoch 1877 Loss 28.178900\n",
      "Params: tensor([ 2.3683, -0.3256])\n",
      "Grad:  tensor([-0.5106,  2.8842])\n",
      "Epoch 1878 Loss 28.178047\n",
      "Params: tensor([ 2.3683, -0.3259])\n",
      "Grad:  tensor([-0.5106,  2.8841])\n",
      "Epoch 1879 Loss 28.177185\n",
      "Params: tensor([ 2.3684, -0.3262])\n",
      "Grad:  tensor([-0.5106,  2.8841])\n",
      "Epoch 1880 Loss 28.176331\n",
      "Params: tensor([ 2.3684, -0.3264])\n",
      "Grad:  tensor([-0.5106,  2.8840])\n",
      "Epoch 1881 Loss 28.175470\n",
      "Params: tensor([ 2.3685, -0.3267])\n",
      "Grad:  tensor([-0.5105,  2.8840])\n",
      "Epoch 1882 Loss 28.174614\n",
      "Params: tensor([ 2.3685, -0.3270])\n",
      "Grad:  tensor([-0.5105,  2.8839])\n",
      "Epoch 1883 Loss 28.173759\n",
      "Params: tensor([ 2.3686, -0.3273])\n",
      "Grad:  tensor([-0.5105,  2.8839])\n",
      "Epoch 1884 Loss 28.172905\n",
      "Params: tensor([ 2.3686, -0.3276])\n",
      "Grad:  tensor([-0.5105,  2.8838])\n",
      "Epoch 1885 Loss 28.172041\n",
      "Params: tensor([ 2.3687, -0.3279])\n",
      "Grad:  tensor([-0.5105,  2.8838])\n",
      "Epoch 1886 Loss 28.171185\n",
      "Params: tensor([ 2.3687, -0.3282])\n",
      "Grad:  tensor([-0.5105,  2.8837])\n",
      "Epoch 1887 Loss 28.170324\n",
      "Params: tensor([ 2.3688, -0.3285])\n",
      "Grad:  tensor([-0.5105,  2.8837])\n",
      "Epoch 1888 Loss 28.169466\n",
      "Params: tensor([ 2.3688, -0.3288])\n",
      "Grad:  tensor([-0.5105,  2.8836])\n",
      "Epoch 1889 Loss 28.168612\n",
      "Params: tensor([ 2.3689, -0.3290])\n",
      "Grad:  tensor([-0.5104,  2.8836])\n",
      "Epoch 1890 Loss 28.167753\n",
      "Params: tensor([ 2.3689, -0.3293])\n",
      "Grad:  tensor([-0.5104,  2.8835])\n",
      "Epoch 1891 Loss 28.166893\n",
      "Params: tensor([ 2.3690, -0.3296])\n",
      "Grad:  tensor([-0.5104,  2.8835])\n",
      "Epoch 1892 Loss 28.166040\n",
      "Params: tensor([ 2.3690, -0.3299])\n",
      "Grad:  tensor([-0.5104,  2.8835])\n",
      "Epoch 1893 Loss 28.165174\n",
      "Params: tensor([ 2.3691, -0.3302])\n",
      "Grad:  tensor([-0.5104,  2.8834])\n",
      "Epoch 1894 Loss 28.164324\n",
      "Params: tensor([ 2.3691, -0.3305])\n",
      "Grad:  tensor([-0.5104,  2.8834])\n",
      "Epoch 1895 Loss 28.163465\n",
      "Params: tensor([ 2.3692, -0.3308])\n",
      "Grad:  tensor([-0.5104,  2.8833])\n",
      "Epoch 1896 Loss 28.162605\n",
      "Params: tensor([ 2.3692, -0.3311])\n",
      "Grad:  tensor([-0.5103,  2.8833])\n",
      "Epoch 1897 Loss 28.161751\n",
      "Params: tensor([ 2.3693, -0.3313])\n",
      "Grad:  tensor([-0.5103,  2.8832])\n",
      "Epoch 1898 Loss 28.160894\n",
      "Params: tensor([ 2.3693, -0.3316])\n",
      "Grad:  tensor([-0.5103,  2.8832])\n",
      "Epoch 1899 Loss 28.160034\n",
      "Params: tensor([ 2.3694, -0.3319])\n",
      "Grad:  tensor([-0.5103,  2.8831])\n",
      "Epoch 1900 Loss 28.159180\n",
      "Params: tensor([ 2.3694, -0.3322])\n",
      "Grad:  tensor([-0.5103,  2.8831])\n",
      "Epoch 1901 Loss 28.158319\n",
      "Params: tensor([ 2.3695, -0.3325])\n",
      "Grad:  tensor([-0.5103,  2.8830])\n",
      "Epoch 1902 Loss 28.157459\n",
      "Params: tensor([ 2.3696, -0.3328])\n",
      "Grad:  tensor([-0.5103,  2.8830])\n",
      "Epoch 1903 Loss 28.156605\n",
      "Params: tensor([ 2.3696, -0.3331])\n",
      "Grad:  tensor([-0.5102,  2.8829])\n",
      "Epoch 1904 Loss 28.155750\n",
      "Params: tensor([ 2.3697, -0.3334])\n",
      "Grad:  tensor([-0.5102,  2.8829])\n",
      "Epoch 1905 Loss 28.154890\n",
      "Params: tensor([ 2.3697, -0.3337])\n",
      "Grad:  tensor([-0.5102,  2.8828])\n",
      "Epoch 1906 Loss 28.154039\n",
      "Params: tensor([ 2.3698, -0.3339])\n",
      "Grad:  tensor([-0.5102,  2.8828])\n",
      "Epoch 1907 Loss 28.153181\n",
      "Params: tensor([ 2.3698, -0.3342])\n",
      "Grad:  tensor([-0.5102,  2.8827])\n",
      "Epoch 1908 Loss 28.152321\n",
      "Params: tensor([ 2.3699, -0.3345])\n",
      "Grad:  tensor([-0.5102,  2.8827])\n",
      "Epoch 1909 Loss 28.151464\n",
      "Params: tensor([ 2.3699, -0.3348])\n",
      "Grad:  tensor([-0.5102,  2.8826])\n",
      "Epoch 1910 Loss 28.150612\n",
      "Params: tensor([ 2.3700, -0.3351])\n",
      "Grad:  tensor([-0.5101,  2.8826])\n",
      "Epoch 1911 Loss 28.149750\n",
      "Params: tensor([ 2.3700, -0.3354])\n",
      "Grad:  tensor([-0.5101,  2.8825])\n",
      "Epoch 1912 Loss 28.148893\n",
      "Params: tensor([ 2.3701, -0.3357])\n",
      "Grad:  tensor([-0.5101,  2.8825])\n",
      "Epoch 1913 Loss 28.148033\n",
      "Params: tensor([ 2.3701, -0.3360])\n",
      "Grad:  tensor([-0.5101,  2.8824])\n",
      "Epoch 1914 Loss 28.147184\n",
      "Params: tensor([ 2.3702, -0.3362])\n",
      "Grad:  tensor([-0.5101,  2.8824])\n",
      "Epoch 1915 Loss 28.146320\n",
      "Params: tensor([ 2.3702, -0.3365])\n",
      "Grad:  tensor([-0.5101,  2.8823])\n",
      "Epoch 1916 Loss 28.145466\n",
      "Params: tensor([ 2.3703, -0.3368])\n",
      "Grad:  tensor([-0.5101,  2.8823])\n",
      "Epoch 1917 Loss 28.144611\n",
      "Params: tensor([ 2.3703, -0.3371])\n",
      "Grad:  tensor([-0.5100,  2.8823])\n",
      "Epoch 1918 Loss 28.143755\n",
      "Params: tensor([ 2.3704, -0.3374])\n",
      "Grad:  tensor([-0.5100,  2.8822])\n",
      "Epoch 1919 Loss 28.142895\n",
      "Params: tensor([ 2.3704, -0.3377])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor([-0.5100,  2.8822])\n",
      "Epoch 1920 Loss 28.142036\n",
      "Params: tensor([ 2.3705, -0.3380])\n",
      "Grad:  tensor([-0.5100,  2.8821])\n",
      "Epoch 1921 Loss 28.141186\n",
      "Params: tensor([ 2.3705, -0.3383])\n",
      "Grad:  tensor([-0.5100,  2.8821])\n",
      "Epoch 1922 Loss 28.140326\n",
      "Params: tensor([ 2.3706, -0.3386])\n",
      "Grad:  tensor([-0.5100,  2.8820])\n",
      "Epoch 1923 Loss 28.139467\n",
      "Params: tensor([ 2.3706, -0.3388])\n",
      "Grad:  tensor([-0.5100,  2.8820])\n",
      "Epoch 1924 Loss 28.138617\n",
      "Params: tensor([ 2.3707, -0.3391])\n",
      "Grad:  tensor([-0.5099,  2.8819])\n",
      "Epoch 1925 Loss 28.137756\n",
      "Params: tensor([ 2.3707, -0.3394])\n",
      "Grad:  tensor([-0.5099,  2.8819])\n",
      "Epoch 1926 Loss 28.136896\n",
      "Params: tensor([ 2.3708, -0.3397])\n",
      "Grad:  tensor([-0.5099,  2.8818])\n",
      "Epoch 1927 Loss 28.136042\n",
      "Params: tensor([ 2.3708, -0.3400])\n",
      "Grad:  tensor([-0.5099,  2.8818])\n",
      "Epoch 1928 Loss 28.135187\n",
      "Params: tensor([ 2.3709, -0.3403])\n",
      "Grad:  tensor([-0.5099,  2.8817])\n",
      "Epoch 1929 Loss 28.134331\n",
      "Params: tensor([ 2.3709, -0.3406])\n",
      "Grad:  tensor([-0.5099,  2.8817])\n",
      "Epoch 1930 Loss 28.133472\n",
      "Params: tensor([ 2.3710, -0.3409])\n",
      "Grad:  tensor([-0.5098,  2.8816])\n",
      "Epoch 1931 Loss 28.132616\n",
      "Params: tensor([ 2.3710, -0.3411])\n",
      "Grad:  tensor([-0.5098,  2.8816])\n",
      "Epoch 1932 Loss 28.131762\n",
      "Params: tensor([ 2.3711, -0.3414])\n",
      "Grad:  tensor([-0.5098,  2.8815])\n",
      "Epoch 1933 Loss 28.130898\n",
      "Params: tensor([ 2.3711, -0.3417])\n",
      "Grad:  tensor([-0.5098,  2.8815])\n",
      "Epoch 1934 Loss 28.130053\n",
      "Params: tensor([ 2.3712, -0.3420])\n",
      "Grad:  tensor([-0.5098,  2.8814])\n",
      "Epoch 1935 Loss 28.129192\n",
      "Params: tensor([ 2.3712, -0.3423])\n",
      "Grad:  tensor([-0.5098,  2.8814])\n",
      "Epoch 1936 Loss 28.128334\n",
      "Params: tensor([ 2.3713, -0.3426])\n",
      "Grad:  tensor([-0.5097,  2.8813])\n",
      "Epoch 1937 Loss 28.127478\n",
      "Params: tensor([ 2.3713, -0.3429])\n",
      "Grad:  tensor([-0.5097,  2.8813])\n",
      "Epoch 1938 Loss 28.126623\n",
      "Params: tensor([ 2.3714, -0.3432])\n",
      "Grad:  tensor([-0.5097,  2.8812])\n",
      "Epoch 1939 Loss 28.125765\n",
      "Params: tensor([ 2.3714, -0.3435])\n",
      "Grad:  tensor([-0.5097,  2.8812])\n",
      "Epoch 1940 Loss 28.124908\n",
      "Params: tensor([ 2.3715, -0.3437])\n",
      "Grad:  tensor([-0.5097,  2.8812])\n",
      "Epoch 1941 Loss 28.124054\n",
      "Params: tensor([ 2.3715, -0.3440])\n",
      "Grad:  tensor([-0.5097,  2.8811])\n",
      "Epoch 1942 Loss 28.123199\n",
      "Params: tensor([ 2.3716, -0.3443])\n",
      "Grad:  tensor([-0.5097,  2.8811])\n",
      "Epoch 1943 Loss 28.122343\n",
      "Params: tensor([ 2.3716, -0.3446])\n",
      "Grad:  tensor([-0.5096,  2.8810])\n",
      "Epoch 1944 Loss 28.121489\n",
      "Params: tensor([ 2.3717, -0.3449])\n",
      "Grad:  tensor([-0.5096,  2.8810])\n",
      "Epoch 1945 Loss 28.120634\n",
      "Params: tensor([ 2.3717, -0.3452])\n",
      "Grad:  tensor([-0.5096,  2.8809])\n",
      "Epoch 1946 Loss 28.119776\n",
      "Params: tensor([ 2.3718, -0.3455])\n",
      "Grad:  tensor([-0.5096,  2.8809])\n",
      "Epoch 1947 Loss 28.118914\n",
      "Params: tensor([ 2.3718, -0.3458])\n",
      "Grad:  tensor([-0.5096,  2.8808])\n",
      "Epoch 1948 Loss 28.118059\n",
      "Params: tensor([ 2.3719, -0.3460])\n",
      "Grad:  tensor([-0.5096,  2.8808])\n",
      "Epoch 1949 Loss 28.117207\n",
      "Params: tensor([ 2.3719, -0.3463])\n",
      "Grad:  tensor([-0.5095,  2.8807])\n",
      "Epoch 1950 Loss 28.116350\n",
      "Params: tensor([ 2.3720, -0.3466])\n",
      "Grad:  tensor([-0.5095,  2.8807])\n",
      "Epoch 1951 Loss 28.115492\n",
      "Params: tensor([ 2.3721, -0.3469])\n",
      "Grad:  tensor([-0.5095,  2.8806])\n",
      "Epoch 1952 Loss 28.114637\n",
      "Params: tensor([ 2.3721, -0.3472])\n",
      "Grad:  tensor([-0.5095,  2.8806])\n",
      "Epoch 1953 Loss 28.113787\n",
      "Params: tensor([ 2.3722, -0.3475])\n",
      "Grad:  tensor([-0.5095,  2.8805])\n",
      "Epoch 1954 Loss 28.112926\n",
      "Params: tensor([ 2.3722, -0.3478])\n",
      "Grad:  tensor([-0.5095,  2.8805])\n",
      "Epoch 1955 Loss 28.112074\n",
      "Params: tensor([ 2.3723, -0.3481])\n",
      "Grad:  tensor([-0.5094,  2.8804])\n",
      "Epoch 1956 Loss 28.111217\n",
      "Params: tensor([ 2.3723, -0.3483])\n",
      "Grad:  tensor([-0.5094,  2.8804])\n",
      "Epoch 1957 Loss 28.110363\n",
      "Params: tensor([ 2.3724, -0.3486])\n",
      "Grad:  tensor([-0.5094,  2.8803])\n",
      "Epoch 1958 Loss 28.109505\n",
      "Params: tensor([ 2.3724, -0.3489])\n",
      "Grad:  tensor([-0.5094,  2.8803])\n",
      "Epoch 1959 Loss 28.108654\n",
      "Params: tensor([ 2.3725, -0.3492])\n",
      "Grad:  tensor([-0.5094,  2.8803])\n",
      "Epoch 1960 Loss 28.107794\n",
      "Params: tensor([ 2.3725, -0.3495])\n",
      "Grad:  tensor([-0.5094,  2.8802])\n",
      "Epoch 1961 Loss 28.106936\n",
      "Params: tensor([ 2.3726, -0.3498])\n",
      "Grad:  tensor([-0.5093,  2.8802])\n",
      "Epoch 1962 Loss 28.106085\n",
      "Params: tensor([ 2.3726, -0.3501])\n",
      "Grad:  tensor([-0.5093,  2.8801])\n",
      "Epoch 1963 Loss 28.105230\n",
      "Params: tensor([ 2.3727, -0.3504])\n",
      "Grad:  tensor([-0.5093,  2.8801])\n",
      "Epoch 1964 Loss 28.104370\n",
      "Params: tensor([ 2.3727, -0.3507])\n",
      "Grad:  tensor([-0.5093,  2.8800])\n",
      "Epoch 1965 Loss 28.103521\n",
      "Params: tensor([ 2.3728, -0.3509])\n",
      "Grad:  tensor([-0.5093,  2.8800])\n",
      "Epoch 1966 Loss 28.102661\n",
      "Params: tensor([ 2.3728, -0.3512])\n",
      "Grad:  tensor([-0.5093,  2.8799])\n",
      "Epoch 1967 Loss 28.101807\n",
      "Params: tensor([ 2.3729, -0.3515])\n",
      "Grad:  tensor([-0.5092,  2.8799])\n",
      "Epoch 1968 Loss 28.100952\n",
      "Params: tensor([ 2.3729, -0.3518])\n",
      "Grad:  tensor([-0.5092,  2.8798])\n",
      "Epoch 1969 Loss 28.100098\n",
      "Params: tensor([ 2.3730, -0.3521])\n",
      "Grad:  tensor([-0.5092,  2.8798])\n",
      "Epoch 1970 Loss 28.099243\n",
      "Params: tensor([ 2.3730, -0.3524])\n",
      "Grad:  tensor([-0.5092,  2.8797])\n",
      "Epoch 1971 Loss 28.098381\n",
      "Params: tensor([ 2.3731, -0.3527])\n",
      "Grad:  tensor([-0.5092,  2.8797])\n",
      "Epoch 1972 Loss 28.097532\n",
      "Params: tensor([ 2.3731, -0.3530])\n",
      "Grad:  tensor([-0.5091,  2.8796])\n",
      "Epoch 1973 Loss 28.096668\n",
      "Params: tensor([ 2.3732, -0.3532])\n",
      "Grad:  tensor([-0.5091,  2.8796])\n",
      "Epoch 1974 Loss 28.095819\n",
      "Params: tensor([ 2.3732, -0.3535])\n",
      "Grad:  tensor([-0.5091,  2.8795])\n",
      "Epoch 1975 Loss 28.094963\n",
      "Params: tensor([ 2.3733, -0.3538])\n",
      "Grad:  tensor([-0.5091,  2.8795])\n",
      "Epoch 1976 Loss 28.094109\n",
      "Params: tensor([ 2.3733, -0.3541])\n",
      "Grad:  tensor([-0.5091,  2.8794])\n",
      "Epoch 1977 Loss 28.093254\n",
      "Params: tensor([ 2.3734, -0.3544])\n",
      "Grad:  tensor([-0.5091,  2.8794])\n",
      "Epoch 1978 Loss 28.092400\n",
      "Params: tensor([ 2.3734, -0.3547])\n",
      "Grad:  tensor([-0.5090,  2.8793])\n",
      "Epoch 1979 Loss 28.091547\n",
      "Params: tensor([ 2.3735, -0.3550])\n",
      "Grad:  tensor([-0.5090,  2.8793])\n",
      "Epoch 1980 Loss 28.090691\n",
      "Params: tensor([ 2.3735, -0.3553])\n",
      "Grad:  tensor([-0.5090,  2.8793])\n",
      "Epoch 1981 Loss 28.089836\n",
      "Params: tensor([ 2.3736, -0.3555])\n",
      "Grad:  tensor([-0.5090,  2.8792])\n",
      "Epoch 1982 Loss 28.088982\n",
      "Params: tensor([ 2.3736, -0.3558])\n",
      "Grad:  tensor([-0.5090,  2.8792])\n",
      "Epoch 1983 Loss 28.088127\n",
      "Params: tensor([ 2.3737, -0.3561])\n",
      "Grad:  tensor([-0.5090,  2.8791])\n",
      "Epoch 1984 Loss 28.087273\n",
      "Params: tensor([ 2.3737, -0.3564])\n",
      "Grad:  tensor([-0.5090,  2.8791])\n",
      "Epoch 1985 Loss 28.086412\n",
      "Params: tensor([ 2.3738, -0.3567])\n",
      "Grad:  tensor([-0.5090,  2.8790])\n",
      "Epoch 1986 Loss 28.085554\n",
      "Params: tensor([ 2.3738, -0.3570])\n",
      "Grad:  tensor([-0.5090,  2.8790])\n",
      "Epoch 1987 Loss 28.084703\n",
      "Params: tensor([ 2.3739, -0.3573])\n",
      "Grad:  tensor([-0.5090,  2.8789])\n",
      "Epoch 1988 Loss 28.083857\n",
      "Params: tensor([ 2.3739, -0.3576])\n",
      "Grad:  tensor([-0.5090,  2.8789])\n",
      "Epoch 1989 Loss 28.083000\n",
      "Params: tensor([ 2.3740, -0.3579])\n",
      "Grad:  tensor([-0.5090,  2.8788])\n",
      "Epoch 1990 Loss 28.082142\n",
      "Params: tensor([ 2.3740, -0.3581])\n",
      "Grad:  tensor([-0.5090,  2.8788])\n",
      "Epoch 1991 Loss 28.081287\n",
      "Params: tensor([ 2.3741, -0.3584])\n",
      "Grad:  tensor([-0.5090,  2.8787])\n",
      "Epoch 1992 Loss 28.080427\n",
      "Params: tensor([ 2.3741, -0.3587])\n",
      "Grad:  tensor([-0.5090,  2.8787])\n",
      "Epoch 1993 Loss 28.079578\n",
      "Params: tensor([ 2.3742, -0.3590])\n",
      "Grad:  tensor([-0.5090,  2.8786])\n",
      "Epoch 1994 Loss 28.078722\n",
      "Params: tensor([ 2.3742, -0.3593])\n",
      "Grad:  tensor([-0.5089,  2.8786])\n",
      "Epoch 1995 Loss 28.077868\n",
      "Params: tensor([ 2.3743, -0.3596])\n",
      "Grad:  tensor([-0.5089,  2.8785])\n",
      "Epoch 1996 Loss 28.077013\n",
      "Params: tensor([ 2.3743, -0.3599])\n",
      "Grad:  tensor([-0.5089,  2.8785])\n",
      "Epoch 1997 Loss 28.076164\n",
      "Params: tensor([ 2.3744, -0.3602])\n",
      "Grad:  tensor([-0.5089,  2.8784])\n",
      "Epoch 1998 Loss 28.075310\n",
      "Params: tensor([ 2.3744, -0.3604])\n",
      "Grad:  tensor([-0.5089,  2.8784])\n",
      "Epoch 1999 Loss 28.074455\n",
      "Params: tensor([ 2.3745, -0.3607])\n",
      "Grad:  tensor([-0.5089,  2.8783])\n",
      "Epoch 2000 Loss 28.073601\n",
      "Params: tensor([ 2.3745, -0.3610])\n",
      "Grad:  tensor([-0.5089,  2.8783])\n",
      "Epoch 2001 Loss 28.072748\n",
      "Params: tensor([ 2.3746, -0.3613])\n",
      "Grad:  tensor([-0.5089,  2.8782])\n",
      "Epoch 2002 Loss 28.071888\n",
      "Params: tensor([ 2.3746, -0.3616])\n",
      "Grad:  tensor([-0.5089,  2.8782])\n",
      "Epoch 2003 Loss 28.071033\n",
      "Params: tensor([ 2.3747, -0.3619])\n",
      "Grad:  tensor([-0.5089,  2.8781])\n",
      "Epoch 2004 Loss 28.070177\n",
      "Params: tensor([ 2.3747, -0.3622])\n",
      "Grad:  tensor([-0.5089,  2.8781])\n",
      "Epoch 2005 Loss 28.069324\n",
      "Params: tensor([ 2.3748, -0.3625])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor([-0.5089,  2.8780])\n",
      "Epoch 2006 Loss 28.068474\n",
      "Params: tensor([ 2.3749, -0.3627])\n",
      "Grad:  tensor([-0.5089,  2.8780])\n",
      "Epoch 2007 Loss 28.067619\n",
      "Params: tensor([ 2.3749, -0.3630])\n",
      "Grad:  tensor([-0.5089,  2.8779])\n",
      "Epoch 2008 Loss 28.066765\n",
      "Params: tensor([ 2.3750, -0.3633])\n",
      "Grad:  tensor([-0.5089,  2.8779])\n",
      "Epoch 2009 Loss 28.065907\n",
      "Params: tensor([ 2.3750, -0.3636])\n",
      "Grad:  tensor([-0.5089,  2.8778])\n",
      "Epoch 2010 Loss 28.065052\n",
      "Params: tensor([ 2.3751, -0.3639])\n",
      "Grad:  tensor([-0.5089,  2.8778])\n",
      "Epoch 2011 Loss 28.064207\n",
      "Params: tensor([ 2.3751, -0.3642])\n",
      "Grad:  tensor([-0.5089,  2.8777])\n",
      "Epoch 2012 Loss 28.063347\n",
      "Params: tensor([ 2.3752, -0.3645])\n",
      "Grad:  tensor([-0.5089,  2.8777])\n",
      "Epoch 2013 Loss 28.062494\n",
      "Params: tensor([ 2.3752, -0.3648])\n",
      "Grad:  tensor([-0.5089,  2.8776])\n",
      "Epoch 2014 Loss 28.061638\n",
      "Params: tensor([ 2.3753, -0.3650])\n",
      "Grad:  tensor([-0.5088,  2.8776])\n",
      "Epoch 2015 Loss 28.060785\n",
      "Params: tensor([ 2.3753, -0.3653])\n",
      "Grad:  tensor([-0.5088,  2.8775])\n",
      "Epoch 2016 Loss 28.059931\n",
      "Params: tensor([ 2.3754, -0.3656])\n",
      "Grad:  tensor([-0.5088,  2.8775])\n",
      "Epoch 2017 Loss 28.059080\n",
      "Params: tensor([ 2.3754, -0.3659])\n",
      "Grad:  tensor([-0.5088,  2.8774])\n",
      "Epoch 2018 Loss 28.058229\n",
      "Params: tensor([ 2.3755, -0.3662])\n",
      "Grad:  tensor([-0.5088,  2.8774])\n",
      "Epoch 2019 Loss 28.057371\n",
      "Params: tensor([ 2.3755, -0.3665])\n",
      "Grad:  tensor([-0.5088,  2.8773])\n",
      "Epoch 2020 Loss 28.056517\n",
      "Params: tensor([ 2.3756, -0.3668])\n",
      "Grad:  tensor([-0.5088,  2.8773])\n",
      "Epoch 2021 Loss 28.055662\n",
      "Params: tensor([ 2.3756, -0.3671])\n",
      "Grad:  tensor([-0.5088,  2.8772])\n",
      "Epoch 2022 Loss 28.054810\n",
      "Params: tensor([ 2.3757, -0.3673])\n",
      "Grad:  tensor([-0.5088,  2.8772])\n",
      "Epoch 2023 Loss 28.053955\n",
      "Params: tensor([ 2.3757, -0.3676])\n",
      "Grad:  tensor([-0.5088,  2.8771])\n",
      "Epoch 2024 Loss 28.053101\n",
      "Params: tensor([ 2.3758, -0.3679])\n",
      "Grad:  tensor([-0.5088,  2.8771])\n",
      "Epoch 2025 Loss 28.052252\n",
      "Params: tensor([ 2.3758, -0.3682])\n",
      "Grad:  tensor([-0.5088,  2.8770])\n",
      "Epoch 2026 Loss 28.051397\n",
      "Params: tensor([ 2.3759, -0.3685])\n",
      "Grad:  tensor([-0.5088,  2.8770])\n",
      "Epoch 2027 Loss 28.050543\n",
      "Params: tensor([ 2.3759, -0.3688])\n",
      "Grad:  tensor([-0.5088,  2.8769])\n",
      "Epoch 2028 Loss 28.049688\n",
      "Params: tensor([ 2.3760, -0.3691])\n",
      "Grad:  tensor([-0.5088,  2.8769])\n",
      "Epoch 2029 Loss 28.048836\n",
      "Params: tensor([ 2.3760, -0.3694])\n",
      "Grad:  tensor([-0.5088,  2.8768])\n",
      "Epoch 2030 Loss 28.047981\n",
      "Params: tensor([ 2.3761, -0.3697])\n",
      "Grad:  tensor([-0.5088,  2.8768])\n",
      "Epoch 2031 Loss 28.047127\n",
      "Params: tensor([ 2.3761, -0.3699])\n",
      "Grad:  tensor([-0.5087,  2.8767])\n",
      "Epoch 2032 Loss 28.046276\n",
      "Params: tensor([ 2.3762, -0.3702])\n",
      "Grad:  tensor([-0.5087,  2.8767])\n",
      "Epoch 2033 Loss 28.045418\n",
      "Params: tensor([ 2.3762, -0.3705])\n",
      "Grad:  tensor([-0.5087,  2.8766])\n",
      "Epoch 2034 Loss 28.044569\n",
      "Params: tensor([ 2.3763, -0.3708])\n",
      "Grad:  tensor([-0.5087,  2.8766])\n",
      "Epoch 2035 Loss 28.043715\n",
      "Params: tensor([ 2.3763, -0.3711])\n",
      "Grad:  tensor([-0.5087,  2.8765])\n",
      "Epoch 2036 Loss 28.042860\n",
      "Params: tensor([ 2.3764, -0.3714])\n",
      "Grad:  tensor([-0.5087,  2.8765])\n",
      "Epoch 2037 Loss 28.042009\n",
      "Params: tensor([ 2.3764, -0.3717])\n",
      "Grad:  tensor([-0.5087,  2.8764])\n",
      "Epoch 2038 Loss 28.041155\n",
      "Params: tensor([ 2.3765, -0.3720])\n",
      "Grad:  tensor([-0.5087,  2.8764])\n",
      "Epoch 2039 Loss 28.040302\n",
      "Params: tensor([ 2.3765, -0.3722])\n",
      "Grad:  tensor([-0.5087,  2.8763])\n",
      "Epoch 2040 Loss 28.039448\n",
      "Params: tensor([ 2.3766, -0.3725])\n",
      "Grad:  tensor([-0.5087,  2.8763])\n",
      "Epoch 2041 Loss 28.038597\n",
      "Params: tensor([ 2.3766, -0.3728])\n",
      "Grad:  tensor([-0.5087,  2.8762])\n",
      "Epoch 2042 Loss 28.037745\n",
      "Params: tensor([ 2.3767, -0.3731])\n",
      "Grad:  tensor([-0.5087,  2.8762])\n",
      "Epoch 2043 Loss 28.036888\n",
      "Params: tensor([ 2.3767, -0.3734])\n",
      "Grad:  tensor([-0.5087,  2.8761])\n",
      "Epoch 2044 Loss 28.036034\n",
      "Params: tensor([ 2.3768, -0.3737])\n",
      "Grad:  tensor([-0.5087,  2.8761])\n",
      "Epoch 2045 Loss 28.035185\n",
      "Params: tensor([ 2.3768, -0.3740])\n",
      "Grad:  tensor([-0.5086,  2.8760])\n",
      "Epoch 2046 Loss 28.034330\n",
      "Params: tensor([ 2.3769, -0.3743])\n",
      "Grad:  tensor([-0.5086,  2.8760])\n",
      "Epoch 2047 Loss 28.033480\n",
      "Params: tensor([ 2.3769, -0.3745])\n",
      "Grad:  tensor([-0.5086,  2.8759])\n",
      "Epoch 2048 Loss 28.032629\n",
      "Params: tensor([ 2.3770, -0.3748])\n",
      "Grad:  tensor([-0.5086,  2.8759])\n",
      "Epoch 2049 Loss 28.031775\n",
      "Params: tensor([ 2.3770, -0.3751])\n",
      "Grad:  tensor([-0.5086,  2.8758])\n",
      "Epoch 2050 Loss 28.030920\n",
      "Params: tensor([ 2.3771, -0.3754])\n",
      "Grad:  tensor([-0.5086,  2.8758])\n",
      "Epoch 2051 Loss 28.030067\n",
      "Params: tensor([ 2.3771, -0.3757])\n",
      "Grad:  tensor([-0.5086,  2.8757])\n",
      "Epoch 2052 Loss 28.029211\n",
      "Params: tensor([ 2.3772, -0.3760])\n",
      "Grad:  tensor([-0.5086,  2.8757])\n",
      "Epoch 2053 Loss 28.028358\n",
      "Params: tensor([ 2.3772, -0.3763])\n",
      "Grad:  tensor([-0.5086,  2.8757])\n",
      "Epoch 2054 Loss 28.027504\n",
      "Params: tensor([ 2.3773, -0.3766])\n",
      "Grad:  tensor([-0.5086,  2.8756])\n",
      "Epoch 2055 Loss 28.026653\n",
      "Params: tensor([ 2.3773, -0.3768])\n",
      "Grad:  tensor([-0.5086,  2.8756])\n",
      "Epoch 2056 Loss 28.025801\n",
      "Params: tensor([ 2.3774, -0.3771])\n",
      "Grad:  tensor([-0.5086,  2.8755])\n",
      "Epoch 2057 Loss 28.024946\n",
      "Params: tensor([ 2.3774, -0.3774])\n",
      "Grad:  tensor([-0.5086,  2.8755])\n",
      "Epoch 2058 Loss 28.024096\n",
      "Params: tensor([ 2.3775, -0.3777])\n",
      "Grad:  tensor([-0.5085,  2.8754])\n",
      "Epoch 2059 Loss 28.023249\n",
      "Params: tensor([ 2.3775, -0.3780])\n",
      "Grad:  tensor([-0.5085,  2.8754])\n",
      "Epoch 2060 Loss 28.022394\n",
      "Params: tensor([ 2.3776, -0.3783])\n",
      "Grad:  tensor([-0.5085,  2.8753])\n",
      "Epoch 2061 Loss 28.021540\n",
      "Params: tensor([ 2.3776, -0.3786])\n",
      "Grad:  tensor([-0.5085,  2.8753])\n",
      "Epoch 2062 Loss 28.020685\n",
      "Params: tensor([ 2.3777, -0.3789])\n",
      "Grad:  tensor([-0.5085,  2.8752])\n",
      "Epoch 2063 Loss 28.019831\n",
      "Params: tensor([ 2.3777, -0.3791])\n",
      "Grad:  tensor([-0.5085,  2.8752])\n",
      "Epoch 2064 Loss 28.018982\n",
      "Params: tensor([ 2.3778, -0.3794])\n",
      "Grad:  tensor([-0.5085,  2.8751])\n",
      "Epoch 2065 Loss 28.018127\n",
      "Params: tensor([ 2.3778, -0.3797])\n",
      "Grad:  tensor([-0.5085,  2.8751])\n",
      "Epoch 2066 Loss 28.017279\n",
      "Params: tensor([ 2.3779, -0.3800])\n",
      "Grad:  tensor([-0.5085,  2.8750])\n",
      "Epoch 2067 Loss 28.016430\n",
      "Params: tensor([ 2.3779, -0.3803])\n",
      "Grad:  tensor([-0.5085,  2.8750])\n",
      "Epoch 2068 Loss 28.015570\n",
      "Params: tensor([ 2.3780, -0.3806])\n",
      "Grad:  tensor([-0.5085,  2.8749])\n",
      "Epoch 2069 Loss 28.014721\n",
      "Params: tensor([ 2.3781, -0.3809])\n",
      "Grad:  tensor([-0.5085,  2.8749])\n",
      "Epoch 2070 Loss 28.013861\n",
      "Params: tensor([ 2.3781, -0.3812])\n",
      "Grad:  tensor([-0.5085,  2.8748])\n",
      "Epoch 2071 Loss 28.013012\n",
      "Params: tensor([ 2.3782, -0.3814])\n",
      "Grad:  tensor([-0.5084,  2.8748])\n",
      "Epoch 2072 Loss 28.012159\n",
      "Params: tensor([ 2.3782, -0.3817])\n",
      "Grad:  tensor([-0.5084,  2.8747])\n",
      "Epoch 2073 Loss 28.011309\n",
      "Params: tensor([ 2.3783, -0.3820])\n",
      "Grad:  tensor([-0.5084,  2.8747])\n",
      "Epoch 2074 Loss 28.010460\n",
      "Params: tensor([ 2.3783, -0.3823])\n",
      "Grad:  tensor([-0.5084,  2.8746])\n",
      "Epoch 2075 Loss 28.009600\n",
      "Params: tensor([ 2.3784, -0.3826])\n",
      "Grad:  tensor([-0.5084,  2.8746])\n",
      "Epoch 2076 Loss 28.008757\n",
      "Params: tensor([ 2.3784, -0.3829])\n",
      "Grad:  tensor([-0.5084,  2.8745])\n",
      "Epoch 2077 Loss 28.007904\n",
      "Params: tensor([ 2.3785, -0.3832])\n",
      "Grad:  tensor([-0.5084,  2.8745])\n",
      "Epoch 2078 Loss 28.007051\n",
      "Params: tensor([ 2.3785, -0.3835])\n",
      "Grad:  tensor([-0.5084,  2.8744])\n",
      "Epoch 2079 Loss 28.006195\n",
      "Params: tensor([ 2.3786, -0.3837])\n",
      "Grad:  tensor([-0.5084,  2.8744])\n",
      "Epoch 2080 Loss 28.005346\n",
      "Params: tensor([ 2.3786, -0.3840])\n",
      "Grad:  tensor([-0.5084,  2.8743])\n",
      "Epoch 2081 Loss 28.004494\n",
      "Params: tensor([ 2.3787, -0.3843])\n",
      "Grad:  tensor([-0.5084,  2.8743])\n",
      "Epoch 2082 Loss 28.003639\n",
      "Params: tensor([ 2.3787, -0.3846])\n",
      "Grad:  tensor([-0.5083,  2.8742])\n",
      "Epoch 2083 Loss 28.002789\n",
      "Params: tensor([ 2.3788, -0.3849])\n",
      "Grad:  tensor([-0.5083,  2.8742])\n",
      "Epoch 2084 Loss 28.001942\n",
      "Params: tensor([ 2.3788, -0.3852])\n",
      "Grad:  tensor([-0.5083,  2.8741])\n",
      "Epoch 2085 Loss 28.001087\n",
      "Params: tensor([ 2.3789, -0.3855])\n",
      "Grad:  tensor([-0.5083,  2.8741])\n",
      "Epoch 2086 Loss 28.000237\n",
      "Params: tensor([ 2.3789, -0.3858])\n",
      "Grad:  tensor([-0.5083,  2.8740])\n",
      "Epoch 2087 Loss 27.999382\n",
      "Params: tensor([ 2.3790, -0.3860])\n",
      "Grad:  tensor([-0.5083,  2.8740])\n",
      "Epoch 2088 Loss 27.998528\n",
      "Params: tensor([ 2.3790, -0.3863])\n",
      "Grad:  tensor([-0.5083,  2.8739])\n",
      "Epoch 2089 Loss 27.997681\n",
      "Params: tensor([ 2.3791, -0.3866])\n",
      "Grad:  tensor([-0.5083,  2.8739])\n",
      "Epoch 2090 Loss 27.996826\n",
      "Params: tensor([ 2.3791, -0.3869])\n",
      "Grad:  tensor([-0.5083,  2.8738])\n",
      "Epoch 2091 Loss 27.995974\n",
      "Params: tensor([ 2.3792, -0.3872])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor([-0.5083,  2.8738])\n",
      "Epoch 2092 Loss 27.995123\n",
      "Params: tensor([ 2.3792, -0.3875])\n",
      "Grad:  tensor([-0.5082,  2.8737])\n",
      "Epoch 2093 Loss 27.994274\n",
      "Params: tensor([ 2.3793, -0.3878])\n",
      "Grad:  tensor([-0.5082,  2.8737])\n",
      "Epoch 2094 Loss 27.993422\n",
      "Params: tensor([ 2.3793, -0.3881])\n",
      "Grad:  tensor([-0.5082,  2.8736])\n",
      "Epoch 2095 Loss 27.992565\n",
      "Params: tensor([ 2.3794, -0.3883])\n",
      "Grad:  tensor([-0.5082,  2.8736])\n",
      "Epoch 2096 Loss 27.991716\n",
      "Params: tensor([ 2.3794, -0.3886])\n",
      "Grad:  tensor([-0.5082,  2.8736])\n",
      "Epoch 2097 Loss 27.990864\n",
      "Params: tensor([ 2.3795, -0.3889])\n",
      "Grad:  tensor([-0.5082,  2.8735])\n",
      "Epoch 2098 Loss 27.990015\n",
      "Params: tensor([ 2.3795, -0.3892])\n",
      "Grad:  tensor([-0.5082,  2.8735])\n",
      "Epoch 2099 Loss 27.989164\n",
      "Params: tensor([ 2.3796, -0.3895])\n",
      "Grad:  tensor([-0.5082,  2.8734])\n",
      "Epoch 2100 Loss 27.988310\n",
      "Params: tensor([ 2.3796, -0.3898])\n",
      "Grad:  tensor([-0.5082,  2.8734])\n",
      "Epoch 2101 Loss 27.987465\n",
      "Params: tensor([ 2.3797, -0.3901])\n",
      "Grad:  tensor([-0.5082,  2.8733])\n",
      "Epoch 2102 Loss 27.986610\n",
      "Params: tensor([ 2.3797, -0.3904])\n",
      "Grad:  tensor([-0.5082,  2.8733])\n",
      "Epoch 2103 Loss 27.985754\n",
      "Params: tensor([ 2.3798, -0.3906])\n",
      "Grad:  tensor([-0.5081,  2.8732])\n",
      "Epoch 2104 Loss 27.984901\n",
      "Params: tensor([ 2.3798, -0.3909])\n",
      "Grad:  tensor([-0.5081,  2.8732])\n",
      "Epoch 2105 Loss 27.984053\n",
      "Params: tensor([ 2.3799, -0.3912])\n",
      "Grad:  tensor([-0.5081,  2.8731])\n",
      "Epoch 2106 Loss 27.983204\n",
      "Params: tensor([ 2.3799, -0.3915])\n",
      "Grad:  tensor([-0.5081,  2.8731])\n",
      "Epoch 2107 Loss 27.982349\n",
      "Params: tensor([ 2.3800, -0.3918])\n",
      "Grad:  tensor([-0.5081,  2.8730])\n",
      "Epoch 2108 Loss 27.981504\n",
      "Params: tensor([ 2.3800, -0.3921])\n",
      "Grad:  tensor([-0.5081,  2.8730])\n",
      "Epoch 2109 Loss 27.980650\n",
      "Params: tensor([ 2.3801, -0.3924])\n",
      "Grad:  tensor([-0.5081,  2.8729])\n",
      "Epoch 2110 Loss 27.979797\n",
      "Params: tensor([ 2.3801, -0.3927])\n",
      "Grad:  tensor([-0.5081,  2.8729])\n",
      "Epoch 2111 Loss 27.978945\n",
      "Params: tensor([ 2.3802, -0.3929])\n",
      "Grad:  tensor([-0.5081,  2.8728])\n",
      "Epoch 2112 Loss 27.978094\n",
      "Params: tensor([ 2.3802, -0.3932])\n",
      "Grad:  tensor([-0.5080,  2.8728])\n",
      "Epoch 2113 Loss 27.977247\n",
      "Params: tensor([ 2.3803, -0.3935])\n",
      "Grad:  tensor([-0.5080,  2.8727])\n",
      "Epoch 2114 Loss 27.976391\n",
      "Params: tensor([ 2.3803, -0.3938])\n",
      "Grad:  tensor([-0.5080,  2.8727])\n",
      "Epoch 2115 Loss 27.975542\n",
      "Params: tensor([ 2.3804, -0.3941])\n",
      "Grad:  tensor([-0.5080,  2.8726])\n",
      "Epoch 2116 Loss 27.974695\n",
      "Params: tensor([ 2.3804, -0.3944])\n",
      "Grad:  tensor([-0.5080,  2.8726])\n",
      "Epoch 2117 Loss 27.973845\n",
      "Params: tensor([ 2.3805, -0.3947])\n",
      "Grad:  tensor([-0.5080,  2.8725])\n",
      "Epoch 2118 Loss 27.972992\n",
      "Params: tensor([ 2.3805, -0.3949])\n",
      "Grad:  tensor([-0.5080,  2.8725])\n",
      "Epoch 2119 Loss 27.972139\n",
      "Params: tensor([ 2.3806, -0.3952])\n",
      "Grad:  tensor([-0.5080,  2.8724])\n",
      "Epoch 2120 Loss 27.971285\n",
      "Params: tensor([ 2.3806, -0.3955])\n",
      "Grad:  tensor([-0.5080,  2.8724])\n",
      "Epoch 2121 Loss 27.970436\n",
      "Params: tensor([ 2.3807, -0.3958])\n",
      "Grad:  tensor([-0.5079,  2.8723])\n",
      "Epoch 2122 Loss 27.969585\n",
      "Params: tensor([ 2.3807, -0.3961])\n",
      "Grad:  tensor([-0.5079,  2.8723])\n",
      "Epoch 2123 Loss 27.968733\n",
      "Params: tensor([ 2.3808, -0.3964])\n",
      "Grad:  tensor([-0.5079,  2.8722])\n",
      "Epoch 2124 Loss 27.967884\n",
      "Params: tensor([ 2.3808, -0.3967])\n",
      "Grad:  tensor([-0.5079,  2.8722])\n",
      "Epoch 2125 Loss 27.967039\n",
      "Params: tensor([ 2.3809, -0.3970])\n",
      "Grad:  tensor([-0.5079,  2.8721])\n",
      "Epoch 2126 Loss 27.966179\n",
      "Params: tensor([ 2.3809, -0.3972])\n",
      "Grad:  tensor([-0.5079,  2.8721])\n",
      "Epoch 2127 Loss 27.965332\n",
      "Params: tensor([ 2.3810, -0.3975])\n",
      "Grad:  tensor([-0.5079,  2.8721])\n",
      "Epoch 2128 Loss 27.964483\n",
      "Params: tensor([ 2.3810, -0.3978])\n",
      "Grad:  tensor([-0.5079,  2.8720])\n",
      "Epoch 2129 Loss 27.963634\n",
      "Params: tensor([ 2.3811, -0.3981])\n",
      "Grad:  tensor([-0.5079,  2.8720])\n",
      "Epoch 2130 Loss 27.962786\n",
      "Params: tensor([ 2.3811, -0.3984])\n",
      "Grad:  tensor([-0.5078,  2.8719])\n",
      "Epoch 2131 Loss 27.961926\n",
      "Params: tensor([ 2.3812, -0.3987])\n",
      "Grad:  tensor([-0.5078,  2.8719])\n",
      "Epoch 2132 Loss 27.961079\n",
      "Params: tensor([ 2.3812, -0.3990])\n",
      "Grad:  tensor([-0.5078,  2.8718])\n",
      "Epoch 2133 Loss 27.960230\n",
      "Params: tensor([ 2.3813, -0.3993])\n",
      "Grad:  tensor([-0.5078,  2.8718])\n",
      "Epoch 2134 Loss 27.959379\n",
      "Params: tensor([ 2.3814, -0.3995])\n",
      "Grad:  tensor([-0.5078,  2.8717])\n",
      "Epoch 2135 Loss 27.958532\n",
      "Params: tensor([ 2.3814, -0.3998])\n",
      "Grad:  tensor([-0.5078,  2.8717])\n",
      "Epoch 2136 Loss 27.957680\n",
      "Params: tensor([ 2.3815, -0.4001])\n",
      "Grad:  tensor([-0.5078,  2.8716])\n",
      "Epoch 2137 Loss 27.956835\n",
      "Params: tensor([ 2.3815, -0.4004])\n",
      "Grad:  tensor([-0.5078,  2.8716])\n",
      "Epoch 2138 Loss 27.955975\n",
      "Params: tensor([ 2.3816, -0.4007])\n",
      "Grad:  tensor([-0.5077,  2.8715])\n",
      "Epoch 2139 Loss 27.955126\n",
      "Params: tensor([ 2.3816, -0.4010])\n",
      "Grad:  tensor([-0.5077,  2.8715])\n",
      "Epoch 2140 Loss 27.954271\n",
      "Params: tensor([ 2.3817, -0.4013])\n",
      "Grad:  tensor([-0.5077,  2.8714])\n",
      "Epoch 2141 Loss 27.953428\n",
      "Params: tensor([ 2.3817, -0.4016])\n",
      "Grad:  tensor([-0.5077,  2.8714])\n",
      "Epoch 2142 Loss 27.952574\n",
      "Params: tensor([ 2.3818, -0.4018])\n",
      "Grad:  tensor([-0.5077,  2.8713])\n",
      "Epoch 2143 Loss 27.951723\n",
      "Params: tensor([ 2.3818, -0.4021])\n",
      "Grad:  tensor([-0.5077,  2.8713])\n",
      "Epoch 2144 Loss 27.950878\n",
      "Params: tensor([ 2.3819, -0.4024])\n",
      "Grad:  tensor([-0.5077,  2.8712])\n",
      "Epoch 2145 Loss 27.950026\n",
      "Params: tensor([ 2.3819, -0.4027])\n",
      "Grad:  tensor([-0.5077,  2.8712])\n",
      "Epoch 2146 Loss 27.949175\n",
      "Params: tensor([ 2.3820, -0.4030])\n",
      "Grad:  tensor([-0.5077,  2.8711])\n",
      "Epoch 2147 Loss 27.948328\n",
      "Params: tensor([ 2.3820, -0.4033])\n",
      "Grad:  tensor([-0.5076,  2.8711])\n",
      "Epoch 2148 Loss 27.947477\n",
      "Params: tensor([ 2.3821, -0.4036])\n",
      "Grad:  tensor([-0.5076,  2.8710])\n",
      "Epoch 2149 Loss 27.946625\n",
      "Params: tensor([ 2.3821, -0.4039])\n",
      "Grad:  tensor([-0.5076,  2.8710])\n",
      "Epoch 2150 Loss 27.945776\n",
      "Params: tensor([ 2.3822, -0.4041])\n",
      "Grad:  tensor([-0.5076,  2.8709])\n",
      "Epoch 2151 Loss 27.944927\n",
      "Params: tensor([ 2.3822, -0.4044])\n",
      "Grad:  tensor([-0.5076,  2.8709])\n",
      "Epoch 2152 Loss 27.944078\n",
      "Params: tensor([ 2.3823, -0.4047])\n",
      "Grad:  tensor([-0.5076,  2.8708])\n",
      "Epoch 2153 Loss 27.943226\n",
      "Params: tensor([ 2.3823, -0.4050])\n",
      "Grad:  tensor([-0.5076,  2.8708])\n",
      "Epoch 2154 Loss 27.942377\n",
      "Params: tensor([ 2.3824, -0.4053])\n",
      "Grad:  tensor([-0.5076,  2.8707])\n",
      "Epoch 2155 Loss 27.941523\n",
      "Params: tensor([ 2.3824, -0.4056])\n",
      "Grad:  tensor([-0.5075,  2.8707])\n",
      "Epoch 2156 Loss 27.940674\n",
      "Params: tensor([ 2.3825, -0.4059])\n",
      "Grad:  tensor([-0.5075,  2.8707])\n",
      "Epoch 2157 Loss 27.939821\n",
      "Params: tensor([ 2.3825, -0.4061])\n",
      "Grad:  tensor([-0.5075,  2.8706])\n",
      "Epoch 2158 Loss 27.938972\n",
      "Params: tensor([ 2.3826, -0.4064])\n",
      "Grad:  tensor([-0.5075,  2.8706])\n",
      "Epoch 2159 Loss 27.938124\n",
      "Params: tensor([ 2.3826, -0.4067])\n",
      "Grad:  tensor([-0.5075,  2.8705])\n",
      "Epoch 2160 Loss 27.937279\n",
      "Params: tensor([ 2.3827, -0.4070])\n",
      "Grad:  tensor([-0.5075,  2.8705])\n",
      "Epoch 2161 Loss 27.936424\n",
      "Params: tensor([ 2.3827, -0.4073])\n",
      "Grad:  tensor([-0.5075,  2.8704])\n",
      "Epoch 2162 Loss 27.935579\n",
      "Params: tensor([ 2.3828, -0.4076])\n",
      "Grad:  tensor([-0.5075,  2.8704])\n",
      "Epoch 2163 Loss 27.934725\n",
      "Params: tensor([ 2.3828, -0.4079])\n",
      "Grad:  tensor([-0.5074,  2.8703])\n",
      "Epoch 2164 Loss 27.933880\n",
      "Params: tensor([ 2.3829, -0.4082])\n",
      "Grad:  tensor([-0.5074,  2.8703])\n",
      "Epoch 2165 Loss 27.933027\n",
      "Params: tensor([ 2.3829, -0.4084])\n",
      "Grad:  tensor([-0.5074,  2.8702])\n",
      "Epoch 2166 Loss 27.932182\n",
      "Params: tensor([ 2.3830, -0.4087])\n",
      "Grad:  tensor([-0.5074,  2.8702])\n",
      "Epoch 2167 Loss 27.931330\n",
      "Params: tensor([ 2.3830, -0.4090])\n",
      "Grad:  tensor([-0.5074,  2.8701])\n",
      "Epoch 2168 Loss 27.930483\n",
      "Params: tensor([ 2.3831, -0.4093])\n",
      "Grad:  tensor([-0.5074,  2.8701])\n",
      "Epoch 2169 Loss 27.929626\n",
      "Params: tensor([ 2.3831, -0.4096])\n",
      "Grad:  tensor([-0.5074,  2.8700])\n",
      "Epoch 2170 Loss 27.928783\n",
      "Params: tensor([ 2.3832, -0.4099])\n",
      "Grad:  tensor([-0.5073,  2.8700])\n",
      "Epoch 2171 Loss 27.927935\n",
      "Params: tensor([ 2.3832, -0.4102])\n",
      "Grad:  tensor([-0.5073,  2.8699])\n",
      "Epoch 2172 Loss 27.927082\n",
      "Params: tensor([ 2.3833, -0.4105])\n",
      "Grad:  tensor([-0.5073,  2.8699])\n",
      "Epoch 2173 Loss 27.926231\n",
      "Params: tensor([ 2.3833, -0.4107])\n",
      "Grad:  tensor([-0.5073,  2.8698])\n",
      "Epoch 2174 Loss 27.925379\n",
      "Params: tensor([ 2.3834, -0.4110])\n",
      "Grad:  tensor([-0.5073,  2.8698])\n",
      "Epoch 2175 Loss 27.924532\n",
      "Params: tensor([ 2.3834, -0.4113])\n",
      "Grad:  tensor([-0.5073,  2.8697])\n",
      "Epoch 2176 Loss 27.923677\n",
      "Params: tensor([ 2.3835, -0.4116])\n",
      "Grad:  tensor([-0.5073,  2.8697])\n",
      "Epoch 2177 Loss 27.922838\n",
      "Params: tensor([ 2.3835, -0.4119])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor([-0.5073,  2.8696])\n",
      "Epoch 2178 Loss 27.921986\n",
      "Params: tensor([ 2.3836, -0.4122])\n",
      "Grad:  tensor([-0.5072,  2.8696])\n",
      "Epoch 2179 Loss 27.921141\n",
      "Params: tensor([ 2.3836, -0.4125])\n",
      "Grad:  tensor([-0.5072,  2.8696])\n",
      "Epoch 2180 Loss 27.920286\n",
      "Params: tensor([ 2.3837, -0.4127])\n",
      "Grad:  tensor([-0.5072,  2.8695])\n",
      "Epoch 2181 Loss 27.919439\n",
      "Params: tensor([ 2.3837, -0.4130])\n",
      "Grad:  tensor([-0.5072,  2.8695])\n",
      "Epoch 2182 Loss 27.918591\n",
      "Params: tensor([ 2.3838, -0.4133])\n",
      "Grad:  tensor([-0.5072,  2.8694])\n",
      "Epoch 2183 Loss 27.917742\n",
      "Params: tensor([ 2.3838, -0.4136])\n",
      "Grad:  tensor([-0.5072,  2.8694])\n",
      "Epoch 2184 Loss 27.916893\n",
      "Params: tensor([ 2.3839, -0.4139])\n",
      "Grad:  tensor([-0.5072,  2.8693])\n",
      "Epoch 2185 Loss 27.916040\n",
      "Params: tensor([ 2.3839, -0.4142])\n",
      "Grad:  tensor([-0.5071,  2.8693])\n",
      "Epoch 2186 Loss 27.915194\n",
      "Params: tensor([ 2.3840, -0.4145])\n",
      "Grad:  tensor([-0.5071,  2.8692])\n",
      "Epoch 2187 Loss 27.914343\n",
      "Params: tensor([ 2.3840, -0.4148])\n",
      "Grad:  tensor([-0.5071,  2.8692])\n",
      "Epoch 2188 Loss 27.913502\n",
      "Params: tensor([ 2.3841, -0.4150])\n",
      "Grad:  tensor([-0.5071,  2.8691])\n",
      "Epoch 2189 Loss 27.912647\n",
      "Params: tensor([ 2.3841, -0.4153])\n",
      "Grad:  tensor([-0.5071,  2.8691])\n",
      "Epoch 2190 Loss 27.911797\n",
      "Params: tensor([ 2.3842, -0.4156])\n",
      "Grad:  tensor([-0.5071,  2.8690])\n",
      "Epoch 2191 Loss 27.910948\n",
      "Params: tensor([ 2.3842, -0.4159])\n",
      "Grad:  tensor([-0.5070,  2.8690])\n",
      "Epoch 2192 Loss 27.910093\n",
      "Params: tensor([ 2.3843, -0.4162])\n",
      "Grad:  tensor([-0.5070,  2.8689])\n",
      "Epoch 2193 Loss 27.909248\n",
      "Params: tensor([ 2.3843, -0.4165])\n",
      "Grad:  tensor([-0.5070,  2.8689])\n",
      "Epoch 2194 Loss 27.908400\n",
      "Params: tensor([ 2.3844, -0.4168])\n",
      "Grad:  tensor([-0.5070,  2.8688])\n",
      "Epoch 2195 Loss 27.907557\n",
      "Params: tensor([ 2.3844, -0.4171])\n",
      "Grad:  tensor([-0.5070,  2.8688])\n",
      "Epoch 2196 Loss 27.906706\n",
      "Params: tensor([ 2.3845, -0.4173])\n",
      "Grad:  tensor([-0.5070,  2.8687])\n",
      "Epoch 2197 Loss 27.905855\n",
      "Params: tensor([ 2.3846, -0.4176])\n",
      "Grad:  tensor([-0.5070,  2.8687])\n",
      "Epoch 2198 Loss 27.905006\n",
      "Params: tensor([ 2.3846, -0.4179])\n",
      "Grad:  tensor([-0.5070,  2.8686])\n",
      "Epoch 2199 Loss 27.904156\n",
      "Params: tensor([ 2.3847, -0.4182])\n",
      "Grad:  tensor([-0.5069,  2.8686])\n",
      "Epoch 2200 Loss 27.903313\n",
      "Params: tensor([ 2.3847, -0.4185])\n",
      "Grad:  tensor([-0.5069,  2.8685])\n",
      "Epoch 2201 Loss 27.902458\n",
      "Params: tensor([ 2.3848, -0.4188])\n",
      "Grad:  tensor([-0.5069,  2.8685])\n",
      "Epoch 2202 Loss 27.901611\n",
      "Params: tensor([ 2.3848, -0.4191])\n",
      "Grad:  tensor([-0.5069,  2.8685])\n",
      "Epoch 2203 Loss 27.900764\n",
      "Params: tensor([ 2.3849, -0.4193])\n",
      "Grad:  tensor([-0.5069,  2.8684])\n",
      "Epoch 2204 Loss 27.899916\n",
      "Params: tensor([ 2.3849, -0.4196])\n",
      "Grad:  tensor([-0.5069,  2.8684])\n",
      "Epoch 2205 Loss 27.899071\n",
      "Params: tensor([ 2.3850, -0.4199])\n",
      "Grad:  tensor([-0.5068,  2.8683])\n",
      "Epoch 2206 Loss 27.898218\n",
      "Params: tensor([ 2.3850, -0.4202])\n",
      "Grad:  tensor([-0.5068,  2.8683])\n",
      "Epoch 2207 Loss 27.897367\n",
      "Params: tensor([ 2.3851, -0.4205])\n",
      "Grad:  tensor([-0.5068,  2.8682])\n",
      "Epoch 2208 Loss 27.896521\n",
      "Params: tensor([ 2.3851, -0.4208])\n",
      "Grad:  tensor([-0.5068,  2.8682])\n",
      "Epoch 2209 Loss 27.895672\n",
      "Params: tensor([ 2.3852, -0.4211])\n",
      "Grad:  tensor([-0.5068,  2.8681])\n",
      "Epoch 2210 Loss 27.894825\n",
      "Params: tensor([ 2.3852, -0.4214])\n",
      "Grad:  tensor([-0.5068,  2.8681])\n",
      "Epoch 2211 Loss 27.893976\n",
      "Params: tensor([ 2.3853, -0.4216])\n",
      "Grad:  tensor([-0.5068,  2.8680])\n",
      "Epoch 2212 Loss 27.893127\n",
      "Params: tensor([ 2.3853, -0.4219])\n",
      "Grad:  tensor([-0.5067,  2.8680])\n",
      "Epoch 2213 Loss 27.892281\n",
      "Params: tensor([ 2.3854, -0.4222])\n",
      "Grad:  tensor([-0.5067,  2.8679])\n",
      "Epoch 2214 Loss 27.891432\n",
      "Params: tensor([ 2.3854, -0.4225])\n",
      "Grad:  tensor([-0.5067,  2.8679])\n",
      "Epoch 2215 Loss 27.890583\n",
      "Params: tensor([ 2.3855, -0.4228])\n",
      "Grad:  tensor([-0.5067,  2.8678])\n",
      "Epoch 2216 Loss 27.889738\n",
      "Params: tensor([ 2.3855, -0.4231])\n",
      "Grad:  tensor([-0.5067,  2.8678])\n",
      "Epoch 2217 Loss 27.888887\n",
      "Params: tensor([ 2.3856, -0.4234])\n",
      "Grad:  tensor([-0.5067,  2.8677])\n",
      "Epoch 2218 Loss 27.888039\n",
      "Params: tensor([ 2.3856, -0.4236])\n",
      "Grad:  tensor([-0.5066,  2.8677])\n",
      "Epoch 2219 Loss 27.887194\n",
      "Params: tensor([ 2.3857, -0.4239])\n",
      "Grad:  tensor([-0.5066,  2.8676])\n",
      "Epoch 2220 Loss 27.886347\n",
      "Params: tensor([ 2.3857, -0.4242])\n",
      "Grad:  tensor([-0.5066,  2.8676])\n",
      "Epoch 2221 Loss 27.885496\n",
      "Params: tensor([ 2.3858, -0.4245])\n",
      "Grad:  tensor([-0.5066,  2.8675])\n",
      "Epoch 2222 Loss 27.884649\n",
      "Params: tensor([ 2.3858, -0.4248])\n",
      "Grad:  tensor([-0.5066,  2.8675])\n",
      "Epoch 2223 Loss 27.883802\n",
      "Params: tensor([ 2.3859, -0.4251])\n",
      "Grad:  tensor([-0.5066,  2.8674])\n",
      "Epoch 2224 Loss 27.882948\n",
      "Params: tensor([ 2.3859, -0.4254])\n",
      "Grad:  tensor([-0.5066,  2.8674])\n",
      "Epoch 2225 Loss 27.882109\n",
      "Params: tensor([ 2.3860, -0.4257])\n",
      "Grad:  tensor([-0.5066,  2.8673])\n",
      "Epoch 2226 Loss 27.881256\n",
      "Params: tensor([ 2.3860, -0.4259])\n",
      "Grad:  tensor([-0.5066,  2.8673])\n",
      "Epoch 2227 Loss 27.880407\n",
      "Params: tensor([ 2.3861, -0.4262])\n",
      "Grad:  tensor([-0.5066,  2.8672])\n",
      "Epoch 2228 Loss 27.879564\n",
      "Params: tensor([ 2.3861, -0.4265])\n",
      "Grad:  tensor([-0.5066,  2.8672])\n",
      "Epoch 2229 Loss 27.878706\n",
      "Params: tensor([ 2.3862, -0.4268])\n",
      "Grad:  tensor([-0.5066,  2.8671])\n",
      "Epoch 2230 Loss 27.877867\n",
      "Params: tensor([ 2.3862, -0.4271])\n",
      "Grad:  tensor([-0.5066,  2.8671])\n",
      "Epoch 2231 Loss 27.877020\n",
      "Params: tensor([ 2.3863, -0.4274])\n",
      "Grad:  tensor([-0.5066,  2.8670])\n",
      "Epoch 2232 Loss 27.876171\n",
      "Params: tensor([ 2.3863, -0.4277])\n",
      "Grad:  tensor([-0.5066,  2.8670])\n",
      "Epoch 2233 Loss 27.875319\n",
      "Params: tensor([ 2.3864, -0.4280])\n",
      "Grad:  tensor([-0.5066,  2.8669])\n",
      "Epoch 2234 Loss 27.874475\n",
      "Params: tensor([ 2.3864, -0.4282])\n",
      "Grad:  tensor([-0.5041,  2.8518])\n",
      "Epoch 2546 Loss 27.611439\n",
      "Params: tensor([ 2.4022, -0.5174])\n",
      "Grad:  tensor([-0.5041,  2.8517])\n",
      "Epoch 2547 Loss 27.610596\n",
      "Params: tensor([ 2.4022, -0.5177])\n",
      "Grad:  tensor([-0.5041,  2.8517])\n",
      "Epoch 2548 Loss 27.609760\n",
      "Params: tensor([ 2.4023, -0.5180])\n",
      "Grad:  tensor([-0.5041,  2.8516])\n",
      "Epoch 2549 Loss 27.608919\n",
      "Params: tensor([ 2.4023, -0.5183])\n",
      "Grad:  tensor([-0.5041,  2.8516])\n",
      "Epoch 2550 Loss 27.608080\n",
      "Params: tensor([ 2.4024, -0.5186])\n",
      "Grad:  tensor([-0.5041,  2.8515])\n",
      "Epoch 2551 Loss 27.607246\n",
      "Params: tensor([ 2.4024, -0.5189])\n",
      "Grad:  tensor([-0.5041,  2.8515])\n",
      "Epoch 2552 Loss 27.606407\n",
      "Params: tensor([ 2.4025, -0.5192])\n",
      "Grad:  tensor([-0.5041,  2.8514])\n",
      "Epoch 2553 Loss 27.605566\n",
      "Params: tensor([ 2.4025, -0.5194])\n",
      "Grad:  tensor([-0.5040,  2.8514])\n",
      "Epoch 2554 Loss 27.604729\n",
      "Params: tensor([ 2.4026, -0.5197])\n",
      "Grad:  tensor([-0.5040,  2.8513])\n",
      "Epoch 2555 Loss 27.603889\n",
      "Params: tensor([ 2.4026, -0.5200])\n",
      "Grad:  tensor([-0.5040,  2.8513])\n",
      "Epoch 2556 Loss 27.603052\n",
      "Params: tensor([ 2.4027, -0.5203])\n",
      "Grad:  tensor([-0.5040,  2.8512])\n",
      "Epoch 2557 Loss 27.602209\n",
      "Params: tensor([ 2.4027, -0.5206])\n",
      "Grad:  tensor([-0.5040,  2.8512])\n",
      "Epoch 2558 Loss 27.601374\n",
      "Params: tensor([ 2.4028, -0.5209])\n",
      "Grad:  tensor([-0.5040,  2.8511])\n",
      "Epoch 2559 Loss 27.600531\n",
      "Params: tensor([ 2.4028, -0.5212])\n",
      "Grad:  tensor([-0.5040,  2.8511])\n",
      "Epoch 2560 Loss 27.599695\n",
      "Params: tensor([ 2.4029, -0.5214])\n",
      "Grad:  tensor([-0.5040,  2.8510])\n",
      "Epoch 2561 Loss 27.598858\n",
      "Params: tensor([ 2.4029, -0.5217])\n",
      "Grad:  tensor([-0.5040,  2.8510])\n",
      "Epoch 2562 Loss 27.598017\n",
      "Params: tensor([ 2.4030, -0.5220])\n",
      "Grad:  tensor([-0.5040,  2.8509])\n",
      "Epoch 2563 Loss 27.597176\n",
      "Params: tensor([ 2.4030, -0.5223])\n",
      "Grad:  tensor([-0.5040,  2.8509])\n",
      "Epoch 2564 Loss 27.596342\n",
      "Params: tensor([ 2.4031, -0.5226])\n",
      "Grad:  tensor([-0.5040,  2.8508])\n",
      "Epoch 2565 Loss 27.595507\n",
      "Params: tensor([ 2.4031, -0.5229])\n",
      "Grad:  tensor([-0.5040,  2.8508])\n",
      "Epoch 2566 Loss 27.594666\n",
      "Params: tensor([ 2.4032, -0.5232])\n",
      "Grad:  tensor([-0.5040,  2.8507])\n",
      "Epoch 2567 Loss 27.593824\n",
      "Params: tensor([ 2.4032, -0.5234])\n",
      "Grad:  tensor([-0.5040,  2.8507])\n",
      "Epoch 2568 Loss 27.592993\n",
      "Params: tensor([ 2.4033, -0.5237])\n",
      "Grad:  tensor([-0.5040,  2.8506])\n",
      "Epoch 2569 Loss 27.592157\n",
      "Params: tensor([ 2.4033, -0.5240])\n",
      "Grad:  tensor([-0.5040,  2.8506])\n",
      "Epoch 2570 Loss 27.591314\n",
      "Params: tensor([ 2.4034, -0.5243])\n",
      "Grad:  tensor([-0.5040,  2.8505])\n",
      "Epoch 2571 Loss 27.590479\n",
      "Params: tensor([ 2.4034, -0.5246])\n",
      "Grad:  tensor([-0.5040,  2.8505])\n",
      "Epoch 2572 Loss 27.589638\n",
      "Params: tensor([ 2.4035, -0.5249])\n",
      "Grad:  tensor([-0.5039,  2.8504])\n",
      "Epoch 2573 Loss 27.588795\n",
      "Params: tensor([ 2.4035, -0.5251])\n",
      "Grad:  tensor([-0.5039,  2.8504])\n",
      "Epoch 2574 Loss 27.587963\n",
      "Params: tensor([ 2.4036, -0.5254])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor([-0.5039,  2.8503])\n",
      "Epoch 2575 Loss 27.587122\n",
      "Params: tensor([ 2.4036, -0.5257])\n",
      "Grad:  tensor([-0.5039,  2.8503])\n",
      "Epoch 2576 Loss 27.586290\n",
      "Params: tensor([ 2.4037, -0.5260])\n",
      "Grad:  tensor([-0.5039,  2.8502])\n",
      "Epoch 2577 Loss 27.585447\n",
      "Params: tensor([ 2.4037, -0.5263])\n",
      "Grad:  tensor([-0.5039,  2.8502])\n",
      "Epoch 2578 Loss 27.584612\n",
      "Params: tensor([ 2.4038, -0.5266])\n",
      "Grad:  tensor([-0.5039,  2.8501])\n",
      "Epoch 2579 Loss 27.583773\n",
      "Params: tensor([ 2.4038, -0.5269])\n",
      "Grad:  tensor([-0.5039,  2.8501])\n",
      "Epoch 2580 Loss 27.582935\n",
      "Params: tensor([ 2.4039, -0.5271])\n",
      "Grad:  tensor([-0.5039,  2.8500])\n",
      "Epoch 2581 Loss 27.582100\n",
      "Params: tensor([ 2.4039, -0.5274])\n",
      "Grad:  tensor([-0.5039,  2.8500])\n",
      "Epoch 2582 Loss 27.581257\n",
      "Params: tensor([ 2.4040, -0.5277])\n",
      "Grad:  tensor([-0.5039,  2.8500])\n",
      "Epoch 2583 Loss 27.580425\n",
      "Params: tensor([ 2.4040, -0.5280])\n",
      "Grad:  tensor([-0.5039,  2.8499])\n",
      "Epoch 2584 Loss 27.579584\n",
      "Params: tensor([ 2.4041, -0.5283])\n",
      "Grad:  tensor([-0.5039,  2.8499])\n",
      "Epoch 2585 Loss 27.578749\n",
      "Params: tensor([ 2.4041, -0.5286])\n",
      "Grad:  tensor([-0.5039,  2.8498])\n",
      "Epoch 2586 Loss 27.577911\n",
      "Params: tensor([ 2.4042, -0.5289])\n",
      "Grad:  tensor([-0.5039,  2.8498])\n",
      "Epoch 2587 Loss 27.577074\n",
      "Params: tensor([ 2.4042, -0.5291])\n",
      "Grad:  tensor([-0.5039,  2.8497])\n",
      "Epoch 2588 Loss 27.576235\n",
      "Params: tensor([ 2.4043, -0.5294])\n",
      "Grad:  tensor([-0.5039,  2.8497])\n",
      "Epoch 2589 Loss 27.575396\n",
      "Params: tensor([ 2.4043, -0.5297])\n",
      "Grad:  tensor([-0.5038,  2.8496])\n",
      "Epoch 2590 Loss 27.574556\n",
      "Params: tensor([ 2.4044, -0.5300])\n",
      "Grad:  tensor([-0.5038,  2.8496])\n",
      "Epoch 2591 Loss 27.573725\n",
      "Params: tensor([ 2.4044, -0.5303])\n",
      "Grad:  tensor([-0.5038,  2.8495])\n",
      "Epoch 2592 Loss 27.572887\n",
      "Params: tensor([ 2.4045, -0.5306])\n",
      "Grad:  tensor([-0.5038,  2.8495])\n",
      "Epoch 2593 Loss 27.572050\n",
      "Params: tensor([ 2.4045, -0.5308])\n",
      "Grad:  tensor([-0.5038,  2.8494])\n",
      "Epoch 2594 Loss 27.571211\n",
      "Params: tensor([ 2.4046, -0.5311])\n",
      "Grad:  tensor([-0.5038,  2.8494])\n",
      "Epoch 2595 Loss 27.570375\n",
      "Params: tensor([ 2.4046, -0.5314])\n",
      "Grad:  tensor([-0.5038,  2.8493])\n",
      "Epoch 2596 Loss 27.569542\n",
      "Params: tensor([ 2.4047, -0.5317])\n",
      "Grad:  tensor([-0.5038,  2.8493])\n",
      "Epoch 2597 Loss 27.568697\n",
      "Params: tensor([ 2.4047, -0.5320])\n",
      "Grad:  tensor([-0.5038,  2.8492])\n",
      "Epoch 2598 Loss 27.567863\n",
      "Params: tensor([ 2.4048, -0.5323])\n",
      "Grad:  tensor([-0.5038,  2.8492])\n",
      "Epoch 2599 Loss 27.567024\n",
      "Params: tensor([ 2.4048, -0.5326])\n",
      "Grad:  tensor([-0.5038,  2.8491])\n",
      "Epoch 2600 Loss 27.566191\n",
      "Params: tensor([ 2.4049, -0.5328])\n",
      "Grad:  tensor([-0.5038,  2.8491])\n",
      "Epoch 2601 Loss 27.565351\n",
      "Params: tensor([ 2.4049, -0.5331])\n",
      "Grad:  tensor([-0.5038,  2.8490])\n",
      "Epoch 2602 Loss 27.564512\n",
      "Params: tensor([ 2.4050, -0.5334])\n",
      "Grad:  tensor([-0.5038,  2.8490])\n",
      "Epoch 2603 Loss 27.563679\n",
      "Params: tensor([ 2.4050, -0.5337])\n",
      "Grad:  tensor([-0.5037,  2.8489])\n",
      "Epoch 2604 Loss 27.562838\n",
      "Params: tensor([ 2.4051, -0.5340])\n",
      "Grad:  tensor([-0.5037,  2.8489])\n",
      "Epoch 2605 Loss 27.562004\n",
      "Params: tensor([ 2.4052, -0.5343])\n",
      "Grad:  tensor([-0.5037,  2.8488])\n",
      "Epoch 2606 Loss 27.561165\n",
      "Params: tensor([ 2.4052, -0.5345])\n",
      "Grad:  tensor([-0.5037,  2.8488])\n",
      "Epoch 2607 Loss 27.560328\n",
      "Params: tensor([ 2.4053, -0.5348])\n",
      "Grad:  tensor([-0.5037,  2.8487])\n",
      "Epoch 2608 Loss 27.559492\n",
      "Params: tensor([ 2.4053, -0.5351])\n",
      "Grad:  tensor([-0.5037,  2.8487])\n",
      "Epoch 2609 Loss 27.558657\n",
      "Params: tensor([ 2.4054, -0.5354])\n",
      "Grad:  tensor([-0.5037,  2.8486])\n",
      "Epoch 2610 Loss 27.557819\n",
      "Params: tensor([ 2.4054, -0.5357])\n",
      "Grad:  tensor([-0.5037,  2.8486])\n",
      "Epoch 2611 Loss 27.556984\n",
      "Params: tensor([ 2.4055, -0.5360])\n",
      "Grad:  tensor([-0.5037,  2.8485])\n",
      "Epoch 2612 Loss 27.556147\n",
      "Params: tensor([ 2.4055, -0.5363])\n",
      "Grad:  tensor([-0.5037,  2.8485])\n",
      "Epoch 2613 Loss 27.555309\n",
      "Params: tensor([ 2.4056, -0.5365])\n",
      "Grad:  tensor([-0.5037,  2.8484])\n",
      "Epoch 2614 Loss 27.554476\n",
      "Params: tensor([ 2.4056, -0.5368])\n",
      "Grad:  tensor([-0.5037,  2.8484])\n",
      "Epoch 2615 Loss 27.553637\n",
      "Params: tensor([ 2.4057, -0.5371])\n",
      "Grad:  tensor([-0.5037,  2.8483])\n",
      "Epoch 2616 Loss 27.552797\n",
      "Params: tensor([ 2.4057, -0.5374])\n",
      "Grad:  tensor([-0.5036,  2.8483])\n",
      "Epoch 2617 Loss 27.551970\n",
      "Params: tensor([ 2.4058, -0.5377])\n",
      "Grad:  tensor([-0.5036,  2.8482])\n",
      "Epoch 2618 Loss 27.551128\n",
      "Params: tensor([ 2.4058, -0.5380])\n",
      "Grad:  tensor([-0.5036,  2.8482])\n",
      "Epoch 2619 Loss 27.550291\n",
      "Params: tensor([ 2.4059, -0.5383])\n",
      "Grad:  tensor([-0.5036,  2.8482])\n",
      "Epoch 2620 Loss 27.549458\n",
      "Params: tensor([ 2.4059, -0.5385])\n",
      "Grad:  tensor([-0.5036,  2.8481])\n",
      "Epoch 2621 Loss 27.548620\n",
      "Params: tensor([ 2.4060, -0.5388])\n",
      "Grad:  tensor([-0.5036,  2.8481])\n",
      "Epoch 2622 Loss 27.547783\n",
      "Params: tensor([ 2.4060, -0.5391])\n",
      "Grad:  tensor([-0.5036,  2.8480])\n",
      "Epoch 2623 Loss 27.546942\n",
      "Params: tensor([ 2.4061, -0.5394])\n",
      "Grad:  tensor([-0.5036,  2.8480])\n",
      "Epoch 2624 Loss 27.546106\n",
      "Params: tensor([ 2.4061, -0.5397])\n",
      "Grad:  tensor([-0.5036,  2.8479])\n",
      "Epoch 2625 Loss 27.545275\n",
      "Params: tensor([ 2.4062, -0.5400])\n",
      "Grad:  tensor([-0.5036,  2.8479])\n",
      "Epoch 2626 Loss 27.544434\n",
      "Params: tensor([ 2.4062, -0.5402])\n",
      "Grad:  tensor([-0.5036,  2.8478])\n",
      "Epoch 2627 Loss 27.543602\n",
      "Params: tensor([ 2.4063, -0.5405])\n",
      "Grad:  tensor([-0.5036,  2.8478])\n",
      "Epoch 2628 Loss 27.542767\n",
      "Params: tensor([ 2.4063, -0.5408])\n",
      "Grad:  tensor([-0.5036,  2.8477])\n",
      "Epoch 2629 Loss 27.541924\n",
      "Params: tensor([ 2.4064, -0.5411])\n",
      "Grad:  tensor([-0.5035,  2.8477])\n",
      "Epoch 2630 Loss 27.541090\n",
      "Params: tensor([ 2.4064, -0.5414])\n",
      "Grad:  tensor([-0.5035,  2.8476])\n",
      "Epoch 2631 Loss 27.540253\n",
      "Params: tensor([ 2.4065, -0.5417])\n",
      "Grad:  tensor([-0.5035,  2.8476])\n",
      "Epoch 2632 Loss 27.539417\n",
      "Params: tensor([ 2.4065, -0.5420])\n",
      "Grad:  tensor([-0.5035,  2.8475])\n",
      "Epoch 2633 Loss 27.538586\n",
      "Params: tensor([ 2.4066, -0.5422])\n",
      "Grad:  tensor([-0.5035,  2.8475])\n",
      "Epoch 2634 Loss 27.537748\n",
      "Params: tensor([ 2.4066, -0.5425])\n",
      "Grad:  tensor([-0.5035,  2.8474])\n",
      "Epoch 2635 Loss 27.536913\n",
      "Params: tensor([ 2.4067, -0.5428])\n",
      "Grad:  tensor([-0.5035,  2.8474])\n",
      "Epoch 2636 Loss 27.536072\n",
      "Params: tensor([ 2.4067, -0.5431])\n",
      "Grad:  tensor([-0.5035,  2.8473])\n",
      "Epoch 2637 Loss 27.535240\n",
      "Params: tensor([ 2.4068, -0.5434])\n",
      "Grad:  tensor([-0.5035,  2.8473])\n",
      "Epoch 2638 Loss 27.534405\n",
      "Params: tensor([ 2.4068, -0.5437])\n",
      "Grad:  tensor([-0.5035,  2.8472])\n",
      "Epoch 2639 Loss 27.533569\n",
      "Params: tensor([ 2.4069, -0.5439])\n",
      "Grad:  tensor([-0.5034,  2.8472])\n",
      "Epoch 2640 Loss 27.532732\n",
      "Params: tensor([ 2.4069, -0.5442])\n",
      "Grad:  tensor([-0.5034,  2.8471])\n",
      "Epoch 2641 Loss 27.531893\n",
      "Params: tensor([ 2.4070, -0.5445])\n",
      "Grad:  tensor([-0.5034,  2.8471])\n",
      "Epoch 2642 Loss 27.531059\n",
      "Params: tensor([ 2.4070, -0.5448])\n",
      "Grad:  tensor([-0.5034,  2.8470])\n",
      "Epoch 2643 Loss 27.530224\n",
      "Params: tensor([ 2.4071, -0.5451])\n",
      "Grad:  tensor([-0.5034,  2.8470])\n",
      "Epoch 2644 Loss 27.529387\n",
      "Params: tensor([ 2.4071, -0.5454])\n",
      "Grad:  tensor([-0.5034,  2.8469])\n",
      "Epoch 2645 Loss 27.528547\n",
      "Params: tensor([ 2.4072, -0.5457])\n",
      "Grad:  tensor([-0.5034,  2.8469])\n",
      "Epoch 2646 Loss 27.527716\n",
      "Params: tensor([ 2.4072, -0.5459])\n",
      "Grad:  tensor([-0.5034,  2.8468])\n",
      "Epoch 2647 Loss 27.526875\n",
      "Params: tensor([ 2.4073, -0.5462])\n",
      "Grad:  tensor([-0.5034,  2.8468])\n",
      "Epoch 2648 Loss 27.526043\n",
      "Params: tensor([ 2.4073, -0.5465])\n",
      "Grad:  tensor([-0.5034,  2.8468])\n",
      "Epoch 2649 Loss 27.525206\n",
      "Params: tensor([ 2.4074, -0.5468])\n",
      "Grad:  tensor([-0.5034,  2.8467])\n",
      "Epoch 2650 Loss 27.524372\n",
      "Params: tensor([ 2.4074, -0.5471])\n",
      "Grad:  tensor([-0.5033,  2.8467])\n",
      "Epoch 2651 Loss 27.523531\n",
      "Params: tensor([ 2.4075, -0.5474])\n",
      "Grad:  tensor([-0.5033,  2.8466])\n",
      "Epoch 2652 Loss 27.522699\n",
      "Params: tensor([ 2.4075, -0.5476])\n",
      "Grad:  tensor([-0.5033,  2.8466])\n",
      "Epoch 2653 Loss 27.521864\n",
      "Params: tensor([ 2.4076, -0.5479])\n",
      "Grad:  tensor([-0.5033,  2.8465])\n",
      "Epoch 2654 Loss 27.521029\n",
      "Params: tensor([ 2.4076, -0.5482])\n",
      "Grad:  tensor([-0.5033,  2.8465])\n",
      "Epoch 2655 Loss 27.520191\n",
      "Params: tensor([ 2.4077, -0.5485])\n",
      "Grad:  tensor([-0.5033,  2.8464])\n",
      "Epoch 2656 Loss 27.519360\n",
      "Params: tensor([ 2.4077, -0.5488])\n",
      "Grad:  tensor([-0.5033,  2.8464])\n",
      "Epoch 2657 Loss 27.518524\n",
      "Params: tensor([ 2.4078, -0.5491])\n",
      "Grad:  tensor([-0.5033,  2.8463])\n",
      "Epoch 2658 Loss 27.517687\n",
      "Params: tensor([ 2.4078, -0.5494])\n",
      "Grad:  tensor([-0.5033,  2.8463])\n",
      "Epoch 2659 Loss 27.516851\n",
      "Params: tensor([ 2.4079, -0.5496])\n",
      "Grad:  tensor([-0.5033,  2.8462])\n",
      "Epoch 2660 Loss 27.516014\n",
      "Params: tensor([ 2.4079, -0.5499])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor([-0.5032,  2.8462])\n",
      "Epoch 2661 Loss 27.515181\n",
      "Params: tensor([ 2.4080, -0.5502])\n",
      "Grad:  tensor([-0.5032,  2.8461])\n",
      "Epoch 2662 Loss 27.514343\n",
      "Params: tensor([ 2.4080, -0.5505])\n",
      "Grad:  tensor([-0.5032,  2.8461])\n",
      "Epoch 2663 Loss 27.513508\n",
      "Params: tensor([ 2.4081, -0.5508])\n",
      "Grad:  tensor([-0.5032,  2.8460])\n",
      "Epoch 2664 Loss 27.512672\n",
      "Params: tensor([ 2.4081, -0.5511])\n",
      "Grad:  tensor([-0.5032,  2.8460])\n",
      "Epoch 2665 Loss 27.511839\n",
      "Params: tensor([ 2.4082, -0.5513])\n",
      "Grad:  tensor([-0.5032,  2.8459])\n",
      "Epoch 2666 Loss 27.511003\n",
      "Params: tensor([ 2.4082, -0.5516])\n",
      "Grad:  tensor([-0.5032,  2.8459])\n",
      "Epoch 2667 Loss 27.510164\n",
      "Params: tensor([ 2.4083, -0.5519])\n",
      "Grad:  tensor([-0.5032,  2.8458])\n",
      "Epoch 2668 Loss 27.509331\n",
      "Params: tensor([ 2.4083, -0.5522])\n",
      "Grad:  tensor([-0.5032,  2.8458])\n",
      "Epoch 2669 Loss 27.508495\n",
      "Params: tensor([ 2.4084, -0.5525])\n",
      "Grad:  tensor([-0.5032,  2.8457])\n",
      "Epoch 2670 Loss 27.507662\n",
      "Params: tensor([ 2.4084, -0.5528])\n",
      "Grad:  tensor([-0.5031,  2.8457])\n",
      "Epoch 2671 Loss 27.506834\n",
      "Params: tensor([ 2.4085, -0.5531])\n",
      "Grad:  tensor([-0.5031,  2.8456])\n",
      "Epoch 2672 Loss 27.505993\n",
      "Params: tensor([ 2.4085, -0.5533])\n",
      "Grad:  tensor([-0.5031,  2.8456])\n",
      "Epoch 2673 Loss 27.505156\n",
      "Params: tensor([ 2.4086, -0.5536])\n",
      "Grad:  tensor([-0.5031,  2.8455])\n",
      "Epoch 2674 Loss 27.504322\n",
      "Params: tensor([ 2.4086, -0.5539])\n",
      "Grad:  tensor([-0.5031,  2.8455])\n",
      "Epoch 2675 Loss 27.503490\n",
      "Params: tensor([ 2.4087, -0.5542])\n",
      "Grad:  tensor([-0.5031,  2.8455])\n",
      "Epoch 2676 Loss 27.502653\n",
      "Params: tensor([ 2.4087, -0.5545])\n",
      "Grad:  tensor([-0.5031,  2.8454])\n",
      "Epoch 2677 Loss 27.501818\n",
      "Params: tensor([ 2.4088, -0.5548])\n",
      "Grad:  tensor([-0.5031,  2.8454])\n",
      "Epoch 2678 Loss 27.500982\n",
      "Params: tensor([ 2.4088, -0.5550])\n",
      "Grad:  tensor([-0.5031,  2.8453])\n",
      "Epoch 2679 Loss 27.500145\n",
      "Params: tensor([ 2.4089, -0.5553])\n",
      "Grad:  tensor([-0.5030,  2.8453])\n",
      "Epoch 2680 Loss 27.499311\n",
      "Params: tensor([ 2.4089, -0.5556])\n",
      "Grad:  tensor([-0.5030,  2.8452])\n",
      "Epoch 2681 Loss 27.498486\n",
      "Params: tensor([ 2.4090, -0.5559])\n",
      "Grad:  tensor([-0.5030,  2.8452])\n",
      "Epoch 2682 Loss 27.497644\n",
      "Params: tensor([ 2.4090, -0.5562])\n",
      "Grad:  tensor([-0.5030,  2.8451])\n",
      "Epoch 2683 Loss 27.496809\n",
      "Params: tensor([ 2.4091, -0.5565])\n",
      "Grad:  tensor([-0.5030,  2.8451])\n",
      "Epoch 2684 Loss 27.495974\n",
      "Params: tensor([ 2.4091, -0.5568])\n",
      "Grad:  tensor([-0.5030,  2.8450])\n",
      "Epoch 2685 Loss 27.495142\n",
      "Params: tensor([ 2.4092, -0.5570])\n",
      "Grad:  tensor([-0.5030,  2.8450])\n",
      "Epoch 2686 Loss 27.494305\n",
      "Params: tensor([ 2.4092, -0.5573])\n",
      "Grad:  tensor([-0.5030,  2.8449])\n",
      "Epoch 2687 Loss 27.493471\n",
      "Params: tensor([ 2.4093, -0.5576])\n",
      "Grad:  tensor([-0.5030,  2.8449])\n",
      "Epoch 2688 Loss 27.492638\n",
      "Params: tensor([ 2.4093, -0.5579])\n",
      "Grad:  tensor([-0.5029,  2.8448])\n",
      "Epoch 2689 Loss 27.491802\n",
      "Params: tensor([ 2.4094, -0.5582])\n",
      "Grad:  tensor([-0.5029,  2.8448])\n",
      "Epoch 2690 Loss 27.490969\n",
      "Params: tensor([ 2.4094, -0.5585])\n",
      "Grad:  tensor([-0.5029,  2.8447])\n",
      "Epoch 2691 Loss 27.490129\n",
      "Params: tensor([ 2.4095, -0.5587])\n",
      "Grad:  tensor([-0.5029,  2.8447])\n",
      "Epoch 2692 Loss 27.489294\n",
      "Params: tensor([ 2.4095, -0.5590])\n",
      "Grad:  tensor([-0.5029,  2.8446])\n",
      "Epoch 2693 Loss 27.488462\n",
      "Params: tensor([ 2.4096, -0.5593])\n",
      "Grad:  tensor([-0.5029,  2.8446])\n",
      "Epoch 2694 Loss 27.487633\n",
      "Params: tensor([ 2.4096, -0.5596])\n",
      "Grad:  tensor([-0.5029,  2.8445])\n",
      "Epoch 2695 Loss 27.486797\n",
      "Params: tensor([ 2.4097, -0.5599])\n",
      "Grad:  tensor([-0.5029,  2.8445])\n",
      "Epoch 2696 Loss 27.485956\n",
      "Params: tensor([ 2.4097, -0.5602])\n",
      "Grad:  tensor([-0.5029,  2.8444])\n",
      "Epoch 2697 Loss 27.485126\n",
      "Params: tensor([ 2.4098, -0.5605])\n",
      "Grad:  tensor([-0.5028,  2.8444])\n",
      "Epoch 2698 Loss 27.484289\n",
      "Params: tensor([ 2.4098, -0.5607])\n",
      "Grad:  tensor([-0.5028,  2.8444])\n",
      "Epoch 2699 Loss 27.483459\n",
      "Params: tensor([ 2.4099, -0.5610])\n",
      "Grad:  tensor([-0.5028,  2.8443])\n",
      "Epoch 2700 Loss 27.482622\n",
      "Params: tensor([ 2.4099, -0.5613])\n",
      "Grad:  tensor([-0.5028,  2.8443])\n",
      "Epoch 2701 Loss 27.481787\n",
      "Params: tensor([ 2.4100, -0.5616])\n",
      "Grad:  tensor([-0.5028,  2.8442])\n",
      "Epoch 2702 Loss 27.480957\n",
      "Params: tensor([ 2.4100, -0.5619])\n",
      "Grad:  tensor([-0.5028,  2.8442])\n",
      "Epoch 2703 Loss 27.480120\n",
      "Params: tensor([ 2.4101, -0.5622])\n",
      "Grad:  tensor([-0.5028,  2.8441])\n",
      "Epoch 2704 Loss 27.479290\n",
      "Params: tensor([ 2.4101, -0.5624])\n",
      "Grad:  tensor([-0.5028,  2.8441])\n",
      "Epoch 2705 Loss 27.478449\n",
      "Params: tensor([ 2.4102, -0.5627])\n",
      "Grad:  tensor([-0.5027,  2.8440])\n",
      "Epoch 2706 Loss 27.477619\n",
      "Params: tensor([ 2.4102, -0.5630])\n",
      "Grad:  tensor([-0.5027,  2.8440])\n",
      "Epoch 2707 Loss 27.476782\n",
      "Params: tensor([ 2.4103, -0.5633])\n",
      "Grad:  tensor([-0.5027,  2.8439])\n",
      "Epoch 2708 Loss 27.475950\n",
      "Params: tensor([ 2.4103, -0.5636])\n",
      "Grad:  tensor([-0.5027,  2.8439])\n",
      "Epoch 2709 Loss 27.475115\n",
      "Params: tensor([ 2.4104, -0.5639])\n",
      "Grad:  tensor([-0.5027,  2.8438])\n",
      "Epoch 2710 Loss 27.474276\n",
      "Params: tensor([ 2.4104, -0.5642])\n",
      "Grad:  tensor([-0.5027,  2.8438])\n",
      "Epoch 2711 Loss 27.473444\n",
      "Params: tensor([ 2.4105, -0.5644])\n",
      "Grad:  tensor([-0.5027,  2.8437])\n",
      "Epoch 2712 Loss 27.472620\n",
      "Params: tensor([ 2.4105, -0.5647])\n",
      "Grad:  tensor([-0.5027,  2.8437])\n",
      "Epoch 2713 Loss 27.471779\n",
      "Params: tensor([ 2.4106, -0.5650])\n",
      "Grad:  tensor([-0.5026,  2.8436])\n",
      "Epoch 2714 Loss 27.470945\n",
      "Params: tensor([ 2.4106, -0.5653])\n",
      "Grad:  tensor([-0.5026,  2.8436])\n",
      "Epoch 2715 Loss 27.470112\n",
      "Params: tensor([ 2.4107, -0.5656])\n",
      "Grad:  tensor([-0.5026,  2.8435])\n",
      "Epoch 2716 Loss 27.469276\n",
      "Params: tensor([ 2.4107, -0.5659])\n",
      "Grad:  tensor([-0.5026,  2.8435])\n",
      "Epoch 2717 Loss 27.468445\n",
      "Params: tensor([ 2.4108, -0.5661])\n",
      "Grad:  tensor([-0.5026,  2.8434])\n",
      "Epoch 2718 Loss 27.467609\n",
      "Params: tensor([ 2.4108, -0.5664])\n",
      "Grad:  tensor([-0.5026,  2.8434])\n",
      "Epoch 2719 Loss 27.466774\n",
      "Params: tensor([ 2.4109, -0.5667])\n",
      "Grad:  tensor([-0.5026,  2.8434])\n",
      "Epoch 2720 Loss 27.465940\n",
      "Params: tensor([ 2.4109, -0.5670])\n",
      "Grad:  tensor([-0.5026,  2.8433])\n",
      "Epoch 2721 Loss 27.465111\n",
      "Params: tensor([ 2.4110, -0.5673])\n",
      "Grad:  tensor([-0.5025,  2.8433])\n",
      "Epoch 2722 Loss 27.464272\n",
      "Params: tensor([ 2.4110, -0.5676])\n",
      "Grad:  tensor([-0.5025,  2.8432])\n",
      "Epoch 2723 Loss 27.463446\n",
      "Params: tensor([ 2.4111, -0.5678])\n",
      "Grad:  tensor([-0.5025,  2.8432])\n",
      "Epoch 2724 Loss 27.462610\n",
      "Params: tensor([ 2.4111, -0.5681])\n",
      "Grad:  tensor([-0.5025,  2.8431])\n",
      "Epoch 2725 Loss 27.461775\n",
      "Params: tensor([ 2.4112, -0.5684])\n",
      "Grad:  tensor([-0.5025,  2.8431])\n",
      "Epoch 2726 Loss 27.460943\n",
      "Params: tensor([ 2.4112, -0.5687])\n",
      "Grad:  tensor([-0.5025,  2.8430])\n",
      "Epoch 2727 Loss 27.460112\n",
      "Params: tensor([ 2.4113, -0.5690])\n",
      "Grad:  tensor([-0.5025,  2.8430])\n",
      "Epoch 2728 Loss 27.459272\n",
      "Params: tensor([ 2.4113, -0.5693])\n",
      "Grad:  tensor([-0.5025,  2.8429])\n",
      "Epoch 2729 Loss 27.458443\n",
      "Params: tensor([ 2.4114, -0.5696])\n",
      "Grad:  tensor([-0.5024,  2.8429])\n",
      "Epoch 2730 Loss 27.457609\n",
      "Params: tensor([ 2.4114, -0.5698])\n",
      "Grad:  tensor([-0.5024,  2.8428])\n",
      "Epoch 2731 Loss 27.456774\n",
      "Params: tensor([ 2.4115, -0.5701])\n",
      "Grad:  tensor([-0.5024,  2.8428])\n",
      "Epoch 2732 Loss 27.455938\n",
      "Params: tensor([ 2.4115, -0.5704])\n",
      "Grad:  tensor([-0.5024,  2.8427])\n",
      "Epoch 2733 Loss 27.455103\n",
      "Params: tensor([ 2.4116, -0.5707])\n",
      "Grad:  tensor([-0.5024,  2.8427])\n",
      "Epoch 2734 Loss 27.454273\n",
      "Params: tensor([ 2.4116, -0.5710])\n",
      "Grad:  tensor([-0.5024,  2.8426])\n",
      "Epoch 2735 Loss 27.453438\n",
      "Params: tensor([ 2.4117, -0.5713])\n",
      "Grad:  tensor([-0.5023,  2.8426])\n",
      "Epoch 2736 Loss 27.452604\n",
      "Params: tensor([ 2.4117, -0.5715])\n",
      "Grad:  tensor([-0.5023,  2.8425])\n",
      "Epoch 2737 Loss 27.451771\n",
      "Params: tensor([ 2.4118, -0.5718])\n",
      "Grad:  tensor([-0.5023,  2.8425])\n",
      "Epoch 2738 Loss 27.450939\n",
      "Params: tensor([ 2.4118, -0.5721])\n",
      "Grad:  tensor([-0.5023,  2.8425])\n",
      "Epoch 2739 Loss 27.450104\n",
      "Params: tensor([ 2.4119, -0.5724])\n",
      "Grad:  tensor([-0.5023,  2.8424])\n",
      "Epoch 2740 Loss 27.449268\n",
      "Params: tensor([ 2.4119, -0.5727])\n",
      "Grad:  tensor([-0.5023,  2.8424])\n",
      "Epoch 2741 Loss 27.448439\n",
      "Params: tensor([ 2.4120, -0.5730])\n",
      "Grad:  tensor([-0.5023,  2.8423])\n",
      "Epoch 2742 Loss 27.447603\n",
      "Params: tensor([ 2.4120, -0.5732])\n",
      "Grad:  tensor([-0.5023,  2.8423])\n",
      "Epoch 2743 Loss 27.446772\n",
      "Params: tensor([ 2.4121, -0.5735])\n",
      "Grad:  tensor([-0.5022,  2.8422])\n",
      "Epoch 2744 Loss 27.445936\n",
      "Params: tensor([ 2.4121, -0.5738])\n",
      "Grad:  tensor([-0.5022,  2.8422])\n",
      "Epoch 2745 Loss 27.445105\n",
      "Params: tensor([ 2.4122, -0.5741])\n",
      "Grad:  tensor([-0.5022,  2.8421])\n",
      "Epoch 2746 Loss 27.444273\n",
      "Params: tensor([ 2.4122, -0.5744])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor([-0.5022,  2.8421])\n",
      "Epoch 2747 Loss 27.443438\n",
      "Params: tensor([ 2.4123, -0.5747])\n",
      "Grad:  tensor([-0.5022,  2.8420])\n",
      "Epoch 2748 Loss 27.442604\n",
      "Params: tensor([ 2.4123, -0.5750])\n",
      "Grad:  tensor([-0.5022,  2.8420])\n",
      "Epoch 2749 Loss 27.441774\n",
      "Params: tensor([ 2.4124, -0.5752])\n",
      "Grad:  tensor([-0.5022,  2.8419])\n",
      "Epoch 2750 Loss 27.440937\n",
      "Params: tensor([ 2.4124, -0.5755])\n",
      "Grad:  tensor([-0.5021,  2.8419])\n",
      "Epoch 2751 Loss 27.440107\n",
      "Params: tensor([ 2.4125, -0.5758])\n",
      "Grad:  tensor([-0.5021,  2.8418])\n",
      "Epoch 2752 Loss 27.439276\n",
      "Params: tensor([ 2.4125, -0.5761])\n",
      "Grad:  tensor([-0.5021,  2.8418])\n",
      "Epoch 2753 Loss 27.438444\n",
      "Params: tensor([ 2.4126, -0.5764])\n",
      "Grad:  tensor([-0.5021,  2.8417])\n",
      "Epoch 2754 Loss 27.437609\n",
      "Params: tensor([ 2.4126, -0.5767])\n",
      "Grad:  tensor([-0.5021,  2.8417])\n",
      "Epoch 2755 Loss 27.436779\n",
      "Params: tensor([ 2.4127, -0.5769])\n",
      "Grad:  tensor([-0.5021,  2.8416])\n",
      "Epoch 2756 Loss 27.435946\n",
      "Params: tensor([ 2.4127, -0.5772])\n",
      "Grad:  tensor([-0.5020,  2.8416])\n",
      "Epoch 2757 Loss 27.435112\n",
      "Params: tensor([ 2.4128, -0.5775])\n",
      "Grad:  tensor([-0.5020,  2.8416])\n",
      "Epoch 2758 Loss 27.434282\n",
      "Params: tensor([ 2.4128, -0.5778])\n",
      "Grad:  tensor([-0.5020,  2.8415])\n",
      "Epoch 2759 Loss 27.433449\n",
      "Params: tensor([ 2.4129, -0.5781])\n",
      "Grad:  tensor([-0.5020,  2.8415])\n",
      "Epoch 2760 Loss 27.432615\n",
      "Params: tensor([ 2.4129, -0.5784])\n",
      "Grad:  tensor([-0.5020,  2.8414])\n",
      "Epoch 2761 Loss 27.431782\n",
      "Params: tensor([ 2.4130, -0.5786])\n",
      "Grad:  tensor([-0.5020,  2.8414])\n",
      "Epoch 2762 Loss 27.430950\n",
      "Params: tensor([ 2.4130, -0.5789])\n",
      "Grad:  tensor([-0.5020,  2.8413])\n",
      "Epoch 2763 Loss 27.430117\n",
      "Params: tensor([ 2.4131, -0.5792])\n",
      "Grad:  tensor([-0.5019,  2.8413])\n",
      "Epoch 2764 Loss 27.429285\n",
      "Params: tensor([ 2.4131, -0.5795])\n",
      "Grad:  tensor([-0.5019,  2.8412])\n",
      "Epoch 2765 Loss 27.428453\n",
      "Params: tensor([ 2.4132, -0.5798])\n",
      "Grad:  tensor([-0.5019,  2.8412])\n",
      "Epoch 2766 Loss 27.427624\n",
      "Params: tensor([ 2.4132, -0.5801])\n",
      "Grad:  tensor([-0.5019,  2.8411])\n",
      "Epoch 2767 Loss 27.426786\n",
      "Params: tensor([ 2.4133, -0.5804])\n",
      "Grad:  tensor([-0.5019,  2.8411])\n",
      "Epoch 2768 Loss 27.425957\n",
      "Params: tensor([ 2.4134, -0.5806])\n",
      "Grad:  tensor([-0.5019,  2.8410])\n",
      "Epoch 2769 Loss 27.425123\n",
      "Params: tensor([ 2.4134, -0.5809])\n",
      "Grad:  tensor([-0.5019,  2.8410])\n",
      "Epoch 2770 Loss 27.424292\n",
      "Params: tensor([ 2.4135, -0.5812])\n",
      "Grad:  tensor([-0.5019,  2.8409])\n",
      "Epoch 2771 Loss 27.423460\n",
      "Params: tensor([ 2.4135, -0.5815])\n",
      "Grad:  tensor([-0.5019,  2.8409])\n",
      "Epoch 2772 Loss 27.422626\n",
      "Params: tensor([ 2.4136, -0.5818])\n",
      "Grad:  tensor([-0.5019,  2.8408])\n",
      "Epoch 2773 Loss 27.421795\n",
      "Params: tensor([ 2.4136, -0.5821])\n",
      "Grad:  tensor([-0.5019,  2.8408])\n",
      "Epoch 2774 Loss 27.420961\n",
      "Params: tensor([ 2.4137, -0.5823])\n",
      "Grad:  tensor([-0.5019,  2.8407])\n",
      "Epoch 2775 Loss 27.420135\n",
      "Params: tensor([ 2.4137, -0.5826])\n",
      "Grad:  tensor([-0.5019,  2.8407])\n",
      "Epoch 2776 Loss 27.419298\n",
      "Params: tensor([ 2.4138, -0.5829])\n",
      "Grad:  tensor([-0.5019,  2.8406])\n",
      "Epoch 2777 Loss 27.418463\n",
      "Params: tensor([ 2.4138, -0.5832])\n",
      "Grad:  tensor([-0.5019,  2.8406])\n",
      "Epoch 2778 Loss 27.417631\n",
      "Params: tensor([ 2.4139, -0.5835])\n",
      "Grad:  tensor([-0.5018,  2.8405])\n",
      "Epoch 2779 Loss 27.416801\n",
      "Params: tensor([ 2.4139, -0.5838])\n",
      "Grad:  tensor([-0.5018,  2.8405])\n",
      "Epoch 2780 Loss 27.415968\n",
      "Params: tensor([ 2.4140, -0.5840])\n",
      "Grad:  tensor([-0.5018,  2.8404])\n",
      "Epoch 2781 Loss 27.415136\n",
      "Params: tensor([ 2.4140, -0.5843])\n",
      "Grad:  tensor([-0.5018,  2.8404])\n",
      "Epoch 2782 Loss 27.414305\n",
      "Params: tensor([ 2.4141, -0.5846])\n",
      "Grad:  tensor([-0.5018,  2.8403])\n",
      "Epoch 2783 Loss 27.413471\n",
      "Params: tensor([ 2.4141, -0.5849])\n",
      "Grad:  tensor([-0.5018,  2.8403])\n",
      "Epoch 2784 Loss 27.412640\n",
      "Params: tensor([ 2.4142, -0.5852])\n",
      "Grad:  tensor([-0.5018,  2.8402])\n",
      "Epoch 2785 Loss 27.411812\n",
      "Params: tensor([ 2.4142, -0.5855])\n",
      "Grad:  tensor([-0.5018,  2.8402])\n",
      "Epoch 2786 Loss 27.410980\n",
      "Params: tensor([ 2.4143, -0.5857])\n",
      "Grad:  tensor([-0.5018,  2.8401])\n",
      "Epoch 2787 Loss 27.410149\n",
      "Params: tensor([ 2.4143, -0.5860])\n",
      "Grad:  tensor([-0.5018,  2.8401])\n",
      "Epoch 2788 Loss 27.409313\n",
      "Params: tensor([ 2.4144, -0.5863])\n",
      "Grad:  tensor([-0.5018,  2.8400])\n",
      "Epoch 2789 Loss 27.408480\n",
      "Params: tensor([ 2.4144, -0.5866])\n",
      "Grad:  tensor([-0.5018,  2.8400])\n",
      "Epoch 2790 Loss 27.407648\n",
      "Params: tensor([ 2.4145, -0.5869])\n",
      "Grad:  tensor([-0.5018,  2.8400])\n",
      "Epoch 2791 Loss 27.406818\n",
      "Params: tensor([ 2.4145, -0.5872])\n",
      "Grad:  tensor([-0.5018,  2.8399])\n",
      "Epoch 2792 Loss 27.405989\n",
      "Params: tensor([ 2.4146, -0.5875])\n",
      "Grad:  tensor([-0.5018,  2.8399])\n",
      "Epoch 2793 Loss 27.405151\n",
      "Params: tensor([ 2.4146, -0.5877])\n",
      "Grad:  tensor([-0.5018,  2.8398])\n",
      "Epoch 2794 Loss 27.404327\n",
      "Params: tensor([ 2.4147, -0.5880])\n",
      "Grad:  tensor([-0.5018,  2.8398])\n",
      "Epoch 2795 Loss 27.403496\n",
      "Params: tensor([ 2.4147, -0.5883])\n",
      "Grad:  tensor([-0.5018,  2.8397])\n",
      "Epoch 2796 Loss 27.402662\n",
      "Params: tensor([ 2.4148, -0.5886])\n",
      "Grad:  tensor([-0.5018,  2.8397])\n",
      "Epoch 2797 Loss 27.401827\n",
      "Params: tensor([ 2.4148, -0.5889])\n",
      "Grad:  tensor([-0.5018,  2.8396])\n",
      "Epoch 2798 Loss 27.400995\n",
      "Params: tensor([ 2.4149, -0.5892])\n",
      "Grad:  tensor([-0.5018,  2.8396])\n",
      "Epoch 2799 Loss 27.400166\n",
      "Params: tensor([ 2.4149, -0.5894])\n",
      "Grad:  tensor([-0.5018,  2.8395])\n",
      "Epoch 2800 Loss 27.399340\n",
      "Params: tensor([ 2.4150, -0.5897])\n",
      "Grad:  tensor([-0.5018,  2.8395])\n",
      "Epoch 2801 Loss 27.398510\n",
      "Params: tensor([ 2.4150, -0.5900])\n",
      "Grad:  tensor([-0.5018,  2.8394])\n",
      "Epoch 2802 Loss 27.397673\n",
      "Params: tensor([ 2.4151, -0.5903])\n",
      "Grad:  tensor([-0.5018,  2.8394])\n",
      "Epoch 2803 Loss 27.396845\n",
      "Params: tensor([ 2.4151, -0.5906])\n",
      "Grad:  tensor([-0.5018,  2.8393])\n",
      "Epoch 2804 Loss 27.396013\n",
      "Params: tensor([ 2.4152, -0.5909])\n",
      "Grad:  tensor([-0.5018,  2.8393])\n",
      "Epoch 2805 Loss 27.395180\n",
      "Params: tensor([ 2.4152, -0.5911])\n",
      "Grad:  tensor([-0.5018,  2.8392])\n",
      "Epoch 2806 Loss 27.394348\n",
      "Params: tensor([ 2.4153, -0.5914])\n",
      "Grad:  tensor([-0.5018,  2.8392])\n",
      "Epoch 2807 Loss 27.393522\n",
      "Params: tensor([ 2.4153, -0.5917])\n",
      "Grad:  tensor([-0.5018,  2.8391])\n",
      "Epoch 2808 Loss 27.392689\n",
      "Params: tensor([ 2.4154, -0.5920])\n",
      "Grad:  tensor([-0.5018,  2.8391])\n",
      "Epoch 2809 Loss 27.391859\n",
      "Params: tensor([ 2.4154, -0.5923])\n",
      "Grad:  tensor([-0.5018,  2.8390])\n",
      "Epoch 2810 Loss 27.391029\n",
      "Params: tensor([ 2.4155, -0.5926])\n",
      "Grad:  tensor([-0.5018,  2.8390])\n",
      "Epoch 2811 Loss 27.390192\n",
      "Params: tensor([ 2.4155, -0.5928])\n",
      "Grad:  tensor([-0.5018,  2.8389])\n",
      "Epoch 2812 Loss 27.389360\n",
      "Params: tensor([ 2.4156, -0.5931])\n",
      "Grad:  tensor([-0.5018,  2.8389])\n",
      "Epoch 2813 Loss 27.388536\n",
      "Params: tensor([ 2.4156, -0.5934])\n",
      "Grad:  tensor([-0.5017,  2.8388])\n",
      "Epoch 2814 Loss 27.387703\n",
      "Params: tensor([ 2.4157, -0.5937])\n",
      "Grad:  tensor([-0.5017,  2.8388])\n",
      "Epoch 2815 Loss 27.386869\n",
      "Params: tensor([ 2.4157, -0.5940])\n",
      "Grad:  tensor([-0.5017,  2.8387])\n",
      "Epoch 2816 Loss 27.386045\n",
      "Params: tensor([ 2.4158, -0.5943])\n",
      "Grad:  tensor([-0.5017,  2.8387])\n",
      "Epoch 2817 Loss 27.385206\n",
      "Params: tensor([ 2.4158, -0.5946])\n",
      "Grad:  tensor([-0.5017,  2.8386])\n",
      "Epoch 2818 Loss 27.384377\n",
      "Params: tensor([ 2.4159, -0.5948])\n",
      "Grad:  tensor([-0.5017,  2.8386])\n",
      "Epoch 2819 Loss 27.383545\n",
      "Params: tensor([ 2.4159, -0.5951])\n",
      "Grad:  tensor([-0.5017,  2.8385])\n",
      "Epoch 2820 Loss 27.382715\n",
      "Params: tensor([ 2.4160, -0.5954])\n",
      "Grad:  tensor([-0.5017,  2.8385])\n",
      "Epoch 2821 Loss 27.381884\n",
      "Params: tensor([ 2.4160, -0.5957])\n",
      "Grad:  tensor([-0.5017,  2.8384])\n",
      "Epoch 2822 Loss 27.381054\n",
      "Params: tensor([ 2.4161, -0.5960])\n",
      "Grad:  tensor([-0.5017,  2.8384])\n",
      "Epoch 2823 Loss 27.380224\n",
      "Params: tensor([ 2.4161, -0.5963])\n",
      "Grad:  tensor([-0.5017,  2.8383])\n",
      "Epoch 2824 Loss 27.379395\n",
      "Params: tensor([ 2.4162, -0.5965])\n",
      "Grad:  tensor([-0.5017,  2.8383])\n",
      "Epoch 2825 Loss 27.378565\n",
      "Params: tensor([ 2.4162, -0.5968])\n",
      "Grad:  tensor([-0.5017,  2.8382])\n",
      "Epoch 2826 Loss 27.377729\n",
      "Params: tensor([ 2.4163, -0.5971])\n",
      "Grad:  tensor([-0.5017,  2.8382])\n",
      "Epoch 2827 Loss 27.376900\n",
      "Params: tensor([ 2.4163, -0.5974])\n",
      "Grad:  tensor([-0.5017,  2.8381])\n",
      "Epoch 2828 Loss 27.376070\n",
      "Params: tensor([ 2.4164, -0.5977])\n",
      "Grad:  tensor([-0.5017,  2.8381])\n",
      "Epoch 2829 Loss 27.375237\n",
      "Params: tensor([ 2.4164, -0.5980])\n",
      "Grad:  tensor([-0.5017,  2.8380])\n",
      "Epoch 2830 Loss 27.374409\n",
      "Params: tensor([ 2.4165, -0.5982])\n",
      "Grad:  tensor([-0.5017,  2.8380])\n",
      "Epoch 2831 Loss 27.373573\n",
      "Params: tensor([ 2.4165, -0.5985])\n",
      "Grad:  tensor([-0.5017,  2.8379])\n",
      "Epoch 2832 Loss 27.372753\n",
      "Params: tensor([ 2.4166, -0.5988])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor([-0.5017,  2.8379])\n",
      "Epoch 2833 Loss 27.371916\n",
      "Params: tensor([ 2.4166, -0.5991])\n",
      "Grad:  tensor([-0.5017,  2.8378])\n",
      "Epoch 2834 Loss 27.371088\n",
      "Params: tensor([ 2.4167, -0.5994])\n",
      "Grad:  tensor([-0.5017,  2.8378])\n",
      "Epoch 2835 Loss 27.370253\n",
      "Params: tensor([ 2.4167, -0.5997])\n",
      "Grad:  tensor([-0.5017,  2.8377])\n",
      "Epoch 2836 Loss 27.369421\n",
      "Params: tensor([ 2.4168, -0.5999])\n",
      "Grad:  tensor([-0.5017,  2.8377])\n",
      "Epoch 2837 Loss 27.368603\n",
      "Params: tensor([ 2.4168, -0.6002])\n",
      "Grad:  tensor([-0.5016,  2.8376])\n",
      "Epoch 2838 Loss 27.367765\n",
      "Params: tensor([ 2.4169, -0.6005])\n",
      "Grad:  tensor([-0.5016,  2.8376])\n",
      "Epoch 2839 Loss 27.366932\n",
      "Params: tensor([ 2.4169, -0.6008])\n",
      "Grad:  tensor([-0.5016,  2.8375])\n",
      "Epoch 2840 Loss 27.366106\n",
      "Params: tensor([ 2.4170, -0.6011])\n",
      "Grad:  tensor([-0.5016,  2.8375])\n",
      "Epoch 2841 Loss 27.365276\n",
      "Params: tensor([ 2.4170, -0.6014])\n",
      "Grad:  tensor([-0.5016,  2.8374])\n",
      "Epoch 2842 Loss 27.364443\n",
      "Params: tensor([ 2.4171, -0.6016])\n",
      "Grad:  tensor([-0.5016,  2.8374])\n",
      "Epoch 2843 Loss 27.363615\n",
      "Params: tensor([ 2.4171, -0.6019])\n",
      "Grad:  tensor([-0.5016,  2.8374])\n",
      "Epoch 2844 Loss 27.362785\n",
      "Params: tensor([ 2.4172, -0.6022])\n",
      "Grad:  tensor([-0.5016,  2.8373])\n",
      "Epoch 2845 Loss 27.361952\n",
      "Params: tensor([ 2.4172, -0.6025])\n",
      "Grad:  tensor([-0.5016,  2.8373])\n",
      "Epoch 2846 Loss 27.361122\n",
      "Params: tensor([ 2.4173, -0.6028])\n",
      "Grad:  tensor([-0.5016,  2.8372])\n",
      "Epoch 2847 Loss 27.360291\n",
      "Params: tensor([ 2.4173, -0.6031])\n",
      "Grad:  tensor([-0.5016,  2.8372])\n",
      "Epoch 2848 Loss 27.359465\n",
      "Params: tensor([ 2.4174, -0.6033])\n",
      "Grad:  tensor([-0.5016,  2.8371])\n",
      "Epoch 2849 Loss 27.358631\n",
      "Params: tensor([ 2.4174, -0.6036])\n",
      "Grad:  tensor([-0.5016,  2.8371])\n",
      "Epoch 2850 Loss 27.357805\n",
      "Params: tensor([ 2.4175, -0.6039])\n",
      "Grad:  tensor([-0.5016,  2.8370])\n",
      "Epoch 2851 Loss 27.356970\n",
      "Params: tensor([ 2.4175, -0.6042])\n",
      "Grad:  tensor([-0.5016,  2.8370])\n",
      "Epoch 2852 Loss 27.356142\n",
      "Params: tensor([ 2.4176, -0.6045])\n",
      "Grad:  tensor([-0.5016,  2.8369])\n",
      "Epoch 2853 Loss 27.355314\n",
      "Params: tensor([ 2.4176, -0.6048])\n",
      "Grad:  tensor([-0.5016,  2.8369])\n",
      "Epoch 2854 Loss 27.354479\n",
      "Params: tensor([ 2.4177, -0.6051])\n",
      "Grad:  tensor([-0.5016,  2.8368])\n",
      "Epoch 2855 Loss 27.353655\n",
      "Params: tensor([ 2.4177, -0.6053])\n",
      "Grad:  tensor([-0.5015,  2.8368])\n",
      "Epoch 2856 Loss 27.352821\n",
      "Params: tensor([ 2.4178, -0.6056])\n",
      "Grad:  tensor([-0.5015,  2.8367])\n",
      "Epoch 2857 Loss 27.351992\n",
      "Params: tensor([ 2.4178, -0.6059])\n",
      "Grad:  tensor([-0.5015,  2.8367])\n",
      "Epoch 2858 Loss 27.351164\n",
      "Params: tensor([ 2.4179, -0.6062])\n",
      "Grad:  tensor([-0.5015,  2.8366])\n",
      "Epoch 2859 Loss 27.350334\n",
      "Params: tensor([ 2.4179, -0.6065])\n",
      "Grad:  tensor([-0.5015,  2.8366])\n",
      "Epoch 2860 Loss 27.349504\n",
      "Params: tensor([ 2.4180, -0.6068])\n",
      "Grad:  tensor([-0.5015,  2.8365])\n",
      "Epoch 2861 Loss 27.348671\n",
      "Params: tensor([ 2.4180, -0.6070])\n",
      "Grad:  tensor([-0.5015,  2.8365])\n",
      "Epoch 2862 Loss 27.347841\n",
      "Params: tensor([ 2.4181, -0.6073])\n",
      "Grad:  tensor([-0.5015,  2.8364])\n",
      "Epoch 2863 Loss 27.347013\n",
      "Params: tensor([ 2.4181, -0.6076])\n",
      "Grad:  tensor([-0.5015,  2.8364])\n",
      "Epoch 2864 Loss 27.346186\n",
      "Params: tensor([ 2.4182, -0.6079])\n",
      "Grad:  tensor([-0.5015,  2.8363])\n",
      "Epoch 2865 Loss 27.345350\n",
      "Params: tensor([ 2.4182, -0.6082])\n",
      "Grad:  tensor([-0.5015,  2.8363])\n",
      "Epoch 2866 Loss 27.344524\n",
      "Params: tensor([ 2.4183, -0.6085])\n",
      "Grad:  tensor([-0.5015,  2.8362])\n",
      "Epoch 2867 Loss 27.343695\n",
      "Params: tensor([ 2.4183, -0.6087])\n",
      "Grad:  tensor([-0.5015,  2.8362])\n",
      "Epoch 2868 Loss 27.342865\n",
      "Params: tensor([ 2.4184, -0.6090])\n",
      "Grad:  tensor([-0.5015,  2.8361])\n",
      "Epoch 2869 Loss 27.342035\n",
      "Params: tensor([ 2.4184, -0.6093])\n",
      "Grad:  tensor([-0.5015,  2.8361])\n",
      "Epoch 2870 Loss 27.341209\n",
      "Params: tensor([ 2.4185, -0.6096])\n",
      "Grad:  tensor([-0.5014,  2.8360])\n",
      "Epoch 2871 Loss 27.340370\n",
      "Params: tensor([ 2.4185, -0.6099])\n",
      "Grad:  tensor([-0.5014,  2.8360])\n",
      "Epoch 2872 Loss 27.339550\n",
      "Params: tensor([ 2.4186, -0.6102])\n",
      "Grad:  tensor([-0.5014,  2.8359])\n",
      "Epoch 2873 Loss 27.338720\n",
      "Params: tensor([ 2.4186, -0.6104])\n",
      "Grad:  tensor([-0.5014,  2.8359])\n",
      "Epoch 2874 Loss 27.337889\n",
      "Params: tensor([ 2.4187, -0.6107])\n",
      "Grad:  tensor([-0.5014,  2.8358])\n",
      "Epoch 2875 Loss 27.337059\n",
      "Params: tensor([ 2.4187, -0.6110])\n",
      "Grad:  tensor([-0.5014,  2.8358])\n",
      "Epoch 2876 Loss 27.336229\n",
      "Params: tensor([ 2.4188, -0.6113])\n",
      "Grad:  tensor([-0.5014,  2.8357])\n",
      "Epoch 2877 Loss 27.335405\n",
      "Params: tensor([ 2.4188, -0.6116])\n",
      "Grad:  tensor([-0.5014,  2.8357])\n",
      "Epoch 2878 Loss 27.334570\n",
      "Params: tensor([ 2.4189, -0.6119])\n",
      "Grad:  tensor([-0.5014,  2.8357])\n",
      "Epoch 2879 Loss 27.333742\n",
      "Params: tensor([ 2.4189, -0.6121])\n",
      "Grad:  tensor([-0.5014,  2.8356])\n",
      "Epoch 2880 Loss 27.332916\n",
      "Params: tensor([ 2.4190, -0.6124])\n",
      "Grad:  tensor([-0.5014,  2.8356])\n",
      "Epoch 2881 Loss 27.332090\n",
      "Params: tensor([ 2.4190, -0.6127])\n",
      "Grad:  tensor([-0.5014,  2.8355])\n",
      "Epoch 2882 Loss 27.331261\n",
      "Params: tensor([ 2.4191, -0.6130])\n",
      "Grad:  tensor([-0.5014,  2.8355])\n",
      "Epoch 2883 Loss 27.330431\n",
      "Params: tensor([ 2.4191, -0.6133])\n",
      "Grad:  tensor([-0.5014,  2.8354])\n",
      "Epoch 2884 Loss 27.329597\n",
      "Params: tensor([ 2.4192, -0.6136])\n",
      "Grad:  tensor([-0.5014,  2.8354])\n",
      "Epoch 2885 Loss 27.328772\n",
      "Params: tensor([ 2.4192, -0.6138])\n",
      "Grad:  tensor([-0.5014,  2.8353])\n",
      "Epoch 2886 Loss 27.327942\n",
      "Params: tensor([ 2.4193, -0.6141])\n",
      "Grad:  tensor([-0.5013,  2.8353])\n",
      "Epoch 2887 Loss 27.327112\n",
      "Params: tensor([ 2.4193, -0.6144])\n",
      "Grad:  tensor([-0.5013,  2.8352])\n",
      "Epoch 2888 Loss 27.326283\n",
      "Params: tensor([ 2.4194, -0.6147])\n",
      "Grad:  tensor([-0.5013,  2.8352])\n",
      "Epoch 2889 Loss 27.325447\n",
      "Params: tensor([ 2.4194, -0.6150])\n",
      "Grad:  tensor([-0.5013,  2.8351])\n",
      "Epoch 2890 Loss 27.324627\n",
      "Params: tensor([ 2.4195, -0.6153])\n",
      "Grad:  tensor([-0.5013,  2.8351])\n",
      "Epoch 2891 Loss 27.323799\n",
      "Params: tensor([ 2.4195, -0.6155])\n",
      "Grad:  tensor([-0.5013,  2.8350])\n",
      "Epoch 2892 Loss 27.322968\n",
      "Params: tensor([ 2.4196, -0.6158])\n",
      "Grad:  tensor([-0.5013,  2.8350])\n",
      "Epoch 2893 Loss 27.322142\n",
      "Params: tensor([ 2.4196, -0.6161])\n",
      "Grad:  tensor([-0.5013,  2.8349])\n",
      "Epoch 2894 Loss 27.321318\n",
      "Params: tensor([ 2.4197, -0.6164])\n",
      "Grad:  tensor([-0.5013,  2.8349])\n",
      "Epoch 2895 Loss 27.320484\n",
      "Params: tensor([ 2.4197, -0.6167])\n",
      "Grad:  tensor([-0.5013,  2.8348])\n",
      "Epoch 2896 Loss 27.319658\n",
      "Params: tensor([ 2.4198, -0.6170])\n",
      "Grad:  tensor([-0.5013,  2.8348])\n",
      "Epoch 2897 Loss 27.318823\n",
      "Params: tensor([ 2.4198, -0.6172])\n",
      "Grad:  tensor([-0.5013,  2.8347])\n",
      "Epoch 2898 Loss 27.317999\n",
      "Params: tensor([ 2.4199, -0.6175])\n",
      "Grad:  tensor([-0.5012,  2.8347])\n",
      "Epoch 2899 Loss 27.317169\n",
      "Params: tensor([ 2.4199, -0.6178])\n",
      "Grad:  tensor([-0.5012,  2.8346])\n",
      "Epoch 2900 Loss 27.316339\n",
      "Params: tensor([ 2.4200, -0.6181])\n",
      "Grad:  tensor([-0.5012,  2.8346])\n",
      "Epoch 2901 Loss 27.315510\n",
      "Params: tensor([ 2.4200, -0.6184])\n",
      "Grad:  tensor([-0.5012,  2.8345])\n",
      "Epoch 2902 Loss 27.314680\n",
      "Params: tensor([ 2.4201, -0.6187])\n",
      "Grad:  tensor([-0.5012,  2.8345])\n",
      "Epoch 2903 Loss 27.313854\n",
      "Params: tensor([ 2.4201, -0.6189])\n",
      "Grad:  tensor([-0.5012,  2.8344])\n",
      "Epoch 2904 Loss 27.313025\n",
      "Params: tensor([ 2.4202, -0.6192])\n",
      "Grad:  tensor([-0.5012,  2.8344])\n",
      "Epoch 2905 Loss 27.312197\n",
      "Params: tensor([ 2.4202, -0.6195])\n",
      "Grad:  tensor([-0.5012,  2.8343])\n",
      "Epoch 2906 Loss 27.311369\n",
      "Params: tensor([ 2.4203, -0.6198])\n",
      "Grad:  tensor([-0.5012,  2.8343])\n",
      "Epoch 2907 Loss 27.310541\n",
      "Params: tensor([ 2.4203, -0.6201])\n",
      "Grad:  tensor([-0.5012,  2.8343])\n",
      "Epoch 2908 Loss 27.309710\n",
      "Params: tensor([ 2.4204, -0.6204])\n",
      "Grad:  tensor([-0.5012,  2.8342])\n",
      "Epoch 2909 Loss 27.308882\n",
      "Params: tensor([ 2.4204, -0.6206])\n",
      "Grad:  tensor([-0.5012,  2.8342])\n",
      "Epoch 2910 Loss 27.308052\n",
      "Params: tensor([ 2.4205, -0.6209])\n",
      "Grad:  tensor([-0.5011,  2.8341])\n",
      "Epoch 2911 Loss 27.307222\n",
      "Params: tensor([ 2.4205, -0.6212])\n",
      "Grad:  tensor([-0.5011,  2.8341])\n",
      "Epoch 2912 Loss 27.306398\n",
      "Params: tensor([ 2.4206, -0.6215])\n",
      "Grad:  tensor([-0.5011,  2.8340])\n",
      "Epoch 2913 Loss 27.305565\n",
      "Params: tensor([ 2.4206, -0.6218])\n",
      "Grad:  tensor([-0.5011,  2.8340])\n",
      "Epoch 2914 Loss 27.304743\n",
      "Params: tensor([ 2.4207, -0.6221])\n",
      "Grad:  tensor([-0.5011,  2.8339])\n",
      "Epoch 2915 Loss 27.303911\n",
      "Params: tensor([ 2.4207, -0.6223])\n",
      "Grad:  tensor([-0.5011,  2.8339])\n",
      "Epoch 2916 Loss 27.303083\n",
      "Params: tensor([ 2.4208, -0.6226])\n",
      "Grad:  tensor([-0.5011,  2.8338])\n",
      "Epoch 2917 Loss 27.302254\n",
      "Params: tensor([ 2.4208, -0.6229])\n",
      "Grad:  tensor([-0.5011,  2.8338])\n",
      "Epoch 2918 Loss 27.301424\n",
      "Params: tensor([ 2.4209, -0.6232])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor([-0.5011,  2.8337])\n",
      "Epoch 2919 Loss 27.300604\n",
      "Params: tensor([ 2.4209, -0.6235])\n",
      "Grad:  tensor([-0.5011,  2.8337])\n",
      "Epoch 2920 Loss 27.299774\n",
      "Params: tensor([ 2.4210, -0.6238])\n",
      "Grad:  tensor([-0.5011,  2.8336])\n",
      "Epoch 2921 Loss 27.298948\n",
      "Params: tensor([ 2.4210, -0.6240])\n",
      "Grad:  tensor([-0.5010,  2.8336])\n",
      "Epoch 2922 Loss 27.298113\n",
      "Params: tensor([ 2.4211, -0.6243])\n",
      "Grad:  tensor([-0.5010,  2.8335])\n",
      "Epoch 2923 Loss 27.297291\n",
      "Params: tensor([ 2.4211, -0.6246])\n",
      "Grad:  tensor([-0.5010,  2.8335])\n",
      "Epoch 2924 Loss 27.296461\n",
      "Params: tensor([ 2.4212, -0.6249])\n",
      "Grad:  tensor([-0.5010,  2.8334])\n",
      "Epoch 2925 Loss 27.295630\n",
      "Params: tensor([ 2.4212, -0.6252])\n",
      "Grad:  tensor([-0.5010,  2.8334])\n",
      "Epoch 2926 Loss 27.294800\n",
      "Params: tensor([ 2.4213, -0.6255])\n",
      "Grad:  tensor([-0.5010,  2.8333])\n",
      "Epoch 2927 Loss 27.293978\n",
      "Params: tensor([ 2.4213, -0.6257])\n",
      "Grad:  tensor([-0.5010,  2.8333])\n",
      "Epoch 2928 Loss 27.293150\n",
      "Params: tensor([ 2.4214, -0.6260])\n",
      "Grad:  tensor([-0.5010,  2.8332])\n",
      "Epoch 2929 Loss 27.292320\n",
      "Params: tensor([ 2.4214, -0.6263])\n",
      "Grad:  tensor([-0.5010,  2.8332])\n",
      "Epoch 2930 Loss 27.291492\n",
      "Params: tensor([ 2.4215, -0.6266])\n",
      "Grad:  tensor([-0.5010,  2.8331])\n",
      "Epoch 2931 Loss 27.290663\n",
      "Params: tensor([ 2.4215, -0.6269])\n",
      "Grad:  tensor([-0.5009,  2.8331])\n",
      "Epoch 2932 Loss 27.289839\n",
      "Params: tensor([ 2.4216, -0.6272])\n",
      "Grad:  tensor([-0.5009,  2.8331])\n",
      "Epoch 2933 Loss 27.289009\n",
      "Params: tensor([ 2.4216, -0.6274])\n",
      "Grad:  tensor([-0.5009,  2.8330])\n",
      "Epoch 2934 Loss 27.288183\n",
      "Params: tensor([ 2.4217, -0.6277])\n",
      "Grad:  tensor([-0.5009,  2.8330])\n",
      "Epoch 2935 Loss 27.287354\n",
      "Params: tensor([ 2.4217, -0.6280])\n",
      "Grad:  tensor([-0.5009,  2.8329])\n",
      "Epoch 2936 Loss 27.286530\n",
      "Params: tensor([ 2.4218, -0.6283])\n",
      "Grad:  tensor([-0.5009,  2.8329])\n",
      "Epoch 2937 Loss 27.285698\n",
      "Params: tensor([ 2.4218, -0.6286])\n",
      "Grad:  tensor([-0.5009,  2.8328])\n",
      "Epoch 2938 Loss 27.284870\n",
      "Params: tensor([ 2.4219, -0.6289])\n",
      "Grad:  tensor([-0.5009,  2.8328])\n",
      "Epoch 2939 Loss 27.284040\n",
      "Params: tensor([ 2.4219, -0.6291])\n",
      "Grad:  tensor([-0.5009,  2.8327])\n",
      "Epoch 2940 Loss 27.283215\n",
      "Params: tensor([ 2.4220, -0.6294])\n",
      "Grad:  tensor([-0.5009,  2.8327])\n",
      "Epoch 2941 Loss 27.282385\n",
      "Params: tensor([ 2.4220, -0.6297])\n",
      "Grad:  tensor([-0.5009,  2.8326])\n",
      "Epoch 2942 Loss 27.281561\n",
      "Params: tensor([ 2.4221, -0.6300])\n",
      "Grad:  tensor([-0.5008,  2.8326])\n",
      "Epoch 2943 Loss 27.280733\n",
      "Params: tensor([ 2.4221, -0.6303])\n",
      "Grad:  tensor([-0.5008,  2.8325])\n",
      "Epoch 2944 Loss 27.279905\n",
      "Params: tensor([ 2.4222, -0.6306])\n",
      "Grad:  tensor([-0.5008,  2.8325])\n",
      "Epoch 2945 Loss 27.279078\n",
      "Params: tensor([ 2.4222, -0.6308])\n",
      "Grad:  tensor([-0.5008,  2.8324])\n",
      "Epoch 2946 Loss 27.278254\n",
      "Params: tensor([ 2.4223, -0.6311])\n",
      "Grad:  tensor([-0.5008,  2.8324])\n",
      "Epoch 2947 Loss 27.277424\n",
      "Params: tensor([ 2.4223, -0.6314])\n",
      "Grad:  tensor([-0.5008,  2.8323])\n",
      "Epoch 2948 Loss 27.276600\n",
      "Params: tensor([ 2.4224, -0.6317])\n",
      "Grad:  tensor([-0.5008,  2.8323])\n",
      "Epoch 2949 Loss 27.275768\n",
      "Params: tensor([ 2.4224, -0.6320])\n",
      "Grad:  tensor([-0.5008,  2.8322])\n",
      "Epoch 2950 Loss 27.274944\n",
      "Params: tensor([ 2.4225, -0.6323])\n",
      "Grad:  tensor([-0.5008,  2.8322])\n",
      "Epoch 2951 Loss 27.274111\n",
      "Params: tensor([ 2.4225, -0.6325])\n",
      "Grad:  tensor([-0.5007,  2.8321])\n",
      "Epoch 2952 Loss 27.273287\n",
      "Params: tensor([ 2.4226, -0.6328])\n",
      "Grad:  tensor([-0.5007,  2.8321])\n",
      "Epoch 2953 Loss 27.272455\n",
      "Params: tensor([ 2.4226, -0.6331])\n",
      "Grad:  tensor([-0.5007,  2.8320])\n",
      "Epoch 2954 Loss 27.271637\n",
      "Params: tensor([ 2.4227, -0.6334])\n",
      "Grad:  tensor([-0.5007,  2.8320])\n",
      "Epoch 2955 Loss 27.270805\n",
      "Params: tensor([ 2.4227, -0.6337])\n",
      "Grad:  tensor([-0.5007,  2.8319])\n",
      "Epoch 2956 Loss 27.269978\n",
      "Params: tensor([ 2.4228, -0.6340])\n",
      "Grad:  tensor([-0.5007,  2.8319])\n",
      "Epoch 2957 Loss 27.269152\n",
      "Params: tensor([ 2.4228, -0.6342])\n",
      "Grad:  tensor([-0.5007,  2.8319])\n",
      "Epoch 2958 Loss 27.268328\n",
      "Params: tensor([ 2.4229, -0.6345])\n",
      "Grad:  tensor([-0.5007,  2.8318])\n",
      "Epoch 2959 Loss 27.267498\n",
      "Params: tensor([ 2.4229, -0.6348])\n",
      "Grad:  tensor([-0.5007,  2.8318])\n",
      "Epoch 2960 Loss 27.266674\n",
      "Params: tensor([ 2.4230, -0.6351])\n",
      "Grad:  tensor([-0.5007,  2.8317])\n",
      "Epoch 2961 Loss 27.265846\n",
      "Params: tensor([ 2.4230, -0.6354])\n",
      "Grad:  tensor([-0.5006,  2.8317])\n",
      "Epoch 2962 Loss 27.265022\n",
      "Params: tensor([ 2.4231, -0.6357])\n",
      "Grad:  tensor([-0.5006,  2.8316])\n",
      "Epoch 2963 Loss 27.264196\n",
      "Params: tensor([ 2.4231, -0.6359])\n",
      "Grad:  tensor([-0.5006,  2.8316])\n",
      "Epoch 2964 Loss 27.263367\n",
      "Params: tensor([ 2.4232, -0.6362])\n",
      "Grad:  tensor([-0.5006,  2.8315])\n",
      "Epoch 2965 Loss 27.262541\n",
      "Params: tensor([ 2.4232, -0.6365])\n",
      "Grad:  tensor([-0.5006,  2.8315])\n",
      "Epoch 2966 Loss 27.261713\n",
      "Params: tensor([ 2.4233, -0.6368])\n",
      "Grad:  tensor([-0.5006,  2.8314])\n",
      "Epoch 2967 Loss 27.260887\n",
      "Params: tensor([ 2.4233, -0.6371])\n",
      "Grad:  tensor([-0.5006,  2.8314])\n",
      "Epoch 2968 Loss 27.260057\n",
      "Params: tensor([ 2.4234, -0.6374])\n",
      "Grad:  tensor([-0.5006,  2.8313])\n",
      "Epoch 2969 Loss 27.259230\n",
      "Params: tensor([ 2.4234, -0.6376])\n",
      "Grad:  tensor([-0.5006,  2.8313])\n",
      "Epoch 2970 Loss 27.258406\n",
      "Params: tensor([ 2.4235, -0.6379])\n",
      "Grad:  tensor([-0.5005,  2.8312])\n",
      "Epoch 2971 Loss 27.257580\n",
      "Params: tensor([ 2.4235, -0.6382])\n",
      "Grad:  tensor([-0.5005,  2.8312])\n",
      "Epoch 2972 Loss 27.256752\n",
      "Params: tensor([ 2.4236, -0.6385])\n",
      "Grad:  tensor([-0.5005,  2.8311])\n",
      "Epoch 2973 Loss 27.255926\n",
      "Params: tensor([ 2.4236, -0.6388])\n",
      "Grad:  tensor([-0.5005,  2.8311])\n",
      "Epoch 2974 Loss 27.255098\n",
      "Params: tensor([ 2.4237, -0.6391])\n",
      "Grad:  tensor([-0.5005,  2.8310])\n",
      "Epoch 2975 Loss 27.254271\n",
      "Params: tensor([ 2.4237, -0.6393])\n",
      "Grad:  tensor([-0.5005,  2.8310])\n",
      "Epoch 2976 Loss 27.253447\n",
      "Params: tensor([ 2.4238, -0.6396])\n",
      "Grad:  tensor([-0.5005,  2.8309])\n",
      "Epoch 2977 Loss 27.252619\n",
      "Params: tensor([ 2.4238, -0.6399])\n",
      "Grad:  tensor([-0.5005,  2.8309])\n",
      "Epoch 2978 Loss 27.251795\n",
      "Params: tensor([ 2.4239, -0.6402])\n",
      "Grad:  tensor([-0.5004,  2.8309])\n",
      "Epoch 2979 Loss 27.250965\n",
      "Params: tensor([ 2.4239, -0.6405])\n",
      "Grad:  tensor([-0.5004,  2.8308])\n",
      "Epoch 2980 Loss 27.250145\n",
      "Params: tensor([ 2.4240, -0.6408])\n",
      "Grad:  tensor([-0.5004,  2.8308])\n",
      "Epoch 2981 Loss 27.249315\n",
      "Params: tensor([ 2.4240, -0.6410])\n",
      "Grad:  tensor([-0.5004,  2.8307])\n",
      "Epoch 2982 Loss 27.248482\n",
      "Params: tensor([ 2.4241, -0.6413])\n",
      "Grad:  tensor([-0.5004,  2.8307])\n",
      "Epoch 2983 Loss 27.247662\n",
      "Params: tensor([ 2.4241, -0.6416])\n",
      "Grad:  tensor([-0.5004,  2.8306])\n",
      "Epoch 2984 Loss 27.246832\n",
      "Params: tensor([ 2.4242, -0.6419])\n",
      "Grad:  tensor([-0.5004,  2.8306])\n",
      "Epoch 2985 Loss 27.246014\n",
      "Params: tensor([ 2.4242, -0.6422])\n",
      "Grad:  tensor([-0.5004,  2.8305])\n",
      "Epoch 2986 Loss 27.245180\n",
      "Params: tensor([ 2.4243, -0.6425])\n",
      "Grad:  tensor([-0.5003,  2.8305])\n",
      "Epoch 2987 Loss 27.244354\n",
      "Params: tensor([ 2.4243, -0.6427])\n",
      "Grad:  tensor([-0.5003,  2.8304])\n",
      "Epoch 2988 Loss 27.243532\n",
      "Params: tensor([ 2.4244, -0.6430])\n",
      "Grad:  tensor([-0.5003,  2.8304])\n",
      "Epoch 2989 Loss 27.242704\n",
      "Params: tensor([ 2.4244, -0.6433])\n",
      "Grad:  tensor([-0.5003,  2.8303])\n",
      "Epoch 2990 Loss 27.241880\n",
      "Params: tensor([ 2.4245, -0.6436])\n",
      "Grad:  tensor([-0.5003,  2.8303])\n",
      "Epoch 2991 Loss 27.241051\n",
      "Params: tensor([ 2.4245, -0.6439])\n",
      "Grad:  tensor([-0.5003,  2.8302])\n",
      "Epoch 2992 Loss 27.240229\n",
      "Params: tensor([ 2.4246, -0.6442])\n",
      "Grad:  tensor([-0.5003,  2.8302])\n",
      "Epoch 2993 Loss 27.239397\n",
      "Params: tensor([ 2.4246, -0.6444])\n",
      "Grad:  tensor([-0.5003,  2.8301])\n",
      "Epoch 2994 Loss 27.238573\n",
      "Params: tensor([ 2.4247, -0.6447])\n",
      "Grad:  tensor([-0.5002,  2.8301])\n",
      "Epoch 2995 Loss 27.237740\n",
      "Params: tensor([ 2.4247, -0.6450])\n",
      "Grad:  tensor([-0.5002,  2.8300])\n",
      "Epoch 2996 Loss 27.236919\n",
      "Params: tensor([ 2.4248, -0.6453])\n",
      "Grad:  tensor([-0.5002,  2.8300])\n",
      "Epoch 2997 Loss 27.236097\n",
      "Params: tensor([ 2.4248, -0.6456])\n",
      "Grad:  tensor([-0.5002,  2.8300])\n",
      "Epoch 2998 Loss 27.235271\n",
      "Params: tensor([ 2.4249, -0.6459])\n",
      "Grad:  tensor([-0.5002,  2.8299])\n",
      "Epoch 2999 Loss 27.234444\n",
      "Params: tensor([ 2.4249, -0.6461])\n",
      "Grad:  tensor([-0.5002,  2.8299])\n",
      "Epoch 3000 Loss 27.233614\n",
      "Params: tensor([ 2.4250, -0.6464])\n",
      "Grad:  tensor([-0.5002,  2.8298])\n",
      "Epoch 3001 Loss 27.232794\n",
      "Params: tensor([ 2.4250, -0.6467])\n",
      "Grad:  tensor([-0.5002,  2.8298])\n",
      "Epoch 3002 Loss 27.231964\n",
      "Params: tensor([ 2.4251, -0.6470])\n",
      "Grad:  tensor([-0.5001,  2.8297])\n",
      "Epoch 3003 Loss 27.231138\n",
      "Params: tensor([ 2.4251, -0.6473])\n",
      "Grad:  tensor([-0.5001,  2.8297])\n",
      "Epoch 3004 Loss 27.230316\n",
      "Params: tensor([ 2.4252, -0.6476])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor([-0.5001,  2.8296])\n",
      "Epoch 3005 Loss 27.229486\n",
      "Params: tensor([ 2.4252, -0.6478])\n",
      "Grad:  tensor([-0.5001,  2.8296])\n",
      "Epoch 3006 Loss 27.228662\n",
      "Params: tensor([ 2.4253, -0.6481])\n",
      "Grad:  tensor([-0.5001,  2.8295])\n",
      "Epoch 3007 Loss 27.227837\n",
      "Params: tensor([ 2.4253, -0.6484])\n",
      "Grad:  tensor([-0.5001,  2.8295])\n",
      "Epoch 3008 Loss 27.227013\n",
      "Params: tensor([ 2.4254, -0.6487])\n",
      "Grad:  tensor([-0.5001,  2.8294])\n",
      "Epoch 3009 Loss 27.226185\n",
      "Params: tensor([ 2.4254, -0.6490])\n",
      "Grad:  tensor([-0.5000,  2.8294])\n",
      "Epoch 3010 Loss 27.225365\n",
      "Params: tensor([ 2.4255, -0.6492])\n",
      "Grad:  tensor([-0.5000,  2.8293])\n",
      "Epoch 3011 Loss 27.224531\n",
      "Params: tensor([ 2.4255, -0.6495])\n",
      "Grad:  tensor([-0.5000,  2.8293])\n",
      "Epoch 3012 Loss 27.223707\n",
      "Params: tensor([ 2.4256, -0.6498])\n",
      "Grad:  tensor([-0.5000,  2.8292])\n",
      "Epoch 3013 Loss 27.222883\n",
      "Params: tensor([ 2.4256, -0.6501])\n",
      "Grad:  tensor([-0.5000,  2.8292])\n",
      "Epoch 3014 Loss 27.222057\n",
      "Params: tensor([ 2.4257, -0.6504])\n",
      "Grad:  tensor([-0.5000,  2.8292])\n",
      "Epoch 3015 Loss 27.221235\n",
      "Params: tensor([ 2.4257, -0.6507])\n",
      "Grad:  tensor([-0.5000,  2.8291])\n",
      "Epoch 3016 Loss 27.220406\n",
      "Params: tensor([ 2.4258, -0.6509])\n",
      "Grad:  tensor([-0.5000,  2.8291])\n",
      "Epoch 3017 Loss 27.219582\n",
      "Params: tensor([ 2.4258, -0.6512])\n",
      "Grad:  tensor([-0.4999,  2.8290])\n",
      "Epoch 3018 Loss 27.218756\n",
      "Params: tensor([ 2.4259, -0.6515])\n",
      "Grad:  tensor([-0.4999,  2.8290])\n",
      "Epoch 3019 Loss 27.217932\n",
      "Params: tensor([ 2.4259, -0.6518])\n",
      "Grad:  tensor([-0.4999,  2.8289])\n",
      "Epoch 3020 Loss 27.217104\n",
      "Params: tensor([ 2.4260, -0.6521])\n",
      "Grad:  tensor([-0.4999,  2.8289])\n",
      "Epoch 3021 Loss 27.216278\n",
      "Params: tensor([ 2.4260, -0.6524])\n",
      "Grad:  tensor([-0.4999,  2.8288])\n",
      "Epoch 3022 Loss 27.215454\n",
      "Params: tensor([ 2.4261, -0.6526])\n",
      "Grad:  tensor([-0.4999,  2.8288])\n",
      "Epoch 3023 Loss 27.214630\n",
      "Params: tensor([ 2.4261, -0.6529])\n",
      "Grad:  tensor([-0.4999,  2.8287])\n",
      "Epoch 3024 Loss 27.213800\n",
      "Params: tensor([ 2.4262, -0.6532])\n",
      "Grad:  tensor([-0.4998,  2.8287])\n",
      "Epoch 3025 Loss 27.212982\n",
      "Params: tensor([ 2.4262, -0.6535])\n",
      "Grad:  tensor([-0.4998,  2.8286])\n",
      "Epoch 3026 Loss 27.212152\n",
      "Params: tensor([ 2.4263, -0.6538])\n",
      "Grad:  tensor([-0.4998,  2.8286])\n",
      "Epoch 3027 Loss 27.211327\n",
      "Params: tensor([ 2.4263, -0.6541])\n",
      "Grad:  tensor([-0.4998,  2.8285])\n",
      "Epoch 3028 Loss 27.210508\n",
      "Params: tensor([ 2.4264, -0.6543])\n",
      "Grad:  tensor([-0.4998,  2.8285])\n",
      "Epoch 3029 Loss 27.209675\n",
      "Params: tensor([ 2.4264, -0.6546])\n",
      "Grad:  tensor([-0.4998,  2.8284])\n",
      "Epoch 3030 Loss 27.208851\n",
      "Params: tensor([ 2.4265, -0.6549])\n",
      "Grad:  tensor([-0.4998,  2.8284])\n",
      "Epoch 3031 Loss 27.208021\n",
      "Params: tensor([ 2.4265, -0.6552])\n",
      "Grad:  tensor([-0.4997,  2.8284])\n",
      "Epoch 3032 Loss 27.207203\n",
      "Params: tensor([ 2.4266, -0.6555])\n",
      "Grad:  tensor([-0.4997,  2.8283])\n",
      "Epoch 3033 Loss 27.206377\n",
      "Params: tensor([ 2.4266, -0.6558])\n",
      "Grad:  tensor([-0.4997,  2.8283])\n",
      "Epoch 3034 Loss 27.205553\n",
      "Params: tensor([ 2.4267, -0.6560])\n",
      "Grad:  tensor([-0.4997,  2.8282])\n",
      "Epoch 3035 Loss 27.204729\n",
      "Params: tensor([ 2.4267, -0.6563])\n",
      "Grad:  tensor([-0.4997,  2.8282])\n",
      "Epoch 3036 Loss 27.203901\n",
      "Params: tensor([ 2.4268, -0.6566])\n",
      "Grad:  tensor([-0.4997,  2.8281])\n",
      "Epoch 3037 Loss 27.203081\n",
      "Params: tensor([ 2.4268, -0.6569])\n",
      "Grad:  tensor([-0.4996,  2.8281])\n",
      "Epoch 3038 Loss 27.202248\n",
      "Params: tensor([ 2.4269, -0.6572])\n",
      "Grad:  tensor([-0.4996,  2.8280])\n",
      "Epoch 3039 Loss 27.201433\n",
      "Params: tensor([ 2.4269, -0.6575])\n",
      "Grad:  tensor([-0.4996,  2.8280])\n",
      "Epoch 3040 Loss 27.200605\n",
      "Params: tensor([ 2.4270, -0.6577])\n",
      "Grad:  tensor([-0.4996,  2.8279])\n",
      "Epoch 3041 Loss 27.199781\n",
      "Params: tensor([ 2.4270, -0.6580])\n",
      "Grad:  tensor([-0.4996,  2.8279])\n",
      "Epoch 3042 Loss 27.198961\n",
      "Params: tensor([ 2.4271, -0.6583])\n",
      "Grad:  tensor([-0.4996,  2.8278])\n",
      "Epoch 3043 Loss 27.198137\n",
      "Params: tensor([ 2.4271, -0.6586])\n",
      "Grad:  tensor([-0.4996,  2.8278])\n",
      "Epoch 3044 Loss 27.197308\n",
      "Params: tensor([ 2.4272, -0.6589])\n",
      "Grad:  tensor([-0.4995,  2.8277])\n",
      "Epoch 3045 Loss 27.196484\n",
      "Params: tensor([ 2.4272, -0.6591])\n",
      "Grad:  tensor([-0.4995,  2.8277])\n",
      "Epoch 3046 Loss 27.195660\n",
      "Params: tensor([ 2.4273, -0.6594])\n",
      "Grad:  tensor([-0.4995,  2.8276])\n",
      "Epoch 3047 Loss 27.194838\n",
      "Params: tensor([ 2.4273, -0.6597])\n",
      "Grad:  tensor([-0.4995,  2.8276])\n",
      "Epoch 3048 Loss 27.194008\n",
      "Params: tensor([ 2.4274, -0.6600])\n",
      "Grad:  tensor([-0.4995,  2.8276])\n",
      "Epoch 3049 Loss 27.193182\n",
      "Params: tensor([ 2.4274, -0.6603])\n",
      "Grad:  tensor([-0.4995,  2.8275])\n",
      "Epoch 3050 Loss 27.192366\n",
      "Params: tensor([ 2.4275, -0.6606])\n",
      "Grad:  tensor([-0.4995,  2.8275])\n",
      "Epoch 3051 Loss 27.191540\n",
      "Params: tensor([ 2.4275, -0.6608])\n",
      "Grad:  tensor([-0.4995,  2.8274])\n",
      "Epoch 3052 Loss 27.190710\n",
      "Params: tensor([ 2.4276, -0.6611])\n",
      "Grad:  tensor([-0.4995,  2.8274])\n",
      "Epoch 3053 Loss 27.189886\n",
      "Params: tensor([ 2.4276, -0.6614])\n",
      "Grad:  tensor([-0.4995,  2.8273])\n",
      "Epoch 3054 Loss 27.189062\n",
      "Params: tensor([ 2.4277, -0.6617])\n",
      "Grad:  tensor([-0.4995,  2.8273])\n",
      "Epoch 3055 Loss 27.188244\n",
      "Params: tensor([ 2.4277, -0.6620])\n",
      "Grad:  tensor([-0.4995,  2.8272])\n",
      "Epoch 3056 Loss 27.187414\n",
      "Params: tensor([ 2.4278, -0.6623])\n",
      "Grad:  tensor([-0.4995,  2.8272])\n",
      "Epoch 3057 Loss 27.186590\n",
      "Params: tensor([ 2.4278, -0.6625])\n",
      "Grad:  tensor([-0.4995,  2.8271])\n",
      "Epoch 3058 Loss 27.185762\n",
      "Params: tensor([ 2.4279, -0.6628])\n",
      "Grad:  tensor([-0.4995,  2.8271])\n",
      "Epoch 3059 Loss 27.184938\n",
      "Params: tensor([ 2.4279, -0.6631])\n",
      "Grad:  tensor([-0.4995,  2.8270])\n",
      "Epoch 3060 Loss 27.184118\n",
      "Params: tensor([ 2.4280, -0.6634])\n",
      "Grad:  tensor([-0.4995,  2.8270])\n",
      "Epoch 3061 Loss 27.183290\n",
      "Params: tensor([ 2.4280, -0.6637])\n",
      "Grad:  tensor([-0.4995,  2.8269])\n",
      "Epoch 3062 Loss 27.182472\n",
      "Params: tensor([ 2.4281, -0.6640])\n",
      "Grad:  tensor([-0.4995,  2.8269])\n",
      "Epoch 3063 Loss 27.181643\n",
      "Params: tensor([ 2.4281, -0.6642])\n",
      "Grad:  tensor([-0.4995,  2.8268])\n",
      "Epoch 3064 Loss 27.180819\n",
      "Params: tensor([ 2.4282, -0.6645])\n",
      "Grad:  tensor([-0.4995,  2.8268])\n",
      "Epoch 3065 Loss 27.179998\n",
      "Params: tensor([ 2.4282, -0.6648])\n",
      "Grad:  tensor([-0.4995,  2.8267])\n",
      "Epoch 3066 Loss 27.179174\n",
      "Params: tensor([ 2.4283, -0.6651])\n",
      "Grad:  tensor([-0.4995,  2.8267])\n",
      "Epoch 3067 Loss 27.178345\n",
      "Params: tensor([ 2.4283, -0.6654])\n",
      "Grad:  tensor([-0.4994,  2.8266])\n",
      "Epoch 3068 Loss 27.177519\n",
      "Params: tensor([ 2.4284, -0.6657])\n",
      "Grad:  tensor([-0.4994,  2.8266])\n",
      "Epoch 3069 Loss 27.176702\n",
      "Params: tensor([ 2.4284, -0.6659])\n",
      "Grad:  tensor([-0.4994,  2.8265])\n",
      "Epoch 3070 Loss 27.175879\n",
      "Params: tensor([ 2.4285, -0.6662])\n",
      "Grad:  tensor([-0.4994,  2.8265])\n",
      "Epoch 3071 Loss 27.175056\n",
      "Params: tensor([ 2.4285, -0.6665])\n",
      "Grad:  tensor([-0.4994,  2.8264])\n",
      "Epoch 3072 Loss 27.174227\n",
      "Params: tensor([ 2.4286, -0.6668])\n",
      "Grad:  tensor([-0.4994,  2.8264])\n",
      "Epoch 3073 Loss 27.173407\n",
      "Params: tensor([ 2.4286, -0.6671])\n",
      "Grad:  tensor([-0.4994,  2.8263])\n",
      "Epoch 3074 Loss 27.172588\n",
      "Params: tensor([ 2.4287, -0.6673])\n",
      "Grad:  tensor([-0.4994,  2.8263])\n",
      "Epoch 3075 Loss 27.171761\n",
      "Params: tensor([ 2.4287, -0.6676])\n",
      "Grad:  tensor([-0.4994,  2.8262])\n",
      "Epoch 3076 Loss 27.170935\n",
      "Params: tensor([ 2.4288, -0.6679])\n",
      "Grad:  tensor([-0.4994,  2.8262])\n",
      "Epoch 3077 Loss 27.170116\n",
      "Params: tensor([ 2.4288, -0.6682])\n",
      "Grad:  tensor([-0.4994,  2.8261])\n",
      "Epoch 3078 Loss 27.169283\n",
      "Params: tensor([ 2.4289, -0.6685])\n",
      "Grad:  tensor([-0.4994,  2.8261])\n",
      "Epoch 3079 Loss 27.168465\n",
      "Params: tensor([ 2.4289, -0.6688])\n",
      "Grad:  tensor([-0.4994,  2.8260])\n",
      "Epoch 3080 Loss 27.167646\n",
      "Params: tensor([ 2.4290, -0.6690])\n",
      "Grad:  tensor([-0.4994,  2.8260])\n",
      "Epoch 3081 Loss 27.166821\n",
      "Params: tensor([ 2.4290, -0.6693])\n",
      "Grad:  tensor([-0.4994,  2.8259])\n",
      "Epoch 3082 Loss 27.165993\n",
      "Params: tensor([ 2.4291, -0.6696])\n",
      "Grad:  tensor([-0.4994,  2.8259])\n",
      "Epoch 3083 Loss 27.165173\n",
      "Params: tensor([ 2.4291, -0.6699])\n",
      "Grad:  tensor([-0.4994,  2.8258])\n",
      "Epoch 3084 Loss 27.164351\n",
      "Params: tensor([ 2.4292, -0.6702])\n",
      "Grad:  tensor([-0.4994,  2.8258])\n",
      "Epoch 3085 Loss 27.163525\n",
      "Params: tensor([ 2.4292, -0.6705])\n",
      "Grad:  tensor([-0.4994,  2.8257])\n",
      "Epoch 3086 Loss 27.162703\n",
      "Params: tensor([ 2.4293, -0.6707])\n",
      "Grad:  tensor([-0.4994,  2.8257])\n",
      "Epoch 3087 Loss 27.161882\n",
      "Params: tensor([ 2.4293, -0.6710])\n",
      "Grad:  tensor([-0.4994,  2.8256])\n",
      "Epoch 3088 Loss 27.161053\n",
      "Params: tensor([ 2.4294, -0.6713])\n",
      "Grad:  tensor([-0.4994,  2.8256])\n",
      "Epoch 3089 Loss 27.160231\n",
      "Params: tensor([ 2.4294, -0.6716])\n",
      "Grad:  tensor([-0.4994,  2.8255])\n",
      "Epoch 3090 Loss 27.159410\n",
      "Params: tensor([ 2.4295, -0.6719])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor([-0.4994,  2.8255])\n",
      "Epoch 3091 Loss 27.158583\n",
      "Params: tensor([ 2.4295, -0.6722])\n",
      "Grad:  tensor([-0.4994,  2.8254])\n",
      "Epoch 3092 Loss 27.157757\n",
      "Params: tensor([ 2.4296, -0.6724])\n",
      "Grad:  tensor([-0.4994,  2.8254])\n",
      "Epoch 3093 Loss 27.156935\n",
      "Params: tensor([ 2.4296, -0.6727])\n",
      "Grad:  tensor([-0.4994,  2.8253])\n",
      "Epoch 3094 Loss 27.156116\n",
      "Params: tensor([ 2.4297, -0.6730])\n",
      "Grad:  tensor([-0.4994,  2.8253])\n",
      "Epoch 3095 Loss 27.155293\n",
      "Params: tensor([ 2.4297, -0.6733])\n",
      "Grad:  tensor([-0.4994,  2.8253])\n",
      "Epoch 3096 Loss 27.154467\n",
      "Params: tensor([ 2.4298, -0.6736])\n",
      "Grad:  tensor([-0.4994,  2.8252])\n",
      "Epoch 3097 Loss 27.153645\n",
      "Params: tensor([ 2.4298, -0.6738])\n",
      "Grad:  tensor([-0.4994,  2.8252])\n",
      "Epoch 3098 Loss 27.152819\n",
      "Params: tensor([ 2.4299, -0.6741])\n",
      "Grad:  tensor([-0.4994,  2.8251])\n",
      "Epoch 3099 Loss 27.152000\n",
      "Params: tensor([ 2.4299, -0.6744])\n",
      "Grad:  tensor([-0.4993,  2.8251])\n",
      "Epoch 3100 Loss 27.151173\n",
      "Params: tensor([ 2.4300, -0.6747])\n",
      "Grad:  tensor([-0.4993,  2.8250])\n",
      "Epoch 3101 Loss 27.150358\n",
      "Params: tensor([ 2.4300, -0.6750])\n",
      "Grad:  tensor([-0.4993,  2.8250])\n",
      "Epoch 3102 Loss 27.149534\n",
      "Params: tensor([ 2.4301, -0.6753])\n",
      "Grad:  tensor([-0.4993,  2.8249])\n",
      "Epoch 3103 Loss 27.148714\n",
      "Params: tensor([ 2.4301, -0.6755])\n",
      "Grad:  tensor([-0.4993,  2.8249])\n",
      "Epoch 3104 Loss 27.147888\n",
      "Params: tensor([ 2.4302, -0.6758])\n",
      "Grad:  tensor([-0.4993,  2.8248])\n",
      "Epoch 3105 Loss 27.147064\n",
      "Params: tensor([ 2.4302, -0.6761])\n",
      "Grad:  tensor([-0.4993,  2.8248])\n",
      "Epoch 3106 Loss 27.146242\n",
      "Params: tensor([ 2.4303, -0.6764])\n",
      "Grad:  tensor([-0.4993,  2.8247])\n",
      "Epoch 3107 Loss 27.145416\n",
      "Params: tensor([ 2.4303, -0.6767])\n",
      "Grad:  tensor([-0.4993,  2.8247])\n",
      "Epoch 3108 Loss 27.144594\n",
      "Params: tensor([ 2.4304, -0.6770])\n",
      "Grad:  tensor([-0.4993,  2.8246])\n",
      "Epoch 3109 Loss 27.143776\n",
      "Params: tensor([ 2.4304, -0.6772])\n",
      "Grad:  tensor([-0.4993,  2.8246])\n",
      "Epoch 3110 Loss 27.142952\n",
      "Params: tensor([ 2.4305, -0.6775])\n",
      "Grad:  tensor([-0.4993,  2.8245])\n",
      "Epoch 3111 Loss 27.142126\n",
      "Params: tensor([ 2.4305, -0.6778])\n",
      "Grad:  tensor([-0.4993,  2.8245])\n",
      "Epoch 3112 Loss 27.141304\n",
      "Params: tensor([ 2.4306, -0.6781])\n",
      "Grad:  tensor([-0.4993,  2.8244])\n",
      "Epoch 3113 Loss 27.140486\n",
      "Params: tensor([ 2.4306, -0.6784])\n",
      "Grad:  tensor([-0.4993,  2.8244])\n",
      "Epoch 3114 Loss 27.139662\n",
      "Params: tensor([ 2.4307, -0.6786])\n",
      "Grad:  tensor([-0.4993,  2.8243])\n",
      "Epoch 3115 Loss 27.138838\n",
      "Params: tensor([ 2.4307, -0.6789])\n",
      "Grad:  tensor([-0.4993,  2.8243])\n",
      "Epoch 3116 Loss 27.138014\n",
      "Params: tensor([ 2.4308, -0.6792])\n",
      "Grad:  tensor([-0.4993,  2.8242])\n",
      "Epoch 3117 Loss 27.137190\n",
      "Params: tensor([ 2.4308, -0.6795])\n",
      "Grad:  tensor([-0.4993,  2.8242])\n",
      "Epoch 3118 Loss 27.136370\n",
      "Params: tensor([ 2.4309, -0.6798])\n",
      "Grad:  tensor([-0.4993,  2.8241])\n",
      "Epoch 3119 Loss 27.135548\n",
      "Params: tensor([ 2.4309, -0.6801])\n",
      "Grad:  tensor([-0.4993,  2.8241])\n",
      "Epoch 3120 Loss 27.134722\n",
      "Params: tensor([ 2.4310, -0.6803])\n",
      "Grad:  tensor([-0.4993,  2.8240])\n",
      "Epoch 3121 Loss 27.133900\n",
      "Params: tensor([ 2.4310, -0.6806])\n",
      "Grad:  tensor([-0.4992,  2.8240])\n",
      "Epoch 3122 Loss 27.133080\n",
      "Params: tensor([ 2.4311, -0.6809])\n",
      "Grad:  tensor([-0.4992,  2.8239])\n",
      "Epoch 3123 Loss 27.132256\n",
      "Params: tensor([ 2.4311, -0.6812])\n",
      "Grad:  tensor([-0.4992,  2.8239])\n",
      "Epoch 3124 Loss 27.131432\n",
      "Params: tensor([ 2.4312, -0.6815])\n",
      "Grad:  tensor([-0.4992,  2.8238])\n",
      "Epoch 3125 Loss 27.130610\n",
      "Params: tensor([ 2.4312, -0.6818])\n",
      "Grad:  tensor([-0.4992,  2.8238])\n",
      "Epoch 3126 Loss 27.129789\n",
      "Params: tensor([ 2.4313, -0.6820])\n",
      "Grad:  tensor([-0.4992,  2.8237])\n",
      "Epoch 3127 Loss 27.128967\n",
      "Params: tensor([ 2.4313, -0.6823])\n",
      "Grad:  tensor([-0.4992,  2.8237])\n",
      "Epoch 3128 Loss 27.128145\n",
      "Params: tensor([ 2.4314, -0.6826])\n",
      "Grad:  tensor([-0.4992,  2.8236])\n",
      "Epoch 3129 Loss 27.127327\n",
      "Params: tensor([ 2.4314, -0.6829])\n",
      "Grad:  tensor([-0.4992,  2.8236])\n",
      "Epoch 3130 Loss 27.126501\n",
      "Params: tensor([ 2.4315, -0.6832])\n",
      "Grad:  tensor([-0.4992,  2.8235])\n",
      "Epoch 3131 Loss 27.125683\n",
      "Params: tensor([ 2.4315, -0.6834])\n",
      "Grad:  tensor([-0.4992,  2.8235])\n",
      "Epoch 3132 Loss 27.124859\n",
      "Params: tensor([ 2.4316, -0.6837])\n",
      "Grad:  tensor([-0.4992,  2.8235])\n",
      "Epoch 3133 Loss 27.124037\n",
      "Params: tensor([ 2.4316, -0.6840])\n",
      "Grad:  tensor([-0.4992,  2.8234])\n",
      "Epoch 3134 Loss 27.123217\n",
      "Params: tensor([ 2.4317, -0.6843])\n",
      "Grad:  tensor([-0.4992,  2.8234])\n",
      "Epoch 3135 Loss 27.122393\n",
      "Params: tensor([ 2.4317, -0.6846])\n",
      "Grad:  tensor([-0.4992,  2.8233])\n",
      "Epoch 3136 Loss 27.121571\n",
      "Params: tensor([ 2.4318, -0.6849])\n",
      "Grad:  tensor([-0.4992,  2.8233])\n",
      "Epoch 3137 Loss 27.120750\n",
      "Params: tensor([ 2.4318, -0.6851])\n",
      "Grad:  tensor([-0.4992,  2.8232])\n",
      "Epoch 3138 Loss 27.119923\n",
      "Params: tensor([ 2.4319, -0.6854])\n",
      "Grad:  tensor([-0.4992,  2.8232])\n",
      "Epoch 3139 Loss 27.119102\n",
      "Params: tensor([ 2.4319, -0.6857])\n",
      "Grad:  tensor([-0.4992,  2.8231])\n",
      "Epoch 3140 Loss 27.118286\n",
      "Params: tensor([ 2.4320, -0.6860])\n",
      "Grad:  tensor([-0.4991,  2.8231])\n",
      "Epoch 3141 Loss 27.117460\n",
      "Params: tensor([ 2.4320, -0.6863])\n",
      "Grad:  tensor([-0.4991,  2.8230])\n",
      "Epoch 3142 Loss 27.116638\n",
      "Params: tensor([ 2.4321, -0.6866])\n",
      "Grad:  tensor([-0.4991,  2.8230])\n",
      "Epoch 3143 Loss 27.115816\n",
      "Params: tensor([ 2.4321, -0.6868])\n",
      "Grad:  tensor([-0.4991,  2.8229])\n",
      "Epoch 3144 Loss 27.114996\n",
      "Params: tensor([ 2.4322, -0.6871])\n",
      "Grad:  tensor([-0.4991,  2.8229])\n",
      "Epoch 3145 Loss 27.114172\n",
      "Params: tensor([ 2.4322, -0.6874])\n",
      "Grad:  tensor([-0.4991,  2.8228])\n",
      "Epoch 3146 Loss 27.113350\n",
      "Params: tensor([ 2.4323, -0.6877])\n",
      "Grad:  tensor([-0.4991,  2.8228])\n",
      "Epoch 3147 Loss 27.112526\n",
      "Params: tensor([ 2.4323, -0.6880])\n",
      "Grad:  tensor([-0.4991,  2.8227])\n",
      "Epoch 3148 Loss 27.111712\n",
      "Params: tensor([ 2.4324, -0.6882])\n",
      "Grad:  tensor([-0.4991,  2.8227])\n",
      "Epoch 3149 Loss 27.110889\n",
      "Params: tensor([ 2.4324, -0.6885])\n",
      "Grad:  tensor([-0.4991,  2.8226])\n",
      "Epoch 3150 Loss 27.110064\n",
      "Params: tensor([ 2.4325, -0.6888])\n",
      "Grad:  tensor([-0.4991,  2.8226])\n",
      "Epoch 3151 Loss 27.109241\n",
      "Params: tensor([ 2.4325, -0.6891])\n",
      "Grad:  tensor([-0.4991,  2.8225])\n",
      "Epoch 3152 Loss 27.108421\n",
      "Params: tensor([ 2.4326, -0.6894])\n",
      "Grad:  tensor([-0.4991,  2.8225])\n",
      "Epoch 3153 Loss 27.107599\n",
      "Params: tensor([ 2.4326, -0.6897])\n",
      "Grad:  tensor([-0.4991,  2.8224])\n",
      "Epoch 3154 Loss 27.106779\n",
      "Params: tensor([ 2.4327, -0.6899])\n",
      "Grad:  tensor([-0.4991,  2.8224])\n",
      "Epoch 3155 Loss 27.105955\n",
      "Params: tensor([ 2.4327, -0.6902])\n",
      "Grad:  tensor([-0.4990,  2.8223])\n",
      "Epoch 3156 Loss 27.105139\n",
      "Params: tensor([ 2.4328, -0.6905])\n",
      "Grad:  tensor([-0.4990,  2.8223])\n",
      "Epoch 3157 Loss 27.104317\n",
      "Params: tensor([ 2.4328, -0.6908])\n",
      "Grad:  tensor([-0.4990,  2.8222])\n",
      "Epoch 3158 Loss 27.103493\n",
      "Params: tensor([ 2.4329, -0.6911])\n",
      "Grad:  tensor([-0.4990,  2.8222])\n",
      "Epoch 3159 Loss 27.102678\n",
      "Params: tensor([ 2.4329, -0.6914])\n",
      "Grad:  tensor([-0.4990,  2.8221])\n",
      "Epoch 3160 Loss 27.101851\n",
      "Params: tensor([ 2.4330, -0.6916])\n",
      "Grad:  tensor([-0.4990,  2.8221])\n",
      "Epoch 3161 Loss 27.101030\n",
      "Params: tensor([ 2.4330, -0.6919])\n",
      "Grad:  tensor([-0.4990,  2.8220])\n",
      "Epoch 3162 Loss 27.100206\n",
      "Params: tensor([ 2.4330, -0.6922])\n",
      "Grad:  tensor([-0.4990,  2.8220])\n",
      "Epoch 3163 Loss 27.099388\n",
      "Params: tensor([ 2.4331, -0.6925])\n",
      "Grad:  tensor([-0.4990,  2.8220])\n",
      "Epoch 3164 Loss 27.098564\n",
      "Params: tensor([ 2.4331, -0.6928])\n",
      "Grad:  tensor([-0.4990,  2.8219])\n",
      "Epoch 3165 Loss 27.097742\n",
      "Params: tensor([ 2.4332, -0.6930])\n",
      "Grad:  tensor([-0.4990,  2.8219])\n",
      "Epoch 3166 Loss 27.096924\n",
      "Params: tensor([ 2.4332, -0.6933])\n",
      "Grad:  tensor([-0.4990,  2.8218])\n",
      "Epoch 3167 Loss 27.096102\n",
      "Params: tensor([ 2.4333, -0.6936])\n",
      "Grad:  tensor([-0.4990,  2.8218])\n",
      "Epoch 3168 Loss 27.095278\n",
      "Params: tensor([ 2.4333, -0.6939])\n",
      "Grad:  tensor([-0.4990,  2.8217])\n",
      "Epoch 3169 Loss 27.094458\n",
      "Params: tensor([ 2.4334, -0.6942])\n",
      "Grad:  tensor([-0.4989,  2.8217])\n",
      "Epoch 3170 Loss 27.093641\n",
      "Params: tensor([ 2.4334, -0.6945])\n",
      "Grad:  tensor([-0.4989,  2.8216])\n",
      "Epoch 3171 Loss 27.092815\n",
      "Params: tensor([ 2.4335, -0.6947])\n",
      "Grad:  tensor([-0.4989,  2.8216])\n",
      "Epoch 3172 Loss 27.091993\n",
      "Params: tensor([ 2.4335, -0.6950])\n",
      "Grad:  tensor([-0.4989,  2.8215])\n",
      "Epoch 3173 Loss 27.091175\n",
      "Params: tensor([ 2.4336, -0.6953])\n",
      "Grad:  tensor([-0.4989,  2.8215])\n",
      "Epoch 3174 Loss 27.090355\n",
      "Params: tensor([ 2.4336, -0.6956])\n",
      "Grad:  tensor([-0.4989,  2.8214])\n",
      "Epoch 3175 Loss 27.089537\n",
      "Params: tensor([ 2.4337, -0.6959])\n",
      "Grad:  tensor([-0.4989,  2.8214])\n",
      "Epoch 3176 Loss 27.088713\n",
      "Params: tensor([ 2.4337, -0.6961])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor([-0.4989,  2.8213])\n",
      "Epoch 3177 Loss 27.087891\n",
      "Params: tensor([ 2.4338, -0.6964])\n",
      "Grad:  tensor([-0.4989,  2.8213])\n",
      "Epoch 3178 Loss 27.087072\n",
      "Params: tensor([ 2.4338, -0.6967])\n",
      "Grad:  tensor([-0.4989,  2.8212])\n",
      "Epoch 3179 Loss 27.086245\n",
      "Params: tensor([ 2.4339, -0.6970])\n",
      "Grad:  tensor([-0.4989,  2.8212])\n",
      "Epoch 3180 Loss 27.085424\n",
      "Params: tensor([ 2.4339, -0.6973])\n",
      "Grad:  tensor([-0.4989,  2.8211])\n",
      "Epoch 3181 Loss 27.084606\n",
      "Params: tensor([ 2.4340, -0.6976])\n",
      "Grad:  tensor([-0.4988,  2.8211])\n",
      "Epoch 3182 Loss 27.083788\n",
      "Params: tensor([ 2.4340, -0.6978])\n",
      "Grad:  tensor([-0.4988,  2.8210])\n",
      "Epoch 3183 Loss 27.082966\n",
      "Params: tensor([ 2.4341, -0.6981])\n",
      "Grad:  tensor([-0.4988,  2.8210])\n",
      "Epoch 3184 Loss 27.082148\n",
      "Params: tensor([ 2.4341, -0.6984])\n",
      "Grad:  tensor([-0.4988,  2.8209])\n",
      "Epoch 3185 Loss 27.081327\n",
      "Params: tensor([ 2.4342, -0.6987])\n",
      "Grad:  tensor([-0.4988,  2.8209])\n",
      "Epoch 3186 Loss 27.080505\n",
      "Params: tensor([ 2.4342, -0.6990])\n",
      "Grad:  tensor([-0.4988,  2.8208])\n",
      "Epoch 3187 Loss 27.079687\n",
      "Params: tensor([ 2.4343, -0.6993])\n",
      "Grad:  tensor([-0.4988,  2.8208])\n",
      "Epoch 3188 Loss 27.078869\n",
      "Params: tensor([ 2.4343, -0.6995])\n",
      "Grad:  tensor([-0.4988,  2.8208])\n",
      "Epoch 3189 Loss 27.078041\n",
      "Params: tensor([ 2.4344, -0.6998])\n",
      "Grad:  tensor([-0.4988,  2.8207])\n",
      "Epoch 3190 Loss 27.077227\n",
      "Params: tensor([ 2.4344, -0.7001])\n",
      "Grad:  tensor([-0.4988,  2.8207])\n",
      "Epoch 3191 Loss 27.076405\n",
      "Params: tensor([ 2.4345, -0.7004])\n",
      "Grad:  tensor([-0.4988,  2.8206])\n",
      "Epoch 3192 Loss 27.075584\n",
      "Params: tensor([ 2.4345, -0.7007])\n",
      "Grad:  tensor([-0.4988,  2.8206])\n",
      "Epoch 3193 Loss 27.074762\n",
      "Params: tensor([ 2.4346, -0.7009])\n",
      "Grad:  tensor([-0.4988,  2.8205])\n",
      "Epoch 3194 Loss 27.073938\n",
      "Params: tensor([ 2.4346, -0.7012])\n",
      "Grad:  tensor([-0.4987,  2.8205])\n",
      "Epoch 3195 Loss 27.073122\n",
      "Params: tensor([ 2.4347, -0.7015])\n",
      "Grad:  tensor([ 2.4438, -0.7528])\n",
      "Grad:  tensor([-0.4970,  2.8118])\n",
      "Epoch 3378 Loss 26.923468\n",
      "Params: tensor([ 2.4438, -0.7530])\n",
      "Grad:  tensor([-0.4970,  2.8117])\n",
      "Epoch 3379 Loss 26.922651\n",
      "Params: tensor([ 2.4439, -0.7533])\n",
      "Grad:  tensor([-0.4970,  2.8117])\n",
      "Epoch 3380 Loss 26.921831\n",
      "Params: tensor([ 2.4439, -0.7536])\n",
      "Grad:  tensor([-0.4970,  2.8116])\n",
      "Epoch 3381 Loss 26.921015\n",
      "Params: tensor([ 2.4440, -0.7539])\n",
      "Grad:  tensor([-0.4970,  2.8116])\n",
      "Epoch 3382 Loss 26.920204\n",
      "Params: tensor([ 2.4440, -0.7542])\n",
      "Grad:  tensor([-0.4969,  2.8115])\n",
      "Epoch 3383 Loss 26.919390\n",
      "Params: tensor([ 2.4440, -0.7544])\n",
      "Grad:  tensor([-0.4969,  2.8115])\n",
      "Epoch 3384 Loss 26.918571\n",
      "Params: tensor([ 2.4441, -0.7547])\n",
      "Grad:  tensor([-0.4969,  2.8114])\n",
      "Epoch 3385 Loss 26.917755\n",
      "Params: tensor([ 2.4441, -0.7550])\n",
      "Grad:  tensor([-0.4969,  2.8114])\n",
      "Epoch 3386 Loss 26.916939\n",
      "Params: tensor([ 2.4442, -0.7553])\n",
      "Grad:  tensor([-0.4969,  2.8113])\n",
      "Epoch 3387 Loss 26.916126\n",
      "Params: tensor([ 2.4442, -0.7556])\n",
      "Grad:  tensor([-0.4969,  2.8113])\n",
      "Epoch 3388 Loss 26.915314\n",
      "Params: tensor([ 2.4443, -0.7559])\n",
      "Grad:  tensor([-0.4969,  2.8112])\n",
      "Epoch 3389 Loss 26.914497\n",
      "Params: tensor([ 2.4443, -0.7561])\n",
      "Grad:  tensor([-0.4969,  2.8112])\n",
      "Epoch 3390 Loss 26.913679\n",
      "Params: tensor([ 2.4444, -0.7564])\n",
      "Grad:  tensor([-0.4969,  2.8111])\n",
      "Epoch 3391 Loss 26.912870\n",
      "Params: tensor([ 2.4444, -0.7567])\n",
      "Grad:  tensor([-0.4969,  2.8111])\n",
      "Epoch 3392 Loss 26.912054\n",
      "Params: tensor([ 2.4445, -0.7570])\n",
      "Grad:  tensor([-0.4969,  2.8110])\n",
      "Epoch 3393 Loss 26.911242\n",
      "Params: tensor([ 2.4445, -0.7573])\n",
      "Grad:  tensor([-0.4969,  2.8110])\n",
      "Epoch 3394 Loss 26.910423\n",
      "Params: tensor([ 2.4446, -0.7575])\n",
      "Grad:  tensor([-0.4969,  2.8109])\n",
      "Epoch 3395 Loss 26.909607\n",
      "Params: tensor([ 2.4446, -0.7578])\n",
      "Grad:  tensor([-0.4969,  2.8109])\n",
      "Epoch 3396 Loss 26.908796\n",
      "Params: tensor([ 2.4447, -0.7581])\n",
      "Grad:  tensor([-0.4969,  2.8108])\n",
      "Epoch 3397 Loss 26.907982\n",
      "Params: tensor([ 2.4447, -0.7584])\n",
      "Grad:  tensor([-0.4969,  2.8108])\n",
      "Epoch 3398 Loss 26.907160\n",
      "Params: tensor([ 2.4448, -0.7587])\n",
      "Grad:  tensor([-0.4969,  2.8107])\n",
      "Epoch 3399 Loss 26.906349\n",
      "Params: tensor([ 2.4448, -0.7589])\n",
      "Grad:  tensor([-0.4969,  2.8107])\n",
      "Epoch 3400 Loss 26.905531\n",
      "Params: tensor([ 2.4449, -0.7592])\n",
      "Grad:  tensor([-0.4969,  2.8106])\n",
      "Epoch 3401 Loss 26.904718\n",
      "Params: tensor([ 2.4449, -0.7595])\n",
      "Grad:  tensor([-0.4969,  2.8106])\n",
      "Epoch 3402 Loss 26.903906\n",
      "Params: tensor([ 2.4450, -0.7598])\n",
      "Grad:  tensor([-0.4969,  2.8105])\n",
      "Epoch 3403 Loss 26.903090\n",
      "Params: tensor([ 2.4450, -0.7601])\n",
      "Grad:  tensor([-0.4968,  2.8105])\n",
      "Epoch 3404 Loss 26.902275\n",
      "Params: tensor([ 2.4451, -0.7604])\n",
      "Grad:  tensor([-0.4968,  2.8104])\n",
      "Epoch 3405 Loss 26.901461\n",
      "Params: tensor([ 2.4451, -0.7606])\n",
      "Grad:  tensor([-0.4968,  2.8104])\n",
      "Epoch 3406 Loss 26.900646\n",
      "Params: tensor([ 2.4452, -0.7609])\n",
      "Grad:  tensor([-0.4968,  2.8103])\n",
      "Epoch 3407 Loss 26.899834\n",
      "Params: tensor([ 2.4452, -0.7612])\n",
      "Grad:  tensor([-0.4968,  2.8103])\n",
      "Epoch 3408 Loss 26.899017\n",
      "Params: tensor([ 2.4453, -0.7615])\n",
      "Grad:  tensor([-0.4968,  2.8102])\n",
      "Epoch 3409 Loss 26.898201\n",
      "Params: tensor([ 2.4453, -0.7618])\n",
      "Grad:  tensor([-0.4968,  2.8102])\n",
      "Epoch 3410 Loss 26.897394\n",
      "Params: tensor([ 2.4454, -0.7620])\n",
      "Grad:  tensor([-0.4968,  2.8102])\n",
      "Epoch 3411 Loss 26.896576\n",
      "Params: tensor([ 2.4454, -0.7623])\n",
      "Grad:  tensor([-0.4968,  2.8101])\n",
      "Epoch 3412 Loss 26.895758\n",
      "Params: tensor([ 2.4455, -0.7626])\n",
      "Grad:  tensor([-0.4968,  2.8101])\n",
      "Epoch 3413 Loss 26.894947\n",
      "Params: tensor([ 2.4455, -0.7629])\n",
      "Grad:  tensor([-0.4968,  2.8100])\n",
      "Epoch 3414 Loss 26.894135\n",
      "Params: tensor([ 2.4456, -0.7632])\n",
      "Grad:  tensor([-0.4968,  2.8100])\n",
      "Epoch 3415 Loss 26.893318\n",
      "Params: tensor([ 2.4456, -0.7634])\n",
      "Grad:  tensor([-0.4968,  2.8099])\n",
      "Epoch 3416 Loss 26.892509\n",
      "Params: tensor([ 2.4457, -0.7637])\n",
      "Grad:  tensor([-0.4968,  2.8099])\n",
      "Epoch 3417 Loss 26.891691\n",
      "Params: tensor([ 2.4457, -0.7640])\n",
      "Grad:  tensor([-0.4968,  2.8098])\n",
      "Epoch 3418 Loss 26.890875\n",
      "Params: tensor([ 2.4458, -0.7643])\n",
      "Grad:  tensor([-0.4968,  2.8098])\n",
      "Epoch 3419 Loss 26.890064\n",
      "Params: tensor([ 2.4458, -0.7646])\n",
      "Grad:  tensor([-0.4968,  2.8097])\n",
      "Epoch 3420 Loss 26.889250\n",
      "Params: tensor([ 2.4459, -0.7648])\n",
      "Grad:  tensor([-0.4968,  2.8097])\n",
      "Epoch 3421 Loss 26.888435\n",
      "Params: tensor([ 2.4459, -0.7651])\n",
      "Grad:  tensor([-0.4967,  2.8096])\n",
      "Epoch 3422 Loss 26.887617\n",
      "Params: tensor([ 2.4460, -0.7654])\n",
      "Grad:  tensor([-0.4967,  2.8096])\n",
      "Epoch 3423 Loss 26.886805\n",
      "Params: tensor([ 2.4460, -0.7657])\n",
      "Grad:  tensor([-0.4967,  2.8095])\n",
      "Epoch 3424 Loss 26.885992\n",
      "Params: tensor([ 2.4461, -0.7660])\n",
      "Grad:  tensor([-0.4967,  2.8095])\n",
      "Epoch 3425 Loss 26.885176\n",
      "Params: tensor([ 2.4461, -0.7663])\n",
      "Grad:  tensor([-0.4967,  2.8094])\n",
      "Epoch 3426 Loss 26.884361\n",
      "Params: tensor([ 2.4462, -0.7665])\n",
      "Grad:  tensor([-0.4967,  2.8094])\n",
      "Epoch 3427 Loss 26.883556\n",
      "Params: tensor([ 2.4462, -0.7668])\n",
      "Grad:  tensor([-0.4967,  2.8093])\n",
      "Epoch 3428 Loss 26.882738\n",
      "Params: tensor([ 2.4463, -0.7671])\n",
      "Grad:  tensor([-0.4967,  2.8093])\n",
      "Epoch 3429 Loss 26.881926\n",
      "Params: tensor([ 2.4463, -0.7674])\n",
      "Grad:  tensor([-0.4967,  2.8092])\n",
      "Epoch 3430 Loss 26.881111\n",
      "Params: tensor([ 2.4464, -0.7677])\n",
      "Grad:  tensor([-0.4967,  2.8092])\n",
      "Epoch 3431 Loss 26.880293\n",
      "Params: tensor([ 2.4464, -0.7679])\n",
      "Grad:  tensor([-0.4967,  2.8091])\n",
      "Epoch 3432 Loss 26.879484\n",
      "Params: tensor([ 2.4465, -0.7682])\n",
      "Grad:  tensor([-0.4967,  2.8091])\n",
      "Epoch 3433 Loss 26.878668\n",
      "Params: tensor([ 2.4465, -0.7685])\n",
      "Grad:  tensor([-0.4967,  2.8090])\n",
      "Epoch 3434 Loss 26.877855\n",
      "Params: tensor([ 2.4466, -0.7688])\n",
      "Grad:  tensor([-0.4967,  2.8090])\n",
      "Epoch 3435 Loss 26.877037\n",
      "Params: tensor([ 2.4466, -0.7691])\n",
      "Grad:  tensor([-0.4967,  2.8089])\n",
      "Epoch 3436 Loss 26.876223\n",
      "Params: tensor([ 2.4467, -0.7693])\n",
      "Grad:  tensor([-0.4967,  2.8089])\n",
      "Epoch 3437 Loss 26.875414\n",
      "Params: tensor([ 2.4467, -0.7696])\n",
      "Grad:  tensor([-0.4967,  2.8089])\n",
      "Epoch 3438 Loss 26.874601\n",
      "Params: tensor([ 2.4468, -0.7699])\n",
      "Grad:  tensor([-0.4966,  2.8088])\n",
      "Epoch 3439 Loss 26.873781\n",
      "Params: tensor([ 2.4468, -0.7702])\n",
      "Grad:  tensor([-0.4966,  2.8088])\n",
      "Epoch 3440 Loss 26.872969\n",
      "Params: tensor([ 2.4469, -0.7705])\n",
      "Grad:  tensor([-0.4966,  2.8087])\n",
      "Epoch 3441 Loss 26.872156\n",
      "Params: tensor([ 2.4469, -0.7707])\n",
      "Grad:  tensor([-0.4966,  2.8087])\n",
      "Epoch 3442 Loss 26.871346\n",
      "Params: tensor([ 2.4470, -0.7710])\n",
      "Grad:  tensor([-0.4966,  2.8086])\n",
      "Epoch 3443 Loss 26.870531\n",
      "Params: tensor([ 2.4470, -0.7713])\n",
      "Grad:  tensor([-0.4966,  2.8086])\n",
      "Epoch 3444 Loss 26.869719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([ 2.4471, -0.7716])\n",
      "Grad:  tensor([-0.4966,  2.8085])\n",
      "Epoch 3445 Loss 26.868908\n",
      "Params: tensor([ 2.4471, -0.7719])\n",
      "Grad:  tensor([-0.4966,  2.8085])\n",
      "Epoch 3446 Loss 26.868095\n",
      "Params: tensor([ 2.4472, -0.7722])\n",
      "Grad:  tensor([-0.4966,  2.8084])\n",
      "Epoch 3447 Loss 26.867277\n",
      "Params: tensor([ 2.4472, -0.7724])\n",
      "Grad:  tensor([-0.4966,  2.8084])\n",
      "Epoch 3448 Loss 26.866463\n",
      "Params: tensor([ 2.4473, -0.7727])\n",
      "Grad:  tensor([-0.4966,  2.8083])\n",
      "Epoch 3449 Loss 26.865654\n",
      "Params: tensor([ 2.4473, -0.7730])\n",
      "Grad:  tensor([-0.4966,  2.8083])\n",
      "Epoch 3450 Loss 26.864838\n",
      "Params: tensor([ 2.4474, -0.7733])\n",
      "Grad:  tensor([-0.4966,  2.8082])\n",
      "Epoch 3451 Loss 26.864019\n",
      "Params: tensor([ 2.4474, -0.7736])\n",
      "Grad:  tensor([-0.4965,  2.8082])\n",
      "Epoch 3452 Loss 26.863209\n",
      "Params: tensor([ 2.4475, -0.7738])\n",
      "Grad:  tensor([-0.4965,  2.8081])\n",
      "Epoch 3453 Loss 26.862398\n",
      "Params: tensor([ 2.4475, -0.7741])\n",
      "Grad:  tensor([-0.4965,  2.8081])\n",
      "Epoch 3454 Loss 26.861584\n",
      "Params: tensor([ 2.4476, -0.7744])\n",
      "Grad:  tensor([-0.4965,  2.8080])\n",
      "Epoch 3455 Loss 26.860773\n",
      "Params: tensor([ 2.4476, -0.7747])\n",
      "Grad:  tensor([-0.4965,  2.8080])\n",
      "Epoch 3456 Loss 26.859955\n",
      "Params: tensor([ 2.4477, -0.7750])\n",
      "Grad:  tensor([-0.4965,  2.8079])\n",
      "Epoch 3457 Loss 26.859148\n",
      "Params: tensor([ 2.4477, -0.7752])\n",
      "Grad:  tensor([-0.4965,  2.8079])\n",
      "Epoch 3458 Loss 26.858335\n",
      "Params: tensor([ 2.4478, -0.7755])\n",
      "Grad:  tensor([-0.4965,  2.8078])\n",
      "Epoch 3459 Loss 26.857521\n",
      "Params: tensor([ 2.4478, -0.7758])\n",
      "Grad:  tensor([-0.4965,  2.8078])\n",
      "Epoch 3460 Loss 26.856709\n",
      "Params: tensor([ 2.4479, -0.7761])\n",
      "Grad:  tensor([-0.4965,  2.8078])\n",
      "Epoch 3461 Loss 26.855894\n",
      "Params: tensor([ 2.4479, -0.7764])\n",
      "Grad:  tensor([-0.4965,  2.8077])\n",
      "Epoch 3462 Loss 26.855083\n",
      "Params: tensor([ 2.4480, -0.7766])\n",
      "Grad:  tensor([-0.4965,  2.8077])\n",
      "Epoch 3463 Loss 26.854267\n",
      "Params: tensor([ 2.4480, -0.7769])\n",
      "Grad:  tensor([-0.4965,  2.8076])\n",
      "Epoch 3464 Loss 26.853456\n",
      "Params: tensor([ 2.4481, -0.7772])\n",
      "Grad:  tensor([-0.4965,  2.8076])\n",
      "Epoch 3465 Loss 26.852644\n",
      "Params: tensor([ 2.4481, -0.7775])\n",
      "Grad:  tensor([-0.4964,  2.8075])\n",
      "Epoch 3466 Loss 26.851830\n",
      "Params: tensor([ 2.4482, -0.7778])\n",
      "Grad:  tensor([-0.4964,  2.8075])\n",
      "Epoch 3467 Loss 26.851019\n",
      "Params: tensor([ 2.4482, -0.7780])\n",
      "Grad:  tensor([-0.4964,  2.8074])\n",
      "Epoch 3468 Loss 26.850203\n",
      "Params: tensor([ 2.4483, -0.7783])\n",
      "Grad:  tensor([-0.4964,  2.8074])\n",
      "Epoch 3469 Loss 26.849388\n",
      "Params: tensor([ 2.4483, -0.7786])\n",
      "Grad:  tensor([-0.4964,  2.8073])\n",
      "Epoch 3470 Loss 26.848579\n",
      "Params: tensor([ 2.4484, -0.7789])\n",
      "Grad:  tensor([-0.4964,  2.8073])\n",
      "Epoch 3471 Loss 26.847765\n",
      "Params: tensor([ 2.4484, -0.7792])\n",
      "Grad:  tensor([-0.4964,  2.8072])\n",
      "Epoch 3472 Loss 26.846954\n",
      "Params: tensor([ 2.4485, -0.7795])\n",
      "Grad:  tensor([-0.4964,  2.8072])\n",
      "Epoch 3473 Loss 26.846138\n",
      "Params: tensor([ 2.4485, -0.7797])\n",
      "Grad:  tensor([-0.4964,  2.8071])\n",
      "Epoch 3474 Loss 26.845325\n",
      "Params: tensor([ 2.4486, -0.7800])\n",
      "Grad:  tensor([-0.4964,  2.8071])\n",
      "Epoch 3475 Loss 26.844515\n",
      "Params: tensor([ 2.4486, -0.7803])\n",
      "Grad:  tensor([-0.4964,  2.8070])\n",
      "Epoch 3476 Loss 26.843700\n",
      "Params: tensor([ 2.4487, -0.7806])\n",
      "Grad:  tensor([-0.4963,  2.8070])\n",
      "Epoch 3477 Loss 26.842890\n",
      "Params: tensor([ 2.4487, -0.7809])\n",
      "Grad:  tensor([-0.4963,  2.8069])\n",
      "Epoch 3478 Loss 26.842083\n",
      "Params: tensor([ 2.4488, -0.7811])\n",
      "Grad:  tensor([-0.4963,  2.8069])\n",
      "Epoch 3479 Loss 26.841267\n",
      "Params: tensor([ 2.4488, -0.7814])\n",
      "Grad:  tensor([-0.4963,  2.8068])\n",
      "Epoch 3480 Loss 26.840454\n",
      "Params: tensor([ 2.4489, -0.7817])\n",
      "Grad:  tensor([-0.4963,  2.8068])\n",
      "Epoch 3481 Loss 26.839642\n",
      "Params: tensor([ 2.4489, -0.7820])\n",
      "Grad:  tensor([-0.4963,  2.8067])\n",
      "Epoch 3482 Loss 26.838829\n",
      "Params: tensor([ 2.4490, -0.7823])\n",
      "Grad:  tensor([-0.4963,  2.8067])\n",
      "Epoch 3483 Loss 26.838015\n",
      "Params: tensor([ 2.4490, -0.7825])\n",
      "Grad:  tensor([-0.4963,  2.8067])\n",
      "Epoch 3484 Loss 26.837202\n",
      "Params: tensor([ 2.4491, -0.7828])\n",
      "Grad:  tensor([-0.4963,  2.8066])\n",
      "Epoch 3485 Loss 26.836395\n",
      "Params: tensor([ 2.4491, -0.7831])\n",
      "Grad:  tensor([-0.4963,  2.8066])\n",
      "Epoch 3486 Loss 26.835579\n",
      "Params: tensor([ 2.4492, -0.7834])\n",
      "Grad:  tensor([-0.4963,  2.8065])\n",
      "Epoch 3487 Loss 26.834764\n",
      "Params: tensor([ 2.4492, -0.7837])\n",
      "Grad:  tensor([-0.4963,  2.8065])\n",
      "Epoch 3488 Loss 26.833954\n",
      "Params: tensor([ 2.4493, -0.7839])\n",
      "Grad:  tensor([-0.4962,  2.8064])\n",
      "Epoch 3489 Loss 26.833138\n",
      "Params: tensor([ 2.4493, -0.7842])\n",
      "Grad:  tensor([-0.4962,  2.8064])\n",
      "Epoch 3490 Loss 26.832331\n",
      "Params: tensor([ 2.4494, -0.7845])\n",
      "Grad:  tensor([-0.4962,  2.8063])\n",
      "Epoch 3491 Loss 26.831514\n",
      "Params: tensor([ 2.4494, -0.7848])\n",
      "Grad:  tensor([-0.4962,  2.8063])\n",
      "Epoch 3492 Loss 26.830708\n",
      "Params: tensor([ 2.4495, -0.7851])\n",
      "Grad:  tensor([-0.4962,  2.8062])\n",
      "Epoch 3493 Loss 26.829893\n",
      "Params: tensor([ 2.4495, -0.7853])\n",
      "Grad:  tensor([-0.4962,  2.8062])\n",
      "Epoch 3494 Loss 26.829082\n",
      "Params: tensor([ 2.4496, -0.7856])\n",
      "Grad:  tensor([-0.4962,  2.8061])\n",
      "Epoch 3495 Loss 26.828272\n",
      "Params: tensor([ 2.4496, -0.7859])\n",
      "Grad:  tensor([-0.4962,  2.8061])\n",
      "Epoch 3496 Loss 26.827461\n",
      "Params: tensor([ 2.4497, -0.7862])\n",
      "Grad:  tensor([-0.4962,  2.8060])\n",
      "Epoch 3497 Loss 26.826643\n",
      "Params: tensor([ 2.4497, -0.7865])\n",
      "Grad:  tensor([-0.4962,  2.8060])\n",
      "Epoch 3498 Loss 26.825836\n",
      "Params: tensor([ 2.4498, -0.7867])\n",
      "Grad:  tensor([-0.4961,  2.8059])\n",
      "Epoch 3499 Loss 26.825020\n",
      "Params: tensor([ 2.4498, -0.7870])\n",
      "Grad:  tensor([-0.4961,  2.8059])\n",
      "Epoch 3500 Loss 26.824211\n",
      "Params: tensor([ 2.4499, -0.7873])\n",
      "Grad:  tensor([-0.4961,  2.8058])\n",
      "Epoch 3501 Loss 26.823397\n",
      "Params: tensor([ 2.4499, -0.7876])\n",
      "Grad:  tensor([-0.4961,  2.8058])\n",
      "Epoch 3502 Loss 26.822590\n",
      "Params: tensor([ 2.4500, -0.7879])\n",
      "Grad:  tensor([-0.4961,  2.8058])\n",
      "Epoch 3503 Loss 26.821779\n",
      "Params: tensor([ 2.4500, -0.7882])\n",
      "Grad:  tensor([-0.4961,  2.8057])\n",
      "Epoch 3504 Loss 26.820961\n",
      "Params: tensor([ 2.4501, -0.7884])\n",
      "Grad:  tensor([-0.4961,  2.8057])\n",
      "Epoch 3505 Loss 26.820152\n",
      "Params: tensor([ 2.4501, -0.7887])\n",
      "Grad:  tensor([-0.4961,  2.8056])\n",
      "Epoch 3506 Loss 26.819338\n",
      "Params: tensor([ 2.4501, -0.7890])\n",
      "Grad:  tensor([-0.4961,  2.8056])\n",
      "Epoch 3507 Loss 26.818525\n",
      "Params: tensor([ 2.4502, -0.7893])\n",
      "Grad:  tensor([-0.4960,  2.8055])\n",
      "Epoch 3508 Loss 26.817711\n",
      "Params: tensor([ 2.4502, -0.7896])\n",
      "Grad:  tensor([-0.4960,  2.8055])\n",
      "Epoch 3509 Loss 26.816906\n",
      "Params: tensor([ 2.4503, -0.7898])\n",
      "Grad:  tensor([-0.4960,  2.8054])\n",
      "Epoch 3510 Loss 26.816093\n",
      "Params: tensor([ 2.4503, -0.7901])\n",
      "Grad:  tensor([-0.4960,  2.8054])\n",
      "Epoch 3511 Loss 26.815283\n",
      "Params: tensor([ 2.4504, -0.7904])\n",
      "Grad:  tensor([-0.4960,  2.8053])\n",
      "Epoch 3512 Loss 26.814470\n",
      "Params: tensor([ 2.4504, -0.7907])\n",
      "Grad:  tensor([-0.4960,  2.8053])\n",
      "Epoch 3513 Loss 26.813658\n",
      "Params: tensor([ 2.4505, -0.7910])\n",
      "Grad:  tensor([-0.4960,  2.8052])\n",
      "Epoch 3514 Loss 26.812849\n",
      "Params: tensor([ 2.4505, -0.7912])\n",
      "Grad:  tensor([-0.4960,  2.8052])\n",
      "Epoch 3515 Loss 26.812037\n",
      "Params: tensor([ 2.4506, -0.7915])\n",
      "Grad:  tensor([-0.4960,  2.8051])\n",
      "Epoch 3516 Loss 26.811224\n",
      "Params: tensor([ 2.4506, -0.7918])\n",
      "Grad:  tensor([-0.4960,  2.8051])\n",
      "Epoch 3517 Loss 26.810413\n",
      "Params: tensor([ 2.4507, -0.7921])\n",
      "Grad:  tensor([-0.4960,  2.8050])\n",
      "Epoch 3518 Loss 26.809599\n",
      "Params: tensor([ 2.4507, -0.7924])\n",
      "Grad:  tensor([-0.4959,  2.8050])\n",
      "Epoch 3519 Loss 26.808788\n",
      "Params: tensor([ 2.4508, -0.7926])\n",
      "Grad:  tensor([-0.4959,  2.8049])\n",
      "Epoch 3520 Loss 26.807972\n",
      "Params: tensor([ 2.4508, -0.7929])\n",
      "Grad:  tensor([-0.4959,  2.8049])\n",
      "Epoch 3521 Loss 26.807167\n",
      "Params: tensor([ 2.4509, -0.7932])\n",
      "Grad:  tensor([-0.4959,  2.8049])\n",
      "Epoch 3522 Loss 26.806353\n",
      "Params: tensor([ 2.4509, -0.7935])\n",
      "Grad:  tensor([-0.4959,  2.8048])\n",
      "Epoch 3523 Loss 26.805542\n",
      "Params: tensor([ 2.4510, -0.7938])\n",
      "Grad:  tensor([-0.4959,  2.8048])\n",
      "Epoch 3524 Loss 26.804729\n",
      "Params: tensor([ 2.4510, -0.7940])\n",
      "Grad:  tensor([-0.4959,  2.8047])\n",
      "Epoch 3525 Loss 26.803919\n",
      "Params: tensor([ 2.4511, -0.7943])\n",
      "Grad:  tensor([-0.4959,  2.8047])\n",
      "Epoch 3526 Loss 26.803112\n",
      "Params: tensor([ 2.4511, -0.7946])\n",
      "Grad:  tensor([-0.4959,  2.8046])\n",
      "Epoch 3527 Loss 26.802305\n",
      "Params: tensor([ 2.4512, -0.7949])\n",
      "Grad:  tensor([-0.4958,  2.8046])\n",
      "Epoch 3528 Loss 26.801489\n",
      "Params: tensor([ 2.4512, -0.7952])\n",
      "Grad:  tensor([-0.4958,  2.8045])\n",
      "Epoch 3529 Loss 26.800678\n",
      "Params: tensor([ 2.4513, -0.7954])\n",
      "Grad:  tensor([-0.4958,  2.8045])\n",
      "Epoch 3530 Loss 26.799868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([ 2.4513, -0.7957])\n",
      "Grad:  tensor([-0.4958,  2.8044])\n",
      "Epoch 3531 Loss 26.799055\n",
      "Params: tensor([ 2.4514, -0.7960])\n",
      "Grad:  tensor([-0.4958,  2.8044])\n",
      "Epoch 3532 Loss 26.798241\n",
      "Params: tensor([ 2.4514, -0.7963])\n",
      "Grad:  tensor([-0.4958,  2.8043])\n",
      "Epoch 3533 Loss 26.797436\n",
      "Params: tensor([ 2.4515, -0.7966])\n",
      "Grad:  tensor([-0.4958,  2.8043])\n",
      "Epoch 3534 Loss 26.796623\n",
      "Params: tensor([ 2.4515, -0.7968])\n",
      "Grad:  tensor([-0.4958,  2.8042])\n",
      "Epoch 3535 Loss 26.795809\n",
      "Params: tensor([ 2.4516, -0.7971])\n",
      "Grad:  tensor([-0.4958,  2.8042])\n",
      "Epoch 3536 Loss 26.794996\n",
      "Params: tensor([ 2.4516, -0.7974])\n",
      "Grad:  tensor([-0.4957,  2.8041])\n",
      "Epoch 3537 Loss 26.794188\n",
      "Params: tensor([ 2.4517, -0.7977])\n",
      "Grad:  tensor([-0.4957,  2.8041])\n",
      "Epoch 3538 Loss 26.793377\n",
      "Params: tensor([ 2.4517, -0.7980])\n",
      "Grad:  tensor([-0.4957,  2.8041])\n",
      "Epoch 3539 Loss 26.792566\n",
      "Params: tensor([ 2.4518, -0.7982])\n",
      "Grad:  tensor([-0.4957,  2.8040])\n",
      "Epoch 3540 Loss 26.791759\n",
      "Params: tensor([ 2.4518, -0.7985])\n",
      "Grad:  tensor([-0.4957,  2.8040])\n",
      "Epoch 3541 Loss 26.790943\n",
      "Params: tensor([ 2.4519, -0.7988])\n",
      "Grad:  tensor([-0.4957,  2.8039])\n",
      "Epoch 3542 Loss 26.790138\n",
      "Params: tensor([ 2.4519, -0.7991])\n",
      "Grad:  tensor([-0.4957,  2.8039])\n",
      "Epoch 3543 Loss 26.789320\n",
      "Params: tensor([ 2.4520, -0.7994])\n",
      "Grad:  tensor([-0.4957,  2.8038])\n",
      "Epoch 3544 Loss 26.788513\n",
      "Params: tensor([ 2.4520, -0.7997])\n",
      "Grad:  tensor([-0.4956,  2.8038])\n",
      "Epoch 3545 Loss 26.787701\n",
      "Params: tensor([ 2.4521, -0.7999])\n",
      "Grad:  tensor([-0.4956,  2.8037])\n",
      "Epoch 3546 Loss 26.786894\n",
      "Params: tensor([ 2.4521, -0.8002])\n",
      "Grad:  tensor([-0.4956,  2.8037])\n",
      "Epoch 3547 Loss 26.786079\n",
      "Params: tensor([ 2.4522, -0.8005])\n",
      "Grad:  tensor([-0.4956,  2.8036])\n",
      "Epoch 3548 Loss 26.785271\n",
      "Params: tensor([ 2.4522, -0.8008])\n",
      "Grad:  tensor([-0.4956,  2.8036])\n",
      "Epoch 3549 Loss 26.784460\n",
      "Params: tensor([ 2.4523, -0.8011])\n",
      "Grad:  tensor([-0.4956,  2.8035])\n",
      "Epoch 3550 Loss 26.783648\n",
      "Params: tensor([ 2.4523, -0.8013])\n",
      "Grad:  tensor([-0.4956,  2.8035])\n",
      "Epoch 3551 Loss 26.782837\n",
      "Params: tensor([ 2.4524, -0.8016])\n",
      "Grad:  tensor([-0.4956,  2.8034])\n",
      "Epoch 3552 Loss 26.782032\n",
      "Params: tensor([ 2.4524, -0.8019])\n",
      "Grad:  tensor([-0.4955,  2.8034])\n",
      "Epoch 3553 Loss 26.781218\n",
      "Params: tensor([ 2.4525, -0.8022])\n",
      "Grad:  tensor([-0.4955,  2.8033])\n",
      "Epoch 3554 Loss 26.780407\n",
      "Params: tensor([ 2.4525, -0.8025])\n",
      "Grad:  tensor([-0.4955,  2.8033])\n",
      "Epoch 3555 Loss 26.779600\n",
      "Params: tensor([ 2.4526, -0.8027])\n",
      "Grad:  tensor([-0.4955,  2.8033])\n",
      "Epoch 3556 Loss 26.778786\n",
      "Params: tensor([ 2.4526, -0.8030])\n",
      "Grad:  tensor([-0.4955,  2.8032])\n",
      "Epoch 3557 Loss 26.777973\n",
      "Params: tensor([ 2.4527, -0.8033])\n",
      "Grad:  tensor([-0.4955,  2.8032])\n",
      "Epoch 3558 Loss 26.777163\n",
      "Params: tensor([ 2.4527, -0.8036])\n",
      "Grad:  tensor([-0.4955,  2.8031])\n",
      "Epoch 3559 Loss 26.776361\n",
      "Params: tensor([ 2.4528, -0.8039])\n",
      "Grad:  tensor([-0.4955,  2.8031])\n",
      "Epoch 3560 Loss 26.775545\n",
      "Params: tensor([ 2.4528, -0.8041])\n",
      "Grad:  tensor([-0.4954,  2.8030])\n",
      "Epoch 3561 Loss 26.774736\n",
      "Params: tensor([ 2.4529, -0.8044])\n",
      "Grad:  tensor([-0.4954,  2.8030])\n",
      "Epoch 3562 Loss 26.773932\n",
      "Params: tensor([ 2.4529, -0.8047])\n",
      "Grad:  tensor([-0.4954,  2.8029])\n",
      "Epoch 3563 Loss 26.773113\n",
      "Params: tensor([ 2.4530, -0.8050])\n",
      "Grad:  tensor([-0.4954,  2.8029])\n",
      "Epoch 3564 Loss 26.772303\n",
      "Params: tensor([ 2.4530, -0.8053])\n",
      "Grad:  tensor([-0.4954,  2.8028])\n",
      "Epoch 3565 Loss 26.771496\n",
      "Params: tensor([ 2.4531, -0.8055])\n",
      "Grad:  tensor([-0.4954,  2.8028])\n",
      "Epoch 3566 Loss 26.770685\n",
      "Params: tensor([ 2.4531, -0.8058])\n",
      "Grad:  tensor([-0.4954,  2.8027])\n",
      "Epoch 3567 Loss 26.769873\n",
      "Params: tensor([ 2.4532, -0.8061])\n",
      "Grad:  tensor([-0.4954,  2.8027])\n",
      "Epoch 3568 Loss 26.769060\n",
      "Params: tensor([ 2.4532, -0.8064])\n",
      "Grad:  tensor([-0.4953,  2.8026])\n",
      "Epoch 3569 Loss 26.768250\n",
      "Params: tensor([ 2.4533, -0.8067])\n",
      "Grad:  tensor([-0.4953,  2.8026])\n",
      "Epoch 3570 Loss 26.767443\n",
      "Params: tensor([ 2.4533, -0.8069])\n",
      "Grad:  tensor([-0.4953,  2.8026])\n",
      "Epoch 3571 Loss 26.766634\n",
      "Params: tensor([ 2.4534, -0.8072])\n",
      "Grad:  tensor([-0.4953,  2.8025])\n",
      "Epoch 3572 Loss 26.765821\n",
      "Params: tensor([ 2.4534, -0.8075])\n",
      "Grad:  tensor([-0.4953,  2.8025])\n",
      "Epoch 3573 Loss 26.765015\n",
      "Params: tensor([ 2.4535, -0.8078])\n",
      "Grad:  tensor([-0.4953,  2.8024])\n",
      "Epoch 3574 Loss 26.764204\n",
      "Params: tensor([ 2.4535, -0.8081])\n",
      "Grad:  tensor([-0.4953,  2.8024])\n",
      "Epoch 3575 Loss 26.763395\n",
      "Params: tensor([ 2.4536, -0.8083])\n",
      "Grad:  tensor([-0.4952,  2.8023])\n",
      "Epoch 3576 Loss 26.762585\n",
      "Params: tensor([ 2.4536, -0.8086])\n",
      "Grad:  tensor([-0.4952,  2.8023])\n",
      "Epoch 3577 Loss 26.761780\n",
      "Params: tensor([ 2.4537, -0.8089])\n",
      "Grad:  tensor([-0.4952,  2.8022])\n",
      "Epoch 3578 Loss 26.760963\n",
      "Params: tensor([ 2.4537, -0.8092])\n",
      "Grad:  tensor([-0.4952,  2.8022])\n",
      "Epoch 3579 Loss 26.760155\n",
      "Params: tensor([ 2.4538, -0.8095])\n",
      "Grad:  tensor([-0.4952,  2.8021])\n",
      "Epoch 3580 Loss 26.759346\n",
      "Params: tensor([ 2.4538, -0.8097])\n",
      "Grad:  tensor([-0.4952,  2.8021])\n",
      "Epoch 3581 Loss 26.758533\n",
      "Params: tensor([ 2.4539, -0.8100])\n",
      "Grad:  tensor([-0.4952,  2.8020])\n",
      "Epoch 3582 Loss 26.757727\n",
      "Params: tensor([ 2.4539, -0.8103])\n",
      "Grad:  tensor([-0.4951,  2.8020])\n",
      "Epoch 3583 Loss 26.756914\n",
      "Params: tensor([ 2.4540, -0.8106])\n",
      "Grad:  tensor([-0.4951,  2.8019])\n",
      "Epoch 3584 Loss 26.756109\n",
      "Params: tensor([ 2.4540, -0.8109])\n",
      "Grad:  tensor([-0.4951,  2.8019])\n",
      "Epoch 3585 Loss 26.755297\n",
      "Params: tensor([ 2.4541, -0.8111])\n",
      "Grad:  tensor([-0.4951,  2.8019])\n",
      "Epoch 3586 Loss 26.754486\n",
      "Params: tensor([ 2.4541, -0.8114])\n",
      "Grad:  tensor([-0.4951,  2.8018])\n",
      "Epoch 3587 Loss 26.753675\n",
      "Params: tensor([ 2.4542, -0.8117])\n",
      "Grad:  tensor([-0.4951,  2.8018])\n",
      "Epoch 3588 Loss 26.752869\n",
      "Params: tensor([ 2.4542, -0.8120])\n",
      "Grad:  tensor([-0.4951,  2.8017])\n",
      "Epoch 3589 Loss 26.752056\n",
      "Params: tensor([ 2.4543, -0.8123])\n",
      "Grad:  tensor([-0.4951,  2.8017])\n",
      "Epoch 3590 Loss 26.751245\n",
      "Params: tensor([ 2.4543, -0.8125])\n",
      "Grad:  tensor([-0.4950,  2.8016])\n",
      "Epoch 3591 Loss 26.750439\n",
      "Params: tensor([ 2.4544, -0.8128])\n",
      "Grad:  tensor([-0.4950,  2.8016])\n",
      "Epoch 3592 Loss 26.749634\n",
      "Params: tensor([ 2.4544, -0.8131])\n",
      "Grad:  tensor([-0.4950,  2.8015])\n",
      "Epoch 3593 Loss 26.748821\n",
      "Params: tensor([ 2.4545, -0.8134])\n",
      "Grad:  tensor([-0.4950,  2.8015])\n",
      "Epoch 3594 Loss 26.748011\n",
      "Params: tensor([ 2.4545, -0.8137])\n",
      "Grad:  tensor([-0.4950,  2.8014])\n",
      "Epoch 3595 Loss 26.747204\n",
      "Params: tensor([ 2.4546, -0.8139])\n",
      "Grad:  tensor([-0.4950,  2.8014])\n",
      "Epoch 3596 Loss 26.746397\n",
      "Params: tensor([ 2.4546, -0.8142])\n",
      "Grad:  tensor([-0.4950,  2.8013])\n",
      "Epoch 3597 Loss 26.745583\n",
      "Params: tensor([ 2.4547, -0.8145])\n",
      "Grad:  tensor([-0.4949,  2.8013])\n",
      "Epoch 3598 Loss 26.744774\n",
      "Params: tensor([ 2.4547, -0.8148])\n",
      "Grad:  tensor([-0.4949,  2.8012])\n",
      "Epoch 3599 Loss 26.743963\n",
      "Params: tensor([ 2.4548, -0.8151])\n",
      "Grad:  tensor([-0.4949,  2.8012])\n",
      "Epoch 3600 Loss 26.743153\n",
      "Params: tensor([ 2.4548, -0.8153])\n",
      "Grad:  tensor([-0.4949,  2.8012])\n",
      "Epoch 3601 Loss 26.742342\n",
      "Params: tensor([ 2.4549, -0.8156])\n",
      "Grad:  tensor([-0.4949,  2.8011])\n",
      "Epoch 3602 Loss 26.741541\n",
      "Params: tensor([ 2.4549, -0.8159])\n",
      "Grad:  tensor([-0.4949,  2.8011])\n",
      "Epoch 3603 Loss 26.740730\n",
      "Params: tensor([ 2.4550, -0.8162])\n",
      "Grad:  tensor([-0.4949,  2.8010])\n",
      "Epoch 3604 Loss 26.739918\n",
      "Params: tensor([ 2.4550, -0.8165])\n",
      "Grad:  tensor([-0.4948,  2.8010])\n",
      "Epoch 3605 Loss 26.739111\n",
      "Params: tensor([ 2.4551, -0.8167])\n",
      "Grad:  tensor([-0.4948,  2.8009])\n",
      "Epoch 3606 Loss 26.738300\n",
      "Params: tensor([ 2.4551, -0.8170])\n",
      "Grad:  tensor([-0.4948,  2.8009])\n",
      "Epoch 3607 Loss 26.737490\n",
      "Params: tensor([ 2.4552, -0.8173])\n",
      "Grad:  tensor([-0.4948,  2.8008])\n",
      "Epoch 3608 Loss 26.736683\n",
      "Params: tensor([ 2.4552, -0.8176])\n",
      "Grad:  tensor([-0.4948,  2.8008])\n",
      "Epoch 3609 Loss 26.735876\n",
      "Params: tensor([ 2.4553, -0.8179])\n",
      "Grad:  tensor([-0.4948,  2.8007])\n",
      "Epoch 3610 Loss 26.735069\n",
      "Params: tensor([ 2.4553, -0.8181])\n",
      "Grad:  tensor([-0.4947,  2.8007])\n",
      "Epoch 3611 Loss 26.734253\n",
      "Params: tensor([ 2.4554, -0.8184])\n",
      "Grad:  tensor([-0.4947,  2.8006])\n",
      "Epoch 3612 Loss 26.733446\n",
      "Params: tensor([ 2.4554, -0.8187])\n",
      "Grad:  tensor([-0.4947,  2.8006])\n",
      "Epoch 3613 Loss 26.732637\n",
      "Params: tensor([ 2.4555, -0.8190])\n",
      "Grad:  tensor([-0.4947,  2.8005])\n",
      "Epoch 3614 Loss 26.731831\n",
      "Params: tensor([ 2.4555, -0.8193])\n",
      "Grad:  tensor([-0.4947,  2.8005])\n",
      "Epoch 3615 Loss 26.731020\n",
      "Params: tensor([ 2.4556, -0.8195])\n",
      "Grad:  tensor([-0.4947,  2.8004])\n",
      "Epoch 3616 Loss 26.730207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([ 2.4556, -0.8198])\n",
      "Grad:  tensor([-0.4947,  2.8004])\n",
      "Epoch 3617 Loss 26.729403\n",
      "Params: tensor([ 2.4557, -0.8201])\n",
      "Grad:  tensor([-0.4947,  2.8004])\n",
      "Epoch 3618 Loss 26.728596\n",
      "Params: tensor([ 2.4557, -0.8204])\n",
      "Grad:  tensor([-0.4947,  2.8003])\n",
      "Epoch 3619 Loss 26.727783\n",
      "Params: tensor([ 2.4558, -0.8207])\n",
      "Grad:  tensor([-0.4947,  2.8003])\n",
      "Epoch 3620 Loss 26.726976\n",
      "Params: tensor([ 2.4558, -0.8209])\n",
      "Grad:  tensor([-0.4947,  2.8002])\n",
      "Epoch 3621 Loss 26.726171\n",
      "Params: tensor([ 2.4559, -0.8212])\n",
      "Grad:  tensor([-0.4947,  2.8002])\n",
      "Epoch 3622 Loss 26.725361\n",
      "Params: tensor([ 2.4559, -0.8215])\n",
      "Grad:  tensor([-0.4947,  2.8001])\n",
      "Epoch 3623 Loss 26.724554\n",
      "Params: tensor([ 2.4559, -0.8218])\n",
      "Grad:  tensor([-0.4947,  2.8001])\n",
      "Epoch 3624 Loss 26.723743\n",
      "Params: tensor([ 2.4560, -0.8221])\n",
      "Grad:  tensor([-0.4947,  2.8000])\n",
      "Epoch 3625 Loss 26.722939\n",
      "Params: tensor([ 2.4560, -0.8223])\n",
      "Grad:  tensor([-0.4947,  2.8000])\n",
      "Epoch 3626 Loss 26.722130\n",
      "Params: tensor([ 2.4561, -0.8226])\n",
      "Grad:  tensor([-0.4947,  2.7999])\n",
      "Epoch 3627 Loss 26.721315\n",
      "Params: tensor([ 2.4561, -0.8229])\n",
      "Grad:  tensor([-0.4947,  2.7999])\n",
      "Epoch 3628 Loss 26.720509\n",
      "Params: tensor([ 2.4562, -0.8232])\n",
      "Grad:  tensor([-0.4947,  2.7998])\n",
      "Epoch 3629 Loss 26.719702\n",
      "Params: tensor([ 2.4562, -0.8235])\n",
      "Grad:  tensor([-0.4947,  2.7998])\n",
      "Epoch 3630 Loss 26.718895\n",
      "Params: tensor([ 2.4563, -0.8237])\n",
      "Grad:  tensor([-0.4947,  2.7997])\n",
      "Epoch 3631 Loss 26.718084\n",
      "Params: tensor([ 2.4563, -0.8240])\n",
      "Grad:  tensor([-0.4947,  2.7997])\n",
      "Epoch 3632 Loss 26.717278\n",
      "Params: tensor([ 2.4564, -0.8243])\n",
      "Grad:  tensor([-0.4947,  2.7996])\n",
      "Epoch 3633 Loss 26.716469\n",
      "Params: tensor([ 2.4564, -0.8246])\n",
      "Grad:  tensor([-0.4947,  2.7996])\n",
      "Epoch 3634 Loss 26.715662\n",
      "Params: tensor([ 2.4565, -0.8249])\n",
      "Grad:  tensor([-0.4947,  2.7995])\n",
      "Epoch 3635 Loss 26.714855\n",
      "Params: tensor([ 2.4565, -0.8251])\n",
      "Grad:  tensor([-0.4947,  2.7995])\n",
      "Epoch 3636 Loss 26.714043\n",
      "Params: tensor([ 2.4566, -0.8254])\n",
      "Grad:  tensor([-0.4947,  2.7994])\n",
      "Epoch 3637 Loss 26.713238\n",
      "Params: tensor([ 2.4566, -0.8257])\n",
      "Grad:  tensor([-0.4947,  2.7994])\n",
      "Epoch 3638 Loss 26.712427\n",
      "Params: tensor([ 2.4567, -0.8260])\n",
      "Grad:  tensor([-0.4947,  2.7993])\n",
      "Epoch 3639 Loss 26.711622\n",
      "Params: tensor([ 2.4567, -0.8263])\n",
      "Grad:  tensor([-0.4947,  2.7993])\n",
      "Epoch 3640 Loss 26.710815\n",
      "Params: tensor([ 2.4568, -0.8265])\n",
      "Grad:  tensor([-0.4947,  2.7992])\n",
      "Epoch 3641 Loss 26.710003\n",
      "Params: tensor([ 2.4568, -0.8268])\n",
      "Grad:  tensor([-0.4947,  2.7992])\n",
      "Epoch 3642 Loss 26.709196\n",
      "Params: tensor([ 2.4569, -0.8271])\n",
      "Grad:  tensor([-0.4947,  2.7991])\n",
      "Epoch 3643 Loss 26.708387\n",
      "Params: tensor([ 2.4569, -0.8274])\n",
      "Grad:  tensor([-0.4947,  2.7991])\n",
      "Epoch 3644 Loss 26.707581\n",
      "Params: tensor([ 2.4570, -0.8277])\n",
      "Grad:  tensor([-0.4946,  2.7990])\n",
      "Epoch 3645 Loss 26.706770\n",
      "Params: tensor([ 2.4570, -0.8279])\n",
      "Grad:  tensor([-0.4946,  2.7990])\n",
      "Epoch 3646 Loss 26.705965\n",
      "Params: tensor([ 2.4571, -0.8282])\n",
      "Grad:  tensor([-0.4946,  2.7989])\n",
      "Epoch 3647 Loss 26.705158\n",
      "Params: tensor([ 2.4571, -0.8285])\n",
      "Grad:  tensor([-0.4946,  2.7989])\n",
      "Epoch 3648 Loss 26.704351\n",
      "Params: tensor([ 2.4572, -0.8288])\n",
      "Grad:  tensor([-0.4946,  2.7988])\n",
      "Epoch 3649 Loss 26.703539\n",
      "Params: tensor([ 2.4572, -0.8291])\n",
      "Grad:  tensor([-0.4946,  2.7988])\n",
      "Epoch 3650 Loss 26.702734\n",
      "Params: tensor([ 2.4573, -0.8293])\n",
      "Grad:  tensor([-0.4946,  2.7987])\n",
      "Epoch 3651 Loss 26.701927\n",
      "Params: tensor([ 2.4573, -0.8296])\n",
      "Grad:  tensor([-0.4946,  2.7987])\n",
      "Epoch 3652 Loss 26.701117\n",
      "Params: tensor([ 2.4574, -0.8299])\n",
      "Grad:  tensor([-0.4946,  2.7986])\n",
      "Epoch 3653 Loss 26.700312\n",
      "Params: tensor([ 2.4574, -0.8302])\n",
      "Grad:  tensor([-0.4946,  2.7986])\n",
      "Epoch 3654 Loss 26.699501\n",
      "Params: tensor([ 2.4575, -0.8305])\n",
      "Grad:  tensor([-0.4946,  2.7986])\n",
      "Epoch 3655 Loss 26.698694\n",
      "Params: tensor([ 2.4575, -0.8307])\n",
      "Grad:  tensor([-0.4946,  2.7985])\n",
      "Epoch 3656 Loss 26.697887\n",
      "Params: tensor([ 2.4576, -0.8310])\n",
      "Grad:  tensor([-0.4946,  2.7985])\n",
      "Epoch 3657 Loss 26.697081\n",
      "Params: tensor([ 2.4576, -0.8313])\n",
      "Grad:  tensor([-0.4946,  2.7984])\n",
      "Epoch 3658 Loss 26.696272\n",
      "Params: tensor([ 2.4577, -0.8316])\n",
      "Grad:  tensor([-0.4946,  2.7984])\n",
      "Epoch 3659 Loss 26.695467\n",
      "Params: tensor([ 2.4577, -0.8319])\n",
      "Grad:  tensor([-0.4946,  2.7983])\n",
      "Epoch 3660 Loss 26.694660\n",
      "Params: tensor([ 2.4578, -0.8321])\n",
      "Grad:  tensor([-0.4946,  2.7983])\n",
      "Epoch 3661 Loss 26.693853\n",
      "Params: tensor([ 2.4578, -0.8324])\n",
      "Grad:  tensor([-0.4946,  2.7982])\n",
      "Epoch 3662 Loss 26.693043\n",
      "Params: tensor([ 2.4579, -0.8327])\n",
      "Grad:  tensor([-0.4946,  2.7982])\n",
      "Epoch 3663 Loss 26.692232\n",
      "Params: tensor([ 2.4579, -0.8330])\n",
      "Grad:  tensor([-0.4946,  2.7981])\n",
      "Epoch 3664 Loss 26.691429\n",
      "Params: tensor([ 2.4580, -0.8333])\n",
      "Grad:  tensor([-0.4946,  2.7981])\n",
      "Epoch 3665 Loss 26.690620\n",
      "Params: tensor([ 2.4580, -0.8335])\n",
      "Grad:  tensor([-0.4946,  2.7980])\n",
      "Epoch 3666 Loss 26.689812\n",
      "Params: tensor([ 2.4581, -0.8338])\n",
      "Grad:  tensor([-0.4946,  2.7980])\n",
      "Epoch 3667 Loss 26.689007\n",
      "Params: tensor([ 2.4581, -0.8341])\n",
      "Grad:  tensor([-0.4946,  2.7979])\n",
      "Epoch 3668 Loss 26.688196\n",
      "Params: tensor([ 2.4582, -0.8344])\n",
      "Grad:  tensor([-0.4946,  2.7979])\n",
      "Epoch 3669 Loss 26.687386\n",
      "Params: tensor([ 2.4582, -0.8347])\n",
      "Grad:  tensor([-0.4946,  2.7978])\n",
      "Epoch 3670 Loss 26.686586\n",
      "Params: tensor([ 2.4583, -0.8349])\n",
      "Grad:  tensor([-0.4946,  2.7978])\n",
      "Epoch 3671 Loss 26.685780\n",
      "Params: tensor([ 2.4583, -0.8352])\n",
      "Grad:  tensor([-0.4946,  2.7977])\n",
      "Epoch 3672 Loss 26.684973\n",
      "Params: tensor([ 2.4584, -0.8355])\n",
      "Grad:  tensor([-0.4945,  2.7977])\n",
      "Epoch 3673 Loss 26.684166\n",
      "Params: tensor([ 2.4584, -0.8358])\n",
      "Grad:  tensor([-0.4945,  2.7976])\n",
      "Epoch 3674 Loss 26.683355\n",
      "Params: tensor([ 2.4585, -0.8361])\n",
      "Grad:  tensor([-0.4945,  2.7976])\n",
      "Epoch 3675 Loss 26.682554\n",
      "Params: tensor([ 2.4585, -0.8363])\n",
      "Grad:  tensor([-0.4945,  2.7975])\n",
      "Epoch 3676 Loss 26.681740\n",
      "Params: tensor([ 2.4586, -0.8366])\n",
      "Grad:  tensor([-0.4945,  2.7975])\n",
      "Epoch 3677 Loss 26.680937\n",
      "Params: tensor([ 2.4586, -0.8369])\n",
      "Grad:  tensor([-0.4945,  2.7974])\n",
      "Epoch 3678 Loss 26.680128\n",
      "Params: tensor([ 2.4587, -0.8372])\n",
      "Grad:  tensor([-0.4945,  2.7974])\n",
      "Epoch 3679 Loss 26.679321\n",
      "Params: tensor([ 2.4587, -0.8375])\n",
      "Grad:  tensor([-0.4945,  2.7973])\n",
      "Epoch 3680 Loss 26.678514\n",
      "Params: tensor([ 2.4588, -0.8377])\n",
      "Grad:  tensor([-0.4945,  2.7973])\n",
      "Epoch 3681 Loss 26.677710\n",
      "Params: tensor([ 2.4588, -0.8380])\n",
      "Grad:  tensor([-0.4945,  2.7972])\n",
      "Epoch 3682 Loss 26.676899\n",
      "Params: tensor([ 2.4589, -0.8383])\n",
      "Grad:  tensor([-0.4945,  2.7972])\n",
      "Epoch 3683 Loss 26.676094\n",
      "Params: tensor([ 2.4589, -0.8386])\n",
      "Grad:  tensor([-0.4945,  2.7972])\n",
      "Epoch 3684 Loss 26.675291\n",
      "Params: tensor([ 2.4590, -0.8389])\n",
      "Grad:  tensor([-0.4945,  2.7971])\n",
      "Epoch 3685 Loss 26.674482\n",
      "Params: tensor([ 2.4590, -0.8391])\n",
      "Grad:  tensor([-0.4945,  2.7971])\n",
      "Epoch 3686 Loss 26.673676\n",
      "Params: tensor([ 2.4591, -0.8394])\n",
      "Grad:  tensor([-0.4945,  2.7970])\n",
      "Epoch 3687 Loss 26.672863\n",
      "Params: tensor([ 2.4591, -0.8397])\n",
      "Grad:  tensor([-0.4945,  2.7970])\n",
      "Epoch 3688 Loss 26.672058\n",
      "Params: tensor([ 2.4592, -0.8400])\n",
      "Grad:  tensor([-0.4945,  2.7969])\n",
      "Epoch 3689 Loss 26.671259\n",
      "Params: tensor([ 2.4592, -0.8403])\n",
      "Grad:  tensor([-0.4945,  2.7969])\n",
      "Epoch 3690 Loss 26.670443\n",
      "Params: tensor([ 2.4593, -0.8405])\n",
      "Grad:  tensor([-0.4945,  2.7968])\n",
      "Epoch 3691 Loss 26.669640\n",
      "Params: tensor([ 2.4593, -0.8408])\n",
      "Grad:  tensor([-0.4945,  2.7968])\n",
      "Epoch 3692 Loss 26.668831\n",
      "Params: tensor([ 2.4594, -0.8411])\n",
      "Grad:  tensor([-0.4944,  2.7967])\n",
      "Epoch 3693 Loss 26.668028\n",
      "Params: tensor([ 2.4594, -0.8414])\n",
      "Grad:  tensor([-0.4944,  2.7967])\n",
      "Epoch 3694 Loss 26.667223\n",
      "Params: tensor([ 2.4595, -0.8417])\n",
      "Grad:  tensor([-0.4944,  2.7966])\n",
      "Epoch 3695 Loss 26.666418\n",
      "Params: tensor([ 2.4595, -0.8419])\n",
      "Grad:  tensor([-0.4944,  2.7966])\n",
      "Epoch 3696 Loss 26.665607\n",
      "Params: tensor([ 2.4596, -0.8422])\n",
      "Grad:  tensor([-0.4944,  2.7965])\n",
      "Epoch 3697 Loss 26.664803\n",
      "Params: tensor([ 2.4596, -0.8425])\n",
      "Grad:  tensor([-0.4944,  2.7965])\n",
      "Epoch 3698 Loss 26.663996\n",
      "Params: tensor([ 2.4597, -0.8428])\n",
      "Grad:  tensor([-0.4944,  2.7964])\n",
      "Epoch 3699 Loss 26.663189\n",
      "Params: tensor([ 2.4597, -0.8431])\n",
      "Grad:  tensor([-0.4944,  2.7964])\n",
      "Epoch 3700 Loss 26.662382\n",
      "Params: tensor([ 2.4597, -0.8433])\n",
      "Grad:  tensor([-0.4944,  2.7963])\n",
      "Epoch 3701 Loss 26.661573\n",
      "Params: tensor([ 2.4598, -0.8436])\n",
      "Grad:  tensor([-0.4944,  2.7963])\n",
      "Epoch 3702 Loss 26.660769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([ 2.4598, -0.8439])\n",
      "Grad:  tensor([-0.4944,  2.7962])\n",
      "Epoch 3703 Loss 26.659962\n",
      "Params: tensor([ 2.4599, -0.8442])\n",
      "Grad:  tensor([-0.4944,  2.7962])\n",
      "Epoch 3704 Loss 26.659155\n",
      "Params: tensor([ 2.4599, -0.8445])\n",
      "Grad:  tensor([-0.4944,  2.7961])\n",
      "Epoch 3705 Loss 26.658354\n",
      "Params: tensor([ 2.4600, -0.8447])\n",
      "Grad:  tensor([-0.4944,  2.7961])\n",
      "Epoch 3706 Loss 26.657549\n",
      "Params: tensor([ 2.4600, -0.8450])\n",
      "Grad:  tensor([-0.4944,  2.7960])\n",
      "Epoch 3707 Loss 26.656740\n",
      "Params: tensor([ 2.4601, -0.8453])\n",
      "Grad:  tensor([-0.4944,  2.7960])\n",
      "Epoch 3708 Loss 26.655933\n",
      "Params: tensor([ 2.4601, -0.8456])\n",
      "Grad:  tensor([-0.4944,  2.7960])\n",
      "Epoch 3709 Loss 26.655128\n",
      "Params: tensor([ 2.4602, -0.8458])\n",
      "Grad:  tensor([-0.4944,  2.7959])\n",
      "Epoch 3710 Loss 26.654322\n",
      "Params: tensor([ 2.4602, -0.8461])\n",
      "Grad:  tensor([-0.4943,  2.7959])\n",
      "Epoch 3711 Loss 26.653515\n",
      "Params: tensor([ 2.4603, -0.8464])\n",
      "Grad:  tensor([-0.4943,  2.7958])\n",
      "Epoch 3712 Loss 26.652710\n",
      "Params: tensor([ 2.4603, -0.8467])\n",
      "Grad:  tensor([-0.4943,  2.7958])\n",
      "Epoch 3713 Loss 26.651903\n",
      "Params: tensor([ 2.4604, -0.8470])\n",
      "Grad:  tensor([-0.4943,  2.7957])\n",
      "Epoch 3714 Loss 26.651093\n",
      "Params: tensor([ 2.4604, -0.8472])\n",
      "Grad:  tensor([-0.4943,  2.7957])\n",
      "Epoch 3715 Loss 26.650293\n",
      "Params: tensor([ 2.4605, -0.8475])\n",
      "Grad:  tensor([-0.4943,  2.7956])\n",
      "Epoch 3716 Loss 26.649487\n",
      "Params: tensor([ 2.4605, -0.8478])\n",
      "Grad:  tensor([-0.4943,  2.7956])\n",
      "Epoch 3717 Loss 26.648682\n",
      "Params: tensor([ 2.4606, -0.8481])\n",
      "Grad:  tensor([-0.4943,  2.7955])\n",
      "Epoch 3718 Loss 26.647871\n",
      "Params: tensor([ 2.4606, -0.8484])\n",
      "Grad:  tensor([-0.4943,  2.7955])\n",
      "Epoch 3719 Loss 26.647070\n",
      "Params: tensor([ 2.4607, -0.8486])\n",
      "Grad:  tensor([-0.4943,  2.7954])\n",
      "Epoch 3720 Loss 26.646263\n",
      "Params: tensor([ 2.4607, -0.8489])\n",
      "Grad:  tensor([-0.4943,  2.7954])\n",
      "Epoch 3721 Loss 26.645458\n",
      "Params: tensor([ 2.4608, -0.8492])\n",
      "Grad:  tensor([-0.4943,  2.7953])\n",
      "Epoch 3722 Loss 26.644648\n",
      "Params: tensor([ 2.4608, -0.8495])\n",
      "Grad:  tensor([-0.4943,  2.7953])\n",
      "Epoch 3723 Loss 26.643847\n",
      "Params: tensor([ 2.4609, -0.8498])\n",
      "Grad:  tensor([-0.4943,  2.7952])\n",
      "Epoch 3724 Loss 26.643042\n",
      "Params: tensor([ 2.4609, -0.8500])\n",
      "Grad:  tensor([-0.4943,  2.7952])\n",
      "Epoch 3725 Loss 26.642231\n",
      "Params: tensor([ 2.4610, -0.8503])\n",
      "Grad:  tensor([-0.4942,  2.7951])\n",
      "Epoch 3726 Loss 26.641424\n",
      "Params: tensor([ 2.4610, -0.8506])\n",
      "Grad:  tensor([-0.4942,  2.7951])\n",
      "Epoch 3727 Loss 26.640619\n",
      "Params: tensor([ 2.4611, -0.8509])\n",
      "Grad:  tensor([-0.4942,  2.7950])\n",
      "Epoch 3728 Loss 26.639818\n",
      "Params: tensor([ 2.4611, -0.8512])\n",
      "Grad:  tensor([-0.4942,  2.7950])\n",
      "Epoch 3729 Loss 26.639013\n",
      "Params: tensor([ 2.4612, -0.8514])\n",
      "Grad:  tensor([-0.4942,  2.7949])\n",
      "Epoch 3730 Loss 26.638206\n",
      "Params: tensor([ 2.4612, -0.8517])\n",
      "Grad:  tensor([-0.4942,  2.7949])\n",
      "Epoch 3731 Loss 26.637398\n",
      "Params: tensor([ 2.4613, -0.8520])\n",
      "Grad:  tensor([-0.4942,  2.7949])\n",
      "Epoch 3732 Loss 26.636597\n",
      "Params: tensor([ 2.4613, -0.8523])\n",
      "Grad:  tensor([-0.4942,  2.7948])\n",
      "Epoch 3733 Loss 26.635790\n",
      "Params: tensor([ 2.4614, -0.8526])\n",
      "Grad:  tensor([-0.4942,  2.7948])\n",
      "Epoch 3734 Loss 26.634981\n",
      "Params: tensor([ 2.4614, -0.8528])\n",
      "Grad:  tensor([-0.4942,  2.7947])\n",
      "Epoch 3735 Loss 26.634172\n",
      "Params: tensor([ 2.4615, -0.8531])\n",
      "Grad:  tensor([-0.4942,  2.7947])\n",
      "Epoch 3736 Loss 26.633373\n",
      "Params: tensor([ 2.4615, -0.8534])\n",
      "Grad:  tensor([-0.4942,  2.7946])\n",
      "Epoch 3737 Loss 26.632568\n",
      "Params: tensor([ 2.4616, -0.8537])\n",
      "Grad:  tensor([-0.4942,  2.7946])\n",
      "Epoch 3738 Loss 26.631758\n",
      "Params: tensor([ 2.4616, -0.8540])\n",
      "Grad:  tensor([-0.4942,  2.7945])\n",
      "Epoch 3739 Loss 26.630953\n",
      "Params: tensor([ 2.4617, -0.8542])\n",
      "Grad:  tensor([-0.4941,  2.7945])\n",
      "Epoch 3740 Loss 26.630152\n",
      "Params: tensor([ 2.4617, -0.8545])\n",
      "Grad:  tensor([-0.4941,  2.7944])\n",
      "Epoch 3741 Loss 26.629347\n",
      "Params: tensor([ 2.4618, -0.8548])\n",
      "Grad:  tensor([-0.4941,  2.7944])\n",
      "Epoch 3742 Loss 26.628540\n",
      "Params: tensor([ 2.4618, -0.8551])\n",
      "Grad:  tensor([-0.4941,  2.7943])\n",
      "Epoch 3743 Loss 26.627733\n",
      "Params: tensor([ 2.4619, -0.8554])\n",
      "Grad:  tensor([-0.4941,  2.7943])\n",
      "Epoch 3744 Loss 26.626930\n",
      "Params: tensor([ 2.4619, -0.8556])\n",
      "Grad:  tensor([-0.4941,  2.7942])\n",
      "Epoch 3745 Loss 26.626123\n",
      "Params: tensor([ 2.4620, -0.8559])\n",
      "Grad:  tensor([-0.4941,  2.7942])\n",
      "Epoch 3746 Loss 26.625319\n",
      "Params: tensor([ 2.4620, -0.8562])\n",
      "Grad:  tensor([-0.4941,  2.7941])\n",
      "Epoch 3747 Loss 26.624517\n",
      "Params: tensor([ 2.4621, -0.8565])\n",
      "Grad:  tensor([-0.4941,  2.7941])\n",
      "Epoch 3748 Loss 26.623711\n",
      "Params: tensor([ 2.4621, -0.8568])\n",
      "Grad:  tensor([-0.4941,  2.7940])\n",
      "Epoch 3749 Loss 26.622906\n",
      "Params: tensor([ 2.4622, -0.8570])\n",
      "Grad:  tensor([-0.4941,  2.7940])\n",
      "Epoch 3750 Loss 26.622099\n",
      "Params: tensor([ 2.4622, -0.8573])\n",
      "Grad:  tensor([-0.4941,  2.7939])\n",
      "Epoch 3751 Loss 26.621294\n",
      "Params: tensor([ 2.4623, -0.8576])\n",
      "Grad:  tensor([-0.4941,  2.7939])\n",
      "Epoch 3752 Loss 26.620483\n",
      "Params: tensor([ 2.4623, -0.8579])\n",
      "Grad:  tensor([-0.4940,  2.7939])\n",
      "Epoch 3753 Loss 26.619682\n",
      "Params: tensor([ 2.4624, -0.8581])\n",
      "Grad:  tensor([-0.4940,  2.7938])\n",
      "Epoch 3754 Loss 26.618877\n",
      "Params: tensor([ 2.4624, -0.8584])\n",
      "Grad:  tensor([-0.4940,  2.7938])\n",
      "Epoch 3755 Loss 26.618073\n",
      "Params: tensor([ 2.4625, -0.8587])\n",
      "Grad:  tensor([-0.4940,  2.7937])\n",
      "Epoch 3756 Loss 26.617266\n",
      "Params: tensor([ 2.4625, -0.8590])\n",
      "Grad:  tensor([-0.4940,  2.7937])\n",
      "Epoch 3757 Loss 26.616463\n",
      "Params: tensor([ 2.4626, -0.8593])\n",
      "Grad:  tensor([-0.4940,  2.7936])\n",
      "Epoch 3758 Loss 26.615662\n",
      "Params: tensor([ 2.4626, -0.8595])\n",
      "Grad:  tensor([-0.4940,  2.7936])\n",
      "Epoch 3759 Loss 26.614851\n",
      "Params: tensor([ 2.4627, -0.8598])\n",
      "Grad:  tensor([-0.4940,  2.7935])\n",
      "Epoch 3760 Loss 26.614050\n",
      "Params: tensor([ 2.4627, -0.8601])\n",
      "Grad:  tensor([-0.4940,  2.7935])\n",
      "Epoch 3761 Loss 26.613249\n",
      "Params: tensor([ 2.4628, -0.8604])\n",
      "Grad:  tensor([-0.4940,  2.7934])\n",
      "Epoch 3762 Loss 26.612440\n",
      "Params: tensor([ 2.4628, -0.8607])\n",
      "Grad:  tensor([-0.4940,  2.7934])\n",
      "Epoch 3763 Loss 26.611635\n",
      "Params: tensor([ 2.4629, -0.8609])\n",
      "Grad:  tensor([-0.4939,  2.7933])\n",
      "Epoch 3764 Loss 26.610828\n",
      "Params: tensor([ 2.4629, -0.8612])\n",
      "Grad:  tensor([-0.4939,  2.7933])\n",
      "Epoch 3765 Loss 26.610029\n",
      "Params: tensor([ 2.4630, -0.8615])\n",
      "Grad:  tensor([-0.4939,  2.7932])\n",
      "Epoch 3766 Loss 26.609222\n",
      "Params: tensor([ 2.4630, -0.8618])\n",
      "Grad:  tensor([-0.4939,  2.7932])\n",
      "Epoch 3767 Loss 26.608418\n",
      "Params: tensor([ 2.4631, -0.8621])\n",
      "Grad:  tensor([-0.4939,  2.7931])\n",
      "Epoch 3768 Loss 26.607616\n",
      "Params: tensor([ 2.4631, -0.8623])\n",
      "Grad:  tensor([-0.4939,  2.7931])\n",
      "Epoch 3769 Loss 26.606812\n",
      "Params: tensor([ 2.4632, -0.8626])\n",
      "Grad:  tensor([-0.4939,  2.7930])\n",
      "Epoch 3770 Loss 26.606005\n",
      "Params: tensor([ 2.4632, -0.8629])\n",
      "Grad:  tensor([-0.4939,  2.7930])\n",
      "Epoch 3771 Loss 26.605202\n",
      "Params: tensor([ 2.4633, -0.8632])\n",
      "Grad:  tensor([-0.4939,  2.7930])\n",
      "Epoch 3772 Loss 26.604395\n",
      "Params: tensor([ 2.4633, -0.8635])\n",
      "Grad:  tensor([-0.4939,  2.7929])\n",
      "Epoch 3773 Loss 26.603596\n",
      "Params: tensor([ 2.4634, -0.8637])\n",
      "Grad:  tensor([-0.4939,  2.7929])\n",
      "Epoch 3774 Loss 26.602785\n",
      "Params: tensor([ 2.4634, -0.8640])\n",
      "Grad:  tensor([-0.4939,  2.7928])\n",
      "Epoch 3775 Loss 26.601978\n",
      "Params: tensor([ 2.4635, -0.8643])\n",
      "Grad:  tensor([-0.4938,  2.7928])\n",
      "Epoch 3776 Loss 26.601177\n",
      "Params: tensor([ 2.4635, -0.8646])\n",
      "Grad:  tensor([-0.4938,  2.7927])\n",
      "Epoch 3777 Loss 26.600374\n",
      "Params: tensor([ 2.4635, -0.8649])\n",
      "Grad:  tensor([-0.4938,  2.7927])\n",
      "Epoch 3778 Loss 26.599573\n",
      "Params: tensor([ 2.4636, -0.8651])\n",
      "Grad:  tensor([-0.4938,  2.7926])\n",
      "Epoch 3779 Loss 26.598766\n",
      "Params: tensor([ 2.4636, -0.8654])\n",
      "Grad:  tensor([-0.4938,  2.7926])\n",
      "Epoch 3780 Loss 26.597960\n",
      "Params: tensor([ 2.4637, -0.8657])\n",
      "Grad:  tensor([-0.4938,  2.7925])\n",
      "Epoch 3781 Loss 26.597162\n",
      "Params: tensor([ 2.4637, -0.8660])\n",
      "Grad:  tensor([-0.4938,  2.7925])\n",
      "Epoch 3782 Loss 26.596352\n",
      "Params: tensor([ 2.4638, -0.8662])\n",
      "Grad:  tensor([-0.4938,  2.7924])\n",
      "Epoch 3783 Loss 26.595551\n",
      "Params: tensor([ 2.4638, -0.8665])\n",
      "Grad:  tensor([-0.4938,  2.7924])\n",
      "Epoch 3784 Loss 26.594744\n",
      "Params: tensor([ 2.4639, -0.8668])\n",
      "Grad:  tensor([-0.4938,  2.7923])\n",
      "Epoch 3785 Loss 26.593939\n",
      "Params: tensor([ 2.4639, -0.8671])\n",
      "Grad:  tensor([-0.4937,  2.7923])\n",
      "Epoch 3786 Loss 26.593138\n",
      "Params: tensor([ 2.4640, -0.8674])\n",
      "Grad:  tensor([-0.4937,  2.7922])\n",
      "Epoch 3787 Loss 26.592327\n",
      "Params: tensor([ 2.4640, -0.8676])\n",
      "Grad:  tensor([-0.4937,  2.7922])\n",
      "Epoch 3788 Loss 26.591524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([ 2.4641, -0.8679])\n",
      "Grad:  tensor([-0.4937,  2.7922])\n",
      "Epoch 3789 Loss 26.590727\n",
      "Params: tensor([ 2.4641, -0.8682])\n",
      "Grad:  tensor([-0.4937,  2.7921])\n",
      "Epoch 3790 Loss 26.589918\n",
      "Params: tensor([ 2.4642, -0.8685])\n",
      "Grad:  tensor([-0.4937,  2.7921])\n",
      "Epoch 3791 Loss 26.589119\n",
      "Params: tensor([ 2.4642, -0.8688])\n",
      "Grad:  tensor([-0.4937,  2.7920])\n",
      "Epoch 3792 Loss 26.588310\n",
      "Params: tensor([ 2.4643, -0.8690])\n",
      "Grad:  tensor([-0.4937,  2.7920])\n",
      "Epoch 3793 Loss 26.587511\n",
      "Params: tensor([ 2.4643, -0.8693])\n",
      "Grad:  tensor([-0.4937,  2.7919])\n",
      "Epoch 3794 Loss 26.586702\n",
      "Params: tensor([ 2.4644, -0.8696])\n",
      "Grad:  tensor([-0.4937,  2.7919])\n",
      "Epoch 3795 Loss 26.585899\n",
      "Params: tensor([ 2.4644, -0.8699])\n",
      "Grad:  tensor([-0.4936,  2.7918])\n",
      "Epoch 3796 Loss 26.585100\n",
      "Params: tensor([ 2.4645, -0.8702])\n",
      "Grad:  tensor([-0.4936,  2.7918])\n",
      "Epoch 3797 Loss 26.584291\n",
      "Params: tensor([ 2.4645, -0.8704])\n",
      "Grad:  tensor([-0.4936,  2.7917])\n",
      "Epoch 3798 Loss 26.583485\n",
      "Params: tensor([ 2.4646, -0.8707])\n",
      "Grad:  tensor([-0.4936,  2.7917])\n",
      "Epoch 3799 Loss 26.582685\n",
      "Params: tensor([ 2.4646, -0.8710])\n",
      "Grad:  tensor([-0.4936,  2.7916])\n",
      "Epoch 3800 Loss 26.581881\n",
      "Params: tensor([ 2.4647, -0.8713])\n",
      "Grad:  tensor([-0.4936,  2.7916])\n",
      "Epoch 3801 Loss 26.581083\n",
      "Params: tensor([ 2.4647, -0.8716])\n",
      "Grad:  tensor([-0.4936,  2.7915])\n",
      "Epoch 3802 Loss 26.580280\n",
      "Params: tensor([ 2.4648, -0.8718])\n",
      "Grad:  tensor([-0.4936,  2.7915])\n",
      "Epoch 3803 Loss 26.579470\n",
      "Params: tensor([ 2.4648, -0.8721])\n",
      "Grad:  tensor([-0.4936,  2.7914])\n",
      "Epoch 3804 Loss 26.578672\n",
      "Params: tensor([ 2.4649, -0.8724])\n",
      "Grad:  tensor([-0.4935,  2.7914])\n",
      "Epoch 3805 Loss 26.577864\n",
      "Params: tensor([ 2.4649, -0.8727])\n",
      "Grad:  tensor([-0.4935,  2.7914])\n",
      "Epoch 3806 Loss 26.577055\n",
      "Params: tensor([ 2.4650, -0.8729])\n",
      "Grad:  tensor([-0.4935,  2.7913])\n",
      "Epoch 3807 Loss 26.576256\n",
      "Params: tensor([ 2.4650, -0.8732])\n",
      "Grad:  tensor([-0.4935,  2.7913])\n",
      "Epoch 3808 Loss 26.575453\n",
      "Params: tensor([ 2.4651, -0.8735])\n",
      "Grad:  tensor([-0.4935,  2.7912])\n",
      "Epoch 3809 Loss 26.574648\n",
      "Params: tensor([ 2.4651, -0.8738])\n",
      "Grad:  tensor([-0.4935,  2.7912])\n",
      "Epoch 3810 Loss 26.573847\n",
      "Params: tensor([ 2.4652, -0.8741])\n",
      "Grad:  tensor([-0.4935,  2.7911])\n",
      "Epoch 3811 Loss 26.573046\n",
      "Params: tensor([ 2.4652, -0.8743])\n",
      "Grad:  tensor([-0.4935,  2.7911])\n",
      "Epoch 3812 Loss 26.572237\n",
      "Params: tensor([ 2.4653, -0.8746])\n",
      "Grad:  tensor([-0.4935,  2.7910])\n",
      "Epoch 3813 Loss 26.571436\n",
      "Params: tensor([ 2.4653, -0.8749])\n",
      "Grad:  tensor([-0.4935,  2.7910])\n",
      "Epoch 3814 Loss 26.570635\n",
      "Params: tensor([ 2.4654, -0.8752])\n",
      "Grad:  tensor([-0.4934,  2.7909])\n",
      "Epoch 3815 Loss 26.569830\n",
      "Params: tensor([ 2.4654, -0.8755])\n",
      "Grad:  tensor([-0.4934,  2.7909])\n",
      "Epoch 3816 Loss 26.569031\n",
      "Params: tensor([ 2.4655, -0.8757])\n",
      "Grad:  tensor([-0.4934,  2.7908])\n",
      "Epoch 3817 Loss 26.568224\n",
      "Params: tensor([ 2.4655, -0.8760])\n",
      "Grad:  tensor([-0.4934,  2.7908])\n",
      "Epoch 3818 Loss 26.567419\n",
      "Params: tensor([ 2.4656, -0.8763])\n",
      "Grad:  tensor([-0.4934,  2.7907])\n",
      "Epoch 3819 Loss 26.566618\n",
      "Params: tensor([ 2.4656, -0.8766])\n",
      "Grad:  tensor([-0.4934,  2.7907])\n",
      "Epoch 3820 Loss 26.565815\n",
      "Params: tensor([ 2.4657, -0.8769])\n",
      "Grad:  tensor([-0.4934,  2.7906])\n",
      "Epoch 3821 Loss 26.565004\n",
      "Params: tensor([ 2.4657, -0.8771])\n",
      "Grad:  tensor([-0.4934,  2.7906])\n",
      "Epoch 3822 Loss 26.564201\n",
      "Params: tensor([ 2.4658, -0.8774])\n",
      "Grad:  tensor([-0.4933,  2.7906])\n",
      "Epoch 3823 Loss 26.563408\n",
      "Params: tensor([ 2.4658, -0.8777])\n",
      "Grad:  tensor([-0.4933,  2.7905])\n",
      "Epoch 3824 Loss 26.562603\n",
      "Params: tensor([ 2.4659, -0.8780])\n",
      "Grad:  tensor([-0.4933,  2.7905])\n",
      "Epoch 3825 Loss 26.561798\n",
      "Params: tensor([ 2.4659, -0.8783])\n",
      "Grad:  tensor([-0.4933,  2.7904])\n",
      "Epoch 3826 Loss 26.560997\n",
      "Params: tensor([ 2.4660, -0.8785])\n",
      "Grad:  tensor([-0.4933,  2.7904])\n",
      "Epoch 3827 Loss 26.560192\n",
      "Params: tensor([ 2.4660, -0.8788])\n",
      "Grad:  tensor([-0.4933,  2.7903])\n",
      "Epoch 3828 Loss 26.559389\n",
      "Params: tensor([ 2.4661, -0.8791])\n",
      "Grad:  tensor([-0.4933,  2.7903])\n",
      "Epoch 3829 Loss 26.558588\n",
      "Params: tensor([ 2.4661, -0.8794])\n",
      "Grad:  tensor([-0.4933,  2.7902])\n",
      "Epoch 3830 Loss 26.557781\n",
      "Params: tensor([ 2.4662, -0.8796])\n",
      "Grad:  tensor([-0.4933,  2.7902])\n",
      "Epoch 3831 Loss 26.556982\n",
      "Params: tensor([ 2.4662, -0.8799])\n",
      "Grad:  tensor([-0.4932,  2.7901])\n",
      "Epoch 3832 Loss 26.556175\n",
      "Params: tensor([ 2.4663, -0.8802])\n",
      "Grad:  tensor([-0.4932,  2.7901])\n",
      "Epoch 3833 Loss 26.555376\n",
      "Params: tensor([ 2.4663, -0.8805])\n",
      "Grad:  tensor([-0.4932,  2.7900])\n",
      "Epoch 3834 Loss 26.554573\n",
      "Params: tensor([ 2.4664, -0.8808])\n",
      "Grad:  tensor([-0.4932,  2.7900])\n",
      "Epoch 3835 Loss 26.553772\n",
      "Params: tensor([ 2.4664, -0.8810])\n",
      "Grad:  tensor([-0.4932,  2.7899])\n",
      "Epoch 3836 Loss 26.552967\n",
      "Params: tensor([ 2.4665, -0.8813])\n",
      "Grad:  tensor([-0.4932,  2.7899])\n",
      "Epoch 3837 Loss 26.552162\n",
      "Params: tensor([ 2.4665, -0.8816])\n",
      "Grad:  tensor([-0.4932,  2.7899])\n",
      "Epoch 3838 Loss 26.551367\n",
      "Params: tensor([ 2.4666, -0.8819])\n",
      "Grad:  tensor([-0.4932,  2.7898])\n",
      "Epoch 3839 Loss 26.550562\n",
      "Params: tensor([ 2.4666, -0.8822])\n",
      "Grad:  tensor([-0.4931,  2.7898])\n",
      "Epoch 3840 Loss 26.549757\n",
      "Params: tensor([ 2.4667, -0.8824])\n",
      "Grad:  tensor([-0.4931,  2.7897])\n",
      "Epoch 3841 Loss 26.548952\n",
      "Params: tensor([ 2.4667, -0.8827])\n",
      "Grad:  tensor([-0.4931,  2.7897])\n",
      "Epoch 3842 Loss 26.548155\n",
      "Params: tensor([ 2.4668, -0.8830])\n",
      "Grad:  tensor([-0.4931,  2.7896])\n",
      "Epoch 3843 Loss 26.547350\n",
      "Params: tensor([ 2.4668, -0.8833])\n",
      "Grad:  tensor([-0.4931,  2.7896])\n",
      "Epoch 3844 Loss 26.546547\n",
      "Params: tensor([ 2.4669, -0.8836])\n",
      "Grad:  tensor([-0.4931,  2.7895])\n",
      "Epoch 3845 Loss 26.545744\n",
      "Params: tensor([ 2.4669, -0.8838])\n",
      "Grad:  tensor([-0.4931,  2.7895])\n",
      "Epoch 3846 Loss 26.544945\n",
      "Params: tensor([ 2.4670, -0.8841])\n",
      "Grad:  tensor([-0.4930,  2.7894])\n",
      "Epoch 3847 Loss 26.544136\n",
      "Params: tensor([ 2.4670, -0.8844])\n",
      "Grad:  tensor([-0.4930,  2.7894])\n",
      "Epoch 3848 Loss 26.543335\n",
      "Params: tensor([ 2.4671, -0.8847])\n",
      "Grad:  tensor([-0.4930,  2.7893])\n",
      "Epoch 3849 Loss 26.542536\n",
      "Params: tensor([ 2.4671, -0.8849])\n",
      "Grad:  tensor([-0.4930,  2.7893])\n",
      "Epoch 3850 Loss 26.541735\n",
      "Params: tensor([ 2.4672, -0.8852])\n",
      "Grad:  tensor([-0.4930,  2.7892])\n",
      "Epoch 3851 Loss 26.540930\n",
      "Params: tensor([ 2.4672, -0.8855])\n",
      "Grad:  tensor([-0.4930,  2.7892])\n",
      "Epoch 3852 Loss 26.540129\n",
      "Params: tensor([ 2.4673, -0.8858])\n",
      "Grad:  tensor([-0.4930,  2.7892])\n",
      "Epoch 3853 Loss 26.539330\n",
      "Params: tensor([ 2.4673, -0.8861])\n",
      "Grad:  tensor([-0.4930,  2.7891])\n",
      "Epoch 3854 Loss 26.538527\n",
      "Params: tensor([ 2.4673, -0.8863])\n",
      "Grad:  tensor([-0.4929,  2.7891])\n",
      "Epoch 3855 Loss 26.537720\n",
      "Params: tensor([ 2.4674, -0.8866])\n",
      "Grad:  tensor([-0.4929,  2.7890])\n",
      "Epoch 3856 Loss 26.536919\n",
      "Params: tensor([ 2.4674, -0.8869])\n",
      "Grad:  tensor([-0.4929,  2.7890])\n",
      "Epoch 3857 Loss 26.536116\n",
      "Params: tensor([ 2.4675, -0.8872])\n",
      "Grad:  tensor([-0.4929,  2.7889])\n",
      "Epoch 3858 Loss 26.535311\n",
      "Params: tensor([ 2.4675, -0.8875])\n",
      "Grad:  tensor([-0.4929,  2.7889])\n",
      "Epoch 3859 Loss 26.534510\n",
      "Params: tensor([ 2.4676, -0.8877])\n",
      "Grad:  tensor([-0.4929,  2.7888])\n",
      "Epoch 3860 Loss 26.533709\n",
      "Params: tensor([ 2.4676, -0.8880])\n",
      "Grad:  tensor([-0.4929,  2.7888])\n",
      "Epoch 3861 Loss 26.532911\n",
      "Params: tensor([ 2.4677, -0.8883])\n",
      "Grad:  tensor([-0.4929,  2.7887])\n",
      "Epoch 3862 Loss 26.532106\n",
      "Params: tensor([ 2.4677, -0.8886])\n",
      "Grad:  tensor([-0.4928,  2.7887])\n",
      "Epoch 3863 Loss 26.531303\n",
      "Params: tensor([ 2.4678, -0.8889])\n",
      "Grad:  tensor([-0.4928,  2.7886])\n",
      "Epoch 3864 Loss 26.530504\n",
      "Params: tensor([ 2.4678, -0.8891])\n",
      "Grad:  tensor([-0.4928,  2.7886])\n",
      "Epoch 3865 Loss 26.529699\n",
      "Params: tensor([ 2.4679, -0.8894])\n",
      "Grad:  tensor([-0.4928,  2.7885])\n",
      "Epoch 3866 Loss 26.528898\n",
      "Params: tensor([ 2.4679, -0.8897])\n",
      "Grad:  tensor([-0.4928,  2.7885])\n",
      "Epoch 3867 Loss 26.528099\n",
      "Params: tensor([ 2.4680, -0.8900])\n",
      "Grad:  tensor([-0.4928,  2.7885])\n",
      "Epoch 3868 Loss 26.527294\n",
      "Params: tensor([ 2.4680, -0.8902])\n",
      "Grad:  tensor([-0.4928,  2.7884])\n",
      "Epoch 3869 Loss 26.526489\n",
      "Params: tensor([ 2.4681, -0.8905])\n",
      "Grad:  tensor([-0.4927,  2.7884])\n",
      "Epoch 3870 Loss 26.525684\n",
      "Params: tensor([ 2.4681, -0.8908])\n",
      "Grad:  tensor([-0.4927,  2.7883])\n",
      "Epoch 3871 Loss 26.524889\n",
      "Params: tensor([ 2.4682, -0.8911])\n",
      "Grad:  tensor([-0.4927,  2.7883])\n",
      "Epoch 3872 Loss 26.524086\n",
      "Params: tensor([ 2.4682, -0.8914])\n",
      "Grad:  tensor([-0.4927,  2.7882])\n",
      "Epoch 3873 Loss 26.523287\n",
      "Params: tensor([ 2.4683, -0.8916])\n",
      "Grad:  tensor([-0.4927,  2.7882])\n",
      "Epoch 3874 Loss 26.522486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([ 2.4683, -0.8919])\n",
      "Grad:  tensor([-0.4927,  2.7881])\n",
      "Epoch 3875 Loss 26.521681\n",
      "Params: tensor([ 2.4684, -0.8922])\n",
      "Grad:  tensor([-0.4927,  2.7881])\n",
      "Epoch 3876 Loss 26.520882\n",
      "Params: tensor([ 2.4684, -0.8925])\n",
      "Grad:  tensor([-0.4926,  2.7880])\n",
      "Epoch 3877 Loss 26.520081\n",
      "Params: tensor([ 2.4685, -0.8928])\n",
      "Grad:  tensor([-0.4926,  2.7880])\n",
      "Epoch 3878 Loss 26.519281\n",
      "Params: tensor([ 2.4685, -0.8930])\n",
      "Grad:  tensor([-0.4926,  2.7879])\n",
      "Epoch 3879 Loss 26.518476\n",
      "Params: tensor([ 2.4686, -0.8933])\n",
      "Grad:  tensor([-0.4926,  2.7879])\n",
      "Epoch 3880 Loss 26.517670\n",
      "Params: tensor([ 2.4686, -0.8936])\n",
      "Grad:  tensor([-0.4926,  2.7879])\n",
      "Epoch 3881 Loss 26.516874\n",
      "Params: tensor([ 2.4687, -0.8939])\n",
      "Grad:  tensor([-0.4926,  2.7878])\n",
      "Epoch 3882 Loss 26.516071\n",
      "Params: tensor([ 2.4687, -0.8941])\n",
      "Grad:  tensor([-0.4926,  2.7878])\n",
      "Epoch 3883 Loss 26.515270\n",
      "Params: tensor([ 2.4688, -0.8944])\n",
      "Grad:  tensor([-0.4925,  2.7877])\n",
      "Epoch 3884 Loss 26.514465\n",
      "Params: tensor([ 2.4688, -0.8947])\n",
      "Grad:  tensor([-0.4925,  2.7877])\n",
      "Epoch 3885 Loss 26.513670\n",
      "Params: tensor([ 2.4689, -0.8950])\n",
      "Grad:  tensor([-0.4925,  2.7876])\n",
      "Epoch 3886 Loss 26.512861\n",
      "Params: tensor([ 2.4689, -0.8953])\n",
      "Grad:  tensor([-0.4925,  2.7876])\n",
      "Epoch 3887 Loss 26.512060\n",
      "Params: tensor([ 2.4690, -0.8955])\n",
      "Grad:  tensor([-0.4925,  2.7875])\n",
      "Epoch 3888 Loss 26.511261\n",
      "Params: tensor([ 2.4690, -0.8958])\n",
      "Grad:  tensor([-0.4925,  2.7875])\n",
      "Epoch 3889 Loss 26.510462\n",
      "Params: tensor([ 2.4691, -0.8961])\n",
      "Grad:  tensor([-0.4925,  2.7874])\n",
      "Epoch 3890 Loss 26.509661\n",
      "Params: tensor([ 2.4691, -0.8964])\n",
      "Grad:  tensor([-0.4924,  2.7874])\n",
      "Epoch 3891 Loss 26.508858\n",
      "Params: tensor([ 2.4692, -0.8967])\n",
      "Grad:  tensor([-0.4924,  2.7873])\n",
      "Epoch 3892 Loss 26.508055\n",
      "Params: tensor([ 2.4692, -0.8969])\n",
      "Grad:  tensor([-0.4924,  2.7873])\n",
      "Epoch 3893 Loss 26.507257\n",
      "Params: tensor([ 2.4693, -0.8972])\n",
      "Grad:  tensor([-0.4924,  2.7873])\n",
      "Epoch 3894 Loss 26.506453\n",
      "Params: tensor([ 2.4693, -0.8975])\n",
      "Grad:  tensor([-0.4924,  2.7872])\n",
      "Epoch 3895 Loss 26.505651\n",
      "Params: tensor([ 2.4694, -0.8978])\n",
      "Grad:  tensor([-0.4924,  2.7872])\n",
      "Epoch 3896 Loss 26.504847\n",
      "Params: tensor([ 2.4694, -0.8981])\n",
      "Grad:  tensor([-0.4923,  2.7871])\n",
      "Epoch 3897 Loss 26.504057\n",
      "Params: tensor([ 2.4695, -0.8983])\n",
      "Grad:  tensor([-0.4923,  2.7871])\n",
      "Epoch 3898 Loss 26.503252\n",
      "Params: tensor([ 2.4695, -0.8986])\n",
      "Grad:  tensor([-0.4923,  2.7870])\n",
      "Epoch 3899 Loss 26.502455\n",
      "Params: tensor([ 2.4696, -0.8989])\n",
      "Grad:  tensor([-0.4923,  2.7870])\n",
      "Epoch 3900 Loss 26.501650\n",
      "Params: tensor([ 2.4696, -0.8992])\n",
      "Grad:  tensor([-0.4923,  2.7869])\n",
      "Epoch 3901 Loss 26.500849\n",
      "Params: tensor([ 2.4697, -0.8994])\n",
      "Grad:  tensor([-0.4923,  2.7869])\n",
      "Epoch 3902 Loss 26.500048\n",
      "Params: tensor([ 2.4697, -0.8997])\n",
      "Grad:  tensor([-0.4923,  2.7868])\n",
      "Epoch 3903 Loss 26.499243\n",
      "Params: tensor([ 2.4698, -0.9000])\n",
      "Grad:  tensor([-0.4923,  2.7868])\n",
      "Epoch 3904 Loss 26.498444\n",
      "Params: tensor([ 2.4698, -0.9003])\n",
      "Grad:  tensor([-0.4923,  2.7867])\n",
      "Epoch 3905 Loss 26.497644\n",
      "Params: tensor([ 2.4699, -0.9006])\n",
      "Grad:  tensor([-0.4923,  2.7867])\n",
      "Epoch 3906 Loss 26.496849\n",
      "Params: tensor([ 2.4699, -0.9008])\n",
      "Grad:  tensor([-0.4923,  2.7866])\n",
      "Epoch 3907 Loss 26.496044\n",
      "Params: tensor([ 2.4700, -0.9011])\n",
      "Grad:  tensor([-0.4923,  2.7866])\n",
      "Epoch 3908 Loss 26.495241\n",
      "Params: tensor([ 2.4700, -0.9014])\n",
      "Grad:  tensor([-0.4923,  2.7865])\n",
      "Epoch 3909 Loss 26.494444\n",
      "Params: tensor([ 2.4701, -0.9017])\n",
      "Grad:  tensor([-0.4923,  2.7865])\n",
      "Epoch 3910 Loss 26.493639\n",
      "Params: tensor([ 2.4701, -0.9020])\n",
      "Grad:  tensor([-0.4923,  2.7864])\n",
      "Epoch 3911 Loss 26.492840\n",
      "Params: tensor([ 2.4702, -0.9022])\n",
      "Grad:  tensor([-0.4923,  2.7864])\n",
      "Epoch 3912 Loss 26.492037\n",
      "Params: tensor([ 2.4702, -0.9025])\n",
      "Grad:  tensor([-0.4923,  2.7863])\n",
      "Epoch 3913 Loss 26.491236\n",
      "Params: tensor([ 2.4703, -0.9028])\n",
      "Grad:  tensor([-0.4923,  2.7863])\n",
      "Epoch 3914 Loss 26.490440\n",
      "Params: tensor([ 2.4703, -0.9031])\n",
      "Grad:  tensor([-0.4923,  2.7862])\n",
      "Epoch 3915 Loss 26.489635\n",
      "Params: tensor([ 2.4704, -0.9033])\n",
      "Grad:  tensor([-0.4923,  2.7862])\n",
      "Epoch 3916 Loss 26.488842\n",
      "Params: tensor([ 2.4704, -0.9036])\n",
      "Grad:  tensor([-0.4923,  2.7861])\n",
      "Epoch 3917 Loss 26.488037\n",
      "Params: tensor([ 2.4705, -0.9039])\n",
      "Grad:  tensor([-0.4923,  2.7861])\n",
      "Epoch 3918 Loss 26.487238\n",
      "Params: tensor([ 2.4705, -0.9042])\n",
      "Grad:  tensor([-0.4923,  2.7860])\n",
      "Epoch 3919 Loss 26.486439\n",
      "Params: tensor([ 2.4706, -0.9045])\n",
      "Grad:  tensor([-0.4923,  2.7860])\n",
      "Epoch 3920 Loss 26.485640\n",
      "Params: tensor([ 2.4706, -0.9047])\n",
      "Grad:  tensor([-0.4923,  2.7860])\n",
      "Epoch 3921 Loss 26.484835\n",
      "Params: tensor([ 2.4707, -0.9050])\n",
      "Grad:  tensor([-0.4923,  2.7859])\n",
      "Epoch 3922 Loss 26.484034\n",
      "Params: tensor([ 2.4707, -0.9053])\n",
      "Grad:  tensor([-0.4923,  2.7859])\n",
      "Epoch 3923 Loss 26.483234\n",
      "Params: tensor([ 2.4707, -0.9056])\n",
      "Grad:  tensor([-0.4923,  2.7858])\n",
      "Epoch 3924 Loss 26.482435\n",
      "Params: tensor([ 2.4708, -0.9059])\n",
      "Grad:  tensor([-0.4923,  2.7858])\n",
      "Epoch 3925 Loss 26.481636\n",
      "Params: tensor([ 2.4708, -0.9061])\n",
      "Grad:  tensor([-0.4923,  2.7857])\n",
      "Epoch 3926 Loss 26.480837\n",
      "Params: tensor([ 2.4709, -0.9064])\n",
      "Grad:  tensor([-0.4923,  2.7857])\n",
      "Epoch 3927 Loss 26.480034\n",
      "Params: tensor([ 2.4709, -0.9067])\n",
      "Grad:  tensor([-0.4923,  2.7856])\n",
      "Epoch 3928 Loss 26.479235\n",
      "Params: tensor([ 2.4710, -0.9070])\n",
      "Grad:  tensor([-0.4923,  2.7856])\n",
      "Epoch 3929 Loss 26.478436\n",
      "Params: tensor([ 2.4710, -0.9072])\n",
      "Grad:  tensor([-0.4923,  2.7855])\n",
      "Epoch 3930 Loss 26.477633\n",
      "Params: tensor([ 2.4711, -0.9075])\n",
      "Grad:  tensor([-0.4923,  2.7855])\n",
      "Epoch 3931 Loss 26.476835\n",
      "Params: tensor([ 2.4711, -0.9078])\n",
      "Grad:  tensor([-0.4923,  2.7854])\n",
      "Epoch 3932 Loss 26.476030\n",
      "Params: tensor([ 2.4712, -0.9081])\n",
      "Grad:  tensor([-0.4923,  2.7854])\n",
      "Epoch 3933 Loss 26.475237\n",
      "Params: tensor([ 2.4712, -0.9084])\n",
      "Grad:  tensor([-0.4923,  2.7853])\n",
      "Epoch 3934 Loss 26.474434\n",
      "Params: tensor([ 2.4713, -0.9086])\n",
      "Grad:  tensor([-0.4923,  2.7853])\n",
      "Epoch 3935 Loss 26.473635\n",
      "Params: tensor([ 2.4713, -0.9089])\n",
      "Grad:  tensor([-0.4923,  2.7852])\n",
      "Epoch 3936 Loss 26.472832\n",
      "Params: tensor([ 2.4714, -0.9092])\n",
      "Grad:  tensor([-0.4922,  2.7852])\n",
      "Epoch 3937 Loss 26.472038\n",
      "Params: tensor([ 2.4714, -0.9095])\n",
      "Grad:  tensor([-0.4922,  2.7851])\n",
      "Epoch 3938 Loss 26.471239\n",
      "Params: tensor([ 2.4715, -0.9098])\n",
      "Grad:  tensor([-0.4922,  2.7851])\n",
      "Epoch 3939 Loss 26.470436\n",
      "Params: tensor([ 2.4715, -0.9100])\n",
      "Grad:  tensor([-0.4922,  2.7850])\n",
      "Epoch 3940 Loss 26.469637\n",
      "Params: tensor([ 2.4716, -0.9103])\n",
      "Grad:  tensor([-0.4922,  2.7850])\n",
      "Epoch 3941 Loss 26.468840\n",
      "Params: tensor([ 2.4716, -0.9106])\n",
      "Grad:  tensor([-0.4922,  2.7849])\n",
      "Epoch 3942 Loss 26.468037\n",
      "Params: tensor([ 2.4717, -0.9109])\n",
      "Grad:  tensor([-0.4922,  2.7849])\n",
      "Epoch 3943 Loss 26.467241\n",
      "Params: tensor([ 2.4717, -0.9111])\n",
      "Grad:  tensor([-0.4922,  2.7848])\n",
      "Epoch 3944 Loss 26.466438\n",
      "Params: tensor([ 2.4718, -0.9114])\n",
      "Grad:  tensor([-0.4922,  2.7848])\n",
      "Epoch 3945 Loss 26.465639\n",
      "Params: tensor([ 2.4718, -0.9117])\n",
      "Grad:  tensor([-0.4922,  2.7847])\n",
      "Epoch 3946 Loss 26.464836\n",
      "Params: tensor([ 2.4719, -0.9120])\n",
      "Grad:  tensor([-0.4922,  2.7847])\n",
      "Epoch 3947 Loss 26.464043\n",
      "Params: tensor([ 2.4719, -0.9123])\n",
      "Grad:  tensor([-0.4922,  2.7847])\n",
      "Epoch 3948 Loss 26.463243\n",
      "Params: tensor([ 2.4720, -0.9125])\n",
      "Grad:  tensor([-0.4922,  2.7846])\n",
      "Epoch 3949 Loss 26.462440\n",
      "Params: tensor([ 2.4720, -0.9128])\n",
      "Grad:  tensor([-0.4922,  2.7846])\n",
      "Epoch 3950 Loss 26.461637\n",
      "Params: tensor([ 2.4721, -0.9131])\n",
      "Grad:  tensor([-0.4922,  2.7845])\n",
      "Epoch 3951 Loss 26.460838\n",
      "Params: tensor([ 2.4721, -0.9134])\n",
      "Grad:  tensor([-0.4922,  2.7845])\n",
      "Epoch 3952 Loss 26.460039\n",
      "Params: tensor([ 2.4722, -0.9137])\n",
      "Grad:  tensor([-0.4922,  2.7844])\n",
      "Epoch 3953 Loss 26.459242\n",
      "Params: tensor([ 2.4722, -0.9139])\n",
      "Grad:  tensor([-0.4922,  2.7844])\n",
      "Epoch 3954 Loss 26.458443\n",
      "Params: tensor([ 2.4723, -0.9142])\n",
      "Grad:  tensor([-0.4922,  2.7843])\n",
      "Epoch 3955 Loss 26.457647\n",
      "Params: tensor([ 2.4723, -0.9145])\n",
      "Grad:  tensor([-0.4922,  2.7843])\n",
      "Epoch 3956 Loss 26.456841\n",
      "Params: tensor([ 2.4724, -0.9148])\n",
      "Grad:  tensor([-0.4922,  2.7842])\n",
      "Epoch 3957 Loss 26.456041\n",
      "Params: tensor([ 2.4724, -0.9150])\n",
      "Grad:  tensor([-0.4922,  2.7842])\n",
      "Epoch 3958 Loss 26.455244\n",
      "Params: tensor([ 2.4725, -0.9153])\n",
      "Grad:  tensor([-0.4922,  2.7841])\n",
      "Epoch 3959 Loss 26.454439\n",
      "Params: tensor([ 2.4725, -0.9156])\n",
      "Grad:  tensor([-0.4922,  2.7841])\n",
      "Epoch 3960 Loss 26.453642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([ 2.4726, -0.9159])\n",
      "Grad:  tensor([-0.4922,  2.7840])\n",
      "Epoch 3961 Loss 26.452848\n",
      "Params: tensor([ 2.4726, -0.9162])\n",
      "Grad:  tensor([-0.4922,  2.7840])\n",
      "Epoch 3962 Loss 26.452049\n",
      "Params: tensor([ 2.4727, -0.9164])\n",
      "Grad:  tensor([-0.4921,  2.7839])\n",
      "Epoch 3963 Loss 26.451250\n",
      "Params: tensor([ 2.4727, -0.9167])\n",
      "Grad:  tensor([-0.4921,  2.7839])\n",
      "Epoch 3964 Loss 26.450447\n",
      "Params: tensor([ 2.4728, -0.9170])\n",
      "Grad:  tensor([-0.4921,  2.7838])\n",
      "Epoch 3965 Loss 26.449644\n",
      "Params: tensor([ 2.4728, -0.9173])\n",
      "Grad:  tensor([-0.4921,  2.7838])\n",
      "Epoch 3966 Loss 26.448853\n",
      "Params: tensor([ 2.4729, -0.9175])\n",
      "Grad:  tensor([-0.4921,  2.7837])\n",
      "Epoch 3967 Loss 26.448051\n",
      "Params: tensor([ 2.4729, -0.9178])\n",
      "Grad:  tensor([-0.4921,  2.7837])\n",
      "Epoch 3968 Loss 26.447252\n",
      "Params: tensor([ 2.4730, -0.9181])\n",
      "Grad:  tensor([-0.4921,  2.7836])\n",
      "Epoch 3969 Loss 26.446449\n",
      "Params: tensor([ 2.4730, -0.9184])\n",
      "Grad:  tensor([-0.4921,  2.7836])\n",
      "Epoch 3970 Loss 26.445650\n",
      "Params: tensor([ 2.4731, -0.9187])\n",
      "Grad:  tensor([-0.4921,  2.7835])\n",
      "Epoch 3971 Loss 26.444857\n",
      "Params: tensor([ 2.4731, -0.9189])\n",
      "Grad:  tensor([-0.4921,  2.7835])\n",
      "Epoch 3972 Loss 26.444059\n",
      "Params: tensor([ 2.4732, -0.9192])\n",
      "Grad:  tensor([-0.4921,  2.7835])\n",
      "Epoch 3973 Loss 26.443260\n",
      "Params: tensor([ 2.4732, -0.9195])\n",
      "Grad:  tensor([-0.4921,  2.7834])\n",
      "Epoch 3974 Loss 26.442457\n",
      "Params: tensor([ 2.4733, -0.9198])\n",
      "Grad:  tensor([-0.4921,  2.7834])\n",
      "Epoch 3975 Loss 26.441664\n",
      "Params: tensor([ 2.4733, -0.9201])\n",
      "Grad:  tensor([-0.4921,  2.7833])\n",
      "Epoch 3976 Loss 26.440859\n",
      "Params: tensor([ 2.4734, -0.9203])\n",
      "Grad:  tensor([-0.4921,  2.7833])\n",
      "Epoch 3977 Loss 26.440063\n",
      "Params: tensor([ 2.4734, -0.9206])\n",
      "Grad:  tensor([-0.4921,  2.7832])\n",
      "Epoch 3978 Loss 26.439262\n",
      "Params: tensor([ 2.4734, -0.9209])\n",
      "Grad:  tensor([-0.4921,  2.7832])\n",
      "Epoch 3979 Loss 26.438465\n",
      "Params: tensor([ 2.4735, -0.9212])\n",
      "Grad:  tensor([-0.4921,  2.7831])\n",
      "Epoch 3980 Loss 26.437664\n",
      "Params: tensor([ 2.4735, -0.9214])\n",
      "Grad:  tensor([-0.4921,  2.7831])\n",
      "Epoch 3981 Loss 26.436871\n",
      "Params: tensor([ 2.4736, -0.9217])\n",
      "Grad:  tensor([-0.4921,  2.7830])\n",
      "Epoch 3982 Loss 26.436068\n",
      "Params: tensor([ 2.4736, -0.9220])\n",
      "Grad:  tensor([-0.4920,  2.7830])\n",
      "Epoch 3983 Loss 26.435270\n",
      "Params: tensor([ 2.4737, -0.9223])\n",
      "Grad:  tensor([-0.4920,  2.7829])\n",
      "Epoch 3984 Loss 26.434473\n",
      "Params: tensor([ 2.4737, -0.9226])\n",
      "Grad:  tensor([-0.4920,  2.7829])\n",
      "Epoch 3985 Loss 26.433672\n",
      "Params: tensor([ 2.4738, -0.9228])\n",
      "Grad:  tensor([-0.4920,  2.7828])\n",
      "Epoch 3986 Loss 26.432873\n",
      "Params: tensor([ 2.4738, -0.9231])\n",
      "Grad:  tensor([-0.4920,  2.7828])\n",
      "Epoch 3987 Loss 26.432074\n",
      "Params: tensor([ 2.4739, -0.9234])\n",
      "Grad:  tensor([-0.4920,  2.7827])\n",
      "Epoch 3988 Loss 26.431273\n",
      "Params: tensor([ 2.4739, -0.9237])\n",
      "Grad:  tensor([-0.4920,  2.7827])\n",
      "Epoch 3989 Loss 26.430479\n",
      "Params: tensor([ 2.4740, -0.9240])\n",
      "Grad:  tensor([-0.4920,  2.7826])\n",
      "Epoch 3990 Loss 26.429682\n",
      "Params: tensor([ 2.4740, -0.9242])\n",
      "Grad:  tensor([-0.4920,  2.7826])\n",
      "Epoch 3991 Loss 26.428883\n",
      "Params: tensor([ 2.4741, -0.9245])\n",
      "Grad:  tensor([-0.4920,  2.7825])\n",
      "Epoch 3992 Loss 26.428082\n",
      "Params: tensor([ 2.4741, -0.9248])\n",
      "Grad:  tensor([-0.4920,  2.7825])\n",
      "Epoch 3993 Loss 26.427284\n",
      "Params: tensor([ 2.4742, -0.9251])\n",
      "Grad:  tensor([-0.4920,  2.7824])\n",
      "Epoch 3994 Loss 26.426485\n",
      "Params: tensor([ 2.4742, -0.9253])\n",
      "Grad:  tensor([-0.4920,  2.7824])\n",
      "Epoch 3995 Loss 26.425688\n",
      "Params: tensor([ 2.4743, -0.9256])\n",
      "Grad:  tensor([-0.4920,  2.7824])\n",
      "Epoch 3996 Loss 26.424889\n",
      "Params: tensor([ 2.4743, -0.9259])\n",
      "Grad:  tensor([-0.4920,  2.7823])\n",
      "Epoch 3997 Loss 26.424089\n",
      "Params: tensor([ 2.4744, -0.9262])\n",
      "Grad:  tensor([-0.4920,  2.7823])\n",
      "Epoch 3998 Loss 26.423292\n",
      "Params: tensor([ 2.4744, -0.9265])\n",
      "Grad:  tensor([-0.4919,  2.7822])\n",
      "Epoch 3999 Loss 26.422493\n",
      "Params: tensor([ 2.4745, -0.9267])\n",
      "Grad:  tensor([-0.4919,  2.7822])\n",
      "Epoch 4000 Loss 26.421694\n",
      "Params: tensor([ 2.4745, -0.9270])\n",
      "Grad:  tensor([-0.4919,  2.7821])\n",
      "Epoch 4001 Loss 26.420898\n",
      "Params: tensor([ 2.4746, -0.9273])\n",
      "Grad:  tensor([-0.4919,  2.7821])\n",
      "Epoch 4002 Loss 26.420103\n",
      "Params: tensor([ 2.4746, -0.9276])\n",
      "Grad:  tensor([-0.4919,  2.7820])\n",
      "Epoch 4003 Loss 26.419306\n",
      "Params: tensor([ 2.4747, -0.9278])\n",
      "Grad:  tensor([-0.4919,  2.7820])\n",
      "Epoch 4004 Loss 26.418501\n",
      "Params: tensor([ 2.4747, -0.9281])\n",
      "Grad:  tensor([-0.4919,  2.7819])\n",
      "Epoch 4005 Loss 26.417702\n",
      "Params: tensor([ 2.4748, -0.9284])\n",
      "Grad:  tensor([-0.4919,  2.7819])\n",
      "Epoch 4006 Loss 26.416906\n",
      "Params: tensor([ 2.4748, -0.9287])\n",
      "Grad:  tensor([-0.4919,  2.7818])\n",
      "Epoch 4007 Loss 26.416107\n",
      "Params: tensor([ 2.4749, -0.9290])\n",
      "Grad:  tensor([-0.4919,  2.7818])\n",
      "Epoch 4008 Loss 26.415314\n",
      "Params: tensor([ 2.4749, -0.9292])\n",
      "Grad:  tensor([-0.4919,  2.7817])\n",
      "Epoch 4009 Loss 26.414515\n",
      "Params: tensor([ 2.4750, -0.9295])\n",
      "Grad:  tensor([-0.4919,  2.7817])\n",
      "Epoch 4010 Loss 26.413719\n",
      "Params: tensor([ 2.4750, -0.9298])\n",
      "Grad:  tensor([-0.4919,  2.7816])\n",
      "Epoch 4011 Loss 26.412920\n",
      "Params: tensor([ 2.4751, -0.9301])\n",
      "Grad:  tensor([-0.4919,  2.7816])\n",
      "Epoch 4012 Loss 26.412123\n",
      "Params: tensor([ 2.4751, -0.9303])\n",
      "Grad:  tensor([-0.4919,  2.7815])\n",
      "Epoch 4013 Loss 26.411318\n",
      "Params: tensor([ 2.4752, -0.9306])\n",
      "Grad:  tensor([-0.4918,  2.7815])\n",
      "Epoch 4014 Loss 26.410528\n",
      "Params: tensor([ 2.4752, -0.9309])\n",
      "Grad:  tensor([-0.4918,  2.7814])\n",
      "Epoch 4015 Loss 26.409729\n",
      "Params: tensor([ 2.4753, -0.9312])\n",
      "Grad:  tensor([-0.4918,  2.7814])\n",
      "Epoch 4016 Loss 26.408928\n",
      "Params: tensor([ 2.4753, -0.9315])\n",
      "Grad:  tensor([-0.4918,  2.7814])\n",
      "Epoch 4017 Loss 26.408129\n",
      "Params: tensor([ 2.4754, -0.9317])\n",
      "Grad:  tensor([-0.4918,  2.7813])\n",
      "Epoch 4018 Loss 26.407341\n",
      "Params: tensor([ 2.4754, -0.9320])\n",
      "Grad:  tensor([-0.4918,  2.7813])\n",
      "Epoch 4019 Loss 26.406538\n",
      "Params: tensor([ 2.4755, -0.9323])\n",
      "Grad:  tensor([-0.4918,  2.7812])\n",
      "Epoch 4020 Loss 26.405739\n",
      "Params: tensor([ 2.4755, -0.9326])\n",
      "Grad:  tensor([-0.4918,  2.7812])\n",
      "Epoch 4021 Loss 26.404943\n",
      "Params: tensor([ 2.4756, -0.9329])\n",
      "Grad:  tensor([-0.4918,  2.7811])\n",
      "Epoch 4022 Loss 26.404142\n",
      "Params: tensor([ 2.4756, -0.9331])\n",
      "Grad:  tensor([-0.4918,  2.7811])\n",
      "Epoch 4023 Loss 26.403345\n",
      "Params: tensor([ 2.4757, -0.9334])\n",
      "Grad:  tensor([-0.4918,  2.7810])\n",
      "Epoch 4024 Loss 26.402546\n",
      "Params: tensor([ 2.4757, -0.9337])\n",
      "Grad:  tensor([-0.4918,  2.7810])\n",
      "Epoch 4025 Loss 26.401751\n",
      "Params: tensor([ 2.4758, -0.9340])\n",
      "Grad:  tensor([-0.4918,  2.7809])\n",
      "Epoch 4026 Loss 26.400951\n",
      "Params: tensor([ 2.4758, -0.9342])\n",
      "Grad:  tensor([-0.4918,  2.7809])\n",
      "Epoch 4027 Loss 26.400154\n",
      "Params: tensor([ 2.4759, -0.9345])\n",
      "Grad:  tensor([-0.4917,  2.7808])\n",
      "Epoch 4028 Loss 26.399359\n",
      "Params: tensor([ 2.4759, -0.9348])\n",
      "Grad:  tensor([-0.4917,  2.7808])\n",
      "Epoch 4029 Loss 26.398560\n",
      "Params: tensor([ 2.4760, -0.9351])\n",
      "Grad:  tensor([-0.4917,  2.7807])\n",
      "Epoch 4030 Loss 26.397766\n",
      "Params: tensor([ 2.4760, -0.9354])\n",
      "Grad:  tensor([-0.4917,  2.7807])\n",
      "Epoch 4031 Loss 26.396967\n",
      "Params: tensor([ 2.4761, -0.9356])\n",
      "Grad:  tensor([-0.4917,  2.7806])\n",
      "Epoch 4032 Loss 26.396168\n",
      "Params: tensor([ 2.4761, -0.9359])\n",
      "Grad:  tensor([-0.4917,  2.7806])\n",
      "Epoch 4033 Loss 26.395372\n",
      "Params: tensor([ 2.4762, -0.9362])\n",
      "Grad:  tensor([-0.4917,  2.7805])\n",
      "Epoch 4034 Loss 26.394575\n",
      "Params: tensor([ 2.4762, -0.9365])\n",
      "Grad:  tensor([-0.4917,  2.7805])\n",
      "Epoch 4035 Loss 26.393780\n",
      "Params: tensor([ 2.4762, -0.9367])\n",
      "Grad:  tensor([-0.4917,  2.7805])\n",
      "Epoch 4036 Loss 26.392981\n",
      "Params: tensor([ 2.4763, -0.9370])\n",
      "Grad:  tensor([-0.4917,  2.7804])\n",
      "Epoch 4037 Loss 26.392179\n",
      "Params: tensor([ 2.4763, -0.9373])\n",
      "Grad:  tensor([-0.4917,  2.7804])\n",
      "Epoch 4038 Loss 26.391386\n",
      "Params: tensor([ 2.4764, -0.9376])\n",
      "Grad:  tensor([-0.4917,  2.7803])\n",
      "Epoch 4039 Loss 26.390589\n",
      "Params: tensor([ 2.4764, -0.9379])\n",
      "Grad:  tensor([-0.4917,  2.7803])\n",
      "Epoch 4040 Loss 26.389790\n",
      "Params: tensor([ 2.4765, -0.9381])\n",
      "Grad:  tensor([-0.4916,  2.7802])\n",
      "Epoch 4041 Loss 26.388994\n",
      "Params: tensor([ 2.4765, -0.9384])\n",
      "Grad:  tensor([-0.4916,  2.7802])\n",
      "Epoch 4042 Loss 26.388197\n",
      "Params: tensor([ 2.4766, -0.9387])\n",
      "Grad:  tensor([-0.4916,  2.7801])\n",
      "Epoch 4043 Loss 26.387402\n",
      "Params: tensor([ 2.4766, -0.9390])\n",
      "Grad:  tensor([-0.4916,  2.7801])\n",
      "Epoch 4044 Loss 26.386608\n",
      "Params: tensor([ 2.4767, -0.9392])\n",
      "Grad:  tensor([-0.4916,  2.7800])\n",
      "Epoch 4045 Loss 26.385801\n",
      "Params: tensor([ 2.4767, -0.9395])\n",
      "Grad:  tensor([-0.4916,  2.7800])\n",
      "Epoch 4046 Loss 26.385008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([ 2.4768, -0.9398])\n",
      "Grad:  tensor([-0.4916,  2.7799])\n",
      "Epoch 4047 Loss 26.384216\n",
      "Params: tensor([ 2.4768, -0.9401])\n",
      "Grad:  tensor([-0.4916,  2.7799])\n",
      "Epoch 4048 Loss 26.383417\n",
      "Params: tensor([ 2.4769, -0.9404])\n",
      "Grad:  tensor([-0.4916,  2.7798])\n",
      "Epoch 4049 Loss 26.382618\n",
      "Params: tensor([ 2.4769, -0.9406])\n",
      "Grad:  tensor([-0.4916,  2.7798])\n",
      "Epoch 4050 Loss 26.381819\n",
      "Params: tensor([ 2.4770, -0.9409])\n",
      "Grad:  tensor([-0.4916,  2.7797])\n",
      "Epoch 4051 Loss 26.381025\n",
      "Params: tensor([ 2.4770, -0.9412])\n",
      "Grad:  tensor([-0.4915,  2.7797])\n",
      "Epoch 4052 Loss 26.380230\n",
      "Params: tensor([ 2.4771, -0.9415])\n",
      "Grad:  tensor([-0.4915,  2.7797])\n",
      "Epoch 4053 Loss 26.379427\n",
      "Params: tensor([ 2.4771, -0.9418])\n",
      "Grad:  tensor([-0.4915,  2.7796])\n",
      "Epoch 4054 Loss 26.378632\n",
      "Params: tensor([ 2.4772, -0.9420])\n",
      "Grad:  tensor([-0.4915,  2.7796])\n",
      "Epoch 4055 Loss 26.377840\n",
      "Params: tensor([ 2.4772, -0.9423])\n",
      "Grad:  tensor([-0.4915,  2.7795])\n",
      "Epoch 4056 Loss 26.377039\n",
      "Params: tensor([ 2.4773, -0.9426])\n",
      "Grad:  tensor([-0.4915,  2.7795])\n",
      "Epoch 4057 Loss 26.376245\n",
      "Params: tensor([ 2.4773, -0.9429])\n",
      "Grad:  tensor([-0.4915,  2.7794])\n",
      "Epoch 4058 Loss 26.375446\n",
      "Params: tensor([ 2.4774, -0.9431])\n",
      "Grad:  tensor([-0.4915,  2.7794])\n",
      "Epoch 4059 Loss 26.374653\n",
      "Params: tensor([ 2.4774, -0.9434])\n",
      "Grad:  tensor([-0.4915,  2.7793])\n",
      "Epoch 4060 Loss 26.373852\n",
      "Params: tensor([ 2.4775, -0.9437])\n",
      "Grad:  tensor([-0.4915,  2.7793])\n",
      "Epoch 4061 Loss 26.373055\n",
      "Params: tensor([ 2.4775, -0.9440])\n",
      "Grad:  tensor([-0.4915,  2.7792])\n",
      "Epoch 4062 Loss 26.372259\n",
      "Params: tensor([ 2.4776, -0.9443])\n",
      "Grad:  tensor([-0.4914,  2.7792])\n",
      "Epoch 4063 Loss 26.371460\n",
      "Params: tensor([ 2.4776, -0.9445])\n",
      "Grad:  tensor([-0.4914,  2.7791])\n",
      "Epoch 4064 Loss 26.370668\n",
      "Params: tensor([ 2.4777, -0.9448])\n",
      "Grad:  tensor([-0.4914,  2.7791])\n",
      "Epoch 4065 Loss 26.369871\n",
      "Params: tensor([ 2.4777, -0.9451])\n",
      "Grad:  tensor([-0.4914,  2.7790])\n",
      "Epoch 4066 Loss 26.369081\n",
      "Params: tensor([ 2.4778, -0.9454])\n",
      "Grad:  tensor([-0.4914,  2.7790])\n",
      "Epoch 4067 Loss 26.368279\n",
      "Params: tensor([ 2.4778, -0.9456])\n",
      "Grad:  tensor([-0.4914,  2.7789])\n",
      "Epoch 4068 Loss 26.367485\n",
      "Params: tensor([ 2.4779, -0.9459])\n",
      "Grad:  tensor([-0.4914,  2.7789])\n",
      "Epoch 4069 Loss 26.366688\n",
      "Params: tensor([ 2.4779, -0.9462])\n",
      "Grad:  tensor([-0.4914,  2.7789])\n",
      "Epoch 4070 Loss 26.365889\n",
      "Params: tensor([ 2.4780, -0.9465])\n",
      "Grad:  tensor([-0.4914,  2.7788])\n",
      "Epoch 4071 Loss 26.365095\n",
      "Params: tensor([ 2.4780, -0.9468])\n",
      "Grad:  tensor([-0.4914,  2.7788])\n",
      "Epoch 4072 Loss 26.364296\n",
      "Params: tensor([ 2.4781, -0.9470])\n",
      "Grad:  tensor([-0.4913,  2.7787])\n",
      "Epoch 4073 Loss 26.363497\n",
      "Params: tensor([ 2.4781, -0.9473])\n",
      "Grad:  tensor([-0.4913,  2.7787])\n",
      "Epoch 4074 Loss 26.362703\n",
      "Params: tensor([ 2.4782, -0.9476])\n",
      "Grad:  tensor([-0.4913,  2.7786])\n",
      "Epoch 4075 Loss 26.361910\n",
      "Params: tensor([ 2.4782, -0.9479])\n",
      "Grad:  tensor([-0.4913,  2.7786])\n",
      "Epoch 4076 Loss 26.361109\n",
      "Params: tensor([ 2.4783, -0.9481])\n",
      "Grad:  tensor([-0.4913,  2.7785])\n",
      "Epoch 4077 Loss 26.360315\n",
      "Params: tensor([ 2.4783, -0.9484])\n",
      "Grad:  tensor([-0.4913,  2.7785])\n",
      "Epoch 4078 Loss 26.359520\n",
      "Params: tensor([ 2.4784, -0.9487])\n",
      "Grad:  tensor([-0.4913,  2.7784])\n",
      "Epoch 4079 Loss 26.358723\n",
      "Params: tensor([ 2.4784, -0.9490])\n",
      "Grad:  tensor([-0.4913,  2.7784])\n",
      "Epoch 4080 Loss 26.357927\n",
      "Params: tensor([ 2.4785, -0.9493])\n",
      "Grad:  tensor([-0.4913,  2.7783])\n",
      "Epoch 4081 Loss 26.357130\n",
      "Params: tensor([ 2.4785, -0.9495])\n",
      "Grad:  tensor([-0.4913,  2.7783])\n",
      "Epoch 4082 Loss 26.356329\n",
      "Params: tensor([ 2.4786, -0.9498])\n",
      "Grad:  tensor([-0.4912,  2.7782])\n",
      "Epoch 4083 Loss 26.355541\n",
      "Params: tensor([ 2.4786, -0.9501])\n",
      "Grad:  tensor([-0.4912,  2.7782])\n",
      "Epoch 4084 Loss 26.354748\n",
      "Params: tensor([ 2.4787, -0.9504])\n",
      "Grad:  tensor([-0.4912,  2.7781])\n",
      "Epoch 4085 Loss 26.353945\n",
      "Params: tensor([ 2.4787, -0.9506])\n",
      "Grad:  tensor([-0.4912,  2.7781])\n",
      "Epoch 4086 Loss 26.353151\n",
      "Params: tensor([ 2.4788, -0.9509])\n",
      "Grad:  tensor([-0.4912,  2.7781])\n",
      "Epoch 4087 Loss 26.352362\n",
      "Params: tensor([ 2.4788, -0.9512])\n",
      "Grad:  tensor([-0.4912,  2.7780])\n",
      "Epoch 4088 Loss 26.351557\n",
      "Params: tensor([ 2.4789, -0.9515])\n",
      "Grad:  tensor([-0.4912,  2.7780])\n",
      "Epoch 4089 Loss 26.350761\n",
      "Params: tensor([ 2.4789, -0.9518])\n",
      "Grad:  tensor([-0.4912,  2.7779])\n",
      "Epoch 4090 Loss 26.349968\n",
      "Params: tensor([ 2.4790, -0.9520])\n",
      "Grad:  tensor([-0.4912,  2.7779])\n",
      "Epoch 4091 Loss 26.349169\n",
      "Params: tensor([ 2.4790, -0.9523])\n",
      "Grad:  tensor([-0.4912,  2.7778])\n",
      "Epoch 4092 Loss 26.348377\n",
      "Params: tensor([ 2.4790, -0.9526])\n",
      "Grad:  tensor([-0.4911,  2.7778])\n",
      "Epoch 4093 Loss 26.347588\n",
      "Params: tensor([ 2.4791, -0.9529])\n",
      "Grad:  tensor([-0.4911,  2.7777])\n",
      "Epoch 4094 Loss 26.346788\n",
      "Params: tensor([ 2.4791, -0.9531])\n",
      "Grad:  tensor([-0.4911,  2.7777])\n",
      "Epoch 4095 Loss 26.345991\n",
      "Params: tensor([ 2.4792, -0.9534])\n",
      "Grad:  tensor([-0.4911,  2.7776])\n",
      "Epoch 4096 Loss 26.345192\n",
      "Params: tensor([ 2.4792, -0.9537])\n",
      "Grad:  tensor([-0.4911,  2.7776])\n",
      "Epoch 4097 Loss 26.344398\n",
      "Params: tensor([ 2.4793, -0.9540])\n",
      "Grad:  tensor([-0.4911,  2.7775])\n",
      "Epoch 4098 Loss 26.343605\n",
      "Params: tensor([ 2.4793, -0.9543])\n",
      "Grad:  tensor([-0.4911,  2.7775])\n",
      "Epoch 4099 Loss 26.342806\n",
      "Params: tensor([ 2.4794, -0.9545])\n",
      "Grad:  tensor([-0.4911,  2.7774])\n",
      "Epoch 4100 Loss 26.342010\n",
      "Params: tensor([ 2.4794, -0.9548])\n",
      "Grad:  tensor([-0.4911,  2.7774])\n",
      "Epoch 4101 Loss 26.341219\n",
      "Params: tensor([ 2.4795, -0.9551])\n",
      "Grad:  tensor([-0.4910,  2.7774])\n",
      "Epoch 4102 Loss 26.340424\n",
      "Params: tensor([ 2.4795, -0.9554])\n",
      "Grad:  tensor([-0.4910,  2.7773])\n",
      "Epoch 4103 Loss 26.339628\n",
      "Params: tensor([ 2.4796, -0.9556])\n",
      "Grad:  tensor([-0.4910,  2.7773])\n",
      "Epoch 4104 Loss 26.338829\n",
      "Params: tensor([ 2.4796, -0.9559])\n",
      "Grad:  tensor([-0.4910,  2.7772])\n",
      "Epoch 4105 Loss 26.338032\n",
      "Params: tensor([ 2.4797, -0.9562])\n",
      "Grad:  tensor([-0.4910,  2.7772])\n",
      "Epoch 4106 Loss 26.337244\n",
      "Params: tensor([ 2.4797, -0.9565])\n",
      "Grad:  tensor([-0.4910,  2.7771])\n",
      "Epoch 4107 Loss 26.336445\n",
      "Params: tensor([ 2.4798, -0.9568])\n",
      "Grad:  tensor([-0.4910,  2.7771])\n",
      "Epoch 4108 Loss 26.335649\n",
      "Params: tensor([ 2.4798, -0.9570])\n",
      "Grad:  tensor([-0.4910,  2.7770])\n",
      "Epoch 4109 Loss 26.334850\n",
      "Params: tensor([ 2.4799, -0.9573])\n",
      "Grad:  tensor([-0.4909,  2.7770])\n",
      "Epoch 4110 Loss 26.334057\n",
      "Params: tensor([ 2.4799, -0.9576])\n",
      "Grad:  tensor([-0.4909,  2.7769])\n",
      "Epoch 4111 Loss 26.333269\n",
      "Params: tensor([ 2.4800, -0.9579])\n",
      "Grad:  tensor([-0.4909,  2.7769])\n",
      "Epoch 4112 Loss 26.332464\n",
      "Params: tensor([ 2.4800, -0.9581])\n",
      "Grad:  tensor([-0.4909,  2.7768])\n",
      "Epoch 4113 Loss 26.331673\n",
      "Params: tensor([ 2.4801, -0.9584])\n",
      "Grad:  tensor([-0.4909,  2.7768])\n",
      "Epoch 4114 Loss 26.330877\n",
      "Params: tensor([ 2.4801, -0.9587])\n",
      "Grad:  tensor([-0.4909,  2.7767])\n",
      "Epoch 4115 Loss 26.330084\n",
      "Params: tensor([ 2.4802, -0.9590])\n",
      "Grad:  tensor([-0.4909,  2.7767])\n",
      "Epoch 4116 Loss 26.329285\n",
      "Params: tensor([ 2.4802, -0.9593])\n",
      "Grad:  tensor([-0.4909,  2.7767])\n",
      "Epoch 4117 Loss 26.328491\n",
      "Params: tensor([ 2.4803, -0.9595])\n",
      "Grad:  tensor([-0.4909,  2.7766])\n",
      "Epoch 4118 Loss 26.327698\n",
      "Params: tensor([ 2.4803, -0.9598])\n",
      "Grad:  tensor([-0.4908,  2.7766])\n",
      "Epoch 4119 Loss 26.326902\n",
      "Params: tensor([ 2.4804, -0.9601])\n",
      "Grad:  tensor([-0.4908,  2.7765])\n",
      "Epoch 4120 Loss 26.326109\n",
      "Params: tensor([ 2.4804, -0.9604])\n",
      "Grad:  tensor([-0.4908,  2.7765])\n",
      "Epoch 4121 Loss 26.325312\n",
      "Params: tensor([ 2.4805, -0.9606])\n",
      "Grad:  tensor([-0.4908,  2.7764])\n",
      "Epoch 4122 Loss 26.324516\n",
      "Params: tensor([ 2.4805, -0.9609])\n",
      "Grad:  tensor([-0.4908,  2.7764])\n",
      "Epoch 4123 Loss 26.323719\n",
      "Params: tensor([ 2.4806, -0.9612])\n",
      "Grad:  tensor([-0.4908,  2.7763])\n",
      "Epoch 4124 Loss 26.322926\n",
      "Params: tensor([ 2.4806, -0.9615])\n",
      "Grad:  tensor([-0.4908,  2.7763])\n",
      "Epoch 4125 Loss 26.322138\n",
      "Params: tensor([ 2.4807, -0.9618])\n",
      "Grad:  tensor([-0.4908,  2.7762])\n",
      "Epoch 4126 Loss 26.321337\n",
      "Params: tensor([ 2.4807, -0.9620])\n",
      "Grad:  tensor([-0.4907,  2.7762])\n",
      "Epoch 4127 Loss 26.320545\n",
      "Params: tensor([ 2.4808, -0.9623])\n",
      "Grad:  tensor([-0.4907,  2.7761])\n",
      "Epoch 4128 Loss 26.319746\n",
      "Params: tensor([ 2.4808, -0.9626])\n",
      "Grad:  tensor([-0.4907,  2.7761])\n",
      "Epoch 4129 Loss 26.318956\n",
      "Params: tensor([ 2.4809, -0.9629])\n",
      "Grad:  tensor([-0.4907,  2.7761])\n",
      "Epoch 4130 Loss 26.318153\n",
      "Params: tensor([ 2.4809, -0.9631])\n",
      "Grad:  tensor([-0.4907,  2.7760])\n",
      "Epoch 4131 Loss 26.317364\n",
      "Params: tensor([ 2.4810, -0.9634])\n",
      "Grad:  tensor([-0.4907,  2.7760])\n",
      "Epoch 4132 Loss 26.316566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([ 2.4810, -0.9637])\n",
      "Grad:  tensor([-0.4907,  2.7759])\n",
      "Epoch 4133 Loss 26.315777\n",
      "Params: tensor([ 2.4811, -0.9640])\n",
      "Grad:  tensor([-0.4907,  2.7759])\n",
      "Epoch 4134 Loss 26.314980\n",
      "Params: tensor([ 2.4811, -0.9642])\n",
      "Grad:  tensor([-0.4906,  2.7758])\n",
      "Epoch 4135 Loss 26.314186\n",
      "Params: tensor([ 2.4812, -0.9645])\n",
      "Grad:  tensor([-0.4906,  2.7758])\n",
      "Epoch 4136 Loss 26.313391\n",
      "Params: tensor([ 2.4812, -0.9648])\n",
      "Grad:  tensor([-0.4906,  2.7757])\n",
      "Epoch 4137 Loss 26.312593\n",
      "Params: tensor([ 2.4813, -0.9651])\n",
      "Grad:  tensor([-0.4906,  2.7757])\n",
      "Epoch 4138 Loss 26.311798\n",
      "Params: tensor([ 2.4813, -0.9654])\n",
      "Grad:  tensor([-0.4906,  2.7756])\n",
      "Epoch 4139 Loss 26.311010\n",
      "Params: tensor([ 2.4814, -0.9656])\n",
      "Grad:  tensor([-0.4906,  2.7756])\n",
      "Epoch 4140 Loss 26.310213\n",
      "Params: tensor([ 2.4814, -0.9659])\n",
      "Grad:  tensor([-0.4906,  2.7755])\n",
      "Epoch 4141 Loss 26.309418\n",
      "Params: tensor([ 2.4815, -0.9662])\n",
      "Grad:  tensor([-0.4905,  2.7755])\n",
      "Epoch 4142 Loss 26.308624\n",
      "Params: tensor([ 2.4815, -0.9665])\n",
      "Grad:  tensor([-0.4905,  2.7754])\n",
      "Epoch 4143 Loss 26.307829\n",
      "Params: tensor([ 2.4816, -0.9667])\n",
      "Grad:  tensor([-0.4905,  2.7754])\n",
      "Epoch 4144 Loss 26.307035\n",
      "Params: tensor([ 2.4816, -0.9670])\n",
      "Grad:  tensor([-0.4905,  2.7754])\n",
      "Epoch 4145 Loss 26.306238\n",
      "Params: tensor([ 2.4817, -0.9673])\n",
      "Grad:  tensor([-0.4905,  2.7753])\n",
      "Epoch 4146 Loss 26.305445\n",
      "Params: tensor([ 2.4817, -0.9676])\n",
      "Grad:  tensor([-0.4905,  2.7753])\n",
      "Epoch 4147 Loss 26.304651\n",
      "Params: tensor([ 2.4818, -0.9679])\n",
      "Grad:  tensor([-0.4905,  2.7752])\n",
      "Epoch 4148 Loss 26.303858\n",
      "Params: tensor([ 2.4818, -0.9681])\n",
      "Grad:  tensor([-0.4905,  2.7752])\n",
      "Epoch 4149 Loss 26.303062\n",
      "Params: tensor([ 2.4818, -0.9684])\n",
      "Grad:  tensor([-0.4904,  2.7751])\n",
      "Epoch 4150 Loss 26.302275\n",
      "Params: tensor([ 2.4819, -0.9687])\n",
      "Grad:  tensor([-0.4904,  2.7751])\n",
      "Epoch 4151 Loss 26.301472\n",
      "Params: tensor([ 2.4819, -0.9690])\n",
      "Grad:  tensor([-0.4904,  2.7750])\n",
      "Epoch 4152 Loss 26.300678\n",
      "Params: tensor([ 2.4820, -0.9692])\n",
      "Grad:  tensor([-0.4904,  2.7750])\n",
      "Epoch 4153 Loss 26.299885\n",
      "Params: tensor([ 2.4820, -0.9695])\n",
      "Grad:  tensor([-0.4904,  2.7749])\n",
      "Epoch 4154 Loss 26.299091\n",
      "Params: tensor([ 2.4821, -0.9698])\n",
      "Grad:  tensor([-0.4904,  2.7749])\n",
      "Epoch 4155 Loss 26.298296\n",
      "Params: tensor([ 2.4821, -0.9701])\n",
      "Grad:  tensor([-0.4904,  2.7748])\n",
      "Epoch 4156 Loss 26.297504\n",
      "Params: tensor([ 2.4822, -0.9704])\n",
      "Grad:  tensor([-0.4903,  2.7748])\n",
      "Epoch 4157 Loss 26.296709\n",
      "Params: tensor([ 2.4822, -0.9706])\n",
      "Grad:  tensor([-0.4903,  2.7748])\n",
      "Epoch 4158 Loss 26.295918\n",
      "Params: tensor([ 2.4823, -0.9709])\n",
      "Grad:  tensor([-0.4903,  2.7747])\n",
      "Epoch 4159 Loss 26.295122\n",
      "Params: tensor([ 2.4823, -0.9712])\n",
      "Grad:  tensor([-0.4903,  2.7747])\n",
      "Epoch 4160 Loss 26.294334\n",
      "Params: tensor([ 2.4824, -0.9715])\n",
      "Grad:  tensor([-0.4903,  2.7746])\n",
      "Epoch 4161 Loss 26.293531\n",
      "Params: tensor([ 2.4824, -0.9717])\n",
      "Grad:  tensor([-0.4903,  2.7746])\n",
      "Epoch 4162 Loss 26.292742\n",
      "Params: tensor([ 2.4825, -0.9720])\n",
      "Grad:  tensor([-0.4903,  2.7745])\n",
      "Epoch 4163 Loss 26.291945\n",
      "Params: tensor([ 2.4825, -0.9723])\n",
      "Grad:  tensor([-0.4902,  2.7745])\n",
      "Epoch 4164 Loss 26.291149\n",
      "Params: tensor([ 2.4826, -0.9726])\n",
      "Grad:  tensor([-0.4902,  2.7744])\n",
      "Epoch 4165 Loss 26.290358\n",
      "Params: tensor([ 2.4826, -0.9729])\n",
      "Grad:  tensor([-0.4902,  2.7744])\n",
      "Epoch 4166 Loss 26.289558\n",
      "Params: tensor([ 2.4827, -0.9731])\n",
      "Grad:  tensor([-0.4902,  2.7743])\n",
      "Epoch 4167 Loss 26.288771\n",
      "Params: tensor([ 2.4827, -0.9734])\n",
      "Grad:  tensor([-0.4902,  2.7743])\n",
      "Epoch 4168 Loss 26.287977\n",
      "Params: tensor([ 2.4828, -0.9737])\n",
      "Grad:  tensor([-0.4902,  2.7742])\n",
      "Epoch 4169 Loss 26.287184\n",
      "Params: tensor([ 2.4828, -0.9740])\n",
      "Grad:  tensor([-0.4902,  2.7742])\n",
      "Epoch 4170 Loss 26.286390\n",
      "Params: tensor([ 2.4829, -0.9742])\n",
      "Grad:  tensor([-0.4901,  2.7742])\n",
      "Epoch 4171 Loss 26.285597\n",
      "Params: tensor([ 2.4829, -0.9745])\n",
      "Grad:  tensor([-0.4901,  2.7741])\n",
      "Epoch 4172 Loss 26.284801\n",
      "Params: tensor([ 2.4830, -0.9748])\n",
      "Grad:  tensor([-0.4901,  2.7741])\n",
      "Epoch 4173 Loss 26.284004\n",
      "Params: tensor([ 2.4830, -0.9751])\n",
      "Grad:  tensor([-0.4901,  2.7740])\n",
      "Epoch 4174 Loss 26.283216\n",
      "Params: tensor([ 2.4831, -0.9753])\n",
      "Grad:  tensor([-0.4901,  2.7740])\n",
      "Epoch 4175 Loss 26.282417\n",
      "Params: tensor([ 2.4831, -0.9756])\n",
      "Grad:  tensor([-0.4901,  2.7739])\n",
      "Epoch 4176 Loss 26.281628\n",
      "Params: tensor([ 2.4832, -0.9759])\n",
      "Grad:  tensor([-0.4901,  2.7739])\n",
      "Epoch 4177 Loss 26.280836\n",
      "Params: tensor([ 2.4832, -0.9762])\n",
      "Grad:  tensor([-0.4900,  2.7738])\n",
      "Epoch 4178 Loss 26.280043\n",
      "Params: tensor([ 2.4833, -0.9765])\n",
      "Grad:  tensor([-0.4900,  2.7738])\n",
      "Epoch 4179 Loss 26.279249\n",
      "Params: tensor([ 2.4833, -0.9767])\n",
      "Grad:  tensor([-0.4900,  2.7737])\n",
      "Epoch 4180 Loss 26.278456\n",
      "Params: tensor([ 2.4834, -0.9770])\n",
      "Grad:  tensor([-0.4900,  2.7737])\n",
      "Epoch 4181 Loss 26.277666\n",
      "Params: tensor([ 2.4834, -0.9773])\n",
      "Grad:  tensor([-0.4900,  2.7737])\n",
      "Epoch 4182 Loss 26.276863\n",
      "Params: tensor([ 2.4835, -0.9776])\n",
      "Grad:  tensor([-0.4900,  2.7736])\n",
      "Epoch 4183 Loss 26.276073\n",
      "Params: tensor([ 2.4835, -0.9778])\n",
      "Grad:  tensor([-0.4900,  2.7736])\n",
      "Epoch 4184 Loss 26.275282\n",
      "Params: tensor([ 2.4836, -0.9781])\n",
      "Grad:  tensor([-0.4899,  2.7735])\n",
      "Epoch 4185 Loss 26.274492\n",
      "Params: tensor([ 2.4836, -0.9784])\n",
      "Grad:  tensor([-0.4899,  2.7735])\n",
      "Epoch 4186 Loss 26.273695\n",
      "Params: tensor([ 2.4837, -0.9787])\n",
      "Grad:  tensor([-0.4899,  2.7734])\n",
      "Epoch 4187 Loss 26.272900\n",
      "Params: tensor([ 2.4837, -0.9790])\n",
      "Grad:  tensor([-0.4899,  2.7734])\n",
      "Epoch 4188 Loss 26.272106\n",
      "Params: tensor([ 2.4838, -0.9792])\n",
      "Grad:  tensor([-0.4899,  2.7733])\n",
      "Epoch 4189 Loss 26.271315\n",
      "Params: tensor([ 2.4838, -0.9795])\n",
      "Grad:  tensor([-0.4899,  2.7733])\n",
      "Epoch 4190 Loss 26.270525\n",
      "Params: tensor([ 2.4839, -0.9798])\n",
      "Grad:  tensor([-0.4899,  2.7732])\n",
      "Epoch 4191 Loss 26.269726\n",
      "Params: tensor([ 2.4839, -0.9801])\n",
      "Grad:  tensor([-0.4899,  2.7732])\n",
      "Epoch 4192 Loss 26.268932\n",
      "Params: tensor([ 2.4840, -0.9803])\n",
      "Grad:  tensor([-0.4899,  2.7731])\n",
      "Epoch 4193 Loss 26.268145\n",
      "Params: tensor([ 2.4840, -0.9806])\n",
      "Grad:  tensor([-0.4899,  2.7731])\n",
      "Epoch 4194 Loss 26.267351\n",
      "Params: tensor([ 2.4841, -0.9809])\n",
      "Grad:  tensor([-0.4899,  2.7730])\n",
      "Epoch 4195 Loss 26.266560\n",
      "Params: tensor([ 2.4841, -0.9812])\n",
      "Grad:  tensor([-0.4899,  2.7730])\n",
      "Epoch 4196 Loss 26.265764\n",
      "Params: tensor([ 2.4842, -0.9815])\n",
      "Grad:  tensor([-0.4899,  2.7729])\n",
      "Epoch 4197 Loss 26.264971\n",
      "Params: tensor([ 2.4842, -0.9817])\n",
      "Grad:  tensor([-0.4899,  2.7729])\n",
      "Epoch 4198 Loss 26.264177\n",
      "Params: tensor([ 2.4843, -0.9820])\n",
      "Grad:  tensor([-0.4899,  2.7728])\n",
      "Epoch 4199 Loss 26.263386\n",
      "Params: tensor([ 2.4843, -0.9823])\n",
      "Grad:  tensor([-0.4899,  2.7728])\n",
      "Epoch 4200 Loss 26.262596\n",
      "Params: tensor([ 2.4843, -0.9826])\n",
      "Grad:  tensor([-0.4899,  2.7727])\n",
      "Epoch 4201 Loss 26.261805\n",
      "Params: tensor([ 2.4844, -0.9828])\n",
      "Grad:  tensor([-0.4899,  2.7727])\n",
      "Epoch 4202 Loss 26.261009\n",
      "Params: tensor([ 2.4844, -0.9831])\n",
      "Grad:  tensor([-0.4899,  2.7726])\n",
      "Epoch 4203 Loss 26.260218\n",
      "Params: tensor([ 2.4845, -0.9834])\n",
      "Grad:  tensor([-0.4899,  2.7726])\n",
      "Epoch 4204 Loss 26.259424\n",
      "Params: tensor([ 2.4845, -0.9837])\n",
      "Grad:  tensor([-0.4899,  2.7725])\n",
      "Epoch 4205 Loss 26.258631\n",
      "Params: tensor([ 2.4846, -0.9839])\n",
      "Grad:  tensor([-0.4899,  2.7725])\n",
      "Epoch 4206 Loss 26.257837\n",
      "Params: tensor([ 2.4846, -0.9842])\n",
      "Grad:  tensor([-0.4899,  2.7725])\n",
      "Epoch 4207 Loss 26.257044\n",
      "Params: tensor([ 2.4847, -0.9845])\n",
      "Grad:  tensor([-0.4899,  2.7724])\n",
      "Epoch 4208 Loss 26.256248\n",
      "Params: tensor([ 2.4847, -0.9848])\n",
      "Grad:  tensor([-0.4899,  2.7724])\n",
      "Epoch 4209 Loss 26.255463\n",
      "Params: tensor([ 2.4848, -0.9851])\n",
      "Grad:  tensor([-0.4899,  2.7723])\n",
      "Epoch 4210 Loss 26.254669\n",
      "Params: tensor([ 2.4848, -0.9853])\n",
      "Grad:  tensor([-0.4899,  2.7723])\n",
      "Epoch 4211 Loss 26.253874\n",
      "Params: tensor([ 2.4849, -0.9856])\n",
      "Grad:  tensor([-0.4899,  2.7722])\n",
      "Epoch 4212 Loss 26.253084\n",
      "Params: tensor([ 2.4849, -0.9859])\n",
      "Grad:  tensor([-0.4899,  2.7722])\n",
      "Epoch 4213 Loss 26.252289\n",
      "Params: tensor([ 2.4850, -0.9862])\n",
      "Grad:  tensor([-0.4899,  2.7721])\n",
      "Epoch 4214 Loss 26.251493\n",
      "Params: tensor([ 2.4850, -0.9864])\n",
      "Grad:  tensor([-0.4899,  2.7721])\n",
      "Epoch 4215 Loss 26.250704\n",
      "Params: tensor([ 2.4851, -0.9867])\n",
      "Grad:  tensor([-0.4899,  2.7720])\n",
      "Epoch 4216 Loss 26.249914\n",
      "Params: tensor([ 2.4851, -0.9870])\n",
      "Grad:  tensor([-0.4899,  2.7720])\n",
      "Epoch 4217 Loss 26.249115\n",
      "Params: tensor([ 2.4852, -0.9873])\n",
      "Grad:  tensor([-0.4899,  2.7719])\n",
      "Epoch 4218 Loss 26.248329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([ 2.4852, -0.9876])\n",
      "Grad:  tensor([-0.4899,  2.7719])\n",
      "Epoch 4219 Loss 26.247534\n",
      "Params: tensor([ 2.4853, -0.9878])\n",
      "Grad:  tensor([-0.4899,  2.7718])\n",
      "Epoch 4220 Loss 26.246742\n",
      "Params: tensor([ 2.4853, -0.9881])\n",
      "Grad:  tensor([-0.4899,  2.7718])\n",
      "Epoch 4221 Loss 26.245953\n",
      "Params: tensor([ 2.4854, -0.9884])\n",
      "Grad:  tensor([-0.4899,  2.7717])\n",
      "Epoch 4222 Loss 26.245159\n",
      "Params: tensor([ 2.4854, -0.9887])\n",
      "Grad:  tensor([-0.4898,  2.7717])\n",
      "Epoch 4223 Loss 26.244362\n",
      "Params: tensor([ 2.4855, -0.9889])\n",
      "Grad:  tensor([-0.4898,  2.7716])\n",
      "Epoch 4224 Loss 26.243578\n",
      "Params: tensor([ 2.4855, -0.9892])\n",
      "Grad:  tensor([-0.4898,  2.7716])\n",
      "Epoch 4225 Loss 26.242785\n",
      "Params: tensor([ 2.4856, -0.9895])\n",
      "Grad:  tensor([-0.4898,  2.7715])\n",
      "Epoch 4226 Loss 26.241991\n",
      "Params: tensor([ 2.4856, -0.9898])\n",
      "Grad:  tensor([-0.4898,  2.7715])\n",
      "Epoch 4227 Loss 26.241199\n",
      "Params: tensor([ 2.4857, -0.9900])\n",
      "Grad:  tensor([-0.4898,  2.7714])\n",
      "Epoch 4228 Loss 26.240406\n",
      "Params: tensor([ 2.4857, -0.9903])\n",
      "Grad:  tensor([-0.4898,  2.7714])\n",
      "Epoch 4229 Loss 26.239613\n",
      "Params: tensor([ 2.4858, -0.9906])\n",
      "Grad:  tensor([-0.4898,  2.7713])\n",
      "Epoch 4230 Loss 26.238825\n",
      "Params: tensor([ 2.4858, -0.9909])\n",
      "Grad:  tensor([-0.4898,  2.7713])\n",
      "Epoch 4231 Loss 26.238035\n",
      "Params: tensor([ 2.4859, -0.9912])\n",
      "Grad:  tensor([-0.4898,  2.7713])\n",
      "Epoch 4232 Loss 26.237242\n",
      "Params: tensor([ 2.4859, -0.9914])\n",
      "Grad:  tensor([-0.4898,  2.7712])\n",
      "Epoch 4233 Loss 26.236444\n",
      "Params: tensor([ 2.4860, -0.9917])\n",
      "Grad:  tensor([-0.4898,  2.7712])\n",
      "Epoch 4234 Loss 26.235659\n",
      "Params: tensor([ 2.4860, -0.9920])\n",
      "Grad:  tensor([-0.4898,  2.7711])\n",
      "Epoch 4235 Loss 26.234863\n",
      "Params: tensor([ 2.4861, -0.9923])\n",
      "Grad:  tensor([-0.4898,  2.7711])\n",
      "Epoch 4236 Loss 26.234072\n",
      "Params: tensor([ 2.4861, -0.9925])\n",
      "Grad:  tensor([-0.4898,  2.7710])\n",
      "Epoch 4237 Loss 26.233278\n",
      "Params: tensor([ 2.4862, -0.9928])\n",
      "Grad:  tensor([-0.4898,  2.7710])\n",
      "Epoch 4238 Loss 26.232489\n",
      "Params: tensor([ 2.4862, -0.9931])\n",
      "Grad:  tensor([-0.4898,  2.7709])\n",
      "Epoch 4239 Loss 26.231697\n",
      "Params: tensor([ 2.4863, -0.9934])\n",
      "Grad:  tensor([-0.4898,  2.7709])\n",
      "Epoch 4240 Loss 26.230902\n",
      "Params: tensor([ 2.4863, -0.9936])\n",
      "Grad:  tensor([-0.4898,  2.7708])\n",
      "Epoch 4241 Loss 26.230110\n",
      "Params: tensor([ 2.4864, -0.9939])\n",
      "Grad:  tensor([-0.4898,  2.7708])\n",
      "Epoch 4242 Loss 26.229321\n",
      "Params: tensor([ 2.4864, -0.9942])\n",
      "Grad:  tensor([-0.4898,  2.7707])\n",
      "Epoch 4243 Loss 26.228527\n",
      "Params: tensor([ 2.4865, -0.9945])\n",
      "Grad:  tensor([-0.4898,  2.7707])\n",
      "Epoch 4244 Loss 26.227739\n",
      "Params: tensor([ 2.4865, -0.9948])\n",
      "Grad:  tensor([-0.4898,  2.7706])\n",
      "Epoch 4245 Loss 26.226946\n",
      "Params: tensor([ 2.4865, -0.9950])\n",
      "Grad:  tensor([-0.4898,  2.7706])\n",
      "Epoch 4246 Loss 26.226154\n",
      "Params: tensor([ 2.4866, -0.9953])\n",
      "Grad:  tensor([-0.4898,  2.7705])\n",
      "Epoch 4247 Loss 26.225367\n",
      "Params: tensor([ 2.4866, -0.9956])\n",
      "Grad:  tensor([-0.4898,  2.7705])\n",
      "Epoch 4248 Loss 26.224573\n",
      "Params: tensor([ 2.4867, -0.9959])\n",
      "Grad:  tensor([-0.4897,  2.7704])\n",
      "Epoch 4249 Loss 26.223780\n",
      "Params: tensor([ 2.4867, -0.9961])\n",
      "Grad:  tensor([-0.4897,  2.7704])\n",
      "Epoch 4250 Loss 26.222990\n",
      "Params: tensor([ 2.4868, -0.9964])\n",
      "Grad:  tensor([-0.4897,  2.7703])\n",
      "Epoch 4251 Loss 26.222200\n",
      "Params: tensor([ 2.4868, -0.9967])\n",
      "Grad:  tensor([-0.4897,  2.7703])\n",
      "Epoch 4252 Loss 26.221403\n",
      "Params: tensor([ 2.4869, -0.9970])\n",
      "Grad:  tensor([-0.4897,  2.7702])\n",
      "Epoch 4253 Loss 26.220613\n",
      "Params: tensor([ 2.4869, -0.9972])\n",
      "Grad:  tensor([-0.4897,  2.7702])\n",
      "Epoch 4254 Loss 26.219826\n",
      "Params: tensor([ 2.4870, -0.9975])\n",
      "Grad:  tensor([-0.4897,  2.7702])\n",
      "Epoch 4255 Loss 26.219030\n",
      "Params: tensor([ 2.4870, -0.9978])\n",
      "Grad:  tensor([-0.4897,  2.7701])\n",
      "Epoch 4256 Loss 26.218239\n",
      "Params: tensor([ 2.4871, -0.9981])\n",
      "Grad:  tensor([-0.4897,  2.7701])\n",
      "Epoch 4257 Loss 26.217455\n",
      "Params: tensor([ 2.4871, -0.9984])\n",
      "Grad:  tensor([-0.4897,  2.7700])\n",
      "Epoch 4258 Loss 26.216658\n",
      "Params: tensor([ 2.4872, -0.9986])\n",
      "Grad:  tensor([-0.4897,  2.7700])\n",
      "Epoch 4259 Loss 26.215868\n",
      "Params: tensor([ 2.4872, -0.9989])\n",
      "Grad:  tensor([-0.4897,  2.7699])\n",
      "Epoch 4260 Loss 26.215075\n",
      "Params: tensor([ 2.4873, -0.9992])\n",
      "Grad:  tensor([-0.4897,  2.7699])\n",
      "Epoch 4261 Loss 26.214293\n",
      "Params: tensor([ 2.4873, -0.9995])\n",
      "Grad:  tensor([-0.4897,  2.7698])\n",
      "Epoch 4262 Loss 26.213499\n",
      "Params: tensor([ 2.4874, -0.9997])\n",
      "Grad:  tensor([-0.4897,  2.7698])\n",
      "Epoch 4263 Loss 26.212702\n",
      "Params: tensor([ 2.4874, -1.0000])\n",
      "Grad:  tensor([-0.4897,  2.7697])\n",
      "Epoch 4264 Loss 26.211912\n",
      "Params: tensor([ 2.4875, -1.0003])\n",
      "Grad:  tensor([-0.4897,  2.7697])\n",
      "Epoch 4265 Loss 26.211119\n",
      "Params: tensor([ 2.4875, -1.0006])\n",
      "Grad:  tensor([-0.4897,  2.7696])\n",
      "Epoch 4266 Loss 26.210327\n",
      "Params: tensor([ 2.4876, -1.0008])\n",
      "Grad:  tensor([-0.4897,  2.7696])\n",
      "Epoch 4267 Loss 26.209539\n",
      "Params: tensor([ 2.4876, -1.0011])\n",
      "Grad:  tensor([-0.4897,  2.7695])\n",
      "Epoch 4268 Loss 26.208746\n",
      "Params: tensor([ 2.4877, -1.0014])\n",
      "Grad:  tensor([-0.4896,  2.7695])\n",
      "Epoch 4269 Loss 26.207960\n",
      "Params: tensor([ 2.4877, -1.0017])\n",
      "Grad:  tensor([-0.4896,  2.7694])\n",
      "Epoch 4270 Loss 26.207167\n",
      "Params: tensor([ 2.4878, -1.0020])\n",
      "Grad:  tensor([-0.4896,  2.7694])\n",
      "Epoch 4271 Loss 26.206377\n",
      "Params: tensor([ 2.4878, -1.0022])\n",
      "Grad:  tensor([-0.4896,  2.7693])\n",
      "Epoch 4272 Loss 26.205589\n",
      "Params: tensor([ 2.4879, -1.0025])\n",
      "Grad:  tensor([-0.4896,  2.7693])\n",
      "Epoch 4273 Loss 26.204792\n",
      "Params: tensor([ 2.4879, -1.0028])\n",
      "Grad:  tensor([-0.4896,  2.7692])\n",
      "Epoch 4274 Loss 26.204002\n",
      "Params: tensor([ 2.4880, -1.0031])\n",
      "Grad:  tensor([-0.4896,  2.7692])\n",
      "Epoch 4275 Loss 26.203215\n",
      "Params: tensor([ 2.4880, -1.0033])\n",
      "Grad:  tensor([-0.4896,  2.7692])\n",
      "Epoch 4276 Loss 26.202415\n",
      "Params: tensor([ 2.4881, -1.0036])\n",
      "Grad:  tensor([-0.4896,  2.7691])\n",
      "Epoch 4277 Loss 26.201635\n",
      "Params: tensor([ 2.4881, -1.0039])\n",
      "Grad:  tensor([-0.4896,  2.7691])\n",
      "Epoch 4278 Loss 26.200844\n",
      "Params: tensor([ 2.4882, -1.0042])\n",
      "Grad:  tensor([-0.4896,  2.7690])\n",
      "Epoch 4279 Loss 26.200050\n",
      "Params: tensor([ 2.4882, -1.0044])\n",
      "Grad:  tensor([-0.4896,  2.7690])\n",
      "Epoch 4280 Loss 26.199261\n",
      "Params: tensor([ 2.4883, -1.0047])\n",
      "Grad:  tensor([-0.4896,  2.7689])\n",
      "Epoch 4281 Loss 26.198463\n",
      "Params: tensor([ 2.4883, -1.0050])\n",
      "Grad:  tensor([-0.4896,  2.7689])\n",
      "Epoch 4282 Loss 26.197680\n",
      "Params: tensor([ 2.4884, -1.0053])\n",
      "Grad:  tensor([-0.4896,  2.7688])\n",
      "Epoch 4283 Loss 26.196888\n",
      "Params: tensor([ 2.4884, -1.0056])\n",
      "Grad:  tensor([-0.4896,  2.7688])\n",
      "Epoch 4284 Loss 26.196098\n",
      "Params: tensor([ 2.4885, -1.0058])\n",
      "Grad:  tensor([-0.4896,  2.7687])\n",
      "Epoch 4285 Loss 26.195305\n",
      "Params: tensor([ 2.4885, -1.0061])\n",
      "Grad:  tensor([-0.4895,  2.7687])\n",
      "Epoch 4286 Loss 26.194517\n",
      "Params: tensor([ 2.4886, -1.0064])\n",
      "Grad:  tensor([-0.4895,  2.7686])\n",
      "Epoch 4287 Loss 26.193724\n",
      "Params: tensor([ 2.4886, -1.0067])\n",
      "Grad:  tensor([-0.4895,  2.7686])\n",
      "Epoch 4288 Loss 26.192938\n",
      "Params: tensor([ 2.4887, -1.0069])\n",
      "Grad:  tensor([-0.4895,  2.7685])\n",
      "Epoch 4289 Loss 26.192144\n",
      "Params: tensor([ 2.4887, -1.0072])\n",
      "Grad:  tensor([-0.4895,  2.7685])\n",
      "Epoch 4290 Loss 26.191353\n",
      "Params: tensor([ 2.4887, -1.0075])\n",
      "Grad:  tensor([-0.4895,  2.7684])\n",
      "Epoch 4291 Loss 26.190563\n",
      "Params: tensor([ 2.4888, -1.0078])\n",
      "Grad:  tensor([-0.4895,  2.7684])\n",
      "Epoch 4292 Loss 26.189772\n",
      "Params: tensor([ 2.4888, -1.0080])\n",
      "Grad:  tensor([-0.4895,  2.7683])\n",
      "Epoch 4293 Loss 26.188984\n",
      "Params: tensor([ 2.4889, -1.0083])\n",
      "Grad:  tensor([-0.4895,  2.7683])\n",
      "Epoch 4294 Loss 26.188194\n",
      "Params: tensor([ 2.4889, -1.0086])\n",
      "Grad:  tensor([-0.4895,  2.7683])\n",
      "Epoch 4295 Loss 26.187403\n",
      "Params: tensor([ 2.4890, -1.0089])\n",
      "Grad:  tensor([-0.4895,  2.7682])\n",
      "Epoch 4296 Loss 26.186615\n",
      "Params: tensor([ 2.4890, -1.0092])\n",
      "Grad:  tensor([-0.4895,  2.7682])\n",
      "Epoch 4297 Loss 26.185822\n",
      "Params: tensor([ 2.4891, -1.0094])\n",
      "Grad:  tensor([-0.4895,  2.7681])\n",
      "Epoch 4298 Loss 26.185034\n",
      "Params: tensor([ 2.4891, -1.0097])\n",
      "Grad:  tensor([-0.4895,  2.7681])\n",
      "Epoch 4299 Loss 26.184242\n",
      "Params: tensor([ 2.4892, -1.0100])\n",
      "Grad:  tensor([-0.4895,  2.7680])\n",
      "Epoch 4300 Loss 26.183455\n",
      "Params: tensor([ 2.4892, -1.0103])\n",
      "Grad:  tensor([-0.4894,  2.7680])\n",
      "Epoch 4301 Loss 26.182665\n",
      "Params: tensor([ 2.4893, -1.0105])\n",
      "Grad:  tensor([-0.4894,  2.7679])\n",
      "Epoch 4302 Loss 26.181873\n",
      "Params: tensor([ 2.4893, -1.0108])\n",
      "Grad:  tensor([-0.4894,  2.7679])\n",
      "Epoch 4303 Loss 26.181078\n",
      "Params: tensor([ 2.4894, -1.0111])\n",
      "Grad:  tensor([-0.4894,  2.7678])\n",
      "Epoch 4304 Loss 26.180292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([ 2.4894, -1.0114])\n",
      "Grad:  tensor([-0.4894,  2.7678])\n",
      "Epoch 4305 Loss 26.179504\n",
      "Params: tensor([ 2.4895, -1.0116])\n",
      "Grad:  tensor([-0.4894,  2.7677])\n",
      "Epoch 4306 Loss 26.178711\n",
      "Params: tensor([ 2.4895, -1.0119])\n",
      "Grad:  tensor([-0.4894,  2.7677])\n",
      "Epoch 4307 Loss 26.177923\n",
      "Params: tensor([ 2.4896, -1.0122])\n",
      "Grad:  tensor([-0.4894,  2.7676])\n",
      "Epoch 4308 Loss 26.177135\n",
      "Params: tensor([ 2.4896, -1.0125])\n",
      "Grad:  tensor([-0.4894,  2.7676])\n",
      "Epoch 4309 Loss 26.176344\n",
      "Params: tensor([ 2.4897, -1.0128])\n",
      "Grad:  tensor([-0.4894,  2.7675])\n",
      "Epoch 4310 Loss 26.175550\n",
      "Params: tensor([ 2.4897, -1.0130])\n",
      "Grad:  tensor([-0.4894,  2.7675])\n",
      "Epoch 4311 Loss 26.174761\n",
      "Params: tensor([ 2.4898, -1.0133])\n",
      "Grad:  tensor([-0.4894,  2.7675])\n",
      "Epoch 4312 Loss 26.173975\n",
      "Params: tensor([ 2.4898, -1.0136])\n",
      "Grad:  tensor([-0.4894,  2.7674])\n",
      "Epoch 4313 Loss 26.173185\n",
      "Params: tensor([ 2.4899, -1.0139])\n",
      "Grad:  tensor([-0.4894,  2.7674])\n",
      "Epoch 4314 Loss 26.172394\n",
      "Params: tensor([ 2.4899, -1.0141])\n",
      "Grad:  tensor([-0.4893,  2.7673])\n",
      "Epoch 4315 Loss 26.171606\n",
      "Params: tensor([ 2.4900, -1.0144])\n",
      "Grad:  tensor([-0.4893,  2.7673])\n",
      "Epoch 4316 Loss 26.170815\n",
      "Params: tensor([ 2.4900, -1.0147])\n",
      "Grad:  tensor([-0.4893,  2.7672])\n",
      "Epoch 4317 Loss 26.170027\n",
      "Params: tensor([ 2.4901, -1.0150])\n",
      "Grad:  tensor([-0.4893,  2.7672])\n",
      "Epoch 4318 Loss 26.169231\n",
      "Params: tensor([ 2.4901, -1.0152])\n",
      "Grad:  tensor([-0.4893,  2.7671])\n",
      "Epoch 4319 Loss 26.168446\n",
      "Params: tensor([ 2.4902, -1.0155])\n",
      "Grad:  tensor([-0.4893,  2.7671])\n",
      "Epoch 4320 Loss 26.167656\n",
      "Params: tensor([ 2.4902, -1.0158])\n",
      "Grad:  tensor([-0.4893,  2.7670])\n",
      "Epoch 4321 Loss 26.166864\n",
      "Params: tensor([ 2.4903, -1.0161])\n",
      "Grad:  tensor([-0.4893,  2.7670])\n",
      "Epoch 4322 Loss 26.166077\n",
      "Params: tensor([ 2.4903, -1.0164])\n",
      "Grad:  tensor([-0.4893,  2.7669])\n",
      "Epoch 4323 Loss 26.165285\n",
      "Params: tensor([ 2.4904, -1.0166])\n",
      "Grad:  tensor([-0.4893,  2.7669])\n",
      "Epoch 4324 Loss 26.164497\n",
      "Params: tensor([ 2.4904, -1.0169])\n",
      "Grad:  tensor([-0.4893,  2.7668])\n",
      "Epoch 4325 Loss 26.163704\n",
      "Params: tensor([ 2.4905, -1.0172])\n",
      "Grad:  tensor([-0.4893,  2.7668])\n",
      "Epoch 4326 Loss 26.162916\n",
      "Params: tensor([ 2.4905, -1.0175])\n",
      "Grad:  tensor([-0.4892,  2.7667])\n",
      "Epoch 4327 Loss 26.162127\n",
      "Params: tensor([ 2.4906, -1.0177])\n",
      "Grad:  tensor([-0.4892,  2.7667])\n",
      "Epoch 4328 Loss 26.161335\n",
      "Params: tensor([ 2.4906, -1.0180])\n",
      "Grad:  tensor([-0.4892,  2.7667])\n",
      "Epoch 4329 Loss 26.160555\n",
      "Params: tensor([ 2.4907, -1.0183])\n",
      "Grad:  tensor([-0.4892,  2.7666])\n",
      "Epoch 4330 Loss 26.159765\n",
      "Params: tensor([ 2.4907, -1.0186])\n",
      "Grad:  tensor([-0.4892,  2.7666])\n",
      "Epoch 4331 Loss 26.158974\n",
      "Params: tensor([ 2.4908, -1.0188])\n",
      "Grad:  tensor([-0.4892,  2.7665])\n",
      "Epoch 4332 Loss 26.158184\n",
      "Params: tensor([ 2.4908, -1.0191])\n",
      "Grad:  tensor([-0.4892,  2.7665])\n",
      "Epoch 4333 Loss 26.157393\n",
      "Params: tensor([ 2.4908, -1.0194])\n",
      "Grad:  tensor([-0.4892,  2.7664])\n",
      "Epoch 4334 Loss 26.156605\n",
      "Params: tensor([ 2.4909, -1.0197])\n",
      "Grad:  tensor([-0.4892,  2.7664])\n",
      "Epoch 4335 Loss 26.155815\n",
      "Params: tensor([ 2.4909, -1.0199])\n",
      "Grad:  tensor([-0.4892,  2.7663])\n",
      "Epoch 4336 Loss 26.155022\n",
      "Params: tensor([ 2.4910, -1.0202])\n",
      "Grad:  tensor([-0.4892,  2.7663])\n",
      "Epoch 4337 Loss 26.154238\n",
      "Params: tensor([ 2.4910, -1.0205])\n",
      "Grad:  tensor([-0.4892,  2.7662])\n",
      "Epoch 4338 Loss 26.153448\n",
      "Params: tensor([ 2.4911, -1.0208])\n",
      "Grad:  tensor([-0.4891,  2.7662])\n",
      "Epoch 4339 Loss 26.152660\n",
      "Params: tensor([ 2.4911, -1.0211])\n",
      "Grad:  tensor([-0.4891,  2.7661])\n",
      "Epoch 4340 Loss 26.151869\n",
      "Params: tensor([ 2.4912, -1.0213])\n",
      "Grad:  tensor([-0.4891,  2.7661])\n",
      "Epoch 4341 Loss 26.151081\n",
      "Params: tensor([ 2.4912, -1.0216])\n",
      "Grad:  tensor([-0.4891,  2.7660])\n",
      "Epoch 4342 Loss 26.150291\n",
      "Params: tensor([ 2.4913, -1.0219])\n",
      "Grad:  tensor([-0.4891,  2.7660])\n",
      "Epoch 4343 Loss 26.149504\n",
      "Params: tensor([ 2.4913, -1.0222])\n",
      "Grad:  tensor([-0.4891,  2.7659])\n",
      "Epoch 4344 Loss 26.148712\n",
      "Params: tensor([ 2.4914, -1.0224])\n",
      "Grad:  tensor([-0.4891,  2.7659])\n",
      "Epoch 4345 Loss 26.147924\n",
      "Params: tensor([ 2.4914, -1.0227])\n",
      "Grad:  tensor([-0.4891,  2.7659])\n",
      "Epoch 4346 Loss 26.147137\n",
      "Params: tensor([ 2.4915, -1.0230])\n",
      "Grad:  tensor([-0.4891,  2.7658])\n",
      "Epoch 4347 Loss 26.146345\n",
      "Params: tensor([ 2.4915, -1.0233])\n",
      "Grad:  tensor([-0.4891,  2.7658])\n",
      "Epoch 4348 Loss 26.145561\n",
      "Params: tensor([ 2.4916, -1.0235])\n",
      "Grad:  tensor([-0.4891,  2.7657])\n",
      "Epoch 4349 Loss 26.144770\n",
      "Params: tensor([ 2.4916, -1.0238])\n",
      "Grad:  tensor([-0.4890,  2.7657])\n",
      "Epoch 4350 Loss 26.143980\n",
      "Params: tensor([ 2.4917, -1.0241])\n",
      "Grad:  tensor([-0.4890,  2.7656])\n",
      "Epoch 4351 Loss 26.143190\n",
      "Params: tensor([ 2.4917, -1.0244])\n",
      "Grad:  tensor([-0.4890,  2.7656])\n",
      "Epoch 4352 Loss 26.142401\n",
      "Params: tensor([ 2.4918, -1.0247])\n",
      "Grad:  tensor([-0.4890,  2.7655])\n",
      "Epoch 4353 Loss 26.141619\n",
      "Params: tensor([ 2.4918, -1.0249])\n",
      "Grad:  tensor([-0.4890,  2.7655])\n",
      "Epoch 4354 Loss 26.140827\n",
      "Params: tensor([ 2.4919, -1.0252])\n",
      "Grad:  tensor([-0.4890,  2.7654])\n",
      "Epoch 4355 Loss 26.140043\n",
      "Params: tensor([ 2.4919, -1.0255])\n",
      "Grad:  tensor([-0.4890,  2.7654])\n",
      "Epoch 4356 Loss 26.139250\n",
      "Params: tensor([ 2.4920, -1.0258])\n",
      "Grad:  tensor([-0.4890,  2.7653])\n",
      "Epoch 4357 Loss 26.138460\n",
      "Params: tensor([ 2.4920, -1.0260])\n",
      "Grad:  tensor([-0.4890,  2.7653])\n",
      "Epoch 4358 Loss 26.137671\n",
      "Params: tensor([ 2.4921, -1.0263])\n",
      "Grad:  tensor([-0.4890,  2.7652])\n",
      "Epoch 4359 Loss 26.136883\n",
      "Params: tensor([ 2.4921, -1.0266])\n",
      "Grad:  tensor([-0.4889,  2.7652])\n",
      "Epoch 4360 Loss 26.136101\n",
      "Params: tensor([ 2.4922, -1.0269])\n",
      "Grad:  tensor([-0.4889,  2.7652])\n",
      "Epoch 4361 Loss 26.135307\n",
      "Params: tensor([ 2.4922, -1.0271])\n",
      "Grad:  tensor([-0.4889,  2.7651])\n",
      "Epoch 4362 Loss 26.134521\n",
      "Params: tensor([ 2.4923, -1.0274])\n",
      "Grad:  tensor([-0.4889,  2.7651])\n",
      "Epoch 4363 Loss 26.133728\n",
      "Params: tensor([ 2.4923, -1.0277])\n",
      "Grad:  tensor([-0.4889,  2.7650])\n",
      "Epoch 4364 Loss 26.132940\n",
      "Params: tensor([ 2.4924, -1.0280])\n",
      "Grad:  tensor([-0.4889,  2.7650])\n",
      "Epoch 4365 Loss 26.132149\n",
      "Params: tensor([ 2.4924, -1.0282])\n",
      "Grad:  tensor([-0.4889,  2.7649])\n",
      "Epoch 4366 Loss 26.131365\n",
      "Params: tensor([ 2.4925, -1.0285])\n",
      "Grad:  tensor([-0.4889,  2.7649])\n",
      "Epoch 4367 Loss 26.130577\n",
      "Params: tensor([ 2.4925, -1.0288])\n",
      "Grad:  tensor([-0.4889,  2.7648])\n",
      "Epoch 4368 Loss 26.129791\n",
      "Params: tensor([ 2.4926, -1.0291])\n",
      "Grad:  tensor([-0.4889,  2.7648])\n",
      "Epoch 4369 Loss 26.129000\n",
      "Params: tensor([ 2.4926, -1.0294])\n",
      "Grad:  tensor([-0.4888,  2.7647])\n",
      "Epoch 4370 Loss 26.128212\n",
      "Params: tensor([ 2.4927, -1.0296])\n",
      "Grad:  tensor([-0.4888,  2.7647])\n",
      "Epoch 4371 Loss 26.127422\n",
      "Params: tensor([ 2.4927, -1.0299])\n",
      "Grad:  tensor([-0.4888,  2.7646])\n",
      "Epoch 4372 Loss 26.126631\n",
      "Params: tensor([ 2.4928, -1.0302])\n",
      "Grad:  tensor([-0.4888,  2.7646])\n",
      "Epoch 4373 Loss 26.125843\n",
      "Params: tensor([ 2.4928, -1.0305])\n",
      "Grad:  tensor([-0.4888,  2.7645])\n",
      "Epoch 4374 Loss 26.125059\n",
      "Params: tensor([ 2.4929, -1.0307])\n",
      "Grad:  tensor([-0.4888,  2.7645])\n",
      "Epoch 4375 Loss 26.124266\n",
      "Params: tensor([ 2.4929, -1.0310])\n",
      "Grad:  tensor([-0.4888,  2.7645])\n",
      "Epoch 4376 Loss 26.123482\n",
      "Params: tensor([ 2.4930, -1.0313])\n",
      "Grad:  tensor([-0.4888,  2.7644])\n",
      "Epoch 4377 Loss 26.122694\n",
      "Params: tensor([ 2.4930, -1.0316])\n",
      "Grad:  tensor([-0.4888,  2.7644])\n",
      "Epoch 4378 Loss 26.121906\n",
      "Params: tensor([ 2.4930, -1.0318])\n",
      "Grad:  tensor([-0.4887,  2.7643])\n",
      "Epoch 4379 Loss 26.121113\n",
      "Params: tensor([ 2.4931, -1.0321])\n",
      "Grad:  tensor([-0.4887,  2.7643])\n",
      "Epoch 4380 Loss 26.120331\n",
      "Params: tensor([ 2.4931, -1.0324])\n",
      "Grad:  tensor([-0.4887,  2.7642])\n",
      "Epoch 4381 Loss 26.119537\n",
      "Params: tensor([ 2.4932, -1.0327])\n",
      "Grad:  tensor([-0.4887,  2.7642])\n",
      "Epoch 4382 Loss 26.118750\n",
      "Params: tensor([ 2.4932, -1.0329])\n",
      "Grad:  tensor([-0.4887,  2.7641])\n",
      "Epoch 4383 Loss 26.117964\n",
      "Params: tensor([ 2.4933, -1.0332])\n",
      "Grad:  tensor([-0.4887,  2.7641])\n",
      "Epoch 4384 Loss 26.117180\n",
      "Params: tensor([ 2.4933, -1.0335])\n",
      "Grad:  tensor([-0.4887,  2.7640])\n",
      "Epoch 4385 Loss 26.116392\n",
      "Params: tensor([ 2.4934, -1.0338])\n",
      "Grad:  tensor([-0.4887,  2.7640])\n",
      "Epoch 4386 Loss 26.115606\n",
      "Params: tensor([ 2.4934, -1.0341])\n",
      "Grad:  tensor([-0.4887,  2.7639])\n",
      "Epoch 4387 Loss 26.114809\n",
      "Params: tensor([ 2.4935, -1.0343])\n",
      "Grad:  tensor([-0.4887,  2.7639])\n",
      "Epoch 4388 Loss 26.114025\n",
      "Params: tensor([ 2.4935, -1.0346])\n",
      "Grad:  tensor([-0.4886,  2.7638])\n",
      "Epoch 4389 Loss 26.113237\n",
      "Params: tensor([ 2.4936, -1.0349])\n",
      "Grad:  tensor([-0.4886,  2.7638])\n",
      "Epoch 4390 Loss 26.112450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([ 2.4936, -1.0352])\n",
      "Grad:  tensor([-0.4886,  2.7638])\n",
      "Epoch 4391 Loss 26.111666\n",
      "Params: tensor([ 2.4937, -1.0354])\n",
      "Grad:  tensor([-0.4886,  2.7637])\n",
      "Epoch 4392 Loss 26.110878\n",
      "Params: tensor([ 2.4937, -1.0357])\n",
      "Grad:  tensor([-0.4886,  2.7637])\n",
      "Epoch 4393 Loss 26.110090\n",
      "Params: tensor([ 2.4938, -1.0360])\n",
      "Grad:  tensor([-0.4886,  2.7636])\n",
      "Epoch 4394 Loss 26.109301\n",
      "Params: tensor([ 2.4938, -1.0363])\n",
      "Grad:  tensor([-0.4886,  2.7636])\n",
      "Epoch 4395 Loss 26.108519\n",
      "Params: tensor([ 2.4939, -1.0365])\n",
      "Grad:  tensor([-0.4886,  2.7635])\n",
      "Epoch 4396 Loss 26.107721\n",
      "Params: tensor([ 2.4939, -1.0368])\n",
      "Grad:  tensor([-0.4885,  2.7635])\n",
      "Epoch 4397 Loss 26.106939\n",
      "Params: tensor([ 2.4940, -1.0371])\n",
      "Grad:  tensor([-0.4885,  2.7634])\n",
      "Epoch 4398 Loss 26.106152\n",
      "Params: tensor([ 2.4940, -1.0374])\n",
      "Grad:  tensor([-0.4885,  2.7634])\n",
      "Epoch 4399 Loss 26.105366\n",
      "Params: tensor([ 2.4941, -1.0376])\n",
      "Grad:  tensor([-0.4885,  2.7633])\n",
      "Epoch 4400 Loss 26.104572\n",
      "Params: tensor([ 2.4941, -1.0379])\n",
      "Grad:  tensor([-0.4885,  2.7633])\n",
      "Epoch 4401 Loss 26.103788\n",
      "Params: tensor([ 2.4942, -1.0382])\n",
      "Grad:  tensor([-0.4885,  2.7632])\n",
      "Epoch 4402 Loss 26.103003\n",
      "Params: tensor([ 2.4942, -1.0385])\n",
      "Grad:  tensor([-0.4885,  2.7632])\n",
      "Epoch 4403 Loss 26.102211\n",
      "Params: tensor([ 2.4943, -1.0387])\n",
      "Grad:  tensor([-0.4885,  2.7632])\n",
      "Epoch 4404 Loss 26.101421\n",
      "Params: tensor([ 2.4943, -1.0390])\n",
      "Grad:  tensor([-0.4884,  2.7631])\n",
      "Epoch 4405 Loss 26.100636\n",
      "Params: tensor([ 2.4944, -1.0393])\n",
      "Grad:  tensor([-0.4884,  2.7631])\n",
      "Epoch 4406 Loss 26.099854\n",
      "Params: tensor([ 2.4944, -1.0396])\n",
      "Grad:  tensor([-0.4884,  2.7630])\n",
      "Epoch 4407 Loss 26.099062\n",
      "Params: tensor([ 2.4945, -1.0399])\n",
      "Grad:  tensor([-0.4884,  2.7630])\n",
      "Epoch 4408 Loss 26.098272\n",
      "Params: tensor([ 2.4945, -1.0401])\n",
      "Grad:  tensor([-0.4884,  2.7629])\n",
      "Epoch 4409 Loss 26.097485\n",
      "Params: tensor([ 2.4946, -1.0404])\n",
      "Grad:  tensor([-0.4884,  2.7629])\n",
      "Epoch 4410 Loss 26.096703\n",
      "Params: tensor([ 2.4946, -1.0407])\n",
      "Grad:  tensor([-0.4884,  2.7628])\n",
      "Epoch 4411 Loss 26.095917\n",
      "Params: tensor([ 2.4947, -1.0410])\n",
      "Grad:  tensor([-0.4884,  2.7628])\n",
      "Epoch 4412 Loss 26.095121\n",
      "Params: tensor([ 2.4947, -1.0412])\n",
      "Grad:  tensor([-0.4884,  2.7627])\n",
      "Epoch 4413 Loss 26.094337\n",
      "Params: tensor([ 2.4948, -1.0415])\n",
      "Grad:  tensor([-0.4883,  2.7627])\n",
      "Epoch 4414 Loss 26.093554\n",
      "Params: tensor([ 2.4948, -1.0418])\n",
      "Grad:  tensor([-0.4883,  2.7626])\n",
      "Epoch 4415 Loss 26.092768\n",
      "Params: tensor([ 2.4949, -1.0421])\n",
      "Grad:  tensor([-0.4883,  2.7626])\n",
      "Epoch 4416 Loss 26.091978\n",
      "Params: tensor([ 2.4949, -1.0423])\n",
      "Grad:  tensor([-0.4883,  2.7626])\n",
      "Epoch 4417 Loss 26.091194\n",
      "Params: tensor([ 2.4950, -1.0426])\n",
      "Grad:  tensor([-0.4883,  2.7625])\n",
      "Epoch 4418 Loss 26.090405\n",
      "Params: tensor([ 2.4950, -1.0429])\n",
      "Grad:  tensor([-0.4883,  2.7625])\n",
      "Epoch 4419 Loss 26.089613\n",
      "Params: tensor([ 2.4951, -1.0432])\n",
      "Grad:  tensor([-0.4883,  2.7624])\n",
      "Epoch 4420 Loss 26.088831\n",
      "Params: tensor([ 2.4951, -1.0434])\n",
      "Grad:  tensor([-0.4882,  2.7624])\n",
      "Epoch 4421 Loss 26.088043\n",
      "Params: tensor([ 2.4952, -1.0437])\n",
      "Grad:  tensor([-0.4882,  2.7623])\n",
      "Epoch 4422 Loss 26.087261\n",
      "Params: tensor([ 2.4952, -1.0440])\n",
      "Grad:  tensor([-0.4882,  2.7623])\n",
      "Epoch 4423 Loss 26.086470\n",
      "Params: tensor([ 2.4952, -1.0443])\n",
      "Grad:  tensor([-0.4882,  2.7622])\n",
      "Epoch 4424 Loss 26.085682\n",
      "Params: tensor([ 2.4953, -1.0446])\n",
      "Grad:  tensor([-0.4882,  2.7622])\n",
      "Epoch 4425 Loss 26.084894\n",
      "Params: tensor([ 2.4953, -1.0448])\n",
      "Grad:  tensor([-0.4882,  2.7621])\n",
      "Epoch 4426 Loss 26.084108\n",
      "Params: tensor([ 2.4954, -1.0451])\n",
      "Grad:  tensor([-0.4882,  2.7621])\n",
      "Epoch 4427 Loss 26.083321\n",
      "Params: tensor([ 2.4954, -1.0454])\n",
      "Grad:  tensor([-0.4882,  2.7620])\n",
      "Epoch 4428 Loss 26.082539\n",
      "Params: tensor([ 2.4955, -1.0457])\n",
      "Grad:  tensor([-0.4881,  2.7620])\n",
      "Epoch 4429 Loss 26.081749\n",
      "Params: tensor([ 2.4955, -1.0459])\n",
      "Grad:  tensor([-0.4881,  2.7620])\n",
      "Epoch 4430 Loss 26.080963\n",
      "Params: tensor([ 2.4956, -1.0462])\n",
      "Grad:  tensor([-0.4881,  2.7619])\n",
      "Epoch 4431 Loss 26.080177\n",
      "Params: tensor([ 2.4956, -1.0465])\n",
      "Grad:  tensor([-0.4881,  2.7619])\n",
      "Epoch 4432 Loss 26.079390\n",
      "Params: tensor([ 2.4957, -1.0468])\n",
      "Grad:  tensor([-0.4881,  2.7618])\n",
      "Epoch 4433 Loss 26.078608\n",
      "Params: tensor([ 2.4957, -1.0470])\n",
      "Grad:  tensor([-0.4881,  2.7618])\n",
      "Epoch 4434 Loss 26.077812\n",
      "Params: tensor([ 2.4958, -1.0473])\n",
      "Grad:  tensor([-0.4881,  2.7617])\n",
      "Epoch 4435 Loss 26.077028\n",
      "Params: tensor([ 2.4958, -1.0476])\n",
      "Grad:  tensor([-0.4881,  2.7617])\n",
      "Epoch 4436 Loss 26.076241\n",
      "Params: tensor([ 2.4959, -1.0479])\n",
      "Grad:  tensor([-0.4880,  2.7616])\n",
      "Epoch 4437 Loss 26.075457\n",
      "Params: tensor([ 2.4959, -1.0481])\n",
      "Grad:  tensor([-0.4880,  2.7616])\n",
      "Epoch 4438 Loss 26.074675\n",
      "Params: tensor([ 2.4960, -1.0484])\n",
      "Grad:  tensor([-0.4880,  2.7615])\n",
      "Epoch 4439 Loss 26.073883\n",
      "Params: tensor([ 2.4960, -1.0487])\n",
      "Grad:  tensor([-0.4880,  2.7615])\n",
      "Epoch 4440 Loss 26.073095\n",
      "Params: tensor([ 2.4961, -1.0490])\n",
      "Grad:  tensor([-0.4880,  2.7614])\n",
      "Epoch 4441 Loss 26.072309\n",
      "Params: tensor([ 2.4961, -1.0492])\n",
      "Grad:  tensor([-0.4880,  2.7614])\n",
      "Epoch 4442 Loss 26.071526\n",
      "Params: tensor([ 2.4962, -1.0495])\n",
      "Grad:  tensor([-0.4880,  2.7614])\n",
      "Epoch 4443 Loss 26.070742\n",
      "Params: tensor([ 2.4962, -1.0498])\n",
      "Grad:  tensor([-0.4879,  2.7613])\n",
      "Epoch 4444 Loss 26.069952\n",
      "Params: tensor([ 2.4963, -1.0501])\n",
      "Grad:  tensor([-0.4879,  2.7613])\n",
      "Epoch 4445 Loss 26.069166\n",
      "Params: tensor([ 2.4963, -1.0504])\n",
      "Grad:  tensor([-0.4879,  2.7612])\n",
      "Epoch 4446 Loss 26.068378\n",
      "Params: tensor([ 2.4964, -1.0506])\n",
      "Grad:  tensor([-0.4879,  2.7612])\n",
      "Epoch 4447 Loss 26.067598\n",
      "Params: tensor([ 2.4964, -1.0509])\n",
      "Grad:  tensor([-0.4879,  2.7611])\n",
      "Epoch 4448 Loss 26.066803\n",
      "Params: tensor([ 2.4965, -1.0512])\n",
      "Grad:  tensor([-0.4879,  2.7611])\n",
      "Epoch 4449 Loss 26.066021\n",
      "Params: tensor([ 2.4965, -1.0515])\n",
      "Grad:  tensor([-0.4879,  2.7610])\n",
      "Epoch 4450 Loss 26.065229\n",
      "Params: tensor([ 2.4966, -1.0517])\n",
      "Grad:  tensor([-0.4879,  2.7610])\n",
      "Epoch 4451 Loss 26.064451\n",
      "Params: tensor([ 2.4966, -1.0520])\n",
      "Grad:  tensor([-0.4878,  2.7609])\n",
      "Epoch 4452 Loss 26.063658\n",
      "Params: tensor([ 2.4967, -1.0523])\n",
      "Grad:  tensor([-0.4878,  2.7609])\n",
      "Epoch 4453 Loss 26.062878\n",
      "Params: tensor([ 2.4967, -1.0526])\n",
      "Grad:  tensor([-0.4878,  2.7609])\n",
      "Epoch 4454 Loss 26.062092\n",
      "Params: tensor([ 2.4968, -1.0528])\n",
      "Grad:  tensor([-0.4878,  2.7608])\n",
      "Epoch 4455 Loss 26.061304\n",
      "Params: tensor([ 2.4968, -1.0531])\n",
      "Grad:  tensor([-0.4878,  2.7608])\n",
      "Epoch 4456 Loss 26.060516\n",
      "Params: tensor([ 2.4969, -1.0534])\n",
      "Grad:  tensor([-0.4878,  2.7607])\n",
      "Epoch 4457 Loss 26.059729\n",
      "Params: tensor([ 2.4969, -1.0537])\n",
      "Grad:  tensor([-0.4877,  2.7607])\n",
      "Epoch 4458 Loss 26.058947\n",
      "Params: tensor([ 2.4970, -1.0539])\n",
      "Grad:  tensor([-0.4877,  2.7606])\n",
      "Epoch 4459 Loss 26.058165\n",
      "Params: tensor([ 2.4970, -1.0542])\n",
      "Grad:  tensor([-0.4877,  2.7606])\n",
      "Epoch 4460 Loss 26.057373\n",
      "Params: tensor([ 2.4971, -1.0545])\n",
      "Grad:  tensor([-0.4877,  2.7605])\n",
      "Epoch 4461 Loss 26.056591\n",
      "Params: tensor([ 2.4971, -1.0548])\n",
      "Grad:  tensor([-0.4877,  2.7605])\n",
      "Epoch 4462 Loss 26.055803\n",
      "Params: tensor([ 2.4972, -1.0550])\n",
      "Grad:  tensor([-0.4877,  2.7604])\n",
      "Epoch 4463 Loss 26.055017\n",
      "Params: tensor([ 2.4972, -1.0553])\n",
      "Grad:  tensor([-0.4877,  2.7604])\n",
      "Epoch 4464 Loss 26.054228\n",
      "Params: tensor([ 2.4973, -1.0556])\n",
      "Grad:  tensor([-0.4876,  2.7603])\n",
      "Epoch 4465 Loss 26.053444\n",
      "Params: tensor([ 2.4973, -1.0559])\n",
      "Grad:  tensor([-0.4876,  2.7603])\n",
      "Epoch 4466 Loss 26.052662\n",
      "Params: tensor([ 2.4974, -1.0561])\n",
      "Grad:  tensor([-0.4876,  2.7603])\n",
      "Epoch 4467 Loss 26.051874\n",
      "Params: tensor([ 2.4974, -1.0564])\n",
      "Grad:  tensor([-0.4876,  2.7602])\n",
      "Epoch 4468 Loss 26.051086\n",
      "Params: tensor([ 2.4974, -1.0567])\n",
      "Grad:  tensor([-0.4876,  2.7602])\n",
      "Epoch 4469 Loss 26.050301\n",
      "Params: tensor([ 2.4975, -1.0570])\n",
      "Grad:  tensor([-0.4876,  2.7601])\n",
      "Epoch 4470 Loss 26.049519\n",
      "Params: tensor([ 2.4975, -1.0573])\n",
      "Grad:  tensor([-0.4876,  2.7601])\n",
      "Epoch 4471 Loss 26.048729\n",
      "Params: tensor([ 2.4976, -1.0575])\n",
      "Grad:  tensor([-0.4876,  2.7600])\n",
      "Epoch 4472 Loss 26.047943\n",
      "Params: tensor([ 2.4976, -1.0578])\n",
      "Grad:  tensor([-0.4876,  2.7600])\n",
      "Epoch 4473 Loss 26.047157\n",
      "Params: tensor([ 2.4977, -1.0581])\n",
      "Grad:  tensor([-0.4875,  2.7599])\n",
      "Epoch 4474 Loss 26.046373\n",
      "Params: tensor([ 2.4977, -1.0584])\n",
      "Grad:  tensor([-0.4875,  2.7599])\n",
      "Epoch 4475 Loss 26.045591\n",
      "Params: tensor([ 2.4978, -1.0586])\n",
      "Grad:  tensor([-0.4875,  2.7598])\n",
      "Epoch 4476 Loss 26.044807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([ 2.4978, -1.0589])\n",
      "Grad:  tensor([-0.4875,  2.7598])\n",
      "Epoch 4477 Loss 26.044014\n",
      "Params: tensor([ 2.4979, -1.0592])\n",
      "Grad:  tensor([-0.4875,  2.7597])\n",
      "Epoch 4478 Loss 26.043232\n",
      "Params: tensor([ 2.4979, -1.0595])\n",
      "Grad:  tensor([-0.4875,  2.7597])\n",
      "Epoch 4479 Loss 26.042450\n",
      "Params: tensor([ 2.4980, -1.0597])\n",
      "Grad:  tensor([-0.4875,  2.7596])\n",
      "Epoch 4480 Loss 26.041658\n",
      "Params: tensor([ 2.4980, -1.0600])\n",
      "Grad:  tensor([-0.4875,  2.7596])\n",
      "Epoch 4481 Loss 26.040876\n",
      "Params: tensor([ 2.4981, -1.0603])\n",
      "Grad:  tensor([-0.4875,  2.7595])\n",
      "Epoch 4482 Loss 26.040092\n",
      "Params: tensor([ 2.4981, -1.0606])\n",
      "Grad:  tensor([-0.4875,  2.7595])\n",
      "Epoch 4483 Loss 26.039305\n",
      "Params: tensor([ 2.4982, -1.0608])\n",
      "Grad:  tensor([-0.4875,  2.7594])\n",
      "Epoch 4484 Loss 26.038521\n",
      "Params: tensor([ 2.4982, -1.0611])\n",
      "Grad:  tensor([-0.4875,  2.7594])\n",
      "Epoch 4485 Loss 26.037737\n",
      "Params: tensor([ 2.4983, -1.0614])\n",
      "Grad:  tensor([-0.4875,  2.7594])\n",
      "Epoch 4486 Loss 26.036955\n",
      "Params: tensor([ 2.4983, -1.0617])\n",
      "Grad:  tensor([-0.4875,  2.7593])\n",
      "Epoch 4487 Loss 26.036165\n",
      "Params: tensor([ 2.4984, -1.0619])\n",
      "Grad:  tensor([-0.4875,  2.7593])\n",
      "Epoch 4488 Loss 26.035381\n",
      "Params: tensor([ 2.4984, -1.0622])\n",
      "Grad:  tensor([-0.4875,  2.7592])\n",
      "Epoch 4489 Loss 26.034599\n",
      "Params: tensor([ 2.4985, -1.0625])\n",
      "Grad:  tensor([-0.4875,  2.7592])\n",
      "Epoch 4490 Loss 26.033808\n",
      "Params: tensor([ 2.4985, -1.0628])\n",
      "Grad:  tensor([-0.4875,  2.7591])\n",
      "Epoch 4491 Loss 26.033026\n",
      "Params: tensor([ 2.4986, -1.0630])\n",
      "Grad:  tensor([-0.4875,  2.7591])\n",
      "Epoch 4492 Loss 26.032240\n",
      "Params: tensor([ 2.4986, -1.0633])\n",
      "Grad:  tensor([-0.4875,  2.7590])\n",
      "Epoch 4493 Loss 26.031452\n",
      "Params: tensor([ 2.4987, -1.0636])\n",
      "Grad:  tensor([-0.4875,  2.7590])\n",
      "Epoch 4494 Loss 26.030670\n",
      "Params: tensor([ 2.4987, -1.0639])\n",
      "Grad:  tensor([-0.4875,  2.7589])\n",
      "Epoch 4495 Loss 26.029882\n",
      "Params: tensor([ 2.4988, -1.0642])\n",
      "Grad:  tensor([-0.4875,  2.7589])\n",
      "Epoch 4496 Loss 26.029100\n",
      "Params: tensor([ 2.4988, -1.0644])\n",
      "Grad:  tensor([-0.4875,  2.7588])\n",
      "Epoch 4497 Loss 26.028315\n",
      "Params: tensor([ 2.4989, -1.0647])\n",
      "Grad:  tensor([-0.4875,  2.7588])\n",
      "Epoch 4498 Loss 26.027529\n",
      "Params: tensor([ 2.4989, -1.0650])\n",
      "Grad:  tensor([-0.4875,  2.7587])\n",
      "Epoch 4499 Loss 26.026747\n",
      "Params: tensor([ 2.4990, -1.0653])\n",
      "Grad:  tensor([-0.4875,  2.7587])\n",
      "Epoch 4500 Loss 26.025965\n",
      "Params: tensor([ 2.4990, -1.0655])\n",
      "Grad:  tensor([-0.4875,  2.7586])\n",
      "Epoch 4501 Loss 26.025177\n",
      "Params: tensor([ 2.4991, -1.0658])\n",
      "Grad:  tensor([-0.4875,  2.7586])\n",
      "Epoch 4502 Loss 26.024391\n",
      "Params: tensor([ 2.4991, -1.0661])\n",
      "Grad:  tensor([-0.4875,  2.7585])\n",
      "Epoch 4503 Loss 26.023607\n",
      "Params: tensor([ 2.4992, -1.0664])\n",
      "Grad:  tensor([-0.4875,  2.7585])\n",
      "Epoch 4504 Loss 26.022827\n",
      "Params: tensor([ 2.4992, -1.0666])\n",
      "Grad:  tensor([-0.4875,  2.7584])\n",
      "Epoch 4505 Loss 26.022039\n",
      "Params: tensor([ 2.4992, -1.0669])\n",
      "Grad:  tensor([-0.4875,  2.7584])\n",
      "Epoch 4506 Loss 26.021252\n",
      "Params: tensor([ 2.4993, -1.0672])\n",
      "Grad:  tensor([-0.4875,  2.7583])\n",
      "Epoch 4507 Loss 26.020470\n",
      "Params: tensor([ 2.4993, -1.0675])\n",
      "Grad:  tensor([-0.4875,  2.7583])\n",
      "Epoch 4508 Loss 26.019686\n",
      "Params: tensor([ 2.4994, -1.0677])\n",
      "Grad:  tensor([-0.4875,  2.7583])\n",
      "Epoch 4509 Loss 26.018898\n",
      "Params: tensor([ 2.4994, -1.0680])\n",
      "Grad:  tensor([-0.4875,  2.7582])\n",
      "Epoch 4510 Loss 26.018120\n",
      "Params: tensor([ 2.4995, -1.0683])\n",
      "Grad:  tensor([-0.4875,  2.7582])\n",
      "Epoch 4511 Loss 26.017328\n",
      "Params: tensor([ 2.4995, -1.0686])\n",
      "Grad:  tensor([-0.4875,  2.7581])\n",
      "Epoch 4512 Loss 26.016546\n",
      "Params: tensor([ 2.4996, -1.0688])\n",
      "Grad:  tensor([-0.4875,  2.7581])\n",
      "Epoch 4513 Loss 26.015764\n",
      "Params: tensor([ 2.4996, -1.0691])\n",
      "Grad:  tensor([-0.4875,  2.7580])\n",
      "Epoch 4514 Loss 26.014977\n",
      "Params: tensor([ 2.4997, -1.0694])\n",
      "Grad:  tensor([-0.4875,  2.7580])\n",
      "Epoch 4515 Loss 26.014193\n",
      "Params: tensor([ 2.4997, -1.0697])\n",
      "Grad:  tensor([-0.4875,  2.7579])\n",
      "Epoch 4516 Loss 26.013411\n",
      "Params: tensor([ 2.4998, -1.0699])\n",
      "Grad:  tensor([-0.4874,  2.7579])\n",
      "Epoch 4517 Loss 26.012621\n",
      "Params: tensor([ 2.4998, -1.0702])\n",
      "Grad:  tensor([-0.4874,  2.7578])\n",
      "Epoch 4518 Loss 26.011841\n",
      "Params: tensor([ 2.4999, -1.0705])\n",
      "Grad:  tensor([-0.4874,  2.7578])\n",
      "Epoch 4519 Loss 26.011059\n",
      "Params: tensor([ 2.4999, -1.0708])\n",
      "Grad:  tensor([-0.4874,  2.7577])\n",
      "Epoch 4520 Loss 26.010277\n",
      "Params: tensor([ 2.5000, -1.0710])\n",
      "Grad:  tensor([-0.4874,  2.7577])\n",
      "Epoch 4521 Loss 26.009483\n",
      "Params: tensor([ 2.5000, -1.0713])\n",
      "Grad:  tensor([-0.4874,  2.7576])\n",
      "Epoch 4522 Loss 26.008703\n",
      "Params: tensor([ 2.5001, -1.0716])\n",
      "Grad:  tensor([-0.4874,  2.7576])\n",
      "Epoch 4523 Loss 26.007923\n",
      "Params: tensor([ 2.5001, -1.0719])\n",
      "Grad:  tensor([-0.4874,  2.7575])\n",
      "Epoch 4524 Loss 26.007139\n",
      "Params: tensor([ 2.5002, -1.0721])\n",
      "Grad:  tensor([-0.4874,  2.7575])\n",
      "Epoch 4525 Loss 26.006353\n",
      "Params: tensor([ 2.5002, -1.0724])\n",
      "Grad:  tensor([-0.4874,  2.7574])\n",
      "Epoch 4526 Loss 26.005568\n",
      "Params: tensor([ 2.5003, -1.0727])\n",
      "Grad:  tensor([-0.4874,  2.7574])\n",
      "Epoch 4527 Loss 26.004789\n",
      "Params: tensor([ 2.5003, -1.0730])\n",
      "Grad:  tensor([-0.4874,  2.7573])\n",
      "Epoch 4528 Loss 26.003998\n",
      "Params: tensor([ 2.5004, -1.0733])\n",
      "Grad:  tensor([-0.4874,  2.7573])\n",
      "Epoch 4529 Loss 26.003222\n",
      "Params: tensor([ 2.5004, -1.0735])\n",
      "Grad:  tensor([-0.4874,  2.7572])\n",
      "Epoch 4530 Loss 26.002430\n",
      "Params: tensor([ 2.5005, -1.0738])\n",
      "Grad:  tensor([-0.4874,  2.7572])\n",
      "Epoch 4531 Loss 26.001648\n",
      "Params: tensor([ 2.5005, -1.0741])\n",
      "Grad:  tensor([-0.4874,  2.7572])\n",
      "Epoch 4532 Loss 26.000872\n",
      "Params: tensor([ 2.5006, -1.0744])\n",
      "Grad:  tensor([-0.4874,  2.7571])\n",
      "Epoch 4533 Loss 26.000080\n",
      "Params: tensor([ 2.5006, -1.0746])\n",
      "Grad:  tensor([-0.4874,  2.7571])\n",
      "Epoch 4534 Loss 25.999298\n",
      "Params: tensor([ 2.5007, -1.0749])\n",
      "Grad:  tensor([-0.4874,  2.7570])\n",
      "Epoch 4535 Loss 25.998510\n",
      "Params: tensor([ 2.5007, -1.0752])\n",
      "Grad:  tensor([-0.4874,  2.7570])\n",
      "Epoch 4536 Loss 25.997728\n",
      "Params: tensor([ 2.5008, -1.0755])\n",
      "Grad:  tensor([-0.4874,  2.7569])\n",
      "Epoch 4537 Loss 25.996941\n",
      "Params: tensor([ 2.5008, -1.0757])\n",
      "Grad:  tensor([-0.4874,  2.7569])\n",
      "Epoch 4538 Loss 25.996161\n",
      "Params: tensor([ 2.5009, -1.0760])\n",
      "Grad:  tensor([-0.4874,  2.7568])\n",
      "Epoch 4539 Loss 25.995378\n",
      "Params: tensor([ 2.5009, -1.0763])\n",
      "Grad:  tensor([-0.4874,  2.7568])\n",
      "Epoch 4540 Loss 25.994593\n",
      "Params: tensor([ 2.5010, -1.0766])\n",
      "Grad:  tensor([-0.4873,  2.7567])\n",
      "Epoch 4541 Loss 25.993813\n",
      "Params: tensor([ 2.5010, -1.0768])\n",
      "Grad:  tensor([-0.4873,  2.7567])\n",
      "Epoch 4542 Loss 25.993025\n",
      "Params: tensor([ 2.5010, -1.0771])\n",
      "Grad:  tensor([-0.4873,  2.7566])\n",
      "Epoch 4543 Loss 25.992247\n",
      "Params: tensor([ 2.5011, -1.0774])\n",
      "Grad:  tensor([-0.4873,  2.7566])\n",
      "Epoch 4544 Loss 25.991455\n",
      "Params: tensor([ 2.5011, -1.0777])\n",
      "Grad:  tensor([-0.4873,  2.7565])\n",
      "Epoch 4545 Loss 25.990679\n",
      "Params: tensor([ 2.5012, -1.0779])\n",
      "Grad:  tensor([-0.4873,  2.7565])\n",
      "Epoch 4546 Loss 25.989893\n",
      "Params: tensor([ 2.5012, -1.0782])\n",
      "Grad:  tensor([-0.4873,  2.7564])\n",
      "Epoch 4547 Loss 25.989107\n",
      "Params: tensor([ 2.5013, -1.0785])\n",
      "Grad:  tensor([-0.4873,  2.7564])\n",
      "Epoch 4548 Loss 25.988325\n",
      "Params: tensor([ 2.5013, -1.0788])\n",
      "Grad:  tensor([-0.4873,  2.7563])\n",
      "Epoch 4549 Loss 25.987541\n",
      "Params: tensor([ 2.5014, -1.0790])\n",
      "Grad:  tensor([-0.4873,  2.7563])\n",
      "Epoch 4550 Loss 25.986761\n",
      "Params: tensor([ 2.5014, -1.0793])\n",
      "Grad:  tensor([-0.4873,  2.7563])\n",
      "Epoch 4551 Loss 25.985979\n",
      "Params: tensor([ 2.5015, -1.0796])\n",
      "Grad:  tensor([-0.4873,  2.7562])\n",
      "Epoch 4552 Loss 25.985191\n",
      "Params: tensor([ 2.5015, -1.0799])\n",
      "Grad:  tensor([-0.4873,  2.7562])\n",
      "Epoch 4553 Loss 25.984411\n",
      "Params: tensor([ 2.5016, -1.0801])\n",
      "Grad:  tensor([-0.4873,  2.7561])\n",
      "Epoch 4554 Loss 25.983625\n",
      "Params: tensor([ 2.5016, -1.0804])\n",
      "Grad:  tensor([-0.4873,  2.7561])\n",
      "Epoch 4555 Loss 25.982843\n",
      "Params: tensor([ 2.5017, -1.0807])\n",
      "Grad:  tensor([-0.4873,  2.7560])\n",
      "Epoch 4556 Loss 25.982058\n",
      "Params: tensor([ 2.5017, -1.0810])\n",
      "Grad:  tensor([-0.4873,  2.7560])\n",
      "Epoch 4557 Loss 25.981279\n",
      "Params: tensor([ 2.5018, -1.0812])\n",
      "Grad:  tensor([-0.4873,  2.7559])\n",
      "Epoch 4558 Loss 25.980494\n",
      "Params: tensor([ 2.5018, -1.0815])\n",
      "Grad:  tensor([-0.4873,  2.7559])\n",
      "Epoch 4559 Loss 25.979712\n",
      "Params: tensor([ 2.5019, -1.0818])\n",
      "Grad:  tensor([-0.4872,  2.7558])\n",
      "Epoch 4560 Loss 25.978926\n",
      "Params: tensor([ 2.5019, -1.0821])\n",
      "Grad:  tensor([-0.4872,  2.7558])\n",
      "Epoch 4561 Loss 25.978144\n",
      "Params: tensor([ 2.5020, -1.0823])\n",
      "Grad:  tensor([-0.4872,  2.7557])\n",
      "Epoch 4562 Loss 25.977356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([ 2.5020, -1.0826])\n",
      "Grad:  tensor([-0.4872,  2.7557])\n",
      "Epoch 4563 Loss 25.976580\n",
      "Params: tensor([ 2.5021, -1.0829])\n",
      "Grad:  tensor([-0.4872,  2.7556])\n",
      "Epoch 4564 Loss 25.975792\n",
      "Params: tensor([ 2.5021, -1.0832])\n",
      "Grad:  tensor([-0.4872,  2.7556])\n",
      "Epoch 4565 Loss 25.975012\n",
      "Params: tensor([ 2.5022, -1.0835])\n",
      "Grad:  tensor([-0.4872,  2.7555])\n",
      "Epoch 4566 Loss 25.974224\n",
      "Params: tensor([ 2.5022, -1.0837])\n",
      "Grad:  tensor([-0.4872,  2.7555])\n",
      "Epoch 4567 Loss 25.973448\n",
      "Params: tensor([ 2.5023, -1.0840])\n",
      "Grad:  tensor([-0.4872,  2.7554])\n",
      "Epoch 4568 Loss 25.972668\n",
      "Params: tensor([ 2.5023, -1.0843])\n",
      "Grad:  tensor([-0.4872,  2.7554])\n",
      "Epoch 4569 Loss 25.971880\n",
      "Params: tensor([ 2.5024, -1.0846])\n",
      "Grad:  tensor([-0.4872,  2.7554])\n",
      "Epoch 4570 Loss 25.971094\n",
      "Params: tensor([ 2.5024, -1.0848])\n",
      "Grad:  tensor([-0.4872,  2.7553])\n",
      "Epoch 4571 Loss 25.970312\n",
      "Params: tensor([ 2.5025, -1.0851])\n",
      "Grad:  tensor([-0.4872,  2.7553])\n",
      "Epoch 4572 Loss 25.969532\n",
      "Params: tensor([ 2.5025, -1.0854])\n",
      "Grad:  tensor([-0.4872,  2.7552])\n",
      "Epoch 4573 Loss 25.968744\n",
      "Params: tensor([ 2.5026, -1.0857])\n",
      "Grad:  tensor([-0.4872,  2.7552])\n",
      "Epoch 4574 Loss 25.967964\n",
      "Params: tensor([ 2.5026, -1.0859])\n",
      "Grad:  tensor([-0.4872,  2.7551])\n",
      "Epoch 4575 Loss 25.967182\n",
      "Params: tensor([ 2.5027, -1.0862])\n",
      "Grad:  tensor([-0.4871,  2.7551])\n",
      "Epoch 4576 Loss 25.966402\n",
      "Params: tensor([ 2.5027, -1.0865])\n",
      "Grad:  tensor([-0.4871,  2.7550])\n",
      "Epoch 4577 Loss 25.965624\n",
      "Params: tensor([ 2.5028, -1.0868])\n",
      "Grad:  tensor([-0.4871,  2.7550])\n",
      "Epoch 4578 Loss 25.964832\n",
      "Params: tensor([ 2.5028, -1.0870])\n",
      "Grad:  tensor([-0.4871,  2.7549])\n",
      "Epoch 4579 Loss 25.964052\n",
      "Params: tensor([ 2.5028, -1.0873])\n",
      "Grad:  tensor([-0.4871,  2.7549])\n",
      "Epoch 4580 Loss 25.963264\n",
      "Params: tensor([ 2.5029, -1.0876])\n",
      "Grad:  tensor([-0.4871,  2.7548])\n",
      "Epoch 4581 Loss 25.962488\n",
      "Params: tensor([ 2.5029, -1.0879])\n",
      "Grad:  tensor([-0.4871,  2.7548])\n",
      "Epoch 4582 Loss 25.961702\n",
      "Params: tensor([ 2.5030, -1.0881])\n",
      "Grad:  tensor([-0.4871,  2.7547])\n",
      "Epoch 4583 Loss 25.960924\n",
      "Params: tensor([ 2.5030, -1.0884])\n",
      "Grad:  tensor([-0.4871,  2.7547])\n",
      "Epoch 4584 Loss 25.960142\n",
      "Params: tensor([ 2.5031, -1.0887])\n",
      "Grad:  tensor([-0.4871,  2.7546])\n",
      "Epoch 4585 Loss 25.959352\n",
      "Params: tensor([ 2.5031, -1.0890])\n",
      "Grad:  tensor([-0.4871,  2.7546])\n",
      "Epoch 4586 Loss 25.958576\n",
      "Params: tensor([ 2.5032, -1.0892])\n",
      "Grad:  tensor([-0.4871,  2.7546])\n",
      "Epoch 4587 Loss 25.957788\n",
      "Params: tensor([ 2.5032, -1.0895])\n",
      "Grad:  tensor([-0.4871,  2.7545])\n",
      "Epoch 4588 Loss 25.957008\n",
      "Params: tensor([ 2.5033, -1.0898])\n",
      "Grad:  tensor([-0.4871,  2.7545])\n",
      "Epoch 4589 Loss 25.956224\n",
      "Params: tensor([ 2.5033, -1.0901])\n",
      "Grad:  tensor([-0.4871,  2.7544])\n",
      "Epoch 4590 Loss 25.955444\n",
      "Params: tensor([ 2.5034, -1.0903])\n",
      "Grad:  tensor([-0.4871,  2.7544])\n",
      "Epoch 4591 Loss 25.954662\n",
      "Params: tensor([ 2.5034, -1.0906])\n",
      "Grad:  tensor([-0.4870,  2.7543])\n",
      "Epoch 4592 Loss 25.953875\n",
      "Params: tensor([ 2.5035, -1.0909])\n",
      "Grad:  tensor([-0.4870,  2.7543])\n",
      "Epoch 4593 Loss 25.953093\n",
      "Params: tensor([ 2.5035, -1.0912])\n",
      "Grad:  tensor([-0.4870,  2.7542])\n",
      "Epoch 4594 Loss 25.952312\n",
      "Params: tensor([ 2.5036, -1.0914])\n",
      "Grad:  tensor([-0.4870,  2.7542])\n",
      "Epoch 4595 Loss 25.951536\n",
      "Params: tensor([ 2.5036, -1.0917])\n",
      "Grad:  tensor([-0.4870,  2.7541])\n",
      "Epoch 4596 Loss 25.950747\n",
      "Params: tensor([ 2.5037, -1.0920])\n",
      "Grad:  tensor([-0.4870,  2.7541])\n",
      "Epoch 4597 Loss 25.949965\n",
      "Params: tensor([ 2.5037, -1.0923])\n",
      "Grad:  tensor([-0.4870,  2.7540])\n",
      "Epoch 4598 Loss 25.949188\n",
      "Params: tensor([ 2.5038, -1.0925])\n",
      "Grad:  tensor([-0.4870,  2.7540])\n",
      "Epoch 4599 Loss 25.948402\n",
      "Params: tensor([ 2.5038, -1.0928])\n",
      "Grad:  tensor([-0.4870,  2.7539])\n",
      "Epoch 4600 Loss 25.947620\n",
      "Params: tensor([ 2.5039, -1.0931])\n",
      "Grad:  tensor([-0.4870,  2.7539])\n",
      "Epoch 4601 Loss 25.946838\n",
      "Params: tensor([ 2.5039, -1.0934])\n",
      "Grad:  tensor([-0.4870,  2.7538])\n",
      "Epoch 4602 Loss 25.946056\n",
      "Params: tensor([ 2.5040, -1.0936])\n",
      "Grad:  tensor([-0.4870,  2.7538])\n",
      "Epoch 4603 Loss 25.945280\n",
      "Params: tensor([ 2.5040, -1.0939])\n",
      "Grad:  tensor([-0.4870,  2.7538])\n",
      "Epoch 4604 Loss 25.944494\n",
      "Params: tensor([ 2.5041, -1.0942])\n",
      "Grad:  tensor([-0.4869,  2.7537])\n",
      "Epoch 4605 Loss 25.943708\n",
      "Params: tensor([ 2.5041, -1.0945])\n",
      "Grad:  tensor([-0.4869,  2.7537])\n",
      "Epoch 4606 Loss 25.942930\n",
      "Params: tensor([ 2.5042, -1.0947])\n",
      "Grad:  tensor([-0.4869,  2.7536])\n",
      "Epoch 4607 Loss 25.942146\n",
      "Params: tensor([ 2.5042, -1.0950])\n",
      "Grad:  tensor([-0.4869,  2.7536])\n",
      "Epoch 4608 Loss 25.941368\n",
      "Params: tensor([ 2.5043, -1.0953])\n",
      "Grad:  tensor([-0.4869,  2.7535])\n",
      "Epoch 4609 Loss 25.940582\n",
      "Params: tensor([ 2.5043, -1.0956])\n",
      "Grad:  tensor([-0.4869,  2.7535])\n",
      "Epoch 4610 Loss 25.939802\n",
      "Params: tensor([ 2.5044, -1.0958])\n",
      "Grad:  tensor([-0.4869,  2.7534])\n",
      "Epoch 4611 Loss 25.939024\n",
      "Params: tensor([ 2.5044, -1.0961])\n",
      "Grad:  tensor([-0.4869,  2.7534])\n",
      "Epoch 4612 Loss 25.938240\n",
      "Params: tensor([ 2.5045, -1.0964])\n",
      "Grad:  tensor([-0.4869,  2.7533])\n",
      "Epoch 4613 Loss 25.937456\n",
      "Params: tensor([ 2.5045, -1.0967])\n",
      "Grad:  tensor([-0.4869,  2.7533])\n",
      "Epoch 4614 Loss 25.936676\n",
      "Params: tensor([ 2.5045, -1.0969])\n",
      "Grad:  tensor([-0.4869,  2.7532])\n",
      "Epoch 4615 Loss 25.935894\n",
      "Params: tensor([ 2.5046, -1.0972])\n",
      "Grad:  tensor([-0.4869,  2.7532])\n",
      "Epoch 4616 Loss 25.935108\n",
      "Params: tensor([ 2.5046, -1.0975])\n",
      "Grad:  tensor([-0.4869,  2.7531])\n",
      "Epoch 4617 Loss 25.934328\n",
      "Params: tensor([ 2.5047, -1.0978])\n",
      "Grad:  tensor([-0.4869,  2.7531])\n",
      "Epoch 4618 Loss 25.933550\n",
      "Params: tensor([ 2.5047, -1.0980])\n",
      "Grad:  tensor([-0.4868,  2.7531])\n",
      "Epoch 4619 Loss 25.932768\n",
      "Params: tensor([ 2.5048, -1.0983])\n",
      "Grad:  tensor([-0.4868,  2.7530])\n",
      "Epoch 4620 Loss 25.931984\n",
      "Params: tensor([ 2.5048, -1.0986])\n",
      "Grad:  tensor([-0.4868,  2.7530])\n",
      "Epoch 4621 Loss 25.931202\n",
      "Params: tensor([ 2.5049, -1.0989])\n",
      "Grad:  tensor([-0.4868,  2.7529])\n",
      "Epoch 4622 Loss 25.930422\n",
      "Params: tensor([ 2.5049, -1.0992])\n",
      "Grad:  tensor([-0.4868,  2.7529])\n",
      "Epoch 4623 Loss 25.929644\n",
      "Params: tensor([ 2.5050, -1.0994])\n",
      "Grad:  tensor([-0.4868,  2.7528])\n",
      "Epoch 4624 Loss 25.928862\n",
      "Params: tensor([ 2.5050, -1.0997])\n",
      "Grad:  tensor([-0.4868,  2.7528])\n",
      "Epoch 4625 Loss 25.928076\n",
      "Params: tensor([ 2.5051, -1.1000])\n",
      "Grad:  tensor([-0.4868,  2.7527])\n",
      "Epoch 4626 Loss 25.927301\n",
      "Params: tensor([ 2.5051, -1.1003])\n",
      "Grad:  tensor([-0.4868,  2.7527])\n",
      "Epoch 4627 Loss 25.926516\n",
      "Params: tensor([ 2.5052, -1.1005])\n",
      "Grad:  tensor([-0.4868,  2.7526])\n",
      "Epoch 4628 Loss 25.925732\n",
      "Params: tensor([ 2.5052, -1.1008])\n",
      "Grad:  tensor([-0.4867,  2.7526])\n",
      "Epoch 4629 Loss 25.924957\n",
      "Params: tensor([ 2.5053, -1.1011])\n",
      "Grad:  tensor([-0.4867,  2.7525])\n",
      "Epoch 4630 Loss 25.924171\n",
      "Params: tensor([ 2.5053, -1.1014])\n",
      "Grad:  tensor([-0.4867,  2.7525])\n",
      "Epoch 4631 Loss 25.923389\n",
      "Params: tensor([ 2.5054, -1.1016])\n",
      "Grad:  tensor([-0.4867,  2.7524])\n",
      "Epoch 4632 Loss 25.922607\n",
      "Params: tensor([ 2.5054, -1.1019])\n",
      "Grad:  tensor([-0.4867,  2.7524])\n",
      "Epoch 4633 Loss 25.921827\n",
      "Params: tensor([ 2.5055, -1.1022])\n",
      "Grad:  tensor([-0.4867,  2.7524])\n",
      "Epoch 4634 Loss 25.921043\n",
      "Params: tensor([ 2.5055, -1.1025])\n",
      "Grad:  tensor([-0.4867,  2.7523])\n",
      "Epoch 4635 Loss 25.920265\n",
      "Params: tensor([ 2.5056, -1.1027])\n",
      "Grad:  tensor([-0.4867,  2.7523])\n",
      "Epoch 4636 Loss 25.919487\n",
      "Params: tensor([ 2.5056, -1.1030])\n",
      "Grad:  tensor([-0.4867,  2.7522])\n",
      "Epoch 4637 Loss 25.918703\n",
      "Params: tensor([ 2.5057, -1.1033])\n",
      "Grad:  tensor([-0.4867,  2.7522])\n",
      "Epoch 4638 Loss 25.917921\n",
      "Params: tensor([ 2.5057, -1.1036])\n",
      "Grad:  tensor([-0.4867,  2.7521])\n",
      "Epoch 4639 Loss 25.917143\n",
      "Params: tensor([ 2.5058, -1.1038])\n",
      "Grad:  tensor([-0.4866,  2.7521])\n",
      "Epoch 4640 Loss 25.916359\n",
      "Params: tensor([ 2.5058, -1.1041])\n",
      "Grad:  tensor([-0.4866,  2.7520])\n",
      "Epoch 4641 Loss 25.915575\n",
      "Params: tensor([ 2.5059, -1.1044])\n",
      "Grad:  tensor([-0.4866,  2.7520])\n",
      "Epoch 4642 Loss 25.914803\n",
      "Params: tensor([ 2.5059, -1.1047])\n",
      "Grad:  tensor([-0.4866,  2.7519])\n",
      "Epoch 4643 Loss 25.914019\n",
      "Params: tensor([ 2.5060, -1.1049])\n",
      "Grad:  tensor([-0.4866,  2.7519])\n",
      "Epoch 4644 Loss 25.913233\n",
      "Params: tensor([ 2.5060, -1.1052])\n",
      "Grad:  tensor([-0.4866,  2.7518])\n",
      "Epoch 4645 Loss 25.912451\n",
      "Params: tensor([ 2.5061, -1.1055])\n",
      "Grad:  tensor([-0.4866,  2.7518])\n",
      "Epoch 4646 Loss 25.911676\n",
      "Params: tensor([ 2.5061, -1.1058])\n",
      "Grad:  tensor([-0.4866,  2.7517])\n",
      "Epoch 4647 Loss 25.910891\n",
      "Params: tensor([ 2.5062, -1.1060])\n",
      "Grad:  tensor([-0.4866,  2.7517])\n",
      "Epoch 4648 Loss 25.910112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([ 2.5062, -1.1063])\n",
      "Grad:  tensor([-0.4866,  2.7517])\n",
      "Epoch 4649 Loss 25.909327\n",
      "Params: tensor([ 2.5063, -1.1066])\n",
      "Grad:  tensor([-0.4865,  2.7516])\n",
      "Epoch 4650 Loss 25.908552\n",
      "Params: tensor([ 2.5063, -1.1069])\n",
      "Grad:  tensor([-0.4865,  2.7516])\n",
      "Epoch 4651 Loss 25.907770\n",
      "Params: tensor([ 2.5063, -1.1071])\n",
      "Grad:  tensor([-0.4865,  2.7515])\n",
      "Epoch 4652 Loss 25.906984\n",
      "Params: tensor([ 2.5064, -1.1074])\n",
      "Grad:  tensor([-0.4865,  2.7515])\n",
      "Epoch 4653 Loss 25.906206\n",
      "Params: tensor([ 2.5064, -1.1077])\n",
      "Grad:  tensor([-0.4865,  2.7514])\n",
      "Epoch 4654 Loss 25.905428\n",
      "Params: tensor([ 2.5065, -1.1080])\n",
      "Grad:  tensor([-0.4865,  2.7514])\n",
      "Epoch 4655 Loss 25.904646\n",
      "Params: tensor([ 2.5065, -1.1082])\n",
      "Grad:  tensor([-0.4865,  2.7513])\n",
      "Epoch 4656 Loss 25.903864\n",
      "Params: tensor([ 2.5066, -1.1085])\n",
      "Grad:  tensor([-0.4865,  2.7513])\n",
      "Epoch 4657 Loss 25.903082\n",
      "Params: tensor([ 2.5066, -1.1088])\n",
      "Grad:  tensor([-0.4865,  2.7512])\n",
      "Epoch 4658 Loss 25.902306\n",
      "Params: tensor([ 2.5067, -1.1091])\n",
      "Grad:  tensor([-0.4865,  2.7512])\n",
      "Epoch 4659 Loss 25.901520\n",
      "Params: tensor([ 2.5067, -1.1093])\n",
      "Grad:  tensor([-0.4864,  2.7511])\n",
      "Epoch 4660 Loss 25.900743\n",
      "Params: tensor([ 2.5068, -1.1096])\n",
      "Grad:  tensor([-0.4864,  2.7511])\n",
      "Epoch 4661 Loss 25.899961\n",
      "Params: tensor([ 2.5068, -1.1099])\n",
      "Grad:  tensor([-0.4864,  2.7511])\n",
      "Epoch 4662 Loss 25.899187\n",
      "Params: tensor([ 2.5069, -1.1102])\n",
      "Grad:  tensor([-0.4864,  2.7510])\n",
      "Epoch 4663 Loss 25.898401\n",
      "Params: tensor([ 2.5069, -1.1104])\n",
      "Grad:  tensor([-0.4864,  2.7510])\n",
      "Epoch 4664 Loss 25.897619\n",
      "Params: tensor([ 2.5070, -1.1107])\n",
      "Grad:  tensor([-0.4864,  2.7509])\n",
      "Epoch 4665 Loss 25.896839\n",
      "Params: tensor([ 2.5070, -1.1110])\n",
      "Grad:  tensor([-0.4864,  2.7509])\n",
      "Epoch 4666 Loss 25.896063\n",
      "Params: tensor([ 2.5071, -1.1113])\n",
      "Grad:  tensor([-0.4864,  2.7508])\n",
      "Epoch 4667 Loss 25.895281\n",
      "Params: tensor([ 2.5071, -1.1115])\n",
      "Grad:  tensor([-0.4864,  2.7508])\n",
      "Epoch 4668 Loss 25.894503\n",
      "Params: tensor([ 2.5072, -1.1118])\n",
      "Grad:  tensor([-0.4864,  2.7507])\n",
      "Epoch 4669 Loss 25.893719\n",
      "Params: tensor([ 2.5072, -1.1121])\n",
      "Grad:  tensor([-0.4863,  2.7507])\n",
      "Epoch 4670 Loss 25.892944\n",
      "Params: tensor([ 2.5073, -1.1124])\n",
      "Grad:  tensor([-0.4863,  2.7506])\n",
      "Epoch 4671 Loss 25.892162\n",
      "Params: tensor([ 2.5073, -1.1126])\n",
      "Grad:  tensor([-0.4863,  2.7506])\n",
      "Epoch 4672 Loss 25.891376\n",
      "Params: tensor([ 2.5074, -1.1129])\n",
      "Grad:  tensor([-0.4863,  2.7505])\n",
      "Epoch 4673 Loss 25.890596\n",
      "Params: tensor([ 2.5074, -1.1132])\n",
      "Grad:  tensor([-0.4863,  2.7505])\n",
      "Epoch 4674 Loss 25.889820\n",
      "Params: tensor([ 2.5075, -1.1135])\n",
      "Grad:  tensor([-0.4863,  2.7505])\n",
      "Epoch 4675 Loss 25.889038\n",
      "Params: tensor([ 2.5075, -1.1137])\n",
      "Grad:  tensor([-0.4863,  2.7504])\n",
      "Epoch 4676 Loss 25.888264\n",
      "Params: tensor([ 2.5076, -1.1140])\n",
      "Grad:  tensor([-0.4863,  2.7504])\n",
      "Epoch 4677 Loss 25.887476\n",
      "Params: tensor([ 2.5076, -1.1143])\n",
      "Grad:  tensor([-0.4863,  2.7503])\n",
      "Epoch 4678 Loss 25.886700\n",
      "Params: tensor([ 2.5077, -1.1146])\n",
      "Grad:  tensor([-0.4862,  2.7503])\n",
      "Epoch 4679 Loss 25.885918\n",
      "Params: tensor([ 2.5077, -1.1148])\n",
      "Grad:  tensor([-0.4862,  2.7502])\n",
      "Epoch 4680 Loss 25.885134\n",
      "Params: tensor([ 2.5078, -1.1151])\n",
      "Grad:  tensor([-0.4862,  2.7502])\n",
      "Epoch 4681 Loss 25.884363\n",
      "Params: tensor([ 2.5078, -1.1154])\n",
      "Grad:  tensor([-0.4862,  2.7501])\n",
      "Epoch 4682 Loss 25.883575\n",
      "Params: tensor([ 2.5079, -1.1157])\n",
      "Grad:  tensor([-0.4862,  2.7501])\n",
      "Epoch 4683 Loss 25.882799\n",
      "Params: tensor([ 2.5079, -1.1159])\n",
      "Grad:  tensor([-0.4862,  2.7500])\n",
      "Epoch 4684 Loss 25.882019\n",
      "Params: tensor([ 2.5080, -1.1162])\n",
      "Grad:  tensor([-0.4862,  2.7500])\n",
      "Epoch 4685 Loss 25.881237\n",
      "Params: tensor([ 2.5080, -1.1165])\n",
      "Grad:  tensor([-0.4862,  2.7499])\n",
      "Epoch 4686 Loss 25.880461\n",
      "Params: tensor([ 2.5081, -1.1168])\n",
      "Grad:  tensor([-0.4861,  2.7499])\n",
      "Epoch 4687 Loss 25.879683\n",
      "Params: tensor([ 2.5081, -1.1170])\n",
      "Grad:  tensor([-0.4861,  2.7499])\n",
      "Epoch 4688 Loss 25.878901\n",
      "Params: tensor([ 2.5081, -1.1173])\n",
      "Grad:  tensor([-0.4861,  2.7498])\n",
      "Epoch 4689 Loss 25.878119\n",
      "Params: tensor([ 2.5082, -1.1176])\n",
      "Grad:  tensor([-0.4861,  2.7498])\n",
      "Epoch 4690 Loss 25.877338\n",
      "Params: tensor([ 2.5082, -1.1179])\n",
      "Grad:  tensor([-0.4861,  2.7497])\n",
      "Epoch 4691 Loss 25.876562\n",
      "Params: tensor([ 2.5083, -1.1181])\n",
      "Grad:  tensor([-0.4861,  2.7497])\n",
      "Epoch 4692 Loss 25.875780\n",
      "Params: tensor([ 2.5083, -1.1184])\n",
      "Grad:  tensor([-0.4861,  2.7496])\n",
      "Epoch 4693 Loss 25.875002\n",
      "Params: tensor([ 2.5084, -1.1187])\n",
      "Grad:  tensor([-0.4861,  2.7496])\n",
      "Epoch 4694 Loss 25.874220\n",
      "Params: tensor([ 2.5084, -1.1190])\n",
      "Grad:  tensor([-0.4861,  2.7495])\n",
      "Epoch 4695 Loss 25.873438\n",
      "Params: tensor([ 2.5085, -1.1192])\n",
      "Grad:  tensor([-0.4860,  2.7495])\n",
      "Epoch 4696 Loss 25.872663\n",
      "Params: tensor([ 2.5085, -1.1195])\n",
      "Grad:  tensor([-0.4860,  2.7494])\n",
      "Epoch 4697 Loss 25.871881\n",
      "Params: tensor([ 2.5086, -1.1198])\n",
      "Grad:  tensor([-0.4860,  2.7494])\n",
      "Epoch 4698 Loss 25.871105\n",
      "Params: tensor([ 2.5086, -1.1201])\n",
      "Grad:  tensor([-0.4860,  2.7493])\n",
      "Epoch 4699 Loss 25.870323\n",
      "Params: tensor([ 2.5087, -1.1203])\n",
      "Grad:  tensor([-0.4860,  2.7493])\n",
      "Epoch 4700 Loss 25.869545\n",
      "Params: tensor([ 2.5087, -1.1206])\n",
      "Grad:  tensor([-0.4860,  2.7493])\n",
      "Epoch 4701 Loss 25.868767\n",
      "Params: tensor([ 2.5088, -1.1209])\n",
      "Grad:  tensor([-0.4860,  2.7492])\n",
      "Epoch 4702 Loss 25.867987\n",
      "Params: tensor([ 2.5088, -1.1212])\n",
      "Grad:  tensor([-0.4860,  2.7492])\n",
      "Epoch 4703 Loss 25.867205\n",
      "Params: tensor([ 2.5089, -1.1214])\n",
      "Grad:  tensor([-0.4859,  2.7491])\n",
      "Epoch 4704 Loss 25.866426\n",
      "Params: tensor([ 2.5089, -1.1217])\n",
      "Grad:  tensor([-0.4859,  2.7491])\n",
      "Epoch 4705 Loss 25.865639\n",
      "Params: tensor([ 2.5090, -1.1220])\n",
      "Grad:  tensor([-0.4859,  2.7490])\n",
      "Epoch 4706 Loss 25.864870\n",
      "Params: tensor([ 2.5090, -1.1223])\n",
      "Grad:  tensor([-0.4859,  2.7490])\n",
      "Epoch 4707 Loss 25.864088\n",
      "Params: tensor([ 2.5091, -1.1225])\n",
      "Grad:  tensor([-0.4859,  2.7489])\n",
      "Epoch 4708 Loss 25.863310\n",
      "Params: tensor([ 2.5091, -1.1228])\n",
      "Grad:  tensor([-0.4859,  2.7489])\n",
      "Epoch 4709 Loss 25.862526\n",
      "Params: tensor([ 2.5092, -1.1231])\n",
      "Grad:  tensor([-0.4859,  2.7488])\n",
      "Epoch 4710 Loss 25.861753\n",
      "Params: tensor([ 2.5092, -1.1234])\n",
      "Grad:  tensor([-0.4859,  2.7488])\n",
      "Epoch 4711 Loss 25.860970\n",
      "Params: tensor([ 2.5093, -1.1236])\n",
      "Grad:  tensor([-0.4858,  2.7487])\n",
      "Epoch 4712 Loss 25.860191\n",
      "Params: tensor([ 2.5093, -1.1239])\n",
      "Grad:  tensor([-0.4858,  2.7487])\n",
      "Epoch 4713 Loss 25.859413\n",
      "Params: tensor([ 2.5094, -1.1242])\n",
      "Grad:  tensor([-0.4858,  2.7487])\n",
      "Epoch 4714 Loss 25.858637\n",
      "Params: tensor([ 2.5094, -1.1245])\n",
      "Grad:  tensor([-0.4858,  2.7486])\n",
      "Epoch 4715 Loss 25.857857\n",
      "Params: tensor([ 2.5095, -1.1247])\n",
      "Grad:  tensor([-0.4858,  2.7486])\n",
      "Epoch 4716 Loss 25.857073\n",
      "Params: tensor([ 2.5095, -1.1250])\n",
      "Grad:  tensor([-0.4858,  2.7485])\n",
      "Epoch 4717 Loss 25.856293\n",
      "Params: tensor([ 2.5096, -1.1253])\n",
      "Grad:  tensor([-0.4858,  2.7485])\n",
      "Epoch 4718 Loss 25.855513\n",
      "Params: tensor([ 2.5096, -1.1256])\n",
      "Grad:  tensor([-0.4857,  2.7484])\n",
      "Epoch 4719 Loss 25.854736\n",
      "Params: tensor([ 2.5097, -1.1258])\n",
      "Grad:  tensor([-0.4857,  2.7484])\n",
      "Epoch 4720 Loss 25.853956\n",
      "Params: tensor([ 2.5097, -1.1261])\n",
      "Grad:  tensor([-0.4857,  2.7483])\n",
      "Epoch 4721 Loss 25.853178\n",
      "Params: tensor([ 2.5098, -1.1264])\n",
      "Grad:  tensor([-0.4857,  2.7483])\n",
      "Epoch 4722 Loss 25.852398\n",
      "Params: tensor([ 2.5098, -1.1267])\n",
      "Grad:  tensor([-0.4857,  2.7482])\n",
      "Epoch 4723 Loss 25.851622\n",
      "Params: tensor([ 2.5099, -1.1269])\n",
      "Grad:  tensor([-0.4857,  2.7482])\n",
      "Epoch 4724 Loss 25.850847\n",
      "Params: tensor([ 2.5099, -1.1272])\n",
      "Grad:  tensor([-0.4857,  2.7482])\n",
      "Epoch 4725 Loss 25.850065\n",
      "Params: tensor([ 2.5099, -1.1275])\n",
      "Grad:  tensor([-0.4857,  2.7481])\n",
      "Epoch 4726 Loss 25.849287\n",
      "Params: tensor([ 2.5100, -1.1278])\n",
      "Grad:  tensor([-0.4856,  2.7481])\n",
      "Epoch 4727 Loss 25.848513\n",
      "Params: tensor([ 2.5100, -1.1280])\n",
      "Grad:  tensor([-0.4856,  2.7480])\n",
      "Epoch 4728 Loss 25.847729\n",
      "Params: tensor([ 2.5101, -1.1283])\n",
      "Grad:  tensor([-0.4856,  2.7480])\n",
      "Epoch 4729 Loss 25.846952\n",
      "Params: tensor([ 2.5101, -1.1286])\n",
      "Grad:  tensor([-0.4856,  2.7479])\n",
      "Epoch 4730 Loss 25.846172\n",
      "Params: tensor([ 2.5102, -1.1289])\n",
      "Grad:  tensor([-0.4856,  2.7479])\n",
      "Epoch 4731 Loss 25.845392\n",
      "Params: tensor([ 2.5102, -1.1291])\n",
      "Grad:  tensor([-0.4856,  2.7478])\n",
      "Epoch 4732 Loss 25.844616\n",
      "Params: tensor([ 2.5103, -1.1294])\n",
      "Grad:  tensor([-0.4856,  2.7478])\n",
      "Epoch 4733 Loss 25.843836\n",
      "Params: tensor([ 2.5103, -1.1297])\n",
      "Grad:  tensor([-0.4855,  2.7477])\n",
      "Epoch 4734 Loss 25.843060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([ 2.5104, -1.1300])\n",
      "Grad:  tensor([-0.4855,  2.7477])\n",
      "Epoch 4735 Loss 25.842278\n",
      "Params: tensor([ 2.5104, -1.1302])\n",
      "Grad:  tensor([-0.4855,  2.7476])\n",
      "Epoch 4736 Loss 25.841497\n",
      "Params: tensor([ 2.5105, -1.1305])\n",
      "Grad:  tensor([-0.4855,  2.7476])\n",
      "Epoch 4737 Loss 25.840717\n",
      "Params: tensor([ 2.5105, -1.1308])\n",
      "Grad:  tensor([-0.4855,  2.7476])\n",
      "Epoch 4738 Loss 25.839941\n",
      "Params: tensor([ 2.5106, -1.1311])\n",
      "Grad:  tensor([-0.4855,  2.7475])\n",
      "Epoch 4739 Loss 25.839165\n",
      "Params: tensor([ 2.5106, -1.1313])\n",
      "Grad:  tensor([-0.4855,  2.7475])\n",
      "Epoch 4740 Loss 25.838390\n",
      "Params: tensor([ 2.5107, -1.1316])\n",
      "Grad:  tensor([-0.4854,  2.7474])\n",
      "Epoch 4741 Loss 25.837608\n",
      "Params: tensor([ 2.5107, -1.1319])\n",
      "Grad:  tensor([-0.4854,  2.7474])\n",
      "Epoch 4742 Loss 25.836828\n",
      "Params: tensor([ 2.5108, -1.1322])\n",
      "Grad:  tensor([-0.4854,  2.7473])\n",
      "Epoch 4743 Loss 25.836048\n",
      "Params: tensor([ 2.5108, -1.1324])\n",
      "Grad:  tensor([-0.4854,  2.7473])\n",
      "Epoch 4744 Loss 25.835268\n",
      "Params: tensor([ 2.5109, -1.1327])\n",
      "Grad:  tensor([-0.4854,  2.7472])\n",
      "Epoch 4745 Loss 25.834492\n",
      "Params: tensor([ 2.5109, -1.1330])\n",
      "Grad:  tensor([-0.4854,  2.7472])\n",
      "Epoch 4746 Loss 25.833717\n",
      "Params: tensor([ 2.5110, -1.1333])\n",
      "Grad:  tensor([-0.4854,  2.7471])\n",
      "Epoch 4747 Loss 25.832935\n",
      "Params: tensor([ 2.5110, -1.1335])\n",
      "Grad:  tensor([-0.4853,  2.7471])\n",
      "Epoch 4748 Loss 25.832155\n",
      "Params: tensor([ 2.5111, -1.1338])\n",
      "Grad:  tensor([-0.4853,  2.7471])\n",
      "Epoch 4749 Loss 25.831377\n",
      "Params: tensor([ 2.5111, -1.1341])\n",
      "Grad:  tensor([-0.4853,  2.7470])\n",
      "Epoch 4750 Loss 25.830599\n",
      "Params: tensor([ 2.5112, -1.1343])\n",
      "Grad:  tensor([-0.4853,  2.7470])\n",
      "Epoch 4751 Loss 25.829826\n",
      "Params: tensor([ 2.5112, -1.1346])\n",
      "Grad:  tensor([-0.4853,  2.7469])\n",
      "Epoch 4752 Loss 25.829048\n",
      "Params: tensor([ 2.5113, -1.1349])\n",
      "Grad:  tensor([-0.4853,  2.7469])\n",
      "Epoch 4753 Loss 25.828270\n",
      "Params: tensor([ 2.5113, -1.1352])\n",
      "Grad:  tensor([-0.4853,  2.7468])\n",
      "Epoch 4754 Loss 25.827490\n",
      "Params: tensor([ 2.5114, -1.1354])\n",
      "Grad:  tensor([-0.4852,  2.7468])\n",
      "Epoch 4755 Loss 25.826714\n",
      "Params: tensor([ 2.5114, -1.1357])\n",
      "Grad:  tensor([-0.4852,  2.7467])\n",
      "Epoch 4756 Loss 25.825933\n",
      "Params: tensor([ 2.5115, -1.1360])\n",
      "Grad:  tensor([-0.4852,  2.7467])\n",
      "Epoch 4757 Loss 25.825151\n",
      "Params: tensor([ 2.5115, -1.1363])\n",
      "Grad:  tensor([-0.4852,  2.7466])\n",
      "Epoch 4758 Loss 25.824379\n",
      "Params: tensor([ 2.5116, -1.1365])\n",
      "Grad:  tensor([-0.4852,  2.7466])\n",
      "Epoch 4759 Loss 25.823601\n",
      "Params: tensor([ 2.5116, -1.1368])\n",
      "Grad:  tensor([-0.4852,  2.7466])\n",
      "Epoch 4760 Loss 25.822823\n",
      "Params: tensor([ 2.5117, -1.1371])\n",
      "Grad:  tensor([-0.4852,  2.7465])\n",
      "Epoch 4761 Loss 25.822044\n",
      "Params: tensor([ 2.5117, -1.1374])\n",
      "Grad:  tensor([-0.4852,  2.7465])\n",
      "Epoch 4762 Loss 25.821259\n",
      "Params: tensor([ 2.5117, -1.1376])\n",
      "Grad:  tensor([-0.4852,  2.7464])\n",
      "Epoch 4763 Loss 25.820490\n",
      "Params: tensor([ 2.5118, -1.1379])\n",
      "Grad:  tensor([-0.4852,  2.7464])\n",
      "Epoch 4764 Loss 25.819710\n",
      "Params: tensor([ 2.5118, -1.1382])\n",
      "Grad:  tensor([-0.4852,  2.7463])\n",
      "Epoch 4765 Loss 25.818937\n",
      "Params: tensor([ 2.5119, -1.1385])\n",
      "Grad:  tensor([-0.4852,  2.7463])\n",
      "Epoch 4766 Loss 25.818151\n",
      "Params: tensor([ 2.5119, -1.1387])\n",
      "Grad:  tensor([-0.4852,  2.7462])\n",
      "Epoch 4767 Loss 25.817371\n",
      "Params: tensor([ 2.5120, -1.1390])\n",
      "Grad:  tensor([-0.4852,  2.7462])\n",
      "Epoch 4768 Loss 25.816603\n",
      "Params: tensor([ 2.5120, -1.1393])\n",
      "Grad:  tensor([-0.4852,  2.7461])\n",
      "Epoch 4769 Loss 25.815825\n",
      "Params: tensor([ 2.5121, -1.1396])\n",
      "Grad:  tensor([-0.4852,  2.7461])\n",
      "Epoch 4770 Loss 25.815044\n",
      "Params: tensor([ 2.5121, -1.1398])\n",
      "Grad:  tensor([-0.4852,  2.7460])\n",
      "Epoch 4771 Loss 25.814264\n",
      "Params: tensor([ 2.5122, -1.1401])\n",
      "Grad:  tensor([-0.4852,  2.7460])\n",
      "Epoch 4772 Loss 25.813488\n",
      "Params: tensor([ 2.5122, -1.1404])\n",
      "Grad:  tensor([-0.4852,  2.7459])\n",
      "Epoch 4773 Loss 25.812708\n",
      "Params: tensor([ 2.5123, -1.1407])\n",
      "Grad:  tensor([-0.4851,  2.7459])\n",
      "Epoch 4774 Loss 25.811937\n",
      "Params: tensor([ 2.5123, -1.1409])\n",
      "Grad:  tensor([-0.4851,  2.7458])\n",
      "Epoch 4775 Loss 25.811159\n",
      "Params: tensor([ 2.5124, -1.1412])\n",
      "Grad:  tensor([-0.4851,  2.7458])\n",
      "Epoch 4776 Loss 25.810381\n",
      "Params: tensor([ 2.5124, -1.1415])\n",
      "Grad:  tensor([-0.4851,  2.7457])\n",
      "Epoch 4777 Loss 25.809607\n",
      "Params: tensor([ 2.5125, -1.1418])\n",
      "Grad:  tensor([-0.4851,  2.7457])\n",
      "Epoch 4778 Loss 25.808825\n",
      "Params: tensor([ 2.5125, -1.1420])\n",
      "Grad:  tensor([-0.4851,  2.7456])\n",
      "Epoch 4779 Loss 25.808046\n",
      "Params: tensor([ 2.5126, -1.1423])\n",
      "Grad:  tensor([-0.4851,  2.7456])\n",
      "Epoch 4780 Loss 25.807268\n",
      "Params: tensor([ 2.5126, -1.1426])\n",
      "Grad:  tensor([-0.4851,  2.7456])\n",
      "Epoch 4781 Loss 25.806494\n",
      "Params: tensor([ 2.5127, -1.1429])\n",
      "Grad:  tensor([-0.4851,  2.7455])\n",
      "Epoch 4782 Loss 25.805717\n",
      "Params: tensor([ 2.5127, -1.1431])\n",
      "Grad:  tensor([-0.4851,  2.7455])\n",
      "Epoch 4783 Loss 25.804939\n",
      "Params: tensor([ 2.5128, -1.1434])\n",
      "Grad:  tensor([-0.4851,  2.7454])\n",
      "Epoch 4784 Loss 25.804161\n",
      "Params: tensor([ 2.5128, -1.1437])\n",
      "Grad:  tensor([-0.4851,  2.7454])\n",
      "Epoch 4785 Loss 25.803383\n",
      "Params: tensor([ 2.5129, -1.1440])\n",
      "Grad:  tensor([-0.4851,  2.7453])\n",
      "Epoch 4786 Loss 25.802607\n",
      "Params: tensor([ 2.5129, -1.1442])\n",
      "Grad:  tensor([-0.4851,  2.7453])\n",
      "Epoch 4787 Loss 25.801830\n",
      "Params: tensor([ 2.5130, -1.1445])\n",
      "Grad:  tensor([-0.4851,  2.7452])\n",
      "Epoch 4788 Loss 25.801054\n",
      "Params: tensor([ 2.5130, -1.1448])\n",
      "Grad:  tensor([-0.4851,  2.7452])\n",
      "Epoch 4789 Loss 25.800274\n",
      "Params: tensor([ 2.5131, -1.1451])\n",
      "Grad:  tensor([-0.4851,  2.7451])\n",
      "Epoch 4790 Loss 25.799498\n",
      "Params: tensor([ 2.5131, -1.1453])\n",
      "Grad:  tensor([-0.4851,  2.7451])\n",
      "Epoch 4791 Loss 25.798723\n",
      "Params: tensor([ 2.5132, -1.1456])\n",
      "Grad:  tensor([-0.4851,  2.7450])\n",
      "Epoch 4792 Loss 25.797945\n",
      "Params: tensor([ 2.5132, -1.1459])\n",
      "Grad:  tensor([-0.4851,  2.7450])\n",
      "Epoch 4793 Loss 25.797173\n",
      "Params: tensor([ 2.5132, -1.1462])\n",
      "Grad:  tensor([-0.4851,  2.7449])\n",
      "Epoch 4794 Loss 25.796389\n",
      "Params: tensor([ 2.5133, -1.1464])\n",
      "Grad:  tensor([-0.4851,  2.7449])\n",
      "Epoch 4795 Loss 25.795610\n",
      "Params: tensor([ 2.5133, -1.1467])\n",
      "Grad:  tensor([-0.4851,  2.7448])\n",
      "Epoch 4796 Loss 25.794832\n",
      "Params: tensor([ 2.5134, -1.1470])\n",
      "Grad:  tensor([-0.4851,  2.7448])\n",
      "Epoch 4797 Loss 25.794060\n",
      "Params: tensor([ 2.5134, -1.1473])\n",
      "Grad:  tensor([-0.4851,  2.7447])\n",
      "Epoch 4798 Loss 25.793285\n",
      "Params: tensor([ 2.5135, -1.1475])\n",
      "Grad:  tensor([-0.4851,  2.7447])\n",
      "Epoch 4799 Loss 25.792505\n",
      "Params: tensor([ 2.5135, -1.1478])\n",
      "Grad:  tensor([-0.4851,  2.7446])\n",
      "Epoch 4800 Loss 25.791725\n",
      "Params: tensor([ 2.5136, -1.1481])\n",
      "Grad:  tensor([-0.4851,  2.7446])\n",
      "Epoch 4801 Loss 25.790955\n",
      "Params: tensor([ 2.5136, -1.1484])\n",
      "Grad:  tensor([-0.4851,  2.7446])\n",
      "Epoch 4802 Loss 25.790180\n",
      "Params: tensor([ 2.5137, -1.1486])\n",
      "Grad:  tensor([-0.4851,  2.7445])\n",
      "Epoch 4803 Loss 25.789404\n",
      "Params: tensor([ 2.5137, -1.1489])\n",
      "Grad:  tensor([-0.4851,  2.7445])\n",
      "Epoch 4804 Loss 25.788624\n",
      "Params: tensor([ 2.5138, -1.1492])\n",
      "Grad:  tensor([-0.4851,  2.7444])\n",
      "Epoch 4805 Loss 25.787844\n",
      "Params: tensor([ 2.5138, -1.1495])\n",
      "Grad:  tensor([-0.4851,  2.7444])\n",
      "Epoch 4806 Loss 25.787071\n",
      "Params: tensor([ 2.5139, -1.1497])\n",
      "Grad:  tensor([-0.4851,  2.7443])\n",
      "Epoch 4807 Loss 25.786293\n",
      "Params: tensor([ 2.5139, -1.1500])\n",
      "Grad:  tensor([-0.4851,  2.7443])\n",
      "Epoch 4808 Loss 25.785517\n",
      "Params: tensor([ 2.5140, -1.1503])\n",
      "Grad:  tensor([-0.4850,  2.7442])\n",
      "Epoch 4809 Loss 25.784742\n",
      "Params: tensor([ 2.5140, -1.1505])\n",
      "Grad:  tensor([-0.4850,  2.7442])\n",
      "Epoch 4810 Loss 25.783968\n",
      "Params: tensor([ 2.5141, -1.1508])\n",
      "Grad:  tensor([-0.4850,  2.7441])\n",
      "Epoch 4811 Loss 25.783186\n",
      "Params: tensor([ 2.5141, -1.1511])\n",
      "Grad:  tensor([-0.4850,  2.7441])\n",
      "Epoch 4812 Loss 25.782410\n",
      "Params: tensor([ 2.5142, -1.1514])\n",
      "Grad:  tensor([-0.4850,  2.7440])\n",
      "Epoch 4813 Loss 25.781630\n",
      "Params: tensor([ 2.5142, -1.1516])\n",
      "Grad:  tensor([-0.4850,  2.7440])\n",
      "Epoch 4814 Loss 25.780859\n",
      "Params: tensor([ 2.5143, -1.1519])\n",
      "Grad:  tensor([-0.4850,  2.7439])\n",
      "Epoch 4815 Loss 25.780079\n",
      "Params: tensor([ 2.5143, -1.1522])\n",
      "Grad:  tensor([-0.4850,  2.7439])\n",
      "Epoch 4816 Loss 25.779303\n",
      "Params: tensor([ 2.5144, -1.1525])\n",
      "Grad:  tensor([-0.4850,  2.7438])\n",
      "Epoch 4817 Loss 25.778528\n",
      "Params: tensor([ 2.5144, -1.1527])\n",
      "Grad:  tensor([-0.4850,  2.7438])\n",
      "Epoch 4818 Loss 25.777754\n",
      "Params: tensor([ 2.5145, -1.1530])\n",
      "Grad:  tensor([-0.4850,  2.7437])\n",
      "Epoch 4819 Loss 25.776976\n",
      "Params: tensor([ 2.5145, -1.1533])\n",
      "Grad:  tensor([-0.4850,  2.7437])\n",
      "Epoch 4820 Loss 25.776201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([ 2.5146, -1.1536])\n",
      "Grad:  tensor([-0.4850,  2.7437])\n",
      "Epoch 4821 Loss 25.775423\n",
      "Params: tensor([ 2.5146, -1.1538])\n",
      "Grad:  tensor([-0.4850,  2.7436])\n",
      "Epoch 4822 Loss 25.774647\n",
      "Params: tensor([ 2.5147, -1.1541])\n",
      "Grad:  tensor([-0.4850,  2.7436])\n",
      "Epoch 4823 Loss 25.773870\n",
      "Params: tensor([ 2.5147, -1.1544])\n",
      "Grad:  tensor([-0.4850,  2.7435])\n",
      "Epoch 4824 Loss 25.773094\n",
      "Params: tensor([ 2.5147, -1.1547])\n",
      "Grad:  tensor([-0.4850,  2.7435])\n",
      "Epoch 4825 Loss 25.772326\n",
      "Params: tensor([ 2.5148, -1.1549])\n",
      "Grad:  tensor([-0.4850,  2.7434])\n",
      "Epoch 4826 Loss 25.771544\n",
      "Params: tensor([ 2.5148, -1.1552])\n",
      "Grad:  tensor([-0.4850,  2.7434])\n",
      "Epoch 4827 Loss 25.770769\n",
      "Params: tensor([ 2.5149, -1.1555])\n",
      "Grad:  tensor([-0.4850,  2.7433])\n",
      "Epoch 4828 Loss 25.769989\n",
      "Params: tensor([ 2.5149, -1.1558])\n",
      "Grad:  tensor([-0.4850,  2.7433])\n",
      "Epoch 4829 Loss 25.769215\n",
      "Params: tensor([ 2.5150, -1.1560])\n",
      "Grad:  tensor([-0.4850,  2.7432])\n",
      "Epoch 4830 Loss 25.768444\n",
      "Params: tensor([ 2.5150, -1.1563])\n",
      "Grad:  tensor([-0.4850,  2.7432])\n",
      "Epoch 4831 Loss 25.767662\n",
      "Params: tensor([ 2.5151, -1.1566])\n",
      "Grad:  tensor([-0.4850,  2.7431])\n",
      "Epoch 4832 Loss 25.766884\n",
      "Params: tensor([ 2.5151, -1.1569])\n",
      "Grad:  tensor([-0.4849,  2.7431])\n",
      "Epoch 4833 Loss 25.766108\n",
      "Params: tensor([ 2.5152, -1.1571])\n",
      "Grad:  tensor([-0.4849,  2.7430])\n",
      "Epoch 4834 Loss 25.765333\n",
      "Params: tensor([ 2.5152, -1.1574])\n",
      "Grad:  tensor([-0.4849,  2.7430])\n",
      "Epoch 4835 Loss 25.764559\n",
      "Params: tensor([ 2.5153, -1.1577])\n",
      "Grad:  tensor([-0.4849,  2.7429])\n",
      "Epoch 4836 Loss 25.763783\n",
      "Params: tensor([ 2.5153, -1.1580])\n",
      "Grad:  tensor([-0.4849,  2.7429])\n",
      "Epoch 4837 Loss 25.763008\n",
      "Params: tensor([ 2.5154, -1.1582])\n",
      "Grad:  tensor([-0.4849,  2.7428])\n",
      "Epoch 4838 Loss 25.762232\n",
      "Params: tensor([ 2.5154, -1.1585])\n",
      "Grad:  tensor([-0.4849,  2.7428])\n",
      "Epoch 4839 Loss 25.761457\n",
      "Params: tensor([ 2.5155, -1.1588])\n",
      "Grad:  tensor([-0.4849,  2.7428])\n",
      "Epoch 4840 Loss 25.760679\n",
      "Params: tensor([ 2.5155, -1.1591])\n",
      "Grad:  tensor([-0.4849,  2.7427])\n",
      "Epoch 4841 Loss 25.759905\n",
      "Params: tensor([ 2.5156, -1.1593])\n",
      "Grad:  tensor([-0.4849,  2.7427])\n",
      "Epoch 4842 Loss 25.759127\n",
      "Params: tensor([ 2.5156, -1.1596])\n",
      "Grad:  tensor([-0.4849,  2.7426])\n",
      "Epoch 4843 Loss 25.758350\n",
      "Params: tensor([ 2.5157, -1.1599])\n",
      "Grad:  tensor([-0.4849,  2.7426])\n",
      "Epoch 4844 Loss 25.757576\n",
      "Params: tensor([ 2.5157, -1.1602])\n",
      "Grad:  tensor([-0.4849,  2.7425])\n",
      "Epoch 4845 Loss 25.756804\n",
      "Params: tensor([ 2.5158, -1.1604])\n",
      "Grad:  tensor([-0.4849,  2.7425])\n",
      "Epoch 4846 Loss 25.756029\n",
      "Params: tensor([ 2.5158, -1.1607])\n",
      "Grad:  tensor([-0.4849,  2.7424])\n",
      "Epoch 4847 Loss 25.755247\n",
      "Params: tensor([ 2.5159, -1.1610])\n",
      "Grad:  tensor([-0.4849,  2.7424])\n",
      "Epoch 4848 Loss 25.754475\n",
      "Params: tensor([ 2.5159, -1.1612])\n",
      "Grad:  tensor([-0.4849,  2.7423])\n",
      "Epoch 4849 Loss 25.753700\n",
      "Params: tensor([ 2.5160, -1.1615])\n",
      "Grad:  tensor([-0.4849,  2.7423])\n",
      "Epoch 4850 Loss 25.752928\n",
      "Params: tensor([ 2.5160, -1.1618])\n",
      "Grad:  tensor([-0.4849,  2.7422])\n",
      "Epoch 4851 Loss 25.752144\n",
      "Params: tensor([ 2.5161, -1.1621])\n",
      "Grad:  tensor([-0.4848,  2.7422])\n",
      "Epoch 4852 Loss 25.751373\n",
      "Params: tensor([ 2.5161, -1.1623])\n",
      "Grad:  tensor([-0.4848,  2.7421])\n",
      "Epoch 4853 Loss 25.750599\n",
      "Params: tensor([ 2.5162, -1.1626])\n",
      "Grad:  tensor([-0.4848,  2.7421])\n",
      "Epoch 4854 Loss 25.749825\n",
      "Params: tensor([ 2.5162, -1.1629])\n",
      "Grad:  tensor([-0.4848,  2.7420])\n",
      "Epoch 4855 Loss 25.749043\n",
      "Params: tensor([ 2.5162, -1.1632])\n",
      "Grad:  tensor([-0.4848,  2.7420])\n",
      "Epoch 4856 Loss 25.748268\n",
      "Params: tensor([ 2.5163, -1.1634])\n",
      "Grad:  tensor([-0.4848,  2.7420])\n",
      "Epoch 4857 Loss 25.747498\n",
      "Params: tensor([ 2.5163, -1.1637])\n",
      "Grad:  tensor([-0.4848,  2.7419])\n",
      "Epoch 4858 Loss 25.746721\n",
      "Params: tensor([ 2.5164, -1.1640])\n",
      "Grad:  tensor([-0.4848,  2.7419])\n",
      "Epoch 4859 Loss 25.745943\n",
      "Params: tensor([ 2.5164, -1.1643])\n",
      "Grad:  tensor([-0.4848,  2.7418])\n",
      "Epoch 4860 Loss 25.745171\n",
      "Params: tensor([ 2.5165, -1.1645])\n",
      "Grad:  tensor([-0.4848,  2.7418])\n",
      "Epoch 4861 Loss 25.744396\n",
      "Params: tensor([ 2.5165, -1.1648])\n",
      "Grad:  tensor([-0.4848,  2.7417])\n",
      "Epoch 4862 Loss 25.743622\n",
      "Params: tensor([ 2.5166, -1.1651])\n",
      "Grad:  tensor([-0.4848,  2.7417])\n",
      "Epoch 4863 Loss 25.742842\n",
      "Params: tensor([ 2.5166, -1.1654])\n",
      "Grad:  tensor([-0.4848,  2.7416])\n",
      "Epoch 4864 Loss 25.742071\n",
      "Params: tensor([ 2.5167, -1.1656])\n",
      "Grad:  tensor([-0.4848,  2.7416])\n",
      "Epoch 4865 Loss 25.741297\n",
      "Params: tensor([ 2.5167, -1.1659])\n",
      "Grad:  tensor([-0.4848,  2.7415])\n",
      "Epoch 4866 Loss 25.740522\n",
      "Params: tensor([ 2.5168, -1.1662])\n",
      "Grad:  tensor([-0.4848,  2.7415])\n",
      "Epoch 4867 Loss 25.739740\n",
      "Params: tensor([ 2.5168, -1.1665])\n",
      "Grad:  tensor([-0.4848,  2.7414])\n",
      "Epoch 4868 Loss 25.738970\n",
      "Params: tensor([ 2.5169, -1.1667])\n",
      "Grad:  tensor([-0.4847,  2.7414])\n",
      "Epoch 4869 Loss 25.738190\n",
      "Params: tensor([ 2.5169, -1.1670])\n",
      "Grad:  tensor([-0.4847,  2.7413])\n",
      "Epoch 4870 Loss 25.737425\n",
      "Params: tensor([ 2.5170, -1.1673])\n",
      "Grad:  tensor([-0.4847,  2.7413])\n",
      "Epoch 4871 Loss 25.736645\n",
      "Params: tensor([ 2.5170, -1.1676])\n",
      "Grad:  tensor([-0.4847,  2.7413])\n",
      "Epoch 4872 Loss 25.735868\n",
      "Params: tensor([ 2.5171, -1.1678])\n",
      "Grad:  tensor([-0.4847,  2.7412])\n",
      "Epoch 4873 Loss 25.735094\n",
      "Params: tensor([ 2.5171, -1.1681])\n",
      "Grad:  tensor([-0.4847,  2.7412])\n",
      "Epoch 4874 Loss 25.734322\n",
      "Params: tensor([ 2.5172, -1.1684])\n",
      "Grad:  tensor([-0.4847,  2.7411])\n",
      "Epoch 4875 Loss 25.733543\n",
      "Params: tensor([ 2.5172, -1.1687])\n",
      "Grad:  tensor([-0.4847,  2.7411])\n",
      "Epoch 4876 Loss 25.732771\n",
      "Params: tensor([ 2.5173, -1.1689])\n",
      "Grad:  tensor([-0.4847,  2.7410])\n",
      "Epoch 4877 Loss 25.731997\n",
      "Params: tensor([ 2.5173, -1.1692])\n",
      "Grad:  tensor([-0.4847,  2.7410])\n",
      "Epoch 4878 Loss 25.731220\n",
      "Params: tensor([ 2.5174, -1.1695])\n",
      "Grad:  tensor([-0.4847,  2.7409])\n",
      "Epoch 4879 Loss 25.730444\n",
      "Params: tensor([ 2.5174, -1.1697])\n",
      "Grad:  tensor([-0.4847,  2.7409])\n",
      "Epoch 4880 Loss 25.729673\n",
      "Params: tensor([ 2.5175, -1.1700])\n",
      "Grad:  tensor([-0.4847,  2.7408])\n",
      "Epoch 4881 Loss 25.728895\n",
      "Params: tensor([ 2.5175, -1.1703])\n",
      "Grad:  tensor([-0.4847,  2.7408])\n",
      "Epoch 4882 Loss 25.728125\n",
      "Params: tensor([ 2.5176, -1.1706])\n",
      "Grad:  tensor([-0.4846,  2.7407])\n",
      "Epoch 4883 Loss 25.727350\n",
      "Params: tensor([ 2.5176, -1.1708])\n",
      "Grad:  tensor([-0.4846,  2.7407])\n",
      "Epoch 4884 Loss 25.726570\n",
      "Params: tensor([ 2.5177, -1.1711])\n",
      "Grad:  tensor([-0.4846,  2.7406])\n",
      "Epoch 4885 Loss 25.725798\n",
      "Params: tensor([ 2.5177, -1.1714])\n",
      "Grad:  tensor([-0.4846,  2.7406])\n",
      "Epoch 4886 Loss 25.725023\n",
      "Params: tensor([ 2.5177, -1.1717])\n",
      "Grad:  tensor([-0.4846,  2.7405])\n",
      "Epoch 4887 Loss 25.724251\n",
      "Params: tensor([ 2.5178, -1.1719])\n",
      "Grad:  tensor([-0.4846,  2.7405])\n",
      "Epoch 4888 Loss 25.723475\n",
      "Params: tensor([ 2.5178, -1.1722])\n",
      "Grad:  tensor([-0.4846,  2.7405])\n",
      "Epoch 4889 Loss 25.722700\n",
      "Params: tensor([ 2.5179, -1.1725])\n",
      "Grad:  tensor([-0.4846,  2.7404])\n",
      "Epoch 4890 Loss 25.721926\n",
      "Params: tensor([ 2.5179, -1.1728])\n",
      "Grad:  tensor([-0.4846,  2.7404])\n",
      "Epoch 4891 Loss 25.721153\n",
      "Params: tensor([ 2.5180, -1.1730])\n",
      "Grad:  tensor([-0.4846,  2.7403])\n",
      "Epoch 4892 Loss 25.720379\n",
      "Params: tensor([ 2.5180, -1.1733])\n",
      "Grad:  tensor([-0.4846,  2.7403])\n",
      "Epoch 4893 Loss 25.719606\n",
      "Params: tensor([ 2.5181, -1.1736])\n",
      "Grad:  tensor([-0.4846,  2.7402])\n",
      "Epoch 4894 Loss 25.718824\n",
      "Params: tensor([ 2.5181, -1.1739])\n",
      "Grad:  tensor([-0.4846,  2.7402])\n",
      "Epoch 4895 Loss 25.718056\n",
      "Params: tensor([ 2.5182, -1.1741])\n",
      "Grad:  tensor([-0.4845,  2.7401])\n",
      "Epoch 4896 Loss 25.717285\n",
      "Params: tensor([ 2.5182, -1.1744])\n",
      "Grad:  tensor([-0.4845,  2.7401])\n",
      "Epoch 4897 Loss 25.716503\n",
      "Params: tensor([ 2.5183, -1.1747])\n",
      "Grad:  tensor([-0.4845,  2.7400])\n",
      "Epoch 4898 Loss 25.715731\n",
      "Params: tensor([ 2.5183, -1.1750])\n",
      "Grad:  tensor([-0.4845,  2.7400])\n",
      "Epoch 4899 Loss 25.714960\n",
      "Params: tensor([ 2.5184, -1.1752])\n",
      "Grad:  tensor([-0.4845,  2.7399])\n",
      "Epoch 4900 Loss 25.714184\n",
      "Params: tensor([ 2.5184, -1.1755])\n",
      "Grad:  tensor([-0.4845,  2.7399])\n",
      "Epoch 4901 Loss 25.713409\n",
      "Params: tensor([ 2.5185, -1.1758])\n",
      "Grad:  tensor([-0.4845,  2.7398])\n",
      "Epoch 4902 Loss 25.712633\n",
      "Params: tensor([ 2.5185, -1.1760])\n",
      "Grad:  tensor([-0.4845,  2.7398])\n",
      "Epoch 4903 Loss 25.711864\n",
      "Params: tensor([ 2.5186, -1.1763])\n",
      "Grad:  tensor([-0.4845,  2.7398])\n",
      "Epoch 4904 Loss 25.711090\n",
      "Params: tensor([ 2.5186, -1.1766])\n",
      "Grad:  tensor([-0.4845,  2.7397])\n",
      "Epoch 4905 Loss 25.710310\n",
      "Params: tensor([ 2.5187, -1.1769])\n",
      "Grad:  tensor([-0.4845,  2.7397])\n",
      "Epoch 4906 Loss 25.709539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([ 2.5187, -1.1771])\n",
      "Grad:  tensor([-0.4845,  2.7396])\n",
      "Epoch 4907 Loss 25.708765\n",
      "Params: tensor([ 2.5188, -1.1774])\n",
      "Grad:  tensor([-0.4845,  2.7396])\n",
      "Epoch 4908 Loss 25.707989\n",
      "Params: tensor([ 2.5188, -1.1777])\n",
      "Grad:  tensor([-0.4844,  2.7395])\n",
      "Epoch 4909 Loss 25.707214\n",
      "Params: tensor([ 2.5189, -1.1780])\n",
      "Grad:  tensor([-0.4844,  2.7395])\n",
      "Epoch 4910 Loss 25.706446\n",
      "Params: tensor([ 2.5189, -1.1782])\n",
      "Grad:  tensor([-0.4844,  2.7394])\n",
      "Epoch 4911 Loss 25.705669\n",
      "Params: tensor([ 2.5190, -1.1785])\n",
      "Grad:  tensor([-0.4844,  2.7394])\n",
      "Epoch 4912 Loss 25.704895\n",
      "Params: tensor([ 2.5190, -1.1788])\n",
      "Grad:  tensor([-0.4844,  2.7393])\n",
      "Epoch 4913 Loss 25.704119\n",
      "Params: tensor([ 2.5191, -1.1791])\n",
      "Grad:  tensor([-0.4844,  2.7393])\n",
      "Epoch 4914 Loss 25.703346\n",
      "Params: tensor([ 2.5191, -1.1793])\n",
      "Grad:  tensor([-0.4844,  2.7392])\n",
      "Epoch 4915 Loss 25.702578\n",
      "Params: tensor([ 2.5192, -1.1796])\n",
      "Grad:  tensor([-0.4844,  2.7392])\n",
      "Epoch 4916 Loss 25.701799\n",
      "Params: tensor([ 2.5192, -1.1799])\n",
      "Grad:  tensor([-0.4844,  2.7392])\n",
      "Epoch 4917 Loss 25.701025\n",
      "Params: tensor([ 2.5192, -1.1802])\n",
      "Grad:  tensor([-0.4844,  2.7391])\n",
      "Epoch 4918 Loss 25.700254\n",
      "Params: tensor([ 2.5193, -1.1804])\n",
      "Grad:  tensor([-0.4844,  2.7391])\n",
      "Epoch 4919 Loss 25.699480\n",
      "Params: tensor([ 2.5193, -1.1807])\n",
      "Grad:  tensor([-0.4843,  2.7390])\n",
      "Epoch 4920 Loss 25.698708\n",
      "Params: tensor([ 2.5194, -1.1810])\n",
      "Grad:  tensor([-0.4843,  2.7390])\n",
      "Epoch 4921 Loss 25.697929\n",
      "Params: tensor([ 2.5194, -1.1813])\n",
      "Grad:  tensor([-0.4843,  2.7389])\n",
      "Epoch 4922 Loss 25.697161\n",
      "Params: tensor([ 2.5195, -1.1815])\n",
      "Grad:  tensor([-0.4843,  2.7389])\n",
      "Epoch 4923 Loss 25.696386\n",
      "Params: tensor([ 2.5195, -1.1818])\n",
      "Grad:  tensor([-0.4843,  2.7388])\n",
      "Epoch 4924 Loss 25.695612\n",
      "Params: tensor([ 2.5196, -1.1821])\n",
      "Grad:  tensor([-0.4843,  2.7388])\n",
      "Epoch 4925 Loss 25.694836\n",
      "Params: tensor([ 2.5196, -1.1823])\n",
      "Grad:  tensor([-0.4843,  2.7387])\n",
      "Epoch 4926 Loss 25.694061\n",
      "Params: tensor([ 2.5197, -1.1826])\n",
      "Grad:  tensor([-0.4843,  2.7387])\n",
      "Epoch 4927 Loss 25.693296\n",
      "Params: tensor([ 2.5197, -1.1829])\n",
      "Grad:  tensor([-0.4843,  2.7386])\n",
      "Epoch 4928 Loss 25.692513\n",
      "Params: tensor([ 2.5198, -1.1832])\n",
      "Grad:  tensor([-0.4843,  2.7386])\n",
      "Epoch 4929 Loss 25.691742\n",
      "Params: tensor([ 2.5198, -1.1834])\n",
      "Grad:  tensor([-0.4843,  2.7385])\n",
      "Epoch 4930 Loss 25.690973\n",
      "Params: tensor([ 2.5199, -1.1837])\n",
      "Grad:  tensor([-0.4842,  2.7385])\n",
      "Epoch 4931 Loss 25.690197\n",
      "Params: tensor([ 2.5199, -1.1840])\n",
      "Grad:  tensor([-0.4842,  2.7385])\n",
      "Epoch 4932 Loss 25.689425\n",
      "Params: tensor([ 2.5200, -1.1843])\n",
      "Grad:  tensor([-0.4842,  2.7384])\n",
      "Epoch 4933 Loss 25.688652\n",
      "Params: tensor([ 2.5200, -1.1845])\n",
      "Grad:  tensor([-0.4842,  2.7384])\n",
      "Epoch 4934 Loss 25.687874\n",
      "Params: tensor([ 2.5201, -1.1848])\n",
      "Grad:  tensor([-0.4842,  2.7383])\n",
      "Epoch 4935 Loss 25.687103\n",
      "Params: tensor([ 2.5201, -1.1851])\n",
      "Grad:  tensor([-0.4842,  2.7383])\n",
      "Epoch 4936 Loss 25.686329\n",
      "Params: tensor([ 2.5202, -1.1854])\n",
      "Grad:  tensor([-0.4842,  2.7382])\n",
      "Epoch 4937 Loss 25.685560\n",
      "Params: tensor([ 2.5202, -1.1856])\n",
      "Grad:  tensor([-0.4842,  2.7382])\n",
      "Epoch 4938 Loss 25.684784\n",
      "Params: tensor([ 2.5203, -1.1859])\n",
      "Grad:  tensor([-0.4842,  2.7381])\n",
      "Epoch 4939 Loss 25.684015\n",
      "Params: tensor([ 2.5203, -1.1862])\n",
      "Grad:  tensor([-0.4842,  2.7381])\n",
      "Epoch 4940 Loss 25.683239\n",
      "Params: tensor([ 2.5204, -1.1865])\n",
      "Grad:  tensor([-0.4841,  2.7380])\n",
      "Epoch 4941 Loss 25.682465\n",
      "Params: tensor([ 2.5204, -1.1867])\n",
      "Grad:  tensor([-0.4841,  2.7380])\n",
      "Epoch 4942 Loss 25.681688\n",
      "Params: tensor([ 2.5205, -1.1870])\n",
      "Grad:  tensor([-0.4841,  2.7379])\n",
      "Epoch 4943 Loss 25.680922\n",
      "Params: tensor([ 2.5205, -1.1873])\n",
      "Grad:  tensor([-0.4841,  2.7379])\n",
      "Epoch 4944 Loss 25.680143\n",
      "Params: tensor([ 2.5206, -1.1876])\n",
      "Grad:  tensor([-0.4841,  2.7379])\n",
      "Epoch 4945 Loss 25.679375\n",
      "Params: tensor([ 2.5206, -1.1878])\n",
      "Grad:  tensor([-0.4841,  2.7378])\n",
      "Epoch 4946 Loss 25.678596\n",
      "Params: tensor([ 2.5207, -1.1881])\n",
      "Grad:  tensor([-0.4841,  2.7378])\n",
      "Epoch 4947 Loss 25.677828\n",
      "Params: tensor([ 2.5207, -1.1884])\n",
      "Grad:  tensor([-0.4841,  2.7377])\n",
      "Epoch 4948 Loss 25.677055\n",
      "Params: tensor([ 2.5207, -1.1886])\n",
      "Grad:  tensor([-0.4841,  2.7377])\n",
      "Epoch 4949 Loss 25.676283\n",
      "Params: tensor([ 2.5208, -1.1889])\n",
      "Grad:  tensor([-0.4841,  2.7376])\n",
      "Epoch 4950 Loss 25.675507\n",
      "Params: tensor([ 2.5208, -1.1892])\n",
      "Grad:  tensor([-0.4841,  2.7376])\n",
      "Epoch 4951 Loss 25.674736\n",
      "Params: tensor([ 2.5209, -1.1895])\n",
      "Grad:  tensor([-0.4840,  2.7375])\n",
      "Epoch 4952 Loss 25.673962\n",
      "Params: tensor([ 2.5209, -1.1897])\n",
      "Grad:  tensor([-0.4840,  2.7375])\n",
      "Epoch 4953 Loss 25.673189\n",
      "Params: tensor([ 2.5210, -1.1900])\n",
      "Grad:  tensor([-0.4840,  2.7374])\n",
      "Epoch 4954 Loss 25.672422\n",
      "Params: tensor([ 2.5210, -1.1903])\n",
      "Grad:  tensor([-0.4840,  2.7374])\n",
      "Epoch 4955 Loss 25.671644\n",
      "Params: tensor([ 2.5211, -1.1906])\n",
      "Grad:  tensor([-0.4840,  2.7373])\n",
      "Epoch 4956 Loss 25.670874\n",
      "Params: tensor([ 2.5211, -1.1908])\n",
      "Grad:  tensor([-0.4840,  2.7373])\n",
      "Epoch 4957 Loss 25.670097\n",
      "Params: tensor([ 2.5212, -1.1911])\n",
      "Grad:  tensor([-0.4840,  2.7373])\n",
      "Epoch 4958 Loss 25.669329\n",
      "Params: tensor([ 2.5212, -1.1914])\n",
      "Grad:  tensor([-0.4840,  2.7372])\n",
      "Epoch 4959 Loss 25.668550\n",
      "Params: tensor([ 2.5213, -1.1917])\n",
      "Grad:  tensor([-0.4839,  2.7372])\n",
      "Epoch 4960 Loss 25.667784\n",
      "Params: tensor([ 2.5213, -1.1919])\n",
      "Grad:  tensor([-0.4839,  2.7371])\n",
      "Epoch 4961 Loss 25.667004\n",
      "Params: tensor([ 2.5214, -1.1922])\n",
      "Grad:  tensor([-0.4839,  2.7371])\n",
      "Epoch 4962 Loss 25.666237\n",
      "Params: tensor([ 2.5214, -1.1925])\n",
      "Grad:  tensor([-0.4839,  2.7370])\n",
      "Epoch 4963 Loss 25.665459\n",
      "Params: tensor([ 2.5215, -1.1928])\n",
      "Grad:  tensor([-0.4839,  2.7370])\n",
      "Epoch 4964 Loss 25.664690\n",
      "Params: tensor([ 2.5215, -1.1930])\n",
      "Grad:  tensor([-0.4839,  2.7369])\n",
      "Epoch 4965 Loss 25.663916\n",
      "Params: tensor([ 2.5216, -1.1933])\n",
      "Grad:  tensor([-0.4839,  2.7369])\n",
      "Epoch 4966 Loss 25.663149\n",
      "Params: tensor([ 2.5216, -1.1936])\n",
      "Grad:  tensor([-0.4839,  2.7368])\n",
      "Epoch 4967 Loss 25.662371\n",
      "Params: tensor([ 2.5217, -1.1938])\n",
      "Grad:  tensor([-0.4839,  2.7368])\n",
      "Epoch 4968 Loss 25.661598\n",
      "Params: tensor([ 2.5217, -1.1941])\n",
      "Grad:  tensor([-0.4838,  2.7368])\n",
      "Epoch 4969 Loss 25.660828\n",
      "Params: tensor([ 2.5218, -1.1944])\n",
      "Grad:  tensor([-0.4838,  2.7367])\n",
      "Epoch 4970 Loss 25.660057\n",
      "Params: tensor([ 2.5218, -1.1947])\n",
      "Grad:  tensor([-0.4838,  2.7367])\n",
      "Epoch 4971 Loss 25.659283\n",
      "Params: tensor([ 2.5219, -1.1949])\n",
      "Grad:  tensor([-0.4838,  2.7366])\n",
      "Epoch 4972 Loss 25.658514\n",
      "Params: tensor([ 2.5219, -1.1952])\n",
      "Grad:  tensor([-0.4838,  2.7366])\n",
      "Epoch 4973 Loss 25.657738\n",
      "Params: tensor([ 2.5220, -1.1955])\n",
      "Grad:  tensor([-0.4838,  2.7365])\n",
      "Epoch 4974 Loss 25.656965\n",
      "Params: tensor([ 2.5220, -1.1958])\n",
      "Grad:  tensor([-0.4838,  2.7365])\n",
      "Epoch 4975 Loss 25.656191\n",
      "Params: tensor([ 2.5221, -1.1960])\n",
      "Grad:  tensor([-0.4838,  2.7364])\n",
      "Epoch 4976 Loss 25.655424\n",
      "Params: tensor([ 2.5221, -1.1963])\n",
      "Grad:  tensor([-0.4838,  2.7364])\n",
      "Epoch 4977 Loss 25.654650\n",
      "Params: tensor([ 2.5222, -1.1966])\n",
      "Grad:  tensor([-0.4837,  2.7363])\n",
      "Epoch 4978 Loss 25.653883\n",
      "Params: tensor([ 2.5222, -1.1969])\n",
      "Grad:  tensor([-0.4837,  2.7363])\n",
      "Epoch 4979 Loss 25.653105\n",
      "Params: tensor([ 2.5222, -1.1971])\n",
      "Grad:  tensor([-0.4837,  2.7362])\n",
      "Epoch 4980 Loss 25.652332\n",
      "Params: tensor([ 2.5223, -1.1974])\n",
      "Grad:  tensor([-0.4837,  2.7362])\n",
      "Epoch 4981 Loss 25.651562\n",
      "Params: tensor([ 2.5223, -1.1977])\n",
      "Grad:  tensor([-0.4837,  2.7362])\n",
      "Epoch 4982 Loss 25.650793\n",
      "Params: tensor([ 2.5224, -1.1980])\n",
      "Grad:  tensor([-0.4837,  2.7361])\n",
      "Epoch 4983 Loss 25.650019\n",
      "Params: tensor([ 2.5224, -1.1982])\n",
      "Grad:  tensor([-0.4837,  2.7361])\n",
      "Epoch 4984 Loss 25.649244\n",
      "Params: tensor([ 2.5225, -1.1985])\n",
      "Grad:  tensor([-0.4837,  2.7360])\n",
      "Epoch 4985 Loss 25.648470\n",
      "Params: tensor([ 2.5225, -1.1988])\n",
      "Grad:  tensor([-0.4836,  2.7360])\n",
      "Epoch 4986 Loss 25.647703\n",
      "Params: tensor([ 2.5226, -1.1990])\n",
      "Grad:  tensor([-0.4836,  2.7359])\n",
      "Epoch 4987 Loss 25.646931\n",
      "Params: tensor([ 2.5226, -1.1993])\n",
      "Grad:  tensor([-0.4836,  2.7359])\n",
      "Epoch 4988 Loss 25.646162\n",
      "Params: tensor([ 2.5227, -1.1996])\n",
      "Grad:  tensor([-0.4836,  2.7358])\n",
      "Epoch 4989 Loss 25.645384\n",
      "Params: tensor([ 2.5227, -1.1999])\n",
      "Grad:  tensor([-0.4836,  2.7358])\n",
      "Epoch 4990 Loss 25.644615\n",
      "Params: tensor([ 2.5228, -1.2001])\n",
      "Grad:  tensor([-0.4836,  2.7357])\n",
      "Epoch 4991 Loss 25.643841\n",
      "Params: tensor([ 2.5228, -1.2004])\n",
      "Grad:  tensor([-0.4836,  2.7357])\n",
      "Epoch 4992 Loss 25.643072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: tensor([ 2.5229, -1.2007])\n",
      "Grad:  tensor([-0.4836,  2.7357])\n",
      "Epoch 4993 Loss 25.642302\n",
      "Params: tensor([ 2.5229, -1.2010])\n",
      "Grad:  tensor([-0.4835,  2.7356])\n",
      "Epoch 4994 Loss 25.641533\n",
      "Params: tensor([ 2.5230, -1.2012])\n",
      "Grad:  tensor([-0.4835,  2.7356])\n",
      "Epoch 4995 Loss 25.640759\n",
      "Params: tensor([ 2.5230, -1.2015])\n",
      "Grad:  tensor([-0.4835,  2.7355])\n",
      "Epoch 4996 Loss 25.639984\n",
      "Params: tensor([ 2.5231, -1.2018])\n",
      "Grad:  tensor([-0.4835,  2.7355])\n",
      "Epoch 4997 Loss 25.639210\n",
      "Params: tensor([ 2.5231, -1.2021])\n",
      "Grad:  tensor([-0.4835,  2.7354])\n",
      "Epoch 4998 Loss 25.638433\n",
      "Params: tensor([ 2.5232, -1.2023])\n",
      "Grad:  tensor([-0.4835,  2.7354])\n",
      "Epoch 4999 Loss 25.637672\n",
      "Params: tensor([ 2.5232, -1.2026])\n",
      "Grad:  tensor([-0.4835,  2.7353])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 2.5233, -1.2029])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0,0.0])\n",
    "epochs = 5000\n",
    "\n",
    "lr = 1e-4\n",
    "for epoch in range(epochs):\n",
    "    w,b = params\n",
    "    t_p = model(t_u, w, b)\n",
    "    loss = loss_fn(t_p, t_c)\n",
    "    print(\"Epoch %d Loss %f\"%(epoch, float(loss)))\n",
    "    grad = grad_fn(t_u, t_c, t_p, w, b)\n",
    "    print('Params:',params)\n",
    "    print('Grad: ',grad)\n",
    "    params = params - lr * grad\n",
    "params\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p, t_c):\n",
    "    sq_diff = (t_p-t_c) **2\n",
    "    return sq_diff.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss 80.364342\n",
      "Epoch 1 loss 37.574917\n",
      "Epoch 2 loss 30.871077\n",
      "Epoch 3 loss 29.756193\n",
      "Epoch 4 loss 29.507149\n",
      "Epoch 5 loss 29.392458\n",
      "Epoch 6 loss 29.298828\n",
      "Epoch 7 loss 29.208717\n",
      "Epoch 8 loss 29.119417\n",
      "Epoch 9 loss 29.030487\n",
      "Epoch 10 loss 28.941875\n",
      "Epoch 11 loss 28.853565\n",
      "Epoch 12 loss 28.765556\n",
      "Epoch 13 loss 28.677851\n",
      "Epoch 14 loss 28.590431\n",
      "Epoch 15 loss 28.503321\n",
      "Epoch 16 loss 28.416496\n",
      "Epoch 17 loss 28.329973\n",
      "Epoch 18 loss 28.243738\n",
      "Epoch 19 loss 28.157801\n",
      "Epoch 20 loss 28.072151\n",
      "Epoch 21 loss 27.986799\n",
      "Epoch 22 loss 27.901731\n",
      "Epoch 23 loss 27.816954\n",
      "Epoch 24 loss 27.732460\n",
      "Epoch 25 loss 27.648256\n",
      "Epoch 26 loss 27.564342\n",
      "Epoch 27 loss 27.480711\n",
      "Epoch 28 loss 27.397358\n",
      "Epoch 29 loss 27.314293\n",
      "Epoch 30 loss 27.231512\n",
      "Epoch 31 loss 27.149006\n",
      "Epoch 32 loss 27.066790\n",
      "Epoch 33 loss 26.984844\n",
      "Epoch 34 loss 26.903173\n",
      "Epoch 35 loss 26.821791\n",
      "Epoch 36 loss 26.740675\n",
      "Epoch 37 loss 26.659838\n",
      "Epoch 38 loss 26.579279\n",
      "Epoch 39 loss 26.498987\n",
      "Epoch 40 loss 26.418974\n",
      "Epoch 41 loss 26.339228\n",
      "Epoch 42 loss 26.259752\n",
      "Epoch 43 loss 26.180548\n",
      "Epoch 44 loss 26.101616\n",
      "Epoch 45 loss 26.022947\n",
      "Epoch 46 loss 25.944550\n",
      "Epoch 47 loss 25.866417\n",
      "Epoch 48 loss 25.788549\n",
      "Epoch 49 loss 25.710936\n",
      "Epoch 50 loss 25.633600\n",
      "Epoch 51 loss 25.556524\n",
      "Epoch 52 loss 25.479700\n",
      "Epoch 53 loss 25.403145\n",
      "Epoch 54 loss 25.326849\n",
      "Epoch 55 loss 25.250811\n",
      "Epoch 56 loss 25.175035\n",
      "Epoch 57 loss 25.099510\n",
      "Epoch 58 loss 25.024248\n",
      "Epoch 59 loss 24.949238\n",
      "Epoch 60 loss 24.874483\n",
      "Epoch 61 loss 24.799980\n",
      "Epoch 62 loss 24.725737\n",
      "Epoch 63 loss 24.651735\n",
      "Epoch 64 loss 24.577990\n",
      "Epoch 65 loss 24.504494\n",
      "Epoch 66 loss 24.431250\n",
      "Epoch 67 loss 24.358257\n",
      "Epoch 68 loss 24.285503\n",
      "Epoch 69 loss 24.212996\n",
      "Epoch 70 loss 24.140747\n",
      "Epoch 71 loss 24.068733\n",
      "Epoch 72 loss 23.996967\n",
      "Epoch 73 loss 23.925446\n",
      "Epoch 74 loss 23.854168\n",
      "Epoch 75 loss 23.783129\n",
      "Epoch 76 loss 23.712328\n",
      "Epoch 77 loss 23.641771\n",
      "Epoch 78 loss 23.571455\n",
      "Epoch 79 loss 23.501379\n",
      "Epoch 80 loss 23.431538\n",
      "Epoch 81 loss 23.361933\n",
      "Epoch 82 loss 23.292566\n",
      "Epoch 83 loss 23.223436\n",
      "Epoch 84 loss 23.154539\n",
      "Epoch 85 loss 23.085882\n",
      "Epoch 86 loss 23.017447\n",
      "Epoch 87 loss 22.949249\n",
      "Epoch 88 loss 22.881281\n",
      "Epoch 89 loss 22.813547\n",
      "Epoch 90 loss 22.746044\n",
      "Epoch 91 loss 22.678768\n",
      "Epoch 92 loss 22.611719\n",
      "Epoch 93 loss 22.544899\n",
      "Epoch 94 loss 22.478306\n",
      "Epoch 95 loss 22.411940\n",
      "Epoch 96 loss 22.345793\n",
      "Epoch 97 loss 22.279875\n",
      "Epoch 98 loss 22.214186\n",
      "Epoch 99 loss 22.148710\n",
      "Epoch 100 loss 22.083464\n",
      "Epoch 101 loss 22.018436\n",
      "Epoch 102 loss 21.953630\n",
      "Epoch 103 loss 21.889046\n",
      "Epoch 104 loss 21.824677\n",
      "Epoch 105 loss 21.760530\n",
      "Epoch 106 loss 21.696600\n",
      "Epoch 107 loss 21.632881\n",
      "Epoch 108 loss 21.569389\n",
      "Epoch 109 loss 21.506104\n",
      "Epoch 110 loss 21.443037\n",
      "Epoch 111 loss 21.380190\n",
      "Epoch 112 loss 21.317547\n",
      "Epoch 113 loss 21.255119\n",
      "Epoch 114 loss 21.192904\n",
      "Epoch 115 loss 21.130901\n",
      "Epoch 116 loss 21.069105\n",
      "Epoch 117 loss 21.007528\n",
      "Epoch 118 loss 20.946150\n",
      "Epoch 119 loss 20.884983\n",
      "Epoch 120 loss 20.824026\n",
      "Epoch 121 loss 20.763273\n",
      "Epoch 122 loss 20.702726\n",
      "Epoch 123 loss 20.642384\n",
      "Epoch 124 loss 20.582251\n",
      "Epoch 125 loss 20.522322\n",
      "Epoch 126 loss 20.462589\n",
      "Epoch 127 loss 20.403067\n",
      "Epoch 128 loss 20.343746\n",
      "Epoch 129 loss 20.284622\n",
      "Epoch 130 loss 20.225702\n",
      "Epoch 131 loss 20.166983\n",
      "Epoch 132 loss 20.108461\n",
      "Epoch 133 loss 20.050135\n",
      "Epoch 134 loss 19.992014\n",
      "Epoch 135 loss 19.934088\n",
      "Epoch 136 loss 19.876352\n",
      "Epoch 137 loss 19.818821\n",
      "Epoch 138 loss 19.761480\n",
      "Epoch 139 loss 19.704332\n",
      "Epoch 140 loss 19.647387\n",
      "Epoch 141 loss 19.590626\n",
      "Epoch 142 loss 19.534063\n",
      "Epoch 143 loss 19.477690\n",
      "Epoch 144 loss 19.421507\n",
      "Epoch 145 loss 19.365517\n",
      "Epoch 146 loss 19.309715\n",
      "Epoch 147 loss 19.254107\n",
      "Epoch 148 loss 19.198685\n",
      "Epoch 149 loss 19.143446\n",
      "Epoch 150 loss 19.088400\n",
      "Epoch 151 loss 19.033545\n",
      "Epoch 152 loss 18.978868\n",
      "Epoch 153 loss 18.924377\n",
      "Epoch 154 loss 18.870081\n",
      "Epoch 155 loss 18.815960\n",
      "Epoch 156 loss 18.762022\n",
      "Epoch 157 loss 18.708269\n",
      "Epoch 158 loss 18.654703\n",
      "Epoch 159 loss 18.601313\n",
      "Epoch 160 loss 18.548111\n",
      "Epoch 161 loss 18.495081\n",
      "Epoch 162 loss 18.442234\n",
      "Epoch 163 loss 18.389570\n",
      "Epoch 164 loss 18.337080\n",
      "Epoch 165 loss 18.284777\n",
      "Epoch 166 loss 18.232643\n",
      "Epoch 167 loss 18.180687\n",
      "Epoch 168 loss 18.128904\n",
      "Epoch 169 loss 18.077303\n",
      "Epoch 170 loss 18.025879\n",
      "Epoch 171 loss 17.974623\n",
      "Epoch 172 loss 17.923546\n",
      "Epoch 173 loss 17.872641\n",
      "Epoch 174 loss 17.821907\n",
      "Epoch 175 loss 17.771343\n",
      "Epoch 176 loss 17.720955\n",
      "Epoch 177 loss 17.670738\n",
      "Epoch 178 loss 17.620691\n",
      "Epoch 179 loss 17.570814\n",
      "Epoch 180 loss 17.521105\n",
      "Epoch 181 loss 17.471563\n",
      "Epoch 182 loss 17.422194\n",
      "Epoch 183 loss 17.372992\n",
      "Epoch 184 loss 17.323954\n",
      "Epoch 185 loss 17.275085\n",
      "Epoch 186 loss 17.226379\n",
      "Epoch 187 loss 17.177839\n",
      "Epoch 188 loss 17.129467\n",
      "Epoch 189 loss 17.081255\n",
      "Epoch 190 loss 17.033207\n",
      "Epoch 191 loss 16.985327\n",
      "Epoch 192 loss 16.937605\n",
      "Epoch 193 loss 16.890047\n",
      "Epoch 194 loss 16.842649\n",
      "Epoch 195 loss 16.795412\n",
      "Epoch 196 loss 16.748339\n",
      "Epoch 197 loss 16.701424\n",
      "Epoch 198 loss 16.654661\n",
      "Epoch 199 loss 16.608065\n",
      "Epoch 200 loss 16.561625\n",
      "Epoch 201 loss 16.515343\n",
      "Epoch 202 loss 16.469219\n",
      "Epoch 203 loss 16.423250\n",
      "Epoch 204 loss 16.377434\n",
      "Epoch 205 loss 16.331776\n",
      "Epoch 206 loss 16.286276\n",
      "Epoch 207 loss 16.240925\n",
      "Epoch 208 loss 16.195734\n",
      "Epoch 209 loss 16.150694\n",
      "Epoch 210 loss 16.105806\n",
      "Epoch 211 loss 16.061071\n",
      "Epoch 212 loss 16.016487\n",
      "Epoch 213 loss 15.972058\n",
      "Epoch 214 loss 15.927777\n",
      "Epoch 215 loss 15.883645\n",
      "Epoch 216 loss 15.839664\n",
      "Epoch 217 loss 15.795832\n",
      "Epoch 218 loss 15.752149\n",
      "Epoch 219 loss 15.708612\n",
      "Epoch 220 loss 15.665228\n",
      "Epoch 221 loss 15.621990\n",
      "Epoch 222 loss 15.578897\n",
      "Epoch 223 loss 15.535950\n",
      "Epoch 224 loss 15.493152\n",
      "Epoch 225 loss 15.450497\n",
      "Epoch 226 loss 15.407981\n",
      "Epoch 227 loss 15.365615\n",
      "Epoch 228 loss 15.323395\n",
      "Epoch 229 loss 15.281318\n",
      "Epoch 230 loss 15.239380\n",
      "Epoch 231 loss 15.197586\n",
      "Epoch 232 loss 15.155931\n",
      "Epoch 233 loss 15.114425\n",
      "Epoch 234 loss 15.073053\n",
      "Epoch 235 loss 15.031823\n",
      "Epoch 236 loss 14.990737\n",
      "Epoch 237 loss 14.949784\n",
      "Epoch 238 loss 14.908973\n",
      "Epoch 239 loss 14.868304\n",
      "Epoch 240 loss 14.827767\n",
      "Epoch 241 loss 14.787370\n",
      "Epoch 242 loss 14.747110\n",
      "Epoch 243 loss 14.706989\n",
      "Epoch 244 loss 14.667002\n",
      "Epoch 245 loss 14.627149\n",
      "Epoch 246 loss 14.587436\n",
      "Epoch 247 loss 14.547854\n",
      "Epoch 248 loss 14.508408\n",
      "Epoch 249 loss 14.469095\n",
      "Epoch 250 loss 14.429919\n",
      "Epoch 251 loss 14.390872\n",
      "Epoch 252 loss 14.351956\n",
      "Epoch 253 loss 14.313177\n",
      "Epoch 254 loss 14.274525\n",
      "Epoch 255 loss 14.236008\n",
      "Epoch 256 loss 14.197620\n",
      "Epoch 257 loss 14.159363\n",
      "Epoch 258 loss 14.121234\n",
      "Epoch 259 loss 14.083237\n",
      "Epoch 260 loss 14.045368\n",
      "Epoch 261 loss 14.007627\n",
      "Epoch 262 loss 13.970016\n",
      "Epoch 263 loss 13.932532\n",
      "Epoch 264 loss 13.895172\n",
      "Epoch 265 loss 13.857942\n",
      "Epoch 266 loss 13.820837\n",
      "Epoch 267 loss 13.783858\n",
      "Epoch 268 loss 13.747006\n",
      "Epoch 269 loss 13.710278\n",
      "Epoch 270 loss 13.673676\n",
      "Epoch 271 loss 13.637196\n",
      "Epoch 272 loss 13.600842\n",
      "Epoch 273 loss 13.564609\n",
      "Epoch 274 loss 13.528501\n",
      "Epoch 275 loss 13.492515\n",
      "Epoch 276 loss 13.456651\n",
      "Epoch 277 loss 13.420910\n",
      "Epoch 278 loss 13.385287\n",
      "Epoch 279 loss 13.349787\n",
      "Epoch 280 loss 13.314410\n",
      "Epoch 281 loss 13.279148\n",
      "Epoch 282 loss 13.244009\n",
      "Epoch 283 loss 13.208993\n",
      "Epoch 284 loss 13.174088\n",
      "Epoch 285 loss 13.139307\n",
      "Epoch 286 loss 13.104638\n",
      "Epoch 287 loss 13.070093\n",
      "Epoch 288 loss 13.035663\n",
      "Epoch 289 loss 13.001349\n",
      "Epoch 290 loss 12.967154\n",
      "Epoch 291 loss 12.933074\n",
      "Epoch 292 loss 12.899109\n",
      "Epoch 293 loss 12.865259\n",
      "Epoch 294 loss 12.831525\n",
      "Epoch 295 loss 12.797904\n",
      "Epoch 296 loss 12.764399\n",
      "Epoch 297 loss 12.731007\n",
      "Epoch 298 loss 12.697727\n",
      "Epoch 299 loss 12.664560\n",
      "Epoch 300 loss 12.631507\n",
      "Epoch 301 loss 12.598566\n",
      "Epoch 302 loss 12.565738\n",
      "Epoch 303 loss 12.533021\n",
      "Epoch 304 loss 12.500415\n",
      "Epoch 305 loss 12.467919\n",
      "Epoch 306 loss 12.435533\n",
      "Epoch 307 loss 12.403255\n",
      "Epoch 308 loss 12.371088\n",
      "Epoch 309 loss 12.339031\n",
      "Epoch 310 loss 12.307083\n",
      "Epoch 311 loss 12.275247\n",
      "Epoch 312 loss 12.243509\n",
      "Epoch 313 loss 12.211887\n",
      "Epoch 314 loss 12.180370\n",
      "Epoch 315 loss 12.148962\n",
      "Epoch 316 loss 12.117655\n",
      "Epoch 317 loss 12.086463\n",
      "Epoch 318 loss 12.055373\n",
      "Epoch 319 loss 12.024384\n",
      "Epoch 320 loss 11.993508\n",
      "Epoch 321 loss 11.962732\n",
      "Epoch 322 loss 11.932056\n",
      "Epoch 323 loss 11.901492\n",
      "Epoch 324 loss 11.871029\n",
      "Epoch 325 loss 11.840671\n",
      "Epoch 326 loss 11.810413\n",
      "Epoch 327 loss 11.780257\n",
      "Epoch 328 loss 11.750208\n",
      "Epoch 329 loss 11.720258\n",
      "Epoch 330 loss 11.690412\n",
      "Epoch 331 loss 11.660664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 332 loss 11.631016\n",
      "Epoch 333 loss 11.601473\n",
      "Epoch 334 loss 11.572030\n",
      "Epoch 335 loss 11.542686\n",
      "Epoch 336 loss 11.513440\n",
      "Epoch 337 loss 11.484293\n",
      "Epoch 338 loss 11.455247\n",
      "Epoch 339 loss 11.426300\n",
      "Epoch 340 loss 11.397448\n",
      "Epoch 341 loss 11.368696\n",
      "Epoch 342 loss 11.340043\n",
      "Epoch 343 loss 11.311487\n",
      "Epoch 344 loss 11.283028\n",
      "Epoch 345 loss 11.254662\n",
      "Epoch 346 loss 11.226396\n",
      "Epoch 347 loss 11.198221\n",
      "Epoch 348 loss 11.170149\n",
      "Epoch 349 loss 11.142170\n",
      "Epoch 350 loss 11.114283\n",
      "Epoch 351 loss 11.086493\n",
      "Epoch 352 loss 11.058796\n",
      "Epoch 353 loss 11.031192\n",
      "Epoch 354 loss 11.003686\n",
      "Epoch 355 loss 10.976271\n",
      "Epoch 356 loss 10.948948\n",
      "Epoch 357 loss 10.921718\n",
      "Epoch 358 loss 10.894581\n",
      "Epoch 359 loss 10.867537\n",
      "Epoch 360 loss 10.840583\n",
      "Epoch 361 loss 10.813720\n",
      "Epoch 362 loss 10.786951\n",
      "Epoch 363 loss 10.760270\n",
      "Epoch 364 loss 10.733681\n",
      "Epoch 365 loss 10.707183\n",
      "Epoch 366 loss 10.680775\n",
      "Epoch 367 loss 10.654453\n",
      "Epoch 368 loss 10.628225\n",
      "Epoch 369 loss 10.602084\n",
      "Epoch 370 loss 10.576032\n",
      "Epoch 371 loss 10.550071\n",
      "Epoch 372 loss 10.524195\n",
      "Epoch 373 loss 10.498408\n",
      "Epoch 374 loss 10.472707\n",
      "Epoch 375 loss 10.447094\n",
      "Epoch 376 loss 10.421568\n",
      "Epoch 377 loss 10.396132\n",
      "Epoch 378 loss 10.370778\n",
      "Epoch 379 loss 10.345510\n",
      "Epoch 380 loss 10.320329\n",
      "Epoch 381 loss 10.295236\n",
      "Epoch 382 loss 10.270224\n",
      "Epoch 383 loss 10.245296\n",
      "Epoch 384 loss 10.220456\n",
      "Epoch 385 loss 10.195701\n",
      "Epoch 386 loss 10.171027\n",
      "Epoch 387 loss 10.146436\n",
      "Epoch 388 loss 10.121934\n",
      "Epoch 389 loss 10.097512\n",
      "Epoch 390 loss 10.073174\n",
      "Epoch 391 loss 10.048919\n",
      "Epoch 392 loss 10.024742\n",
      "Epoch 393 loss 10.000652\n",
      "Epoch 394 loss 9.976640\n",
      "Epoch 395 loss 9.952712\n",
      "Epoch 396 loss 9.928863\n",
      "Epoch 397 loss 9.905092\n",
      "Epoch 398 loss 9.881409\n",
      "Epoch 399 loss 9.857802\n",
      "Epoch 400 loss 9.834277\n",
      "Epoch 401 loss 9.810832\n",
      "Epoch 402 loss 9.787466\n",
      "Epoch 403 loss 9.764176\n",
      "Epoch 404 loss 9.740971\n",
      "Epoch 405 loss 9.717843\n",
      "Epoch 406 loss 9.694793\n",
      "Epoch 407 loss 9.671823\n",
      "Epoch 408 loss 9.648926\n",
      "Epoch 409 loss 9.626110\n",
      "Epoch 410 loss 9.603373\n",
      "Epoch 411 loss 9.580710\n",
      "Epoch 412 loss 9.558124\n",
      "Epoch 413 loss 9.535618\n",
      "Epoch 414 loss 9.513185\n",
      "Epoch 415 loss 9.490829\n",
      "Epoch 416 loss 9.468551\n",
      "Epoch 417 loss 9.446347\n",
      "Epoch 418 loss 9.424216\n",
      "Epoch 419 loss 9.402163\n",
      "Epoch 420 loss 9.380185\n",
      "Epoch 421 loss 9.358281\n",
      "Epoch 422 loss 9.336448\n",
      "Epoch 423 loss 9.314696\n",
      "Epoch 424 loss 9.293013\n",
      "Epoch 425 loss 9.271402\n",
      "Epoch 426 loss 9.249870\n",
      "Epoch 427 loss 9.228409\n",
      "Epoch 428 loss 9.207021\n",
      "Epoch 429 loss 9.185704\n",
      "Epoch 430 loss 9.164462\n",
      "Epoch 431 loss 9.143288\n",
      "Epoch 432 loss 9.122189\n",
      "Epoch 433 loss 9.101160\n",
      "Epoch 434 loss 9.080204\n",
      "Epoch 435 loss 9.059317\n",
      "Epoch 436 loss 9.038502\n",
      "Epoch 437 loss 9.017757\n",
      "Epoch 438 loss 8.997085\n",
      "Epoch 439 loss 8.976479\n",
      "Epoch 440 loss 8.955945\n",
      "Epoch 441 loss 8.935481\n",
      "Epoch 442 loss 8.915089\n",
      "Epoch 443 loss 8.894763\n",
      "Epoch 444 loss 8.874508\n",
      "Epoch 445 loss 8.854318\n",
      "Epoch 446 loss 8.834197\n",
      "Epoch 447 loss 8.814149\n",
      "Epoch 448 loss 8.794162\n",
      "Epoch 449 loss 8.774252\n",
      "Epoch 450 loss 8.754406\n",
      "Epoch 451 loss 8.734625\n",
      "Epoch 452 loss 8.714911\n",
      "Epoch 453 loss 8.695266\n",
      "Epoch 454 loss 8.675689\n",
      "Epoch 455 loss 8.656174\n",
      "Epoch 456 loss 8.636728\n",
      "Epoch 457 loss 8.617346\n",
      "Epoch 458 loss 8.598029\n",
      "Epoch 459 loss 8.578781\n",
      "Epoch 460 loss 8.559597\n",
      "Epoch 461 loss 8.540478\n",
      "Epoch 462 loss 8.521426\n",
      "Epoch 463 loss 8.502438\n",
      "Epoch 464 loss 8.483516\n",
      "Epoch 465 loss 8.464652\n",
      "Epoch 466 loss 8.445858\n",
      "Epoch 467 loss 8.427128\n",
      "Epoch 468 loss 8.408456\n",
      "Epoch 469 loss 8.389848\n",
      "Epoch 470 loss 8.371305\n",
      "Epoch 471 loss 8.352828\n",
      "Epoch 472 loss 8.334408\n",
      "Epoch 473 loss 8.316055\n",
      "Epoch 474 loss 8.297764\n",
      "Epoch 475 loss 8.279534\n",
      "Epoch 476 loss 8.261369\n",
      "Epoch 477 loss 8.243261\n",
      "Epoch 478 loss 8.225213\n",
      "Epoch 479 loss 8.207232\n",
      "Epoch 480 loss 8.189310\n",
      "Epoch 481 loss 8.171450\n",
      "Epoch 482 loss 8.153648\n",
      "Epoch 483 loss 8.135907\n",
      "Epoch 484 loss 8.118226\n",
      "Epoch 485 loss 8.100607\n",
      "Epoch 486 loss 8.083045\n",
      "Epoch 487 loss 8.065548\n",
      "Epoch 488 loss 8.048104\n",
      "Epoch 489 loss 8.030723\n",
      "Epoch 490 loss 8.013400\n",
      "Epoch 491 loss 7.996135\n",
      "Epoch 492 loss 7.978929\n",
      "Epoch 493 loss 7.961784\n",
      "Epoch 494 loss 7.944690\n",
      "Epoch 495 loss 7.927662\n",
      "Epoch 496 loss 7.910690\n",
      "Epoch 497 loss 7.893775\n",
      "Epoch 498 loss 7.876915\n",
      "Epoch 499 loss 7.860116\n",
      "Epoch 500 loss 7.843370\n",
      "Epoch 501 loss 7.826681\n",
      "Epoch 502 loss 7.810053\n",
      "Epoch 503 loss 7.793480\n",
      "Epoch 504 loss 7.776962\n",
      "Epoch 505 loss 7.760498\n",
      "Epoch 506 loss 7.744092\n",
      "Epoch 507 loss 7.727745\n",
      "Epoch 508 loss 7.711447\n",
      "Epoch 509 loss 7.695212\n",
      "Epoch 510 loss 7.679024\n",
      "Epoch 511 loss 7.662895\n",
      "Epoch 512 loss 7.646819\n",
      "Epoch 513 loss 7.630803\n",
      "Epoch 514 loss 7.614836\n",
      "Epoch 515 loss 7.598925\n",
      "Epoch 516 loss 7.583069\n",
      "Epoch 517 loss 7.567266\n",
      "Epoch 518 loss 7.551516\n",
      "Epoch 519 loss 7.535819\n",
      "Epoch 520 loss 7.520176\n",
      "Epoch 521 loss 7.504588\n",
      "Epoch 522 loss 7.489048\n",
      "Epoch 523 loss 7.473566\n",
      "Epoch 524 loss 7.458135\n",
      "Epoch 525 loss 7.442751\n",
      "Epoch 526 loss 7.427426\n",
      "Epoch 527 loss 7.412152\n",
      "Epoch 528 loss 7.396928\n",
      "Epoch 529 loss 7.381756\n",
      "Epoch 530 loss 7.366636\n",
      "Epoch 531 loss 7.351566\n",
      "Epoch 532 loss 7.336550\n",
      "Epoch 533 loss 7.321585\n",
      "Epoch 534 loss 7.306670\n",
      "Epoch 535 loss 7.291803\n",
      "Epoch 536 loss 7.276989\n",
      "Epoch 537 loss 7.262227\n",
      "Epoch 538 loss 7.247512\n",
      "Epoch 539 loss 7.232846\n",
      "Epoch 540 loss 7.218231\n",
      "Epoch 541 loss 7.203666\n",
      "Epoch 542 loss 7.189151\n",
      "Epoch 543 loss 7.174683\n",
      "Epoch 544 loss 7.160267\n",
      "Epoch 545 loss 7.145897\n",
      "Epoch 546 loss 7.131578\n",
      "Epoch 547 loss 7.117305\n",
      "Epoch 548 loss 7.103083\n",
      "Epoch 549 loss 7.088911\n",
      "Epoch 550 loss 7.074785\n",
      "Epoch 551 loss 7.060707\n",
      "Epoch 552 loss 7.046677\n",
      "Epoch 553 loss 7.032695\n",
      "Epoch 554 loss 7.018756\n",
      "Epoch 555 loss 7.004869\n",
      "Epoch 556 loss 6.991029\n",
      "Epoch 557 loss 6.977232\n",
      "Epoch 558 loss 6.963488\n",
      "Epoch 559 loss 6.949786\n",
      "Epoch 560 loss 6.936135\n",
      "Epoch 561 loss 6.922528\n",
      "Epoch 562 loss 6.908967\n",
      "Epoch 563 loss 6.895452\n",
      "Epoch 564 loss 6.881980\n",
      "Epoch 565 loss 6.868558\n",
      "Epoch 566 loss 6.855180\n",
      "Epoch 567 loss 6.841848\n",
      "Epoch 568 loss 6.828561\n",
      "Epoch 569 loss 6.815319\n",
      "Epoch 570 loss 6.802118\n",
      "Epoch 571 loss 6.788968\n",
      "Epoch 572 loss 6.775864\n",
      "Epoch 573 loss 6.762798\n",
      "Epoch 574 loss 6.749779\n",
      "Epoch 575 loss 6.736803\n",
      "Epoch 576 loss 6.723875\n",
      "Epoch 577 loss 6.710986\n",
      "Epoch 578 loss 6.698142\n",
      "Epoch 579 loss 6.685344\n",
      "Epoch 580 loss 6.672589\n",
      "Epoch 581 loss 6.659874\n",
      "Epoch 582 loss 6.647207\n",
      "Epoch 583 loss 6.634577\n",
      "Epoch 584 loss 6.621995\n",
      "Epoch 585 loss 6.609454\n",
      "Epoch 586 loss 6.596954\n",
      "Epoch 587 loss 6.584500\n",
      "Epoch 588 loss 6.572087\n",
      "Epoch 589 loss 6.559712\n",
      "Epoch 590 loss 6.547384\n",
      "Epoch 591 loss 6.535097\n",
      "Epoch 592 loss 6.522851\n",
      "Epoch 593 loss 6.510646\n",
      "Epoch 594 loss 6.498481\n",
      "Epoch 595 loss 6.486362\n",
      "Epoch 596 loss 6.474282\n",
      "Epoch 597 loss 6.462242\n",
      "Epoch 598 loss 6.450243\n",
      "Epoch 599 loss 6.438284\n",
      "Epoch 600 loss 6.426367\n",
      "Epoch 601 loss 6.414490\n",
      "Epoch 602 loss 6.402655\n",
      "Epoch 603 loss 6.390859\n",
      "Epoch 604 loss 6.379102\n",
      "Epoch 605 loss 6.367384\n",
      "Epoch 606 loss 6.355706\n",
      "Epoch 607 loss 6.344071\n",
      "Epoch 608 loss 6.332472\n",
      "Epoch 609 loss 6.320912\n",
      "Epoch 610 loss 6.309395\n",
      "Epoch 611 loss 6.297915\n",
      "Epoch 612 loss 6.286473\n",
      "Epoch 613 loss 6.275074\n",
      "Epoch 614 loss 6.263707\n",
      "Epoch 615 loss 6.252382\n",
      "Epoch 616 loss 6.241098\n",
      "Epoch 617 loss 6.229849\n",
      "Epoch 618 loss 6.218639\n",
      "Epoch 619 loss 6.207471\n",
      "Epoch 620 loss 6.196334\n",
      "Epoch 621 loss 6.185240\n",
      "Epoch 622 loss 6.174181\n",
      "Epoch 623 loss 6.163159\n",
      "Epoch 624 loss 6.152177\n",
      "Epoch 625 loss 6.141229\n",
      "Epoch 626 loss 6.130321\n",
      "Epoch 627 loss 6.119448\n",
      "Epoch 628 loss 6.108614\n",
      "Epoch 629 loss 6.097815\n",
      "Epoch 630 loss 6.087054\n",
      "Epoch 631 loss 6.076329\n",
      "Epoch 632 loss 6.065643\n",
      "Epoch 633 loss 6.054988\n",
      "Epoch 634 loss 6.044372\n",
      "Epoch 635 loss 6.033794\n",
      "Epoch 636 loss 6.023247\n",
      "Epoch 637 loss 6.012738\n",
      "Epoch 638 loss 6.002264\n",
      "Epoch 639 loss 5.991829\n",
      "Epoch 640 loss 5.981426\n",
      "Epoch 641 loss 5.971057\n",
      "Epoch 642 loss 5.960727\n",
      "Epoch 643 loss 5.950432\n",
      "Epoch 644 loss 5.940171\n",
      "Epoch 645 loss 5.929944\n",
      "Epoch 646 loss 5.919752\n",
      "Epoch 647 loss 5.909597\n",
      "Epoch 648 loss 5.899473\n",
      "Epoch 649 loss 5.889384\n",
      "Epoch 650 loss 5.879326\n",
      "Epoch 651 loss 5.869310\n",
      "Epoch 652 loss 5.859322\n",
      "Epoch 653 loss 5.849374\n",
      "Epoch 654 loss 5.839453\n",
      "Epoch 655 loss 5.829570\n",
      "Epoch 656 loss 5.819718\n",
      "Epoch 657 loss 5.809900\n",
      "Epoch 658 loss 5.800117\n",
      "Epoch 659 loss 5.790367\n",
      "Epoch 660 loss 5.780647\n",
      "Epoch 661 loss 5.770962\n",
      "Epoch 662 loss 5.761312\n",
      "Epoch 663 loss 5.751693\n",
      "Epoch 664 loss 5.742105\n",
      "Epoch 665 loss 5.732550\n",
      "Epoch 666 loss 5.723031\n",
      "Epoch 667 loss 5.713540\n",
      "Epoch 668 loss 5.704084\n",
      "Epoch 669 loss 5.694658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 670 loss 5.685265\n",
      "Epoch 671 loss 5.675904\n",
      "Epoch 672 loss 5.666573\n",
      "Epoch 673 loss 5.657277\n",
      "Epoch 674 loss 5.648010\n",
      "Epoch 675 loss 5.638776\n",
      "Epoch 676 loss 5.629575\n",
      "Epoch 677 loss 5.620402\n",
      "Epoch 678 loss 5.611260\n",
      "Epoch 679 loss 5.602148\n",
      "Epoch 680 loss 5.593071\n",
      "Epoch 681 loss 5.584022\n",
      "Epoch 682 loss 5.575005\n",
      "Epoch 683 loss 5.566019\n",
      "Epoch 684 loss 5.557063\n",
      "Epoch 685 loss 5.548136\n",
      "Epoch 686 loss 5.539241\n",
      "Epoch 687 loss 5.530376\n",
      "Epoch 688 loss 5.521540\n",
      "Epoch 689 loss 5.512733\n",
      "Epoch 690 loss 5.503958\n",
      "Epoch 691 loss 5.495212\n",
      "Epoch 692 loss 5.486496\n",
      "Epoch 693 loss 5.477808\n",
      "Epoch 694 loss 5.469152\n",
      "Epoch 695 loss 5.460525\n",
      "Epoch 696 loss 5.451928\n",
      "Epoch 697 loss 5.443359\n",
      "Epoch 698 loss 5.434820\n",
      "Epoch 699 loss 5.426310\n",
      "Epoch 700 loss 5.417827\n",
      "Epoch 701 loss 5.409373\n",
      "Epoch 702 loss 5.400949\n",
      "Epoch 703 loss 5.392551\n",
      "Epoch 704 loss 5.384184\n",
      "Epoch 705 loss 5.375845\n",
      "Epoch 706 loss 5.367537\n",
      "Epoch 707 loss 5.359253\n",
      "Epoch 708 loss 5.350998\n",
      "Epoch 709 loss 5.342771\n",
      "Epoch 710 loss 5.334575\n",
      "Epoch 711 loss 5.326403\n",
      "Epoch 712 loss 5.318259\n",
      "Epoch 713 loss 5.310144\n",
      "Epoch 714 loss 5.302055\n",
      "Epoch 715 loss 5.293994\n",
      "Epoch 716 loss 5.285964\n",
      "Epoch 717 loss 5.277958\n",
      "Epoch 718 loss 5.269979\n",
      "Epoch 719 loss 5.262026\n",
      "Epoch 720 loss 5.254103\n",
      "Epoch 721 loss 5.246205\n",
      "Epoch 722 loss 5.238335\n",
      "Epoch 723 loss 5.230491\n",
      "Epoch 724 loss 5.222673\n",
      "Epoch 725 loss 5.214881\n",
      "Epoch 726 loss 5.207120\n",
      "Epoch 727 loss 5.199381\n",
      "Epoch 728 loss 5.191670\n",
      "Epoch 729 loss 5.183984\n",
      "Epoch 730 loss 5.176324\n",
      "Epoch 731 loss 5.168688\n",
      "Epoch 732 loss 5.161084\n",
      "Epoch 733 loss 5.153500\n",
      "Epoch 734 loss 5.145943\n",
      "Epoch 735 loss 5.138412\n",
      "Epoch 736 loss 5.130910\n",
      "Epoch 737 loss 5.123428\n",
      "Epoch 738 loss 5.115977\n",
      "Epoch 739 loss 5.108547\n",
      "Epoch 740 loss 5.101144\n",
      "Epoch 741 loss 5.093765\n",
      "Epoch 742 loss 5.086413\n",
      "Epoch 743 loss 5.079085\n",
      "Epoch 744 loss 5.071782\n",
      "Epoch 745 loss 5.064505\n",
      "Epoch 746 loss 5.057247\n",
      "Epoch 747 loss 5.050022\n",
      "Epoch 748 loss 5.042817\n",
      "Epoch 749 loss 5.035636\n",
      "Epoch 750 loss 5.028476\n",
      "Epoch 751 loss 5.021346\n",
      "Epoch 752 loss 5.014239\n",
      "Epoch 753 loss 5.007157\n",
      "Epoch 754 loss 5.000099\n",
      "Epoch 755 loss 4.993064\n",
      "Epoch 756 loss 4.986051\n",
      "Epoch 757 loss 4.979065\n",
      "Epoch 758 loss 4.972100\n",
      "Epoch 759 loss 4.965159\n",
      "Epoch 760 loss 4.958245\n",
      "Epoch 761 loss 4.951350\n",
      "Epoch 762 loss 4.944479\n",
      "Epoch 763 loss 4.937633\n",
      "Epoch 764 loss 4.930812\n",
      "Epoch 765 loss 4.924009\n",
      "Epoch 766 loss 4.917234\n",
      "Epoch 767 loss 4.910480\n",
      "Epoch 768 loss 4.903749\n",
      "Epoch 769 loss 4.897040\n",
      "Epoch 770 loss 4.890356\n",
      "Epoch 771 loss 4.883691\n",
      "Epoch 772 loss 4.877052\n",
      "Epoch 773 loss 4.870436\n",
      "Epoch 774 loss 4.863839\n",
      "Epoch 775 loss 4.857267\n",
      "Epoch 776 loss 4.850717\n",
      "Epoch 777 loss 4.844189\n",
      "Epoch 778 loss 4.837683\n",
      "Epoch 779 loss 4.831196\n",
      "Epoch 780 loss 4.824737\n",
      "Epoch 781 loss 4.818298\n",
      "Epoch 782 loss 4.811880\n",
      "Epoch 783 loss 4.805481\n",
      "Epoch 784 loss 4.799106\n",
      "Epoch 785 loss 4.792755\n",
      "Epoch 786 loss 4.786422\n",
      "Epoch 787 loss 4.780112\n",
      "Epoch 788 loss 4.773824\n",
      "Epoch 789 loss 4.767559\n",
      "Epoch 790 loss 4.761311\n",
      "Epoch 791 loss 4.755087\n",
      "Epoch 792 loss 4.748885\n",
      "Epoch 793 loss 4.742701\n",
      "Epoch 794 loss 4.736537\n",
      "Epoch 795 loss 4.730397\n",
      "Epoch 796 loss 4.724279\n",
      "Epoch 797 loss 4.718181\n",
      "Epoch 798 loss 4.712101\n",
      "Epoch 799 loss 4.706046\n",
      "Epoch 800 loss 4.700009\n",
      "Epoch 801 loss 4.693989\n",
      "Epoch 802 loss 4.687995\n",
      "Epoch 803 loss 4.682020\n",
      "Epoch 804 loss 4.676063\n",
      "Epoch 805 loss 4.670130\n",
      "Epoch 806 loss 4.664214\n",
      "Epoch 807 loss 4.658320\n",
      "Epoch 808 loss 4.652445\n",
      "Epoch 809 loss 4.646592\n",
      "Epoch 810 loss 4.640753\n",
      "Epoch 811 loss 4.634938\n",
      "Epoch 812 loss 4.629142\n",
      "Epoch 813 loss 4.623368\n",
      "Epoch 814 loss 4.617611\n",
      "Epoch 815 loss 4.611873\n",
      "Epoch 816 loss 4.606156\n",
      "Epoch 817 loss 4.600458\n",
      "Epoch 818 loss 4.594780\n",
      "Epoch 819 loss 4.589119\n",
      "Epoch 820 loss 4.583479\n",
      "Epoch 821 loss 4.577857\n",
      "Epoch 822 loss 4.572256\n",
      "Epoch 823 loss 4.566675\n",
      "Epoch 824 loss 4.561109\n",
      "Epoch 825 loss 4.555565\n",
      "Epoch 826 loss 4.550039\n",
      "Epoch 827 loss 4.544533\n",
      "Epoch 828 loss 4.539044\n",
      "Epoch 829 loss 4.533575\n",
      "Epoch 830 loss 4.528122\n",
      "Epoch 831 loss 4.522691\n",
      "Epoch 832 loss 4.517276\n",
      "Epoch 833 loss 4.511879\n",
      "Epoch 834 loss 4.506504\n",
      "Epoch 835 loss 4.501141\n",
      "Epoch 836 loss 4.495801\n",
      "Epoch 837 loss 4.490474\n",
      "Epoch 838 loss 4.485170\n",
      "Epoch 839 loss 4.479884\n",
      "Epoch 840 loss 4.474614\n",
      "Epoch 841 loss 4.469364\n",
      "Epoch 842 loss 4.464129\n",
      "Epoch 843 loss 4.458913\n",
      "Epoch 844 loss 4.453716\n",
      "Epoch 845 loss 4.448534\n",
      "Epoch 846 loss 4.443372\n",
      "Epoch 847 loss 4.438227\n",
      "Epoch 848 loss 4.433099\n",
      "Epoch 849 loss 4.427989\n",
      "Epoch 850 loss 4.422897\n",
      "Epoch 851 loss 4.417819\n",
      "Epoch 852 loss 4.412762\n",
      "Epoch 853 loss 4.407720\n",
      "Epoch 854 loss 4.402697\n",
      "Epoch 855 loss 4.397688\n",
      "Epoch 856 loss 4.392697\n",
      "Epoch 857 loss 4.387725\n",
      "Epoch 858 loss 4.382769\n",
      "Epoch 859 loss 4.377828\n",
      "Epoch 860 loss 4.372905\n",
      "Epoch 861 loss 4.368000\n",
      "Epoch 862 loss 4.363111\n",
      "Epoch 863 loss 4.358238\n",
      "Epoch 864 loss 4.353383\n",
      "Epoch 865 loss 4.348542\n",
      "Epoch 866 loss 4.343716\n",
      "Epoch 867 loss 4.338911\n",
      "Epoch 868 loss 4.334121\n",
      "Epoch 869 loss 4.329345\n",
      "Epoch 870 loss 4.324588\n",
      "Epoch 871 loss 4.319845\n",
      "Epoch 872 loss 4.315118\n",
      "Epoch 873 loss 4.310409\n",
      "Epoch 874 loss 4.305714\n",
      "Epoch 875 loss 4.301035\n",
      "Epoch 876 loss 4.296376\n",
      "Epoch 877 loss 4.291727\n",
      "Epoch 878 loss 4.287097\n",
      "Epoch 879 loss 4.282482\n",
      "Epoch 880 loss 4.277882\n",
      "Epoch 881 loss 4.273299\n",
      "Epoch 882 loss 4.268732\n",
      "Epoch 883 loss 4.264178\n",
      "Epoch 884 loss 4.259643\n",
      "Epoch 885 loss 4.255120\n",
      "Epoch 886 loss 4.250613\n",
      "Epoch 887 loss 4.246124\n",
      "Epoch 888 loss 4.241648\n",
      "Epoch 889 loss 4.237185\n",
      "Epoch 890 loss 4.232740\n",
      "Epoch 891 loss 4.228308\n",
      "Epoch 892 loss 4.223895\n",
      "Epoch 893 loss 4.219494\n",
      "Epoch 894 loss 4.215109\n",
      "Epoch 895 loss 4.210737\n",
      "Epoch 896 loss 4.206383\n",
      "Epoch 897 loss 4.202042\n",
      "Epoch 898 loss 4.197715\n",
      "Epoch 899 loss 4.193405\n",
      "Epoch 900 loss 4.189108\n",
      "Epoch 901 loss 4.184825\n",
      "Epoch 902 loss 4.180559\n",
      "Epoch 903 loss 4.176305\n",
      "Epoch 904 loss 4.172065\n",
      "Epoch 905 loss 4.167842\n",
      "Epoch 906 loss 4.163631\n",
      "Epoch 907 loss 4.159436\n",
      "Epoch 908 loss 4.155253\n",
      "Epoch 909 loss 4.151086\n",
      "Epoch 910 loss 4.146934\n",
      "Epoch 911 loss 4.142795\n",
      "Epoch 912 loss 4.138669\n",
      "Epoch 913 loss 4.134559\n",
      "Epoch 914 loss 4.130464\n",
      "Epoch 915 loss 4.126378\n",
      "Epoch 916 loss 4.122310\n",
      "Epoch 917 loss 4.118254\n",
      "Epoch 918 loss 4.114213\n",
      "Epoch 919 loss 4.110184\n",
      "Epoch 920 loss 4.106169\n",
      "Epoch 921 loss 4.102170\n",
      "Epoch 922 loss 4.098181\n",
      "Epoch 923 loss 4.094210\n",
      "Epoch 924 loss 4.090249\n",
      "Epoch 925 loss 4.086300\n",
      "Epoch 926 loss 4.082366\n",
      "Epoch 927 loss 4.078448\n",
      "Epoch 928 loss 4.074541\n",
      "Epoch 929 loss 4.070649\n",
      "Epoch 930 loss 4.066768\n",
      "Epoch 931 loss 4.062900\n",
      "Epoch 932 loss 4.059047\n",
      "Epoch 933 loss 4.055204\n",
      "Epoch 934 loss 4.051379\n",
      "Epoch 935 loss 4.047564\n",
      "Epoch 936 loss 4.043762\n",
      "Epoch 937 loss 4.039972\n",
      "Epoch 938 loss 4.036197\n",
      "Epoch 939 loss 4.032434\n",
      "Epoch 940 loss 4.028686\n",
      "Epoch 941 loss 4.024947\n",
      "Epoch 942 loss 4.021224\n",
      "Epoch 943 loss 4.017509\n",
      "Epoch 944 loss 4.013810\n",
      "Epoch 945 loss 4.010122\n",
      "Epoch 946 loss 4.006450\n",
      "Epoch 947 loss 4.002785\n",
      "Epoch 948 loss 3.999137\n",
      "Epoch 949 loss 3.995498\n",
      "Epoch 950 loss 3.991874\n",
      "Epoch 951 loss 3.988262\n",
      "Epoch 952 loss 3.984659\n",
      "Epoch 953 loss 3.981071\n",
      "Epoch 954 loss 3.977497\n",
      "Epoch 955 loss 3.973931\n",
      "Epoch 956 loss 3.970381\n",
      "Epoch 957 loss 3.966840\n",
      "Epoch 958 loss 3.963312\n",
      "Epoch 959 loss 3.959797\n",
      "Epoch 960 loss 3.956295\n",
      "Epoch 961 loss 3.952801\n",
      "Epoch 962 loss 3.949323\n",
      "Epoch 963 loss 3.945855\n",
      "Epoch 964 loss 3.942398\n",
      "Epoch 965 loss 3.938953\n",
      "Epoch 966 loss 3.935521\n",
      "Epoch 967 loss 3.932096\n",
      "Epoch 968 loss 3.928688\n",
      "Epoch 969 loss 3.925292\n",
      "Epoch 970 loss 3.921906\n",
      "Epoch 971 loss 3.918528\n",
      "Epoch 972 loss 3.915166\n",
      "Epoch 973 loss 3.911815\n",
      "Epoch 974 loss 3.908474\n",
      "Epoch 975 loss 3.905144\n",
      "Epoch 976 loss 3.901824\n",
      "Epoch 977 loss 3.898517\n",
      "Epoch 978 loss 3.895222\n",
      "Epoch 979 loss 3.891935\n",
      "Epoch 980 loss 3.888664\n",
      "Epoch 981 loss 3.885400\n",
      "Epoch 982 loss 3.882150\n",
      "Epoch 983 loss 3.878911\n",
      "Epoch 984 loss 3.875680\n",
      "Epoch 985 loss 3.872463\n",
      "Epoch 986 loss 3.869256\n",
      "Epoch 987 loss 3.866060\n",
      "Epoch 988 loss 3.862873\n",
      "Epoch 989 loss 3.859699\n",
      "Epoch 990 loss 3.856535\n",
      "Epoch 991 loss 3.853381\n",
      "Epoch 992 loss 3.850237\n",
      "Epoch 993 loss 3.847109\n",
      "Epoch 994 loss 3.843984\n",
      "Epoch 995 loss 3.840876\n",
      "Epoch 996 loss 3.837775\n",
      "Epoch 997 loss 3.834686\n",
      "Epoch 998 loss 3.831606\n",
      "Epoch 999 loss 3.828538\n",
      "Epoch 1000 loss 3.825484\n",
      "Epoch 1001 loss 3.822433\n",
      "Epoch 1002 loss 3.819398\n",
      "Epoch 1003 loss 3.816369\n",
      "Epoch 1004 loss 3.813350\n",
      "Epoch 1005 loss 3.810344\n",
      "Epoch 1006 loss 3.807348\n",
      "Epoch 1007 loss 3.804360\n",
      "Epoch 1008 loss 3.801384\n",
      "Epoch 1009 loss 3.798421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1010 loss 3.795465\n",
      "Epoch 1011 loss 3.792518\n",
      "Epoch 1012 loss 3.789584\n",
      "Epoch 1013 loss 3.786658\n",
      "Epoch 1014 loss 3.783740\n",
      "Epoch 1015 loss 3.780832\n",
      "Epoch 1016 loss 3.777939\n",
      "Epoch 1017 loss 3.775053\n",
      "Epoch 1018 loss 3.772173\n",
      "Epoch 1019 loss 3.769310\n",
      "Epoch 1020 loss 3.766451\n",
      "Epoch 1021 loss 3.763602\n",
      "Epoch 1022 loss 3.760766\n",
      "Epoch 1023 loss 3.757936\n",
      "Epoch 1024 loss 3.755118\n",
      "Epoch 1025 loss 3.752309\n",
      "Epoch 1026 loss 3.749511\n",
      "Epoch 1027 loss 3.746722\n",
      "Epoch 1028 loss 3.743940\n",
      "Epoch 1029 loss 3.741169\n",
      "Epoch 1030 loss 3.738407\n",
      "Epoch 1031 loss 3.735656\n",
      "Epoch 1032 loss 3.732914\n",
      "Epoch 1033 loss 3.730181\n",
      "Epoch 1034 loss 3.727456\n",
      "Epoch 1035 loss 3.724741\n",
      "Epoch 1036 loss 3.722034\n",
      "Epoch 1037 loss 3.719337\n",
      "Epoch 1038 loss 3.716650\n",
      "Epoch 1039 loss 3.713972\n",
      "Epoch 1040 loss 3.711302\n",
      "Epoch 1041 loss 3.708643\n",
      "Epoch 1042 loss 3.705990\n",
      "Epoch 1043 loss 3.703351\n",
      "Epoch 1044 loss 3.700716\n",
      "Epoch 1045 loss 3.698092\n",
      "Epoch 1046 loss 3.695476\n",
      "Epoch 1047 loss 3.692869\n",
      "Epoch 1048 loss 3.690273\n",
      "Epoch 1049 loss 3.687683\n",
      "Epoch 1050 loss 3.685103\n",
      "Epoch 1051 loss 3.682532\n",
      "Epoch 1052 loss 3.679969\n",
      "Epoch 1053 loss 3.677417\n",
      "Epoch 1054 loss 3.674871\n",
      "Epoch 1055 loss 3.672334\n",
      "Epoch 1056 loss 3.669805\n",
      "Epoch 1057 loss 3.667287\n",
      "Epoch 1058 loss 3.664775\n",
      "Epoch 1059 loss 3.662273\n",
      "Epoch 1060 loss 3.659778\n",
      "Epoch 1061 loss 3.657295\n",
      "Epoch 1062 loss 3.654816\n",
      "Epoch 1063 loss 3.652350\n",
      "Epoch 1064 loss 3.649889\n",
      "Epoch 1065 loss 3.647437\n",
      "Epoch 1066 loss 3.644991\n",
      "Epoch 1067 loss 3.642559\n",
      "Epoch 1068 loss 3.640131\n",
      "Epoch 1069 loss 3.637711\n",
      "Epoch 1070 loss 3.635302\n",
      "Epoch 1071 loss 3.632902\n",
      "Epoch 1072 loss 3.630508\n",
      "Epoch 1073 loss 3.628119\n",
      "Epoch 1074 loss 3.625741\n",
      "Epoch 1075 loss 3.623374\n",
      "Epoch 1076 loss 3.621010\n",
      "Epoch 1077 loss 3.618659\n",
      "Epoch 1078 loss 3.616311\n",
      "Epoch 1079 loss 3.613973\n",
      "Epoch 1080 loss 3.611643\n",
      "Epoch 1081 loss 3.609322\n",
      "Epoch 1082 loss 3.607007\n",
      "Epoch 1083 loss 3.604701\n",
      "Epoch 1084 loss 3.602404\n",
      "Epoch 1085 loss 3.600114\n",
      "Epoch 1086 loss 3.597830\n",
      "Epoch 1087 loss 3.595553\n",
      "Epoch 1088 loss 3.593287\n",
      "Epoch 1089 loss 3.591030\n",
      "Epoch 1090 loss 3.588776\n",
      "Epoch 1091 loss 3.586534\n",
      "Epoch 1092 loss 3.584295\n",
      "Epoch 1093 loss 3.582067\n",
      "Epoch 1094 loss 3.579846\n",
      "Epoch 1095 loss 3.577631\n",
      "Epoch 1096 loss 3.575424\n",
      "Epoch 1097 loss 3.573225\n",
      "Epoch 1098 loss 3.571034\n",
      "Epoch 1099 loss 3.568848\n",
      "Epoch 1100 loss 3.566673\n",
      "Epoch 1101 loss 3.564506\n",
      "Epoch 1102 loss 3.562340\n",
      "Epoch 1103 loss 3.560185\n",
      "Epoch 1104 loss 3.558040\n",
      "Epoch 1105 loss 3.555901\n",
      "Epoch 1106 loss 3.553767\n",
      "Epoch 1107 loss 3.551641\n",
      "Epoch 1108 loss 3.549524\n",
      "Epoch 1109 loss 3.547411\n",
      "Epoch 1110 loss 3.545309\n",
      "Epoch 1111 loss 3.543211\n",
      "Epoch 1112 loss 3.541124\n",
      "Epoch 1113 loss 3.539041\n",
      "Epoch 1114 loss 3.536966\n",
      "Epoch 1115 loss 3.534897\n",
      "Epoch 1116 loss 3.532835\n",
      "Epoch 1117 loss 3.530781\n",
      "Epoch 1118 loss 3.528734\n",
      "Epoch 1119 loss 3.526694\n",
      "Epoch 1120 loss 3.524662\n",
      "Epoch 1121 loss 3.522633\n",
      "Epoch 1122 loss 3.520614\n",
      "Epoch 1123 loss 3.518601\n",
      "Epoch 1124 loss 3.516594\n",
      "Epoch 1125 loss 3.514594\n",
      "Epoch 1126 loss 3.512602\n",
      "Epoch 1127 loss 3.510619\n",
      "Epoch 1128 loss 3.508637\n",
      "Epoch 1129 loss 3.506665\n",
      "Epoch 1130 loss 3.504700\n",
      "Epoch 1131 loss 3.502740\n",
      "Epoch 1132 loss 3.500789\n",
      "Epoch 1133 loss 3.498843\n",
      "Epoch 1134 loss 3.496905\n",
      "Epoch 1135 loss 3.494972\n",
      "Epoch 1136 loss 3.493046\n",
      "Epoch 1137 loss 3.491127\n",
      "Epoch 1138 loss 3.489213\n",
      "Epoch 1139 loss 3.487308\n",
      "Epoch 1140 loss 3.485410\n",
      "Epoch 1141 loss 3.483515\n",
      "Epoch 1142 loss 3.481627\n",
      "Epoch 1143 loss 3.479746\n",
      "Epoch 1144 loss 3.477872\n",
      "Epoch 1145 loss 3.476005\n",
      "Epoch 1146 loss 3.474143\n",
      "Epoch 1147 loss 3.472288\n",
      "Epoch 1148 loss 3.470441\n",
      "Epoch 1149 loss 3.468597\n",
      "Epoch 1150 loss 3.466762\n",
      "Epoch 1151 loss 3.464930\n",
      "Epoch 1152 loss 3.463105\n",
      "Epoch 1153 loss 3.461289\n",
      "Epoch 1154 loss 3.459477\n",
      "Epoch 1155 loss 3.457672\n",
      "Epoch 1156 loss 3.455873\n",
      "Epoch 1157 loss 3.454080\n",
      "Epoch 1158 loss 3.452293\n",
      "Epoch 1159 loss 3.450512\n",
      "Epoch 1160 loss 3.448736\n",
      "Epoch 1161 loss 3.446968\n",
      "Epoch 1162 loss 3.445203\n",
      "Epoch 1163 loss 3.443449\n",
      "Epoch 1164 loss 3.441696\n",
      "Epoch 1165 loss 3.439952\n",
      "Epoch 1166 loss 3.438210\n",
      "Epoch 1167 loss 3.436478\n",
      "Epoch 1168 loss 3.434753\n",
      "Epoch 1169 loss 3.433029\n",
      "Epoch 1170 loss 3.431314\n",
      "Epoch 1171 loss 3.429608\n",
      "Epoch 1172 loss 3.427903\n",
      "Epoch 1173 loss 3.426204\n",
      "Epoch 1174 loss 3.424509\n",
      "Epoch 1175 loss 3.422824\n",
      "Epoch 1176 loss 3.421144\n",
      "Epoch 1177 loss 3.419468\n",
      "Epoch 1178 loss 3.417798\n",
      "Epoch 1179 loss 3.416134\n",
      "Epoch 1180 loss 3.414477\n",
      "Epoch 1181 loss 3.412824\n",
      "Epoch 1182 loss 3.411176\n",
      "Epoch 1183 loss 3.409534\n",
      "Epoch 1184 loss 3.407899\n",
      "Epoch 1185 loss 3.406272\n",
      "Epoch 1186 loss 3.404645\n",
      "Epoch 1187 loss 3.403024\n",
      "Epoch 1188 loss 3.401413\n",
      "Epoch 1189 loss 3.399802\n",
      "Epoch 1190 loss 3.398200\n",
      "Epoch 1191 loss 3.396602\n",
      "Epoch 1192 loss 3.395011\n",
      "Epoch 1193 loss 3.393425\n",
      "Epoch 1194 loss 3.391845\n",
      "Epoch 1195 loss 3.390267\n",
      "Epoch 1196 loss 3.388697\n",
      "Epoch 1197 loss 3.387132\n",
      "Epoch 1198 loss 3.385571\n",
      "Epoch 1199 loss 3.384017\n",
      "Epoch 1200 loss 3.382467\n",
      "Epoch 1201 loss 3.380925\n",
      "Epoch 1202 loss 3.379386\n",
      "Epoch 1203 loss 3.377852\n",
      "Epoch 1204 loss 3.376323\n",
      "Epoch 1205 loss 3.374800\n",
      "Epoch 1206 loss 3.373284\n",
      "Epoch 1207 loss 3.371769\n",
      "Epoch 1208 loss 3.370261\n",
      "Epoch 1209 loss 3.368759\n",
      "Epoch 1210 loss 3.367262\n",
      "Epoch 1211 loss 3.365771\n",
      "Epoch 1212 loss 3.364282\n",
      "Epoch 1213 loss 3.362800\n",
      "Epoch 1214 loss 3.361325\n",
      "Epoch 1215 loss 3.359851\n",
      "Epoch 1216 loss 3.358383\n",
      "Epoch 1217 loss 3.356921\n",
      "Epoch 1218 loss 3.355464\n",
      "Epoch 1219 loss 3.354013\n",
      "Epoch 1220 loss 3.352564\n",
      "Epoch 1221 loss 3.351122\n",
      "Epoch 1222 loss 3.349685\n",
      "Epoch 1223 loss 3.348251\n",
      "Epoch 1224 loss 3.346825\n",
      "Epoch 1225 loss 3.345403\n",
      "Epoch 1226 loss 3.343982\n",
      "Epoch 1227 loss 3.342571\n",
      "Epoch 1228 loss 3.341161\n",
      "Epoch 1229 loss 3.339758\n",
      "Epoch 1230 loss 3.338359\n",
      "Epoch 1231 loss 3.336965\n",
      "Epoch 1232 loss 3.335577\n",
      "Epoch 1233 loss 3.334191\n",
      "Epoch 1234 loss 3.332811\n",
      "Epoch 1235 loss 3.331435\n",
      "Epoch 1236 loss 3.330065\n",
      "Epoch 1237 loss 3.328699\n",
      "Epoch 1238 loss 3.327338\n",
      "Epoch 1239 loss 3.325980\n",
      "Epoch 1240 loss 3.324628\n",
      "Epoch 1241 loss 3.323280\n",
      "Epoch 1242 loss 3.321935\n",
      "Epoch 1243 loss 3.320599\n",
      "Epoch 1244 loss 3.319264\n",
      "Epoch 1245 loss 3.317935\n",
      "Epoch 1246 loss 3.316610\n",
      "Epoch 1247 loss 3.315289\n",
      "Epoch 1248 loss 3.313974\n",
      "Epoch 1249 loss 3.312663\n",
      "Epoch 1250 loss 3.311353\n",
      "Epoch 1251 loss 3.310053\n",
      "Epoch 1252 loss 3.308756\n",
      "Epoch 1253 loss 3.307462\n",
      "Epoch 1254 loss 3.306170\n",
      "Epoch 1255 loss 3.304887\n",
      "Epoch 1256 loss 3.303605\n",
      "Epoch 1257 loss 3.302329\n",
      "Epoch 1258 loss 3.301058\n",
      "Epoch 1259 loss 3.299791\n",
      "Epoch 1260 loss 3.298527\n",
      "Epoch 1261 loss 3.297266\n",
      "Epoch 1262 loss 3.296014\n",
      "Epoch 1263 loss 3.294762\n",
      "Epoch 1264 loss 3.293517\n",
      "Epoch 1265 loss 3.292275\n",
      "Epoch 1266 loss 3.291036\n",
      "Epoch 1267 loss 3.289804\n",
      "Epoch 1268 loss 3.288573\n",
      "Epoch 1269 loss 3.287347\n",
      "Epoch 1270 loss 3.286129\n",
      "Epoch 1271 loss 3.284911\n",
      "Epoch 1272 loss 3.283698\n",
      "Epoch 1273 loss 3.282488\n",
      "Epoch 1274 loss 3.281284\n",
      "Epoch 1275 loss 3.280086\n",
      "Epoch 1276 loss 3.278888\n",
      "Epoch 1277 loss 3.277696\n",
      "Epoch 1278 loss 3.276506\n",
      "Epoch 1279 loss 3.275322\n",
      "Epoch 1280 loss 3.274142\n",
      "Epoch 1281 loss 3.272967\n",
      "Epoch 1282 loss 3.271793\n",
      "Epoch 1283 loss 3.270625\n",
      "Epoch 1284 loss 3.269460\n",
      "Epoch 1285 loss 3.268301\n",
      "Epoch 1286 loss 3.267143\n",
      "Epoch 1287 loss 3.265991\n",
      "Epoch 1288 loss 3.264842\n",
      "Epoch 1289 loss 3.263700\n",
      "Epoch 1290 loss 3.262556\n",
      "Epoch 1291 loss 3.261421\n",
      "Epoch 1292 loss 3.260288\n",
      "Epoch 1293 loss 3.259161\n",
      "Epoch 1294 loss 3.258033\n",
      "Epoch 1295 loss 3.256912\n",
      "Epoch 1296 loss 3.255795\n",
      "Epoch 1297 loss 3.254681\n",
      "Epoch 1298 loss 3.253569\n",
      "Epoch 1299 loss 3.252462\n",
      "Epoch 1300 loss 3.251362\n",
      "Epoch 1301 loss 3.250264\n",
      "Epoch 1302 loss 3.249168\n",
      "Epoch 1303 loss 3.248077\n",
      "Epoch 1304 loss 3.246989\n",
      "Epoch 1305 loss 3.245904\n",
      "Epoch 1306 loss 3.244824\n",
      "Epoch 1307 loss 3.243747\n",
      "Epoch 1308 loss 3.242674\n",
      "Epoch 1309 loss 3.241606\n",
      "Epoch 1310 loss 3.240538\n",
      "Epoch 1311 loss 3.239475\n",
      "Epoch 1312 loss 3.238420\n",
      "Epoch 1313 loss 3.237364\n",
      "Epoch 1314 loss 3.236314\n",
      "Epoch 1315 loss 3.235264\n",
      "Epoch 1316 loss 3.234218\n",
      "Epoch 1317 loss 3.233179\n",
      "Epoch 1318 loss 3.232143\n",
      "Epoch 1319 loss 3.231108\n",
      "Epoch 1320 loss 3.230078\n",
      "Epoch 1321 loss 3.229051\n",
      "Epoch 1322 loss 3.228027\n",
      "Epoch 1323 loss 3.227010\n",
      "Epoch 1324 loss 3.225993\n",
      "Epoch 1325 loss 3.224979\n",
      "Epoch 1326 loss 3.223971\n",
      "Epoch 1327 loss 3.222965\n",
      "Epoch 1328 loss 3.221961\n",
      "Epoch 1329 loss 3.220962\n",
      "Epoch 1330 loss 3.219967\n",
      "Epoch 1331 loss 3.218975\n",
      "Epoch 1332 loss 3.217986\n",
      "Epoch 1333 loss 3.217000\n",
      "Epoch 1334 loss 3.216017\n",
      "Epoch 1335 loss 3.215039\n",
      "Epoch 1336 loss 3.214062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1337 loss 3.213092\n",
      "Epoch 1338 loss 3.212122\n",
      "Epoch 1339 loss 3.211157\n",
      "Epoch 1340 loss 3.210193\n",
      "Epoch 1341 loss 3.209235\n",
      "Epoch 1342 loss 3.208279\n",
      "Epoch 1343 loss 3.207326\n",
      "Epoch 1344 loss 3.206376\n",
      "Epoch 1345 loss 3.205430\n",
      "Epoch 1346 loss 3.204488\n",
      "Epoch 1347 loss 3.203547\n",
      "Epoch 1348 loss 3.202611\n",
      "Epoch 1349 loss 3.201678\n",
      "Epoch 1350 loss 3.200747\n",
      "Epoch 1351 loss 3.199820\n",
      "Epoch 1352 loss 3.198897\n",
      "Epoch 1353 loss 3.197976\n",
      "Epoch 1354 loss 3.197060\n",
      "Epoch 1355 loss 3.196144\n",
      "Epoch 1356 loss 3.195231\n",
      "Epoch 1357 loss 3.194324\n",
      "Epoch 1358 loss 3.193419\n",
      "Epoch 1359 loss 3.192517\n",
      "Epoch 1360 loss 3.191616\n",
      "Epoch 1361 loss 3.190720\n",
      "Epoch 1362 loss 3.189829\n",
      "Epoch 1363 loss 3.188937\n",
      "Epoch 1364 loss 3.188051\n",
      "Epoch 1365 loss 3.187166\n",
      "Epoch 1366 loss 3.186288\n",
      "Epoch 1367 loss 3.185409\n",
      "Epoch 1368 loss 3.184535\n",
      "Epoch 1369 loss 3.183662\n",
      "Epoch 1370 loss 3.182791\n",
      "Epoch 1371 loss 3.181925\n",
      "Epoch 1372 loss 3.181063\n",
      "Epoch 1373 loss 3.180201\n",
      "Epoch 1374 loss 3.179347\n",
      "Epoch 1375 loss 3.178490\n",
      "Epoch 1376 loss 3.177638\n",
      "Epoch 1377 loss 3.176789\n",
      "Epoch 1378 loss 3.175945\n",
      "Epoch 1379 loss 3.175101\n",
      "Epoch 1380 loss 3.174262\n",
      "Epoch 1381 loss 3.173424\n",
      "Epoch 1382 loss 3.172590\n",
      "Epoch 1383 loss 3.171759\n",
      "Epoch 1384 loss 3.170929\n",
      "Epoch 1385 loss 3.170103\n",
      "Epoch 1386 loss 3.169280\n",
      "Epoch 1387 loss 3.168462\n",
      "Epoch 1388 loss 3.167644\n",
      "Epoch 1389 loss 3.166827\n",
      "Epoch 1390 loss 3.166017\n",
      "Epoch 1391 loss 3.165206\n",
      "Epoch 1392 loss 3.164401\n",
      "Epoch 1393 loss 3.163594\n",
      "Epoch 1394 loss 3.162795\n",
      "Epoch 1395 loss 3.161996\n",
      "Epoch 1396 loss 3.161201\n",
      "Epoch 1397 loss 3.160410\n",
      "Epoch 1398 loss 3.159618\n",
      "Epoch 1399 loss 3.158831\n",
      "Epoch 1400 loss 3.158046\n",
      "Epoch 1401 loss 3.157263\n",
      "Epoch 1402 loss 3.156484\n",
      "Epoch 1403 loss 3.155708\n",
      "Epoch 1404 loss 3.154933\n",
      "Epoch 1405 loss 3.154162\n",
      "Epoch 1406 loss 3.153393\n",
      "Epoch 1407 loss 3.152627\n",
      "Epoch 1408 loss 3.151864\n",
      "Epoch 1409 loss 3.151101\n",
      "Epoch 1410 loss 3.150343\n",
      "Epoch 1411 loss 3.149587\n",
      "Epoch 1412 loss 3.148833\n",
      "Epoch 1413 loss 3.148082\n",
      "Epoch 1414 loss 3.147335\n",
      "Epoch 1415 loss 3.146588\n",
      "Epoch 1416 loss 3.145845\n",
      "Epoch 1417 loss 3.145105\n",
      "Epoch 1418 loss 3.144367\n",
      "Epoch 1419 loss 3.143630\n",
      "Epoch 1420 loss 3.142899\n",
      "Epoch 1421 loss 3.142166\n",
      "Epoch 1422 loss 3.141439\n",
      "Epoch 1423 loss 3.140712\n",
      "Epoch 1424 loss 3.139989\n",
      "Epoch 1425 loss 3.139271\n",
      "Epoch 1426 loss 3.138551\n",
      "Epoch 1427 loss 3.137834\n",
      "Epoch 1428 loss 3.137121\n",
      "Epoch 1429 loss 3.136410\n",
      "Epoch 1430 loss 3.135700\n",
      "Epoch 1431 loss 3.134995\n",
      "Epoch 1432 loss 3.134291\n",
      "Epoch 1433 loss 3.133590\n",
      "Epoch 1434 loss 3.132889\n",
      "Epoch 1435 loss 3.132194\n",
      "Epoch 1436 loss 3.131500\n",
      "Epoch 1437 loss 3.130810\n",
      "Epoch 1438 loss 3.130119\n",
      "Epoch 1439 loss 3.129432\n",
      "Epoch 1440 loss 3.128746\n",
      "Epoch 1441 loss 3.128064\n",
      "Epoch 1442 loss 3.127381\n",
      "Epoch 1443 loss 3.126705\n",
      "Epoch 1444 loss 3.126031\n",
      "Epoch 1445 loss 3.125356\n",
      "Epoch 1446 loss 3.124683\n",
      "Epoch 1447 loss 3.124017\n",
      "Epoch 1448 loss 3.123348\n",
      "Epoch 1449 loss 3.122685\n",
      "Epoch 1450 loss 3.122022\n",
      "Epoch 1451 loss 3.121362\n",
      "Epoch 1452 loss 3.120706\n",
      "Epoch 1453 loss 3.120049\n",
      "Epoch 1454 loss 3.119396\n",
      "Epoch 1455 loss 3.118746\n",
      "Epoch 1456 loss 3.118098\n",
      "Epoch 1457 loss 3.117452\n",
      "Epoch 1458 loss 3.116805\n",
      "Epoch 1459 loss 3.116164\n",
      "Epoch 1460 loss 3.115525\n",
      "Epoch 1461 loss 3.114886\n",
      "Epoch 1462 loss 3.114250\n",
      "Epoch 1463 loss 3.113617\n",
      "Epoch 1464 loss 3.112984\n",
      "Epoch 1465 loss 3.112358\n",
      "Epoch 1466 loss 3.111731\n",
      "Epoch 1467 loss 3.111103\n",
      "Epoch 1468 loss 3.110484\n",
      "Epoch 1469 loss 3.109860\n",
      "Epoch 1470 loss 3.109242\n",
      "Epoch 1471 loss 3.108627\n",
      "Epoch 1472 loss 3.108011\n",
      "Epoch 1473 loss 3.107401\n",
      "Epoch 1474 loss 3.106791\n",
      "Epoch 1475 loss 3.106180\n",
      "Epoch 1476 loss 3.105575\n",
      "Epoch 1477 loss 3.104972\n",
      "Epoch 1478 loss 3.104370\n",
      "Epoch 1479 loss 3.103770\n",
      "Epoch 1480 loss 3.103172\n",
      "Epoch 1481 loss 3.102576\n",
      "Epoch 1482 loss 3.101982\n",
      "Epoch 1483 loss 3.101390\n",
      "Epoch 1484 loss 3.100802\n",
      "Epoch 1485 loss 3.100213\n",
      "Epoch 1486 loss 3.099627\n",
      "Epoch 1487 loss 3.099044\n",
      "Epoch 1488 loss 3.098462\n",
      "Epoch 1489 loss 3.097883\n",
      "Epoch 1490 loss 3.097302\n",
      "Epoch 1491 loss 3.096727\n",
      "Epoch 1492 loss 3.096153\n",
      "Epoch 1493 loss 3.095583\n",
      "Epoch 1494 loss 3.095011\n",
      "Epoch 1495 loss 3.094444\n",
      "Epoch 1496 loss 3.093876\n",
      "Epoch 1497 loss 3.093314\n",
      "Epoch 1498 loss 3.092751\n",
      "Epoch 1499 loss 3.092191\n",
      "Epoch 1500 loss 3.091631\n",
      "Epoch 1501 loss 3.091074\n",
      "Epoch 1502 loss 3.090520\n",
      "Epoch 1503 loss 3.089969\n",
      "Epoch 1504 loss 3.089417\n",
      "Epoch 1505 loss 3.088867\n",
      "Epoch 1506 loss 3.088320\n",
      "Epoch 1507 loss 3.087775\n",
      "Epoch 1508 loss 3.087232\n",
      "Epoch 1509 loss 3.086690\n",
      "Epoch 1510 loss 3.086150\n",
      "Epoch 1511 loss 3.085612\n",
      "Epoch 1512 loss 3.085075\n",
      "Epoch 1513 loss 3.084542\n",
      "Epoch 1514 loss 3.084009\n",
      "Epoch 1515 loss 3.083478\n",
      "Epoch 1516 loss 3.082948\n",
      "Epoch 1517 loss 3.082422\n",
      "Epoch 1518 loss 3.081897\n",
      "Epoch 1519 loss 3.081373\n",
      "Epoch 1520 loss 3.080850\n",
      "Epoch 1521 loss 3.080331\n",
      "Epoch 1522 loss 3.079811\n",
      "Epoch 1523 loss 3.079297\n",
      "Epoch 1524 loss 3.078781\n",
      "Epoch 1525 loss 3.078268\n",
      "Epoch 1526 loss 3.077757\n",
      "Epoch 1527 loss 3.077247\n",
      "Epoch 1528 loss 3.076739\n",
      "Epoch 1529 loss 3.076232\n",
      "Epoch 1530 loss 3.075729\n",
      "Epoch 1531 loss 3.075225\n",
      "Epoch 1532 loss 3.074724\n",
      "Epoch 1533 loss 3.074227\n",
      "Epoch 1534 loss 3.073726\n",
      "Epoch 1535 loss 3.073232\n",
      "Epoch 1536 loss 3.072739\n",
      "Epoch 1537 loss 3.072245\n",
      "Epoch 1538 loss 3.071753\n",
      "Epoch 1539 loss 3.071265\n",
      "Epoch 1540 loss 3.070778\n",
      "Epoch 1541 loss 3.070293\n",
      "Epoch 1542 loss 3.069808\n",
      "Epoch 1543 loss 3.069326\n",
      "Epoch 1544 loss 3.068845\n",
      "Epoch 1545 loss 3.068366\n",
      "Epoch 1546 loss 3.067887\n",
      "Epoch 1547 loss 3.067412\n",
      "Epoch 1548 loss 3.066937\n",
      "Epoch 1549 loss 3.066464\n",
      "Epoch 1550 loss 3.065993\n",
      "Epoch 1551 loss 3.065524\n",
      "Epoch 1552 loss 3.065055\n",
      "Epoch 1553 loss 3.064588\n",
      "Epoch 1554 loss 3.064123\n",
      "Epoch 1555 loss 3.063660\n",
      "Epoch 1556 loss 3.063199\n",
      "Epoch 1557 loss 3.062738\n",
      "Epoch 1558 loss 3.062280\n",
      "Epoch 1559 loss 3.061822\n",
      "Epoch 1560 loss 3.061368\n",
      "Epoch 1561 loss 3.060913\n",
      "Epoch 1562 loss 3.060461\n",
      "Epoch 1563 loss 3.060011\n",
      "Epoch 1564 loss 3.059561\n",
      "Epoch 1565 loss 3.059114\n",
      "Epoch 1566 loss 3.058668\n",
      "Epoch 1567 loss 3.058221\n",
      "Epoch 1568 loss 3.057780\n",
      "Epoch 1569 loss 3.057338\n",
      "Epoch 1570 loss 3.056898\n",
      "Epoch 1571 loss 3.056458\n",
      "Epoch 1572 loss 3.056019\n",
      "Epoch 1573 loss 3.055585\n",
      "Epoch 1574 loss 3.055151\n",
      "Epoch 1575 loss 3.054717\n",
      "Epoch 1576 loss 3.054286\n",
      "Epoch 1577 loss 3.053857\n",
      "Epoch 1578 loss 3.053428\n",
      "Epoch 1579 loss 3.053001\n",
      "Epoch 1580 loss 3.052576\n",
      "Epoch 1581 loss 3.052152\n",
      "Epoch 1582 loss 3.051730\n",
      "Epoch 1583 loss 3.051307\n",
      "Epoch 1584 loss 3.050888\n",
      "Epoch 1585 loss 3.050471\n",
      "Epoch 1586 loss 3.050052\n",
      "Epoch 1587 loss 3.049639\n",
      "Epoch 1588 loss 3.049223\n",
      "Epoch 1589 loss 3.048811\n",
      "Epoch 1590 loss 3.048398\n",
      "Epoch 1591 loss 3.047991\n",
      "Epoch 1592 loss 3.047581\n",
      "Epoch 1593 loss 3.047173\n",
      "Epoch 1594 loss 3.046768\n",
      "Epoch 1595 loss 3.046363\n",
      "Epoch 1596 loss 3.045960\n",
      "Epoch 1597 loss 3.045559\n",
      "Epoch 1598 loss 3.045160\n",
      "Epoch 1599 loss 3.044759\n",
      "Epoch 1600 loss 3.044361\n",
      "Epoch 1601 loss 3.043966\n",
      "Epoch 1602 loss 3.043571\n",
      "Epoch 1603 loss 3.043176\n",
      "Epoch 1604 loss 3.042785\n",
      "Epoch 1605 loss 3.042395\n",
      "Epoch 1606 loss 3.042004\n",
      "Epoch 1607 loss 3.041615\n",
      "Epoch 1608 loss 3.041230\n",
      "Epoch 1609 loss 3.040844\n",
      "Epoch 1610 loss 3.040460\n",
      "Epoch 1611 loss 3.040077\n",
      "Epoch 1612 loss 3.039695\n",
      "Epoch 1613 loss 3.039314\n",
      "Epoch 1614 loss 3.038934\n",
      "Epoch 1615 loss 3.038557\n",
      "Epoch 1616 loss 3.038182\n",
      "Epoch 1617 loss 3.037806\n",
      "Epoch 1618 loss 3.037431\n",
      "Epoch 1619 loss 3.037059\n",
      "Epoch 1620 loss 3.036689\n",
      "Epoch 1621 loss 3.036319\n",
      "Epoch 1622 loss 3.035949\n",
      "Epoch 1623 loss 3.035583\n",
      "Epoch 1624 loss 3.035215\n",
      "Epoch 1625 loss 3.034849\n",
      "Epoch 1626 loss 3.034485\n",
      "Epoch 1627 loss 3.034122\n",
      "Epoch 1628 loss 3.033762\n",
      "Epoch 1629 loss 3.033402\n",
      "Epoch 1630 loss 3.033042\n",
      "Epoch 1631 loss 3.032685\n",
      "Epoch 1632 loss 3.032329\n",
      "Epoch 1633 loss 3.031973\n",
      "Epoch 1634 loss 3.031619\n",
      "Epoch 1635 loss 3.031265\n",
      "Epoch 1636 loss 3.030913\n",
      "Epoch 1637 loss 3.030564\n",
      "Epoch 1638 loss 3.030215\n",
      "Epoch 1639 loss 3.029866\n",
      "Epoch 1640 loss 3.029518\n",
      "Epoch 1641 loss 3.029172\n",
      "Epoch 1642 loss 3.028829\n",
      "Epoch 1643 loss 3.028486\n",
      "Epoch 1644 loss 3.028142\n",
      "Epoch 1645 loss 3.027802\n",
      "Epoch 1646 loss 3.027462\n",
      "Epoch 1647 loss 3.027122\n",
      "Epoch 1648 loss 3.026784\n",
      "Epoch 1649 loss 3.026447\n",
      "Epoch 1650 loss 3.026111\n",
      "Epoch 1651 loss 3.025780\n",
      "Epoch 1652 loss 3.025446\n",
      "Epoch 1653 loss 3.025114\n",
      "Epoch 1654 loss 3.024782\n",
      "Epoch 1655 loss 3.024452\n",
      "Epoch 1656 loss 3.024125\n",
      "Epoch 1657 loss 3.023797\n",
      "Epoch 1658 loss 3.023471\n",
      "Epoch 1659 loss 3.023146\n",
      "Epoch 1660 loss 3.022821\n",
      "Epoch 1661 loss 3.022498\n",
      "Epoch 1662 loss 3.022177\n",
      "Epoch 1663 loss 3.021855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1664 loss 3.021534\n",
      "Epoch 1665 loss 3.021217\n",
      "Epoch 1666 loss 3.020898\n",
      "Epoch 1667 loss 3.020582\n",
      "Epoch 1668 loss 3.020266\n",
      "Epoch 1669 loss 3.019952\n",
      "Epoch 1670 loss 3.019639\n",
      "Epoch 1671 loss 3.019325\n",
      "Epoch 1672 loss 3.019016\n",
      "Epoch 1673 loss 3.018706\n",
      "Epoch 1674 loss 3.018395\n",
      "Epoch 1675 loss 3.018089\n",
      "Epoch 1676 loss 3.017780\n",
      "Epoch 1677 loss 3.017475\n",
      "Epoch 1678 loss 3.017170\n",
      "Epoch 1679 loss 3.016867\n",
      "Epoch 1680 loss 3.016564\n",
      "Epoch 1681 loss 3.016262\n",
      "Epoch 1682 loss 3.015959\n",
      "Epoch 1683 loss 3.015661\n",
      "Epoch 1684 loss 3.015362\n",
      "Epoch 1685 loss 3.015064\n",
      "Epoch 1686 loss 3.014768\n",
      "Epoch 1687 loss 3.014472\n",
      "Epoch 1688 loss 3.014179\n",
      "Epoch 1689 loss 3.013884\n",
      "Epoch 1690 loss 3.013591\n",
      "Epoch 1691 loss 3.013299\n",
      "Epoch 1692 loss 3.013008\n",
      "Epoch 1693 loss 3.012719\n",
      "Epoch 1694 loss 3.012431\n",
      "Epoch 1695 loss 3.012141\n",
      "Epoch 1696 loss 3.011855\n",
      "Epoch 1697 loss 3.011570\n",
      "Epoch 1698 loss 3.011284\n",
      "Epoch 1699 loss 3.011001\n",
      "Epoch 1700 loss 3.010718\n",
      "Epoch 1701 loss 3.010436\n",
      "Epoch 1702 loss 3.010156\n",
      "Epoch 1703 loss 3.009876\n",
      "Epoch 1704 loss 3.009595\n",
      "Epoch 1705 loss 3.009319\n",
      "Epoch 1706 loss 3.009040\n",
      "Epoch 1707 loss 3.008763\n",
      "Epoch 1708 loss 3.008487\n",
      "Epoch 1709 loss 3.008214\n",
      "Epoch 1710 loss 3.007941\n",
      "Epoch 1711 loss 3.007668\n",
      "Epoch 1712 loss 3.007396\n",
      "Epoch 1713 loss 3.007126\n",
      "Epoch 1714 loss 3.006856\n",
      "Epoch 1715 loss 3.006586\n",
      "Epoch 1716 loss 3.006318\n",
      "Epoch 1717 loss 3.006052\n",
      "Epoch 1718 loss 3.005785\n",
      "Epoch 1719 loss 3.005520\n",
      "Epoch 1720 loss 3.005256\n",
      "Epoch 1721 loss 3.004993\n",
      "Epoch 1722 loss 3.004729\n",
      "Epoch 1723 loss 3.004467\n",
      "Epoch 1724 loss 3.004207\n",
      "Epoch 1725 loss 3.003947\n",
      "Epoch 1726 loss 3.003690\n",
      "Epoch 1727 loss 3.003430\n",
      "Epoch 1728 loss 3.003174\n",
      "Epoch 1729 loss 3.002918\n",
      "Epoch 1730 loss 3.002661\n",
      "Epoch 1731 loss 3.002406\n",
      "Epoch 1732 loss 3.002152\n",
      "Epoch 1733 loss 3.001901\n",
      "Epoch 1734 loss 3.001649\n",
      "Epoch 1735 loss 3.001395\n",
      "Epoch 1736 loss 3.001145\n",
      "Epoch 1737 loss 3.000898\n",
      "Epoch 1738 loss 3.000648\n",
      "Epoch 1739 loss 3.000400\n",
      "Epoch 1740 loss 3.000154\n",
      "Epoch 1741 loss 2.999907\n",
      "Epoch 1742 loss 2.999662\n",
      "Epoch 1743 loss 2.999417\n",
      "Epoch 1744 loss 2.999174\n",
      "Epoch 1745 loss 2.998930\n",
      "Epoch 1746 loss 2.998688\n",
      "Epoch 1747 loss 2.998448\n",
      "Epoch 1748 loss 2.998208\n",
      "Epoch 1749 loss 2.997968\n",
      "Epoch 1750 loss 2.997730\n",
      "Epoch 1751 loss 2.997490\n",
      "Epoch 1752 loss 2.997254\n",
      "Epoch 1753 loss 2.997018\n",
      "Epoch 1754 loss 2.996783\n",
      "Epoch 1755 loss 2.996548\n",
      "Epoch 1756 loss 2.996313\n",
      "Epoch 1757 loss 2.996081\n",
      "Epoch 1758 loss 2.995847\n",
      "Epoch 1759 loss 2.995615\n",
      "Epoch 1760 loss 2.995387\n",
      "Epoch 1761 loss 2.995156\n",
      "Epoch 1762 loss 2.994929\n",
      "Epoch 1763 loss 2.994699\n",
      "Epoch 1764 loss 2.994472\n",
      "Epoch 1765 loss 2.994245\n",
      "Epoch 1766 loss 2.994018\n",
      "Epoch 1767 loss 2.993793\n",
      "Epoch 1768 loss 2.993569\n",
      "Epoch 1769 loss 2.993344\n",
      "Epoch 1770 loss 2.993122\n",
      "Epoch 1771 loss 2.992900\n",
      "Epoch 1772 loss 2.992678\n",
      "Epoch 1773 loss 2.992457\n",
      "Epoch 1774 loss 2.992238\n",
      "Epoch 1775 loss 2.992017\n",
      "Epoch 1776 loss 2.991798\n",
      "Epoch 1777 loss 2.991583\n",
      "Epoch 1778 loss 2.991366\n",
      "Epoch 1779 loss 2.991146\n",
      "Epoch 1780 loss 2.990932\n",
      "Epoch 1781 loss 2.990719\n",
      "Epoch 1782 loss 2.990503\n",
      "Epoch 1783 loss 2.990289\n",
      "Epoch 1784 loss 2.990078\n",
      "Epoch 1785 loss 2.989866\n",
      "Epoch 1786 loss 2.989654\n",
      "Epoch 1787 loss 2.989443\n",
      "Epoch 1788 loss 2.989233\n",
      "Epoch 1789 loss 2.989025\n",
      "Epoch 1790 loss 2.988817\n",
      "Epoch 1791 loss 2.988609\n",
      "Epoch 1792 loss 2.988401\n",
      "Epoch 1793 loss 2.988195\n",
      "Epoch 1794 loss 2.987989\n",
      "Epoch 1795 loss 2.987784\n",
      "Epoch 1796 loss 2.987581\n",
      "Epoch 1797 loss 2.987377\n",
      "Epoch 1798 loss 2.987174\n",
      "Epoch 1799 loss 2.986974\n",
      "Epoch 1800 loss 2.986771\n",
      "Epoch 1801 loss 2.986570\n",
      "Epoch 1802 loss 2.986371\n",
      "Epoch 1803 loss 2.986171\n",
      "Epoch 1804 loss 2.985972\n",
      "Epoch 1805 loss 2.985774\n",
      "Epoch 1806 loss 2.985578\n",
      "Epoch 1807 loss 2.985381\n",
      "Epoch 1808 loss 2.985184\n",
      "Epoch 1809 loss 2.984989\n",
      "Epoch 1810 loss 2.984793\n",
      "Epoch 1811 loss 2.984601\n",
      "Epoch 1812 loss 2.984406\n",
      "Epoch 1813 loss 2.984215\n",
      "Epoch 1814 loss 2.984022\n",
      "Epoch 1815 loss 2.983830\n",
      "Epoch 1816 loss 2.983639\n",
      "Epoch 1817 loss 2.983449\n",
      "Epoch 1818 loss 2.983260\n",
      "Epoch 1819 loss 2.983073\n",
      "Epoch 1820 loss 2.982884\n",
      "Epoch 1821 loss 2.982697\n",
      "Epoch 1822 loss 2.982510\n",
      "Epoch 1823 loss 2.982322\n",
      "Epoch 1824 loss 2.982137\n",
      "Epoch 1825 loss 2.981953\n",
      "Epoch 1826 loss 2.981769\n",
      "Epoch 1827 loss 2.981586\n",
      "Epoch 1828 loss 2.981402\n",
      "Epoch 1829 loss 2.981219\n",
      "Epoch 1830 loss 2.981037\n",
      "Epoch 1831 loss 2.980856\n",
      "Epoch 1832 loss 2.980676\n",
      "Epoch 1833 loss 2.980495\n",
      "Epoch 1834 loss 2.980316\n",
      "Epoch 1835 loss 2.980137\n",
      "Epoch 1836 loss 2.979958\n",
      "Epoch 1837 loss 2.979782\n",
      "Epoch 1838 loss 2.979604\n",
      "Epoch 1839 loss 2.979428\n",
      "Epoch 1840 loss 2.979253\n",
      "Epoch 1841 loss 2.979078\n",
      "Epoch 1842 loss 2.978902\n",
      "Epoch 1843 loss 2.978729\n",
      "Epoch 1844 loss 2.978555\n",
      "Epoch 1845 loss 2.978382\n",
      "Epoch 1846 loss 2.978211\n",
      "Epoch 1847 loss 2.978039\n",
      "Epoch 1848 loss 2.977867\n",
      "Epoch 1849 loss 2.977696\n",
      "Epoch 1850 loss 2.977527\n",
      "Epoch 1851 loss 2.977357\n",
      "Epoch 1852 loss 2.977188\n",
      "Epoch 1853 loss 2.977021\n",
      "Epoch 1854 loss 2.976853\n",
      "Epoch 1855 loss 2.976687\n",
      "Epoch 1856 loss 2.976520\n",
      "Epoch 1857 loss 2.976354\n",
      "Epoch 1858 loss 2.976189\n",
      "Epoch 1859 loss 2.976023\n",
      "Epoch 1860 loss 2.975860\n",
      "Epoch 1861 loss 2.975697\n",
      "Epoch 1862 loss 2.975532\n",
      "Epoch 1863 loss 2.975369\n",
      "Epoch 1864 loss 2.975208\n",
      "Epoch 1865 loss 2.975046\n",
      "Epoch 1866 loss 2.974886\n",
      "Epoch 1867 loss 2.974725\n",
      "Epoch 1868 loss 2.974565\n",
      "Epoch 1869 loss 2.974406\n",
      "Epoch 1870 loss 2.974248\n",
      "Epoch 1871 loss 2.974088\n",
      "Epoch 1872 loss 2.973930\n",
      "Epoch 1873 loss 2.973776\n",
      "Epoch 1874 loss 2.973618\n",
      "Epoch 1875 loss 2.973463\n",
      "Epoch 1876 loss 2.973307\n",
      "Epoch 1877 loss 2.973150\n",
      "Epoch 1878 loss 2.972996\n",
      "Epoch 1879 loss 2.972844\n",
      "Epoch 1880 loss 2.972690\n",
      "Epoch 1881 loss 2.972536\n",
      "Epoch 1882 loss 2.972383\n",
      "Epoch 1883 loss 2.972232\n",
      "Epoch 1884 loss 2.972081\n",
      "Epoch 1885 loss 2.971931\n",
      "Epoch 1886 loss 2.971780\n",
      "Epoch 1887 loss 2.971629\n",
      "Epoch 1888 loss 2.971481\n",
      "Epoch 1889 loss 2.971332\n",
      "Epoch 1890 loss 2.971185\n",
      "Epoch 1891 loss 2.971035\n",
      "Epoch 1892 loss 2.970888\n",
      "Epoch 1893 loss 2.970741\n",
      "Epoch 1894 loss 2.970596\n",
      "Epoch 1895 loss 2.970449\n",
      "Epoch 1896 loss 2.970304\n",
      "Epoch 1897 loss 2.970159\n",
      "Epoch 1898 loss 2.970015\n",
      "Epoch 1899 loss 2.969871\n",
      "Epoch 1900 loss 2.969727\n",
      "Epoch 1901 loss 2.969586\n",
      "Epoch 1902 loss 2.969443\n",
      "Epoch 1903 loss 2.969302\n",
      "Epoch 1904 loss 2.969160\n",
      "Epoch 1905 loss 2.969017\n",
      "Epoch 1906 loss 2.968879\n",
      "Epoch 1907 loss 2.968739\n",
      "Epoch 1908 loss 2.968599\n",
      "Epoch 1909 loss 2.968460\n",
      "Epoch 1910 loss 2.968322\n",
      "Epoch 1911 loss 2.968184\n",
      "Epoch 1912 loss 2.968046\n",
      "Epoch 1913 loss 2.967908\n",
      "Epoch 1914 loss 2.967772\n",
      "Epoch 1915 loss 2.967636\n",
      "Epoch 1916 loss 2.967499\n",
      "Epoch 1917 loss 2.967365\n",
      "Epoch 1918 loss 2.967230\n",
      "Epoch 1919 loss 2.967095\n",
      "Epoch 1920 loss 2.966961\n",
      "Epoch 1921 loss 2.966827\n",
      "Epoch 1922 loss 2.966693\n",
      "Epoch 1923 loss 2.966561\n",
      "Epoch 1924 loss 2.966429\n",
      "Epoch 1925 loss 2.966297\n",
      "Epoch 1926 loss 2.966167\n",
      "Epoch 1927 loss 2.966036\n",
      "Epoch 1928 loss 2.965904\n",
      "Epoch 1929 loss 2.965776\n",
      "Epoch 1930 loss 2.965646\n",
      "Epoch 1931 loss 2.965517\n",
      "Epoch 1932 loss 2.965387\n",
      "Epoch 1933 loss 2.965261\n",
      "Epoch 1934 loss 2.965131\n",
      "Epoch 1935 loss 2.965005\n",
      "Epoch 1936 loss 2.964878\n",
      "Epoch 1937 loss 2.964751\n",
      "Epoch 1938 loss 2.964625\n",
      "Epoch 1939 loss 2.964500\n",
      "Epoch 1940 loss 2.964375\n",
      "Epoch 1941 loss 2.964250\n",
      "Epoch 1942 loss 2.964126\n",
      "Epoch 1943 loss 2.964001\n",
      "Epoch 1944 loss 2.963879\n",
      "Epoch 1945 loss 2.963756\n",
      "Epoch 1946 loss 2.963632\n",
      "Epoch 1947 loss 2.963511\n",
      "Epoch 1948 loss 2.963388\n",
      "Epoch 1949 loss 2.963266\n",
      "Epoch 1950 loss 2.963149\n",
      "Epoch 1951 loss 2.963026\n",
      "Epoch 1952 loss 2.962907\n",
      "Epoch 1953 loss 2.962788\n",
      "Epoch 1954 loss 2.962666\n",
      "Epoch 1955 loss 2.962547\n",
      "Epoch 1956 loss 2.962429\n",
      "Epoch 1957 loss 2.962312\n",
      "Epoch 1958 loss 2.962195\n",
      "Epoch 1959 loss 2.962078\n",
      "Epoch 1960 loss 2.961959\n",
      "Epoch 1961 loss 2.961843\n",
      "Epoch 1962 loss 2.961728\n",
      "Epoch 1963 loss 2.961611\n",
      "Epoch 1964 loss 2.961497\n",
      "Epoch 1965 loss 2.961382\n",
      "Epoch 1966 loss 2.961268\n",
      "Epoch 1967 loss 2.961153\n",
      "Epoch 1968 loss 2.961038\n",
      "Epoch 1969 loss 2.960926\n",
      "Epoch 1970 loss 2.960814\n",
      "Epoch 1971 loss 2.960699\n",
      "Epoch 1972 loss 2.960587\n",
      "Epoch 1973 loss 2.960475\n",
      "Epoch 1974 loss 2.960365\n",
      "Epoch 1975 loss 2.960254\n",
      "Epoch 1976 loss 2.960143\n",
      "Epoch 1977 loss 2.960033\n",
      "Epoch 1978 loss 2.959923\n",
      "Epoch 1979 loss 2.959812\n",
      "Epoch 1980 loss 2.959703\n",
      "Epoch 1981 loss 2.959594\n",
      "Epoch 1982 loss 2.959486\n",
      "Epoch 1983 loss 2.959378\n",
      "Epoch 1984 loss 2.959271\n",
      "Epoch 1985 loss 2.959163\n",
      "Epoch 1986 loss 2.959055\n",
      "Epoch 1987 loss 2.958950\n",
      "Epoch 1988 loss 2.958842\n",
      "Epoch 1989 loss 2.958738\n",
      "Epoch 1990 loss 2.958632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1991 loss 2.958526\n",
      "Epoch 1992 loss 2.958421\n",
      "Epoch 1993 loss 2.958317\n",
      "Epoch 1994 loss 2.958212\n",
      "Epoch 1995 loss 2.958109\n",
      "Epoch 1996 loss 2.958006\n",
      "Epoch 1997 loss 2.957903\n",
      "Epoch 1998 loss 2.957801\n",
      "Epoch 1999 loss 2.957697\n",
      "Epoch 2000 loss 2.957596\n",
      "Epoch 2001 loss 2.957494\n",
      "Epoch 2002 loss 2.957393\n",
      "Epoch 2003 loss 2.957293\n",
      "Epoch 2004 loss 2.957193\n",
      "Epoch 2005 loss 2.957091\n",
      "Epoch 2006 loss 2.956991\n",
      "Epoch 2007 loss 2.956892\n",
      "Epoch 2008 loss 2.956792\n",
      "Epoch 2009 loss 2.956694\n",
      "Epoch 2010 loss 2.956595\n",
      "Epoch 2011 loss 2.956496\n",
      "Epoch 2012 loss 2.956397\n",
      "Epoch 2013 loss 2.956300\n",
      "Epoch 2014 loss 2.956204\n",
      "Epoch 2015 loss 2.956108\n",
      "Epoch 2016 loss 2.956010\n",
      "Epoch 2017 loss 2.955914\n",
      "Epoch 2018 loss 2.955817\n",
      "Epoch 2019 loss 2.955722\n",
      "Epoch 2020 loss 2.955627\n",
      "Epoch 2021 loss 2.955533\n",
      "Epoch 2022 loss 2.955436\n",
      "Epoch 2023 loss 2.955343\n",
      "Epoch 2024 loss 2.955250\n",
      "Epoch 2025 loss 2.955154\n",
      "Epoch 2026 loss 2.955062\n",
      "Epoch 2027 loss 2.954968\n",
      "Epoch 2028 loss 2.954875\n",
      "Epoch 2029 loss 2.954783\n",
      "Epoch 2030 loss 2.954691\n",
      "Epoch 2031 loss 2.954600\n",
      "Epoch 2032 loss 2.954507\n",
      "Epoch 2033 loss 2.954417\n",
      "Epoch 2034 loss 2.954326\n",
      "Epoch 2035 loss 2.954235\n",
      "Epoch 2036 loss 2.954145\n",
      "Epoch 2037 loss 2.954055\n",
      "Epoch 2038 loss 2.953966\n",
      "Epoch 2039 loss 2.953876\n",
      "Epoch 2040 loss 2.953787\n",
      "Epoch 2041 loss 2.953698\n",
      "Epoch 2042 loss 2.953610\n",
      "Epoch 2043 loss 2.953521\n",
      "Epoch 2044 loss 2.953434\n",
      "Epoch 2045 loss 2.953346\n",
      "Epoch 2046 loss 2.953259\n",
      "Epoch 2047 loss 2.953172\n",
      "Epoch 2048 loss 2.953085\n",
      "Epoch 2049 loss 2.953000\n",
      "Epoch 2050 loss 2.952913\n",
      "Epoch 2051 loss 2.952828\n",
      "Epoch 2052 loss 2.952742\n",
      "Epoch 2053 loss 2.952657\n",
      "Epoch 2054 loss 2.952571\n",
      "Epoch 2055 loss 2.952487\n",
      "Epoch 2056 loss 2.952403\n",
      "Epoch 2057 loss 2.952318\n",
      "Epoch 2058 loss 2.952235\n",
      "Epoch 2059 loss 2.952152\n",
      "Epoch 2060 loss 2.952068\n",
      "Epoch 2061 loss 2.951985\n",
      "Epoch 2062 loss 2.951903\n",
      "Epoch 2063 loss 2.951820\n",
      "Epoch 2064 loss 2.951738\n",
      "Epoch 2065 loss 2.951656\n",
      "Epoch 2066 loss 2.951575\n",
      "Epoch 2067 loss 2.951494\n",
      "Epoch 2068 loss 2.951413\n",
      "Epoch 2069 loss 2.951333\n",
      "Epoch 2070 loss 2.951252\n",
      "Epoch 2071 loss 2.951172\n",
      "Epoch 2072 loss 2.951093\n",
      "Epoch 2073 loss 2.951012\n",
      "Epoch 2074 loss 2.950932\n",
      "Epoch 2075 loss 2.950853\n",
      "Epoch 2076 loss 2.950774\n",
      "Epoch 2077 loss 2.950698\n",
      "Epoch 2078 loss 2.950618\n",
      "Epoch 2079 loss 2.950540\n",
      "Epoch 2080 loss 2.950463\n",
      "Epoch 2081 loss 2.950385\n",
      "Epoch 2082 loss 2.950308\n",
      "Epoch 2083 loss 2.950231\n",
      "Epoch 2084 loss 2.950155\n",
      "Epoch 2085 loss 2.950079\n",
      "Epoch 2086 loss 2.950003\n",
      "Epoch 2087 loss 2.949925\n",
      "Epoch 2088 loss 2.949850\n",
      "Epoch 2089 loss 2.949776\n",
      "Epoch 2090 loss 2.949699\n",
      "Epoch 2091 loss 2.949626\n",
      "Epoch 2092 loss 2.949551\n",
      "Epoch 2093 loss 2.949476\n",
      "Epoch 2094 loss 2.949401\n",
      "Epoch 2095 loss 2.949328\n",
      "Epoch 2096 loss 2.949254\n",
      "Epoch 2097 loss 2.949182\n",
      "Epoch 2098 loss 2.949108\n",
      "Epoch 2099 loss 2.949036\n",
      "Epoch 2100 loss 2.948962\n",
      "Epoch 2101 loss 2.948890\n",
      "Epoch 2102 loss 2.948818\n",
      "Epoch 2103 loss 2.948746\n",
      "Epoch 2104 loss 2.948675\n",
      "Epoch 2105 loss 2.948602\n",
      "Epoch 2106 loss 2.948532\n",
      "Epoch 2107 loss 2.948462\n",
      "Epoch 2108 loss 2.948391\n",
      "Epoch 2109 loss 2.948321\n",
      "Epoch 2110 loss 2.948250\n",
      "Epoch 2111 loss 2.948180\n",
      "Epoch 2112 loss 2.948109\n",
      "Epoch 2113 loss 2.948040\n",
      "Epoch 2114 loss 2.947971\n",
      "Epoch 2115 loss 2.947902\n",
      "Epoch 2116 loss 2.947833\n",
      "Epoch 2117 loss 2.947765\n",
      "Epoch 2118 loss 2.947696\n",
      "Epoch 2119 loss 2.947628\n",
      "Epoch 2120 loss 2.947560\n",
      "Epoch 2121 loss 2.947494\n",
      "Epoch 2122 loss 2.947426\n",
      "Epoch 2123 loss 2.947358\n",
      "Epoch 2124 loss 2.947294\n",
      "Epoch 2125 loss 2.947226\n",
      "Epoch 2126 loss 2.947158\n",
      "Epoch 2127 loss 2.947091\n",
      "Epoch 2128 loss 2.947026\n",
      "Epoch 2129 loss 2.946960\n",
      "Epoch 2130 loss 2.946895\n",
      "Epoch 2131 loss 2.946830\n",
      "Epoch 2132 loss 2.946764\n",
      "Epoch 2133 loss 2.946700\n",
      "Epoch 2134 loss 2.946635\n",
      "Epoch 2135 loss 2.946571\n",
      "Epoch 2136 loss 2.946507\n",
      "Epoch 2137 loss 2.946442\n",
      "Epoch 2138 loss 2.946378\n",
      "Epoch 2139 loss 2.946314\n",
      "Epoch 2140 loss 2.946251\n",
      "Epoch 2141 loss 2.946189\n",
      "Epoch 2142 loss 2.946125\n",
      "Epoch 2143 loss 2.946063\n",
      "Epoch 2144 loss 2.946001\n",
      "Epoch 2145 loss 2.945937\n",
      "Epoch 2146 loss 2.945876\n",
      "Epoch 2147 loss 2.945815\n",
      "Epoch 2148 loss 2.945753\n",
      "Epoch 2149 loss 2.945690\n",
      "Epoch 2150 loss 2.945630\n",
      "Epoch 2151 loss 2.945567\n",
      "Epoch 2152 loss 2.945509\n",
      "Epoch 2153 loss 2.945448\n",
      "Epoch 2154 loss 2.945386\n",
      "Epoch 2155 loss 2.945326\n",
      "Epoch 2156 loss 2.945267\n",
      "Epoch 2157 loss 2.945207\n",
      "Epoch 2158 loss 2.945146\n",
      "Epoch 2159 loss 2.945088\n",
      "Epoch 2160 loss 2.945028\n",
      "Epoch 2161 loss 2.944969\n",
      "Epoch 2162 loss 2.944911\n",
      "Epoch 2163 loss 2.944852\n",
      "Epoch 2164 loss 2.944792\n",
      "Epoch 2165 loss 2.944736\n",
      "Epoch 2166 loss 2.944678\n",
      "Epoch 2167 loss 2.944619\n",
      "Epoch 2168 loss 2.944562\n",
      "Epoch 2169 loss 2.944504\n",
      "Epoch 2170 loss 2.944447\n",
      "Epoch 2171 loss 2.944391\n",
      "Epoch 2172 loss 2.944332\n",
      "Epoch 2173 loss 2.944276\n",
      "Epoch 2174 loss 2.944220\n",
      "Epoch 2175 loss 2.944164\n",
      "Epoch 2176 loss 2.944108\n",
      "Epoch 2177 loss 2.944052\n",
      "Epoch 2178 loss 2.943996\n",
      "Epoch 2179 loss 2.943941\n",
      "Epoch 2180 loss 2.943886\n",
      "Epoch 2181 loss 2.943831\n",
      "Epoch 2182 loss 2.943775\n",
      "Epoch 2183 loss 2.943721\n",
      "Epoch 2184 loss 2.943666\n",
      "Epoch 2185 loss 2.943613\n",
      "Epoch 2186 loss 2.943558\n",
      "Epoch 2187 loss 2.943504\n",
      "Epoch 2188 loss 2.943451\n",
      "Epoch 2189 loss 2.943395\n",
      "Epoch 2190 loss 2.943343\n",
      "Epoch 2191 loss 2.943290\n",
      "Epoch 2192 loss 2.943235\n",
      "Epoch 2193 loss 2.943183\n",
      "Epoch 2194 loss 2.943130\n",
      "Epoch 2195 loss 2.943079\n",
      "Epoch 2196 loss 2.943026\n",
      "Epoch 2197 loss 2.942974\n",
      "Epoch 2198 loss 2.942922\n",
      "Epoch 2199 loss 2.942870\n",
      "Epoch 2200 loss 2.942818\n",
      "Epoch 2201 loss 2.942765\n",
      "Epoch 2202 loss 2.942714\n",
      "Epoch 2203 loss 2.942664\n",
      "Epoch 2204 loss 2.942612\n",
      "Epoch 2205 loss 2.942563\n",
      "Epoch 2206 loss 2.942510\n",
      "Epoch 2207 loss 2.942461\n",
      "Epoch 2208 loss 2.942411\n",
      "Epoch 2209 loss 2.942361\n",
      "Epoch 2210 loss 2.942310\n",
      "Epoch 2211 loss 2.942261\n",
      "Epoch 2212 loss 2.942211\n",
      "Epoch 2213 loss 2.942162\n",
      "Epoch 2214 loss 2.942112\n",
      "Epoch 2215 loss 2.942062\n",
      "Epoch 2216 loss 2.942014\n",
      "Epoch 2217 loss 2.941965\n",
      "Epoch 2218 loss 2.941918\n",
      "Epoch 2219 loss 2.941868\n",
      "Epoch 2220 loss 2.941821\n",
      "Epoch 2221 loss 2.941773\n",
      "Epoch 2222 loss 2.941725\n",
      "Epoch 2223 loss 2.941677\n",
      "Epoch 2224 loss 2.941629\n",
      "Epoch 2225 loss 2.941582\n",
      "Epoch 2226 loss 2.941534\n",
      "Epoch 2227 loss 2.941488\n",
      "Epoch 2228 loss 2.941440\n",
      "Epoch 2229 loss 2.941393\n",
      "Epoch 2230 loss 2.941346\n",
      "Epoch 2231 loss 2.941299\n",
      "Epoch 2232 loss 2.941252\n",
      "Epoch 2233 loss 2.941206\n",
      "Epoch 2234 loss 2.941163\n",
      "Epoch 2235 loss 2.941115\n",
      "Epoch 2236 loss 2.941070\n",
      "Epoch 2237 loss 2.941025\n",
      "Epoch 2238 loss 2.940979\n",
      "Epoch 2239 loss 2.940933\n",
      "Epoch 2240 loss 2.940890\n",
      "Epoch 2241 loss 2.940844\n",
      "Epoch 2242 loss 2.940798\n",
      "Epoch 2243 loss 2.940753\n",
      "Epoch 2244 loss 2.940711\n",
      "Epoch 2245 loss 2.940666\n",
      "Epoch 2246 loss 2.940621\n",
      "Epoch 2247 loss 2.940576\n",
      "Epoch 2248 loss 2.940533\n",
      "Epoch 2249 loss 2.940489\n",
      "Epoch 2250 loss 2.940446\n",
      "Epoch 2251 loss 2.940403\n",
      "Epoch 2252 loss 2.940358\n",
      "Epoch 2253 loss 2.940316\n",
      "Epoch 2254 loss 2.940274\n",
      "Epoch 2255 loss 2.940229\n",
      "Epoch 2256 loss 2.940187\n",
      "Epoch 2257 loss 2.940144\n",
      "Epoch 2258 loss 2.940102\n",
      "Epoch 2259 loss 2.940060\n",
      "Epoch 2260 loss 2.940018\n",
      "Epoch 2261 loss 2.939977\n",
      "Epoch 2262 loss 2.939934\n",
      "Epoch 2263 loss 2.939892\n",
      "Epoch 2264 loss 2.939851\n",
      "Epoch 2265 loss 2.939809\n",
      "Epoch 2266 loss 2.939769\n",
      "Epoch 2267 loss 2.939727\n",
      "Epoch 2268 loss 2.939686\n",
      "Epoch 2269 loss 2.939646\n",
      "Epoch 2270 loss 2.939605\n",
      "Epoch 2271 loss 2.939565\n",
      "Epoch 2272 loss 2.939522\n",
      "Epoch 2273 loss 2.939483\n",
      "Epoch 2274 loss 2.939443\n",
      "Epoch 2275 loss 2.939403\n",
      "Epoch 2276 loss 2.939361\n",
      "Epoch 2277 loss 2.939323\n",
      "Epoch 2278 loss 2.939282\n",
      "Epoch 2279 loss 2.939243\n",
      "Epoch 2280 loss 2.939205\n",
      "Epoch 2281 loss 2.939165\n",
      "Epoch 2282 loss 2.939127\n",
      "Epoch 2283 loss 2.939087\n",
      "Epoch 2284 loss 2.939049\n",
      "Epoch 2285 loss 2.939011\n",
      "Epoch 2286 loss 2.938971\n",
      "Epoch 2287 loss 2.938933\n",
      "Epoch 2288 loss 2.938893\n",
      "Epoch 2289 loss 2.938857\n",
      "Epoch 2290 loss 2.938820\n",
      "Epoch 2291 loss 2.938779\n",
      "Epoch 2292 loss 2.938743\n",
      "Epoch 2293 loss 2.938705\n",
      "Epoch 2294 loss 2.938667\n",
      "Epoch 2295 loss 2.938629\n",
      "Epoch 2296 loss 2.938593\n",
      "Epoch 2297 loss 2.938555\n",
      "Epoch 2298 loss 2.938519\n",
      "Epoch 2299 loss 2.938481\n",
      "Epoch 2300 loss 2.938444\n",
      "Epoch 2301 loss 2.938408\n",
      "Epoch 2302 loss 2.938371\n",
      "Epoch 2303 loss 2.938335\n",
      "Epoch 2304 loss 2.938299\n",
      "Epoch 2305 loss 2.938262\n",
      "Epoch 2306 loss 2.938227\n",
      "Epoch 2307 loss 2.938191\n",
      "Epoch 2308 loss 2.938155\n",
      "Epoch 2309 loss 2.938118\n",
      "Epoch 2310 loss 2.938084\n",
      "Epoch 2311 loss 2.938049\n",
      "Epoch 2312 loss 2.938014\n",
      "Epoch 2313 loss 2.937977\n",
      "Epoch 2314 loss 2.937943\n",
      "Epoch 2315 loss 2.937908\n",
      "Epoch 2316 loss 2.937872\n",
      "Epoch 2317 loss 2.937839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2318 loss 2.937804\n",
      "Epoch 2319 loss 2.937768\n",
      "Epoch 2320 loss 2.937734\n",
      "Epoch 2321 loss 2.937700\n",
      "Epoch 2322 loss 2.937665\n",
      "Epoch 2323 loss 2.937632\n",
      "Epoch 2324 loss 2.937598\n",
      "Epoch 2325 loss 2.937565\n",
      "Epoch 2326 loss 2.937531\n",
      "Epoch 2327 loss 2.937499\n",
      "Epoch 2328 loss 2.937464\n",
      "Epoch 2329 loss 2.937430\n",
      "Epoch 2330 loss 2.937398\n",
      "Epoch 2331 loss 2.937364\n",
      "Epoch 2332 loss 2.937332\n",
      "Epoch 2333 loss 2.937299\n",
      "Epoch 2334 loss 2.937265\n",
      "Epoch 2335 loss 2.937232\n",
      "Epoch 2336 loss 2.937201\n",
      "Epoch 2337 loss 2.937168\n",
      "Epoch 2338 loss 2.937134\n",
      "Epoch 2339 loss 2.937104\n",
      "Epoch 2340 loss 2.937071\n",
      "Epoch 2341 loss 2.937039\n",
      "Epoch 2342 loss 2.937008\n",
      "Epoch 2343 loss 2.936976\n",
      "Epoch 2344 loss 2.936945\n",
      "Epoch 2345 loss 2.936912\n",
      "Epoch 2346 loss 2.936882\n",
      "Epoch 2347 loss 2.936851\n",
      "Epoch 2348 loss 2.936819\n",
      "Epoch 2349 loss 2.936788\n",
      "Epoch 2350 loss 2.936757\n",
      "Epoch 2351 loss 2.936725\n",
      "Epoch 2352 loss 2.936694\n",
      "Epoch 2353 loss 2.936665\n",
      "Epoch 2354 loss 2.936633\n",
      "Epoch 2355 loss 2.936602\n",
      "Epoch 2356 loss 2.936572\n",
      "Epoch 2357 loss 2.936542\n",
      "Epoch 2358 loss 2.936511\n",
      "Epoch 2359 loss 2.936482\n",
      "Epoch 2360 loss 2.936451\n",
      "Epoch 2361 loss 2.936421\n",
      "Epoch 2362 loss 2.936392\n",
      "Epoch 2363 loss 2.936362\n",
      "Epoch 2364 loss 2.936332\n",
      "Epoch 2365 loss 2.936304\n",
      "Epoch 2366 loss 2.936274\n",
      "Epoch 2367 loss 2.936244\n",
      "Epoch 2368 loss 2.936215\n",
      "Epoch 2369 loss 2.936188\n",
      "Epoch 2370 loss 2.936156\n",
      "Epoch 2371 loss 2.936128\n",
      "Epoch 2372 loss 2.936100\n",
      "Epoch 2373 loss 2.936071\n",
      "Epoch 2374 loss 2.936043\n",
      "Epoch 2375 loss 2.936014\n",
      "Epoch 2376 loss 2.935986\n",
      "Epoch 2377 loss 2.935957\n",
      "Epoch 2378 loss 2.935928\n",
      "Epoch 2379 loss 2.935901\n",
      "Epoch 2380 loss 2.935873\n",
      "Epoch 2381 loss 2.935845\n",
      "Epoch 2382 loss 2.935817\n",
      "Epoch 2383 loss 2.935789\n",
      "Epoch 2384 loss 2.935761\n",
      "Epoch 2385 loss 2.935734\n",
      "Epoch 2386 loss 2.935707\n",
      "Epoch 2387 loss 2.935679\n",
      "Epoch 2388 loss 2.935650\n",
      "Epoch 2389 loss 2.935626\n",
      "Epoch 2390 loss 2.935596\n",
      "Epoch 2391 loss 2.935571\n",
      "Epoch 2392 loss 2.935544\n",
      "Epoch 2393 loss 2.935516\n",
      "Epoch 2394 loss 2.935489\n",
      "Epoch 2395 loss 2.935464\n",
      "Epoch 2396 loss 2.935436\n",
      "Epoch 2397 loss 2.935412\n",
      "Epoch 2398 loss 2.935385\n",
      "Epoch 2399 loss 2.935357\n",
      "Epoch 2400 loss 2.935332\n",
      "Epoch 2401 loss 2.935304\n",
      "Epoch 2402 loss 2.935281\n",
      "Epoch 2403 loss 2.935252\n",
      "Epoch 2404 loss 2.935229\n",
      "Epoch 2405 loss 2.935203\n",
      "Epoch 2406 loss 2.935177\n",
      "Epoch 2407 loss 2.935152\n",
      "Epoch 2408 loss 2.935126\n",
      "Epoch 2409 loss 2.935099\n",
      "Epoch 2410 loss 2.935075\n",
      "Epoch 2411 loss 2.935049\n",
      "Epoch 2412 loss 2.935024\n",
      "Epoch 2413 loss 2.935001\n",
      "Epoch 2414 loss 2.934973\n",
      "Epoch 2415 loss 2.934949\n",
      "Epoch 2416 loss 2.934925\n",
      "Epoch 2417 loss 2.934899\n",
      "Epoch 2418 loss 2.934876\n",
      "Epoch 2419 loss 2.934852\n",
      "Epoch 2420 loss 2.934827\n",
      "Epoch 2421 loss 2.934802\n",
      "Epoch 2422 loss 2.934777\n",
      "Epoch 2423 loss 2.934753\n",
      "Epoch 2424 loss 2.934730\n",
      "Epoch 2425 loss 2.934705\n",
      "Epoch 2426 loss 2.934681\n",
      "Epoch 2427 loss 2.934658\n",
      "Epoch 2428 loss 2.934635\n",
      "Epoch 2429 loss 2.934609\n",
      "Epoch 2430 loss 2.934585\n",
      "Epoch 2431 loss 2.934564\n",
      "Epoch 2432 loss 2.934541\n",
      "Epoch 2433 loss 2.934516\n",
      "Epoch 2434 loss 2.934493\n",
      "Epoch 2435 loss 2.934469\n",
      "Epoch 2436 loss 2.934446\n",
      "Epoch 2437 loss 2.934423\n",
      "Epoch 2438 loss 2.934400\n",
      "Epoch 2439 loss 2.934377\n",
      "Epoch 2440 loss 2.934355\n",
      "Epoch 2441 loss 2.934331\n",
      "Epoch 2442 loss 2.934309\n",
      "Epoch 2443 loss 2.934287\n",
      "Epoch 2444 loss 2.934264\n",
      "Epoch 2445 loss 2.934242\n",
      "Epoch 2446 loss 2.934219\n",
      "Epoch 2447 loss 2.934198\n",
      "Epoch 2448 loss 2.934175\n",
      "Epoch 2449 loss 2.934151\n",
      "Epoch 2450 loss 2.934129\n",
      "Epoch 2451 loss 2.934108\n",
      "Epoch 2452 loss 2.934084\n",
      "Epoch 2453 loss 2.934064\n",
      "Epoch 2454 loss 2.934043\n",
      "Epoch 2455 loss 2.934020\n",
      "Epoch 2456 loss 2.934000\n",
      "Epoch 2457 loss 2.933978\n",
      "Epoch 2458 loss 2.933956\n",
      "Epoch 2459 loss 2.933935\n",
      "Epoch 2460 loss 2.933913\n",
      "Epoch 2461 loss 2.933893\n",
      "Epoch 2462 loss 2.933871\n",
      "Epoch 2463 loss 2.933849\n",
      "Epoch 2464 loss 2.933828\n",
      "Epoch 2465 loss 2.933807\n",
      "Epoch 2466 loss 2.933787\n",
      "Epoch 2467 loss 2.933766\n",
      "Epoch 2468 loss 2.933745\n",
      "Epoch 2469 loss 2.933723\n",
      "Epoch 2470 loss 2.933704\n",
      "Epoch 2471 loss 2.933682\n",
      "Epoch 2472 loss 2.933662\n",
      "Epoch 2473 loss 2.933643\n",
      "Epoch 2474 loss 2.933622\n",
      "Epoch 2475 loss 2.933602\n",
      "Epoch 2476 loss 2.933583\n",
      "Epoch 2477 loss 2.933561\n",
      "Epoch 2478 loss 2.933541\n",
      "Epoch 2479 loss 2.933521\n",
      "Epoch 2480 loss 2.933501\n",
      "Epoch 2481 loss 2.933480\n",
      "Epoch 2482 loss 2.933463\n",
      "Epoch 2483 loss 2.933442\n",
      "Epoch 2484 loss 2.933423\n",
      "Epoch 2485 loss 2.933403\n",
      "Epoch 2486 loss 2.933382\n",
      "Epoch 2487 loss 2.933365\n",
      "Epoch 2488 loss 2.933345\n",
      "Epoch 2489 loss 2.933325\n",
      "Epoch 2490 loss 2.933306\n",
      "Epoch 2491 loss 2.933287\n",
      "Epoch 2492 loss 2.933266\n",
      "Epoch 2493 loss 2.933249\n",
      "Epoch 2494 loss 2.933228\n",
      "Epoch 2495 loss 2.933209\n",
      "Epoch 2496 loss 2.933190\n",
      "Epoch 2497 loss 2.933172\n",
      "Epoch 2498 loss 2.933154\n",
      "Epoch 2499 loss 2.933134\n",
      "Epoch 2500 loss 2.933116\n",
      "Epoch 2501 loss 2.933097\n",
      "Epoch 2502 loss 2.933079\n",
      "Epoch 2503 loss 2.933060\n",
      "Epoch 2504 loss 2.933043\n",
      "Epoch 2505 loss 2.933025\n",
      "Epoch 2506 loss 2.933007\n",
      "Epoch 2507 loss 2.932988\n",
      "Epoch 2508 loss 2.932970\n",
      "Epoch 2509 loss 2.932953\n",
      "Epoch 2510 loss 2.932932\n",
      "Epoch 2511 loss 2.932915\n",
      "Epoch 2512 loss 2.932898\n",
      "Epoch 2513 loss 2.932880\n",
      "Epoch 2514 loss 2.932862\n",
      "Epoch 2515 loss 2.932846\n",
      "Epoch 2516 loss 2.932826\n",
      "Epoch 2517 loss 2.932810\n",
      "Epoch 2518 loss 2.932791\n",
      "Epoch 2519 loss 2.932774\n",
      "Epoch 2520 loss 2.932758\n",
      "Epoch 2521 loss 2.932739\n",
      "Epoch 2522 loss 2.932723\n",
      "Epoch 2523 loss 2.932706\n",
      "Epoch 2524 loss 2.932689\n",
      "Epoch 2525 loss 2.932671\n",
      "Epoch 2526 loss 2.932654\n",
      "Epoch 2527 loss 2.932637\n",
      "Epoch 2528 loss 2.932619\n",
      "Epoch 2529 loss 2.932603\n",
      "Epoch 2530 loss 2.932585\n",
      "Epoch 2531 loss 2.932569\n",
      "Epoch 2532 loss 2.932553\n",
      "Epoch 2533 loss 2.932535\n",
      "Epoch 2534 loss 2.932520\n",
      "Epoch 2535 loss 2.932502\n",
      "Epoch 2536 loss 2.932487\n",
      "Epoch 2537 loss 2.932469\n",
      "Epoch 2538 loss 2.932455\n",
      "Epoch 2539 loss 2.932438\n",
      "Epoch 2540 loss 2.932421\n",
      "Epoch 2541 loss 2.932404\n",
      "Epoch 2542 loss 2.932387\n",
      "Epoch 2543 loss 2.932370\n",
      "Epoch 2544 loss 2.932358\n",
      "Epoch 2545 loss 2.932340\n",
      "Epoch 2546 loss 2.932324\n",
      "Epoch 2547 loss 2.932310\n",
      "Epoch 2548 loss 2.932293\n",
      "Epoch 2549 loss 2.932278\n",
      "Epoch 2550 loss 2.932261\n",
      "Epoch 2551 loss 2.932246\n",
      "Epoch 2552 loss 2.932229\n",
      "Epoch 2553 loss 2.932215\n",
      "Epoch 2554 loss 2.932198\n",
      "Epoch 2555 loss 2.932184\n",
      "Epoch 2556 loss 2.932168\n",
      "Epoch 2557 loss 2.932153\n",
      "Epoch 2558 loss 2.932137\n",
      "Epoch 2559 loss 2.932122\n",
      "Epoch 2560 loss 2.932107\n",
      "Epoch 2561 loss 2.932092\n",
      "Epoch 2562 loss 2.932076\n",
      "Epoch 2563 loss 2.932061\n",
      "Epoch 2564 loss 2.932047\n",
      "Epoch 2565 loss 2.932031\n",
      "Epoch 2566 loss 2.932017\n",
      "Epoch 2567 loss 2.932002\n",
      "Epoch 2568 loss 2.931986\n",
      "Epoch 2569 loss 2.931972\n",
      "Epoch 2570 loss 2.931957\n",
      "Epoch 2571 loss 2.931941\n",
      "Epoch 2572 loss 2.931929\n",
      "Epoch 2573 loss 2.931914\n",
      "Epoch 2574 loss 2.931900\n",
      "Epoch 2575 loss 2.931885\n",
      "Epoch 2576 loss 2.931870\n",
      "Epoch 2577 loss 2.931855\n",
      "Epoch 2578 loss 2.931843\n",
      "Epoch 2579 loss 2.931828\n",
      "Epoch 2580 loss 2.931813\n",
      "Epoch 2581 loss 2.931799\n",
      "Epoch 2582 loss 2.931786\n",
      "Epoch 2583 loss 2.931771\n",
      "Epoch 2584 loss 2.931759\n",
      "Epoch 2585 loss 2.931742\n",
      "Epoch 2586 loss 2.931729\n",
      "Epoch 2587 loss 2.931717\n",
      "Epoch 2588 loss 2.931701\n",
      "Epoch 2589 loss 2.931687\n",
      "Epoch 2590 loss 2.931674\n",
      "Epoch 2591 loss 2.931660\n",
      "Epoch 2592 loss 2.931647\n",
      "Epoch 2593 loss 2.931632\n",
      "Epoch 2594 loss 2.931619\n",
      "Epoch 2595 loss 2.931606\n",
      "Epoch 2596 loss 2.931594\n",
      "Epoch 2597 loss 2.931580\n",
      "Epoch 2598 loss 2.931566\n",
      "Epoch 2599 loss 2.931554\n",
      "Epoch 2600 loss 2.931539\n",
      "Epoch 2601 loss 2.931526\n",
      "Epoch 2602 loss 2.931512\n",
      "Epoch 2603 loss 2.931500\n",
      "Epoch 2604 loss 2.931488\n",
      "Epoch 2605 loss 2.931474\n",
      "Epoch 2606 loss 2.931462\n",
      "Epoch 2607 loss 2.931448\n",
      "Epoch 2608 loss 2.931436\n",
      "Epoch 2609 loss 2.931422\n",
      "Epoch 2610 loss 2.931411\n",
      "Epoch 2611 loss 2.931398\n",
      "Epoch 2612 loss 2.931384\n",
      "Epoch 2613 loss 2.931370\n",
      "Epoch 2614 loss 2.931358\n",
      "Epoch 2615 loss 2.931346\n",
      "Epoch 2616 loss 2.931334\n",
      "Epoch 2617 loss 2.931322\n",
      "Epoch 2618 loss 2.931309\n",
      "Epoch 2619 loss 2.931296\n",
      "Epoch 2620 loss 2.931282\n",
      "Epoch 2621 loss 2.931272\n",
      "Epoch 2622 loss 2.931258\n",
      "Epoch 2623 loss 2.931245\n",
      "Epoch 2624 loss 2.931235\n",
      "Epoch 2625 loss 2.931222\n",
      "Epoch 2626 loss 2.931211\n",
      "Epoch 2627 loss 2.931196\n",
      "Epoch 2628 loss 2.931185\n",
      "Epoch 2629 loss 2.931173\n",
      "Epoch 2630 loss 2.931162\n",
      "Epoch 2631 loss 2.931149\n",
      "Epoch 2632 loss 2.931139\n",
      "Epoch 2633 loss 2.931126\n",
      "Epoch 2634 loss 2.931114\n",
      "Epoch 2635 loss 2.931101\n",
      "Epoch 2636 loss 2.931090\n",
      "Epoch 2637 loss 2.931079\n",
      "Epoch 2638 loss 2.931067\n",
      "Epoch 2639 loss 2.931054\n",
      "Epoch 2640 loss 2.931044\n",
      "Epoch 2641 loss 2.931034\n",
      "Epoch 2642 loss 2.931021\n",
      "Epoch 2643 loss 2.931010\n",
      "Epoch 2644 loss 2.930999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2645 loss 2.930987\n",
      "Epoch 2646 loss 2.930976\n",
      "Epoch 2647 loss 2.930964\n",
      "Epoch 2648 loss 2.930953\n",
      "Epoch 2649 loss 2.930941\n",
      "Epoch 2650 loss 2.930932\n",
      "Epoch 2651 loss 2.930920\n",
      "Epoch 2652 loss 2.930908\n",
      "Epoch 2653 loss 2.930899\n",
      "Epoch 2654 loss 2.930885\n",
      "Epoch 2655 loss 2.930876\n",
      "Epoch 2656 loss 2.930864\n",
      "Epoch 2657 loss 2.930854\n",
      "Epoch 2658 loss 2.930841\n",
      "Epoch 2659 loss 2.930833\n",
      "Epoch 2660 loss 2.930821\n",
      "Epoch 2661 loss 2.930811\n",
      "Epoch 2662 loss 2.930801\n",
      "Epoch 2663 loss 2.930788\n",
      "Epoch 2664 loss 2.930778\n",
      "Epoch 2665 loss 2.930766\n",
      "Epoch 2666 loss 2.930757\n",
      "Epoch 2667 loss 2.930746\n",
      "Epoch 2668 loss 2.930735\n",
      "Epoch 2669 loss 2.930724\n",
      "Epoch 2670 loss 2.930715\n",
      "Epoch 2671 loss 2.930704\n",
      "Epoch 2672 loss 2.930694\n",
      "Epoch 2673 loss 2.930685\n",
      "Epoch 2674 loss 2.930674\n",
      "Epoch 2675 loss 2.930663\n",
      "Epoch 2676 loss 2.930654\n",
      "Epoch 2677 loss 2.930644\n",
      "Epoch 2678 loss 2.930631\n",
      "Epoch 2679 loss 2.930622\n",
      "Epoch 2680 loss 2.930614\n",
      "Epoch 2681 loss 2.930603\n",
      "Epoch 2682 loss 2.930592\n",
      "Epoch 2683 loss 2.930582\n",
      "Epoch 2684 loss 2.930572\n",
      "Epoch 2685 loss 2.930562\n",
      "Epoch 2686 loss 2.930552\n",
      "Epoch 2687 loss 2.930543\n",
      "Epoch 2688 loss 2.930534\n",
      "Epoch 2689 loss 2.930524\n",
      "Epoch 2690 loss 2.930514\n",
      "Epoch 2691 loss 2.930502\n",
      "Epoch 2692 loss 2.930493\n",
      "Epoch 2693 loss 2.930482\n",
      "Epoch 2694 loss 2.930474\n",
      "Epoch 2695 loss 2.930465\n",
      "Epoch 2696 loss 2.930454\n",
      "Epoch 2697 loss 2.930446\n",
      "Epoch 2698 loss 2.930436\n",
      "Epoch 2699 loss 2.930426\n",
      "Epoch 2700 loss 2.930417\n",
      "Epoch 2701 loss 2.930408\n",
      "Epoch 2702 loss 2.930398\n",
      "Epoch 2703 loss 2.930388\n",
      "Epoch 2704 loss 2.930380\n",
      "Epoch 2705 loss 2.930370\n",
      "Epoch 2706 loss 2.930360\n",
      "Epoch 2707 loss 2.930352\n",
      "Epoch 2708 loss 2.930342\n",
      "Epoch 2709 loss 2.930334\n",
      "Epoch 2710 loss 2.930325\n",
      "Epoch 2711 loss 2.930315\n",
      "Epoch 2712 loss 2.930306\n",
      "Epoch 2713 loss 2.930298\n",
      "Epoch 2714 loss 2.930288\n",
      "Epoch 2715 loss 2.930279\n",
      "Epoch 2716 loss 2.930270\n",
      "Epoch 2717 loss 2.930262\n",
      "Epoch 2718 loss 2.930254\n",
      "Epoch 2719 loss 2.930244\n",
      "Epoch 2720 loss 2.930235\n",
      "Epoch 2721 loss 2.930226\n",
      "Epoch 2722 loss 2.930218\n",
      "Epoch 2723 loss 2.930209\n",
      "Epoch 2724 loss 2.930201\n",
      "Epoch 2725 loss 2.930190\n",
      "Epoch 2726 loss 2.930182\n",
      "Epoch 2727 loss 2.930173\n",
      "Epoch 2728 loss 2.930167\n",
      "Epoch 2729 loss 2.930156\n",
      "Epoch 2730 loss 2.930149\n",
      "Epoch 2731 loss 2.930139\n",
      "Epoch 2732 loss 2.930131\n",
      "Epoch 2733 loss 2.930123\n",
      "Epoch 2734 loss 2.930113\n",
      "Epoch 2735 loss 2.930107\n",
      "Epoch 2736 loss 2.930099\n",
      "Epoch 2737 loss 2.930090\n",
      "Epoch 2738 loss 2.930081\n",
      "Epoch 2739 loss 2.930073\n",
      "Epoch 2740 loss 2.930064\n",
      "Epoch 2741 loss 2.930056\n",
      "Epoch 2742 loss 2.930048\n",
      "Epoch 2743 loss 2.930041\n",
      "Epoch 2744 loss 2.930032\n",
      "Epoch 2745 loss 2.930022\n",
      "Epoch 2746 loss 2.930016\n",
      "Epoch 2747 loss 2.930008\n",
      "Epoch 2748 loss 2.930000\n",
      "Epoch 2749 loss 2.929992\n",
      "Epoch 2750 loss 2.929983\n",
      "Epoch 2751 loss 2.929975\n",
      "Epoch 2752 loss 2.929968\n",
      "Epoch 2753 loss 2.929960\n",
      "Epoch 2754 loss 2.929953\n",
      "Epoch 2755 loss 2.929945\n",
      "Epoch 2756 loss 2.929937\n",
      "Epoch 2757 loss 2.929929\n",
      "Epoch 2758 loss 2.929921\n",
      "Epoch 2759 loss 2.929914\n",
      "Epoch 2760 loss 2.929905\n",
      "Epoch 2761 loss 2.929896\n",
      "Epoch 2762 loss 2.929891\n",
      "Epoch 2763 loss 2.929882\n",
      "Epoch 2764 loss 2.929874\n",
      "Epoch 2765 loss 2.929869\n",
      "Epoch 2766 loss 2.929859\n",
      "Epoch 2767 loss 2.929852\n",
      "Epoch 2768 loss 2.929845\n",
      "Epoch 2769 loss 2.929838\n",
      "Epoch 2770 loss 2.929830\n",
      "Epoch 2771 loss 2.929822\n",
      "Epoch 2772 loss 2.929816\n",
      "Epoch 2773 loss 2.929806\n",
      "Epoch 2774 loss 2.929799\n",
      "Epoch 2775 loss 2.929793\n",
      "Epoch 2776 loss 2.929786\n",
      "Epoch 2777 loss 2.929778\n",
      "Epoch 2778 loss 2.929771\n",
      "Epoch 2779 loss 2.929765\n",
      "Epoch 2780 loss 2.929757\n",
      "Epoch 2781 loss 2.929750\n",
      "Epoch 2782 loss 2.929743\n",
      "Epoch 2783 loss 2.929735\n",
      "Epoch 2784 loss 2.929729\n",
      "Epoch 2785 loss 2.929722\n",
      "Epoch 2786 loss 2.929714\n",
      "Epoch 2787 loss 2.929707\n",
      "Epoch 2788 loss 2.929701\n",
      "Epoch 2789 loss 2.929692\n",
      "Epoch 2790 loss 2.929685\n",
      "Epoch 2791 loss 2.929680\n",
      "Epoch 2792 loss 2.929672\n",
      "Epoch 2793 loss 2.929666\n",
      "Epoch 2794 loss 2.929658\n",
      "Epoch 2795 loss 2.929652\n",
      "Epoch 2796 loss 2.929646\n",
      "Epoch 2797 loss 2.929638\n",
      "Epoch 2798 loss 2.929632\n",
      "Epoch 2799 loss 2.929626\n",
      "Epoch 2800 loss 2.929620\n",
      "Epoch 2801 loss 2.929611\n",
      "Epoch 2802 loss 2.929605\n",
      "Epoch 2803 loss 2.929600\n",
      "Epoch 2804 loss 2.929593\n",
      "Epoch 2805 loss 2.929586\n",
      "Epoch 2806 loss 2.929579\n",
      "Epoch 2807 loss 2.929572\n",
      "Epoch 2808 loss 2.929566\n",
      "Epoch 2809 loss 2.929559\n",
      "Epoch 2810 loss 2.929552\n",
      "Epoch 2811 loss 2.929545\n",
      "Epoch 2812 loss 2.929540\n",
      "Epoch 2813 loss 2.929533\n",
      "Epoch 2814 loss 2.929527\n",
      "Epoch 2815 loss 2.929520\n",
      "Epoch 2816 loss 2.929513\n",
      "Epoch 2817 loss 2.929507\n",
      "Epoch 2818 loss 2.929501\n",
      "Epoch 2819 loss 2.929496\n",
      "Epoch 2820 loss 2.929489\n",
      "Epoch 2821 loss 2.929482\n",
      "Epoch 2822 loss 2.929476\n",
      "Epoch 2823 loss 2.929471\n",
      "Epoch 2824 loss 2.929463\n",
      "Epoch 2825 loss 2.929457\n",
      "Epoch 2826 loss 2.929452\n",
      "Epoch 2827 loss 2.929445\n",
      "Epoch 2828 loss 2.929439\n",
      "Epoch 2829 loss 2.929433\n",
      "Epoch 2830 loss 2.929427\n",
      "Epoch 2831 loss 2.929421\n",
      "Epoch 2832 loss 2.929415\n",
      "Epoch 2833 loss 2.929409\n",
      "Epoch 2834 loss 2.929404\n",
      "Epoch 2835 loss 2.929396\n",
      "Epoch 2836 loss 2.929391\n",
      "Epoch 2837 loss 2.929383\n",
      "Epoch 2838 loss 2.929380\n",
      "Epoch 2839 loss 2.929373\n",
      "Epoch 2840 loss 2.929368\n",
      "Epoch 2841 loss 2.929362\n",
      "Epoch 2842 loss 2.929356\n",
      "Epoch 2843 loss 2.929351\n",
      "Epoch 2844 loss 2.929344\n",
      "Epoch 2845 loss 2.929338\n",
      "Epoch 2846 loss 2.929332\n",
      "Epoch 2847 loss 2.929328\n",
      "Epoch 2848 loss 2.929321\n",
      "Epoch 2849 loss 2.929316\n",
      "Epoch 2850 loss 2.929309\n",
      "Epoch 2851 loss 2.929304\n",
      "Epoch 2852 loss 2.929300\n",
      "Epoch 2853 loss 2.929293\n",
      "Epoch 2854 loss 2.929288\n",
      "Epoch 2855 loss 2.929282\n",
      "Epoch 2856 loss 2.929277\n",
      "Epoch 2857 loss 2.929271\n",
      "Epoch 2858 loss 2.929266\n",
      "Epoch 2859 loss 2.929260\n",
      "Epoch 2860 loss 2.929255\n",
      "Epoch 2861 loss 2.929250\n",
      "Epoch 2862 loss 2.929244\n",
      "Epoch 2863 loss 2.929237\n",
      "Epoch 2864 loss 2.929234\n",
      "Epoch 2865 loss 2.929227\n",
      "Epoch 2866 loss 2.929222\n",
      "Epoch 2867 loss 2.929216\n",
      "Epoch 2868 loss 2.929212\n",
      "Epoch 2869 loss 2.929207\n",
      "Epoch 2870 loss 2.929202\n",
      "Epoch 2871 loss 2.929195\n",
      "Epoch 2872 loss 2.929191\n",
      "Epoch 2873 loss 2.929184\n",
      "Epoch 2874 loss 2.929180\n",
      "Epoch 2875 loss 2.929175\n",
      "Epoch 2876 loss 2.929170\n",
      "Epoch 2877 loss 2.929166\n",
      "Epoch 2878 loss 2.929160\n",
      "Epoch 2879 loss 2.929155\n",
      "Epoch 2880 loss 2.929150\n",
      "Epoch 2881 loss 2.929144\n",
      "Epoch 2882 loss 2.929138\n",
      "Epoch 2883 loss 2.929133\n",
      "Epoch 2884 loss 2.929127\n",
      "Epoch 2885 loss 2.929122\n",
      "Epoch 2886 loss 2.929118\n",
      "Epoch 2887 loss 2.929113\n",
      "Epoch 2888 loss 2.929108\n",
      "Epoch 2889 loss 2.929103\n",
      "Epoch 2890 loss 2.929099\n",
      "Epoch 2891 loss 2.929093\n",
      "Epoch 2892 loss 2.929090\n",
      "Epoch 2893 loss 2.929084\n",
      "Epoch 2894 loss 2.929079\n",
      "Epoch 2895 loss 2.929075\n",
      "Epoch 2896 loss 2.929069\n",
      "Epoch 2897 loss 2.929065\n",
      "Epoch 2898 loss 2.929059\n",
      "Epoch 2899 loss 2.929054\n",
      "Epoch 2900 loss 2.929050\n",
      "Epoch 2901 loss 2.929044\n",
      "Epoch 2902 loss 2.929040\n",
      "Epoch 2903 loss 2.929036\n",
      "Epoch 2904 loss 2.929031\n",
      "Epoch 2905 loss 2.929025\n",
      "Epoch 2906 loss 2.929021\n",
      "Epoch 2907 loss 2.929017\n",
      "Epoch 2908 loss 2.929012\n",
      "Epoch 2909 loss 2.929006\n",
      "Epoch 2910 loss 2.929002\n",
      "Epoch 2911 loss 2.928999\n",
      "Epoch 2912 loss 2.928994\n",
      "Epoch 2913 loss 2.928988\n",
      "Epoch 2914 loss 2.928984\n",
      "Epoch 2915 loss 2.928980\n",
      "Epoch 2916 loss 2.928976\n",
      "Epoch 2917 loss 2.928971\n",
      "Epoch 2918 loss 2.928967\n",
      "Epoch 2919 loss 2.928962\n",
      "Epoch 2920 loss 2.928958\n",
      "Epoch 2921 loss 2.928953\n",
      "Epoch 2922 loss 2.928947\n",
      "Epoch 2923 loss 2.928946\n",
      "Epoch 2924 loss 2.928941\n",
      "Epoch 2925 loss 2.928935\n",
      "Epoch 2926 loss 2.928932\n",
      "Epoch 2927 loss 2.928926\n",
      "Epoch 2928 loss 2.928923\n",
      "Epoch 2929 loss 2.928919\n",
      "Epoch 2930 loss 2.928915\n",
      "Epoch 2931 loss 2.928909\n",
      "Epoch 2932 loss 2.928904\n",
      "Epoch 2933 loss 2.928902\n",
      "Epoch 2934 loss 2.928897\n",
      "Epoch 2935 loss 2.928893\n",
      "Epoch 2936 loss 2.928887\n",
      "Epoch 2937 loss 2.928883\n",
      "Epoch 2938 loss 2.928880\n",
      "Epoch 2939 loss 2.928877\n",
      "Epoch 2940 loss 2.928871\n",
      "Epoch 2941 loss 2.928867\n",
      "Epoch 2942 loss 2.928864\n",
      "Epoch 2943 loss 2.928860\n",
      "Epoch 2944 loss 2.928855\n",
      "Epoch 2945 loss 2.928850\n",
      "Epoch 2946 loss 2.928845\n",
      "Epoch 2947 loss 2.928843\n",
      "Epoch 2948 loss 2.928838\n",
      "Epoch 2949 loss 2.928833\n",
      "Epoch 2950 loss 2.928830\n",
      "Epoch 2951 loss 2.928826\n",
      "Epoch 2952 loss 2.928822\n",
      "Epoch 2953 loss 2.928818\n",
      "Epoch 2954 loss 2.928815\n",
      "Epoch 2955 loss 2.928811\n",
      "Epoch 2956 loss 2.928805\n",
      "Epoch 2957 loss 2.928801\n",
      "Epoch 2958 loss 2.928799\n",
      "Epoch 2959 loss 2.928795\n",
      "Epoch 2960 loss 2.928789\n",
      "Epoch 2961 loss 2.928789\n",
      "Epoch 2962 loss 2.928783\n",
      "Epoch 2963 loss 2.928779\n",
      "Epoch 2964 loss 2.928775\n",
      "Epoch 2965 loss 2.928771\n",
      "Epoch 2966 loss 2.928767\n",
      "Epoch 2967 loss 2.928765\n",
      "Epoch 2968 loss 2.928761\n",
      "Epoch 2969 loss 2.928758\n",
      "Epoch 2970 loss 2.928752\n",
      "Epoch 2971 loss 2.928750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2972 loss 2.928745\n",
      "Epoch 2973 loss 2.928741\n",
      "Epoch 2974 loss 2.928737\n",
      "Epoch 2975 loss 2.928735\n",
      "Epoch 2976 loss 2.928730\n",
      "Epoch 2977 loss 2.928727\n",
      "Epoch 2978 loss 2.928723\n",
      "Epoch 2979 loss 2.928719\n",
      "Epoch 2980 loss 2.928716\n",
      "Epoch 2981 loss 2.928712\n",
      "Epoch 2982 loss 2.928708\n",
      "Epoch 2983 loss 2.928705\n",
      "Epoch 2984 loss 2.928700\n",
      "Epoch 2985 loss 2.928698\n",
      "Epoch 2986 loss 2.928695\n",
      "Epoch 2987 loss 2.928690\n",
      "Epoch 2988 loss 2.928687\n",
      "Epoch 2989 loss 2.928684\n",
      "Epoch 2990 loss 2.928679\n",
      "Epoch 2991 loss 2.928677\n",
      "Epoch 2992 loss 2.928673\n",
      "Epoch 2993 loss 2.928669\n",
      "Epoch 2994 loss 2.928666\n",
      "Epoch 2995 loss 2.928662\n",
      "Epoch 2996 loss 2.928660\n",
      "Epoch 2997 loss 2.928656\n",
      "Epoch 2998 loss 2.928651\n",
      "Epoch 2999 loss 2.928648\n",
      "Epoch 3000 loss 2.928646\n",
      "Epoch 3001 loss 2.928643\n",
      "Epoch 3002 loss 2.928638\n",
      "Epoch 3003 loss 2.928635\n",
      "Epoch 3004 loss 2.928632\n",
      "Epoch 3005 loss 2.928629\n",
      "Epoch 3006 loss 2.928625\n",
      "Epoch 3007 loss 2.928621\n",
      "Epoch 3008 loss 2.928617\n",
      "Epoch 3009 loss 2.928616\n",
      "Epoch 3010 loss 2.928612\n",
      "Epoch 3011 loss 2.928608\n",
      "Epoch 3012 loss 2.928604\n",
      "Epoch 3013 loss 2.928601\n",
      "Epoch 3014 loss 2.928599\n",
      "Epoch 3015 loss 2.928595\n",
      "Epoch 3016 loss 2.928592\n",
      "Epoch 3017 loss 2.928588\n",
      "Epoch 3018 loss 2.928586\n",
      "Epoch 3019 loss 2.928583\n",
      "Epoch 3020 loss 2.928580\n",
      "Epoch 3021 loss 2.928576\n",
      "Epoch 3022 loss 2.928574\n",
      "Epoch 3023 loss 2.928569\n",
      "Epoch 3024 loss 2.928567\n",
      "Epoch 3025 loss 2.928564\n",
      "Epoch 3026 loss 2.928560\n",
      "Epoch 3027 loss 2.928557\n",
      "Epoch 3028 loss 2.928555\n",
      "Epoch 3029 loss 2.928551\n",
      "Epoch 3030 loss 2.928548\n",
      "Epoch 3031 loss 2.928545\n",
      "Epoch 3032 loss 2.928543\n",
      "Epoch 3033 loss 2.928539\n",
      "Epoch 3034 loss 2.928536\n",
      "Epoch 3035 loss 2.928532\n",
      "Epoch 3036 loss 2.928531\n",
      "Epoch 3037 loss 2.928528\n",
      "Epoch 3038 loss 2.928524\n",
      "Epoch 3039 loss 2.928521\n",
      "Epoch 3040 loss 2.928519\n",
      "Epoch 3041 loss 2.928514\n",
      "Epoch 3042 loss 2.928512\n",
      "Epoch 3043 loss 2.928509\n",
      "Epoch 3044 loss 2.928505\n",
      "Epoch 3045 loss 2.928503\n",
      "Epoch 3046 loss 2.928500\n",
      "Epoch 3047 loss 2.928498\n",
      "Epoch 3048 loss 2.928495\n",
      "Epoch 3049 loss 2.928491\n",
      "Epoch 3050 loss 2.928488\n",
      "Epoch 3051 loss 2.928486\n",
      "Epoch 3052 loss 2.928484\n",
      "Epoch 3053 loss 2.928480\n",
      "Epoch 3054 loss 2.928477\n",
      "Epoch 3055 loss 2.928475\n",
      "Epoch 3056 loss 2.928473\n",
      "Epoch 3057 loss 2.928469\n",
      "Epoch 3058 loss 2.928468\n",
      "Epoch 3059 loss 2.928463\n",
      "Epoch 3060 loss 2.928460\n",
      "Epoch 3061 loss 2.928458\n",
      "Epoch 3062 loss 2.928456\n",
      "Epoch 3063 loss 2.928452\n",
      "Epoch 3064 loss 2.928450\n",
      "Epoch 3065 loss 2.928447\n",
      "Epoch 3066 loss 2.928443\n",
      "Epoch 3067 loss 2.928443\n",
      "Epoch 3068 loss 2.928440\n",
      "Epoch 3069 loss 2.928435\n",
      "Epoch 3070 loss 2.928436\n",
      "Epoch 3071 loss 2.928430\n",
      "Epoch 3072 loss 2.928428\n",
      "Epoch 3073 loss 2.928426\n",
      "Epoch 3074 loss 2.928423\n",
      "Epoch 3075 loss 2.928421\n",
      "Epoch 3076 loss 2.928417\n",
      "Epoch 3077 loss 2.928415\n",
      "Epoch 3078 loss 2.928411\n",
      "Epoch 3079 loss 2.928410\n",
      "Epoch 3080 loss 2.928407\n",
      "Epoch 3081 loss 2.928404\n",
      "Epoch 3082 loss 2.928402\n",
      "Epoch 3083 loss 2.928399\n",
      "Epoch 3084 loss 2.928396\n",
      "Epoch 3085 loss 2.928396\n",
      "Epoch 3086 loss 2.928392\n",
      "Epoch 3087 loss 2.928389\n",
      "Epoch 3088 loss 2.928386\n",
      "Epoch 3089 loss 2.928383\n",
      "Epoch 3090 loss 2.928383\n",
      "Epoch 3091 loss 2.928379\n",
      "Epoch 3092 loss 2.928378\n",
      "Epoch 3093 loss 2.928375\n",
      "Epoch 3094 loss 2.928372\n",
      "Epoch 3095 loss 2.928370\n",
      "Epoch 3096 loss 2.928368\n",
      "Epoch 3097 loss 2.928364\n",
      "Epoch 3098 loss 2.928362\n",
      "Epoch 3099 loss 2.928361\n",
      "Epoch 3100 loss 2.928357\n",
      "Epoch 3101 loss 2.928355\n",
      "Epoch 3102 loss 2.928353\n",
      "Epoch 3103 loss 2.928349\n",
      "Epoch 3104 loss 2.928348\n",
      "Epoch 3105 loss 2.928345\n",
      "Epoch 3106 loss 2.928343\n",
      "Epoch 3107 loss 2.928340\n",
      "Epoch 3108 loss 2.928339\n",
      "Epoch 3109 loss 2.928337\n",
      "Epoch 3110 loss 2.928333\n",
      "Epoch 3111 loss 2.928332\n",
      "Epoch 3112 loss 2.928328\n",
      "Epoch 3113 loss 2.928329\n",
      "Epoch 3114 loss 2.928324\n",
      "Epoch 3115 loss 2.928323\n",
      "Epoch 3116 loss 2.928320\n",
      "Epoch 3117 loss 2.928318\n",
      "Epoch 3118 loss 2.928315\n",
      "Epoch 3119 loss 2.928313\n",
      "Epoch 3120 loss 2.928311\n",
      "Epoch 3121 loss 2.928308\n",
      "Epoch 3122 loss 2.928306\n",
      "Epoch 3123 loss 2.928304\n",
      "Epoch 3124 loss 2.928303\n",
      "Epoch 3125 loss 2.928300\n",
      "Epoch 3126 loss 2.928296\n",
      "Epoch 3127 loss 2.928295\n",
      "Epoch 3128 loss 2.928292\n",
      "Epoch 3129 loss 2.928292\n",
      "Epoch 3130 loss 2.928288\n",
      "Epoch 3131 loss 2.928287\n",
      "Epoch 3132 loss 2.928285\n",
      "Epoch 3133 loss 2.928282\n",
      "Epoch 3134 loss 2.928279\n",
      "Epoch 3135 loss 2.928276\n",
      "Epoch 3136 loss 2.928275\n",
      "Epoch 3137 loss 2.928273\n",
      "Epoch 3138 loss 2.928272\n",
      "Epoch 3139 loss 2.928268\n",
      "Epoch 3140 loss 2.928267\n",
      "Epoch 3141 loss 2.928265\n",
      "Epoch 3142 loss 2.928263\n",
      "Epoch 3143 loss 2.928260\n",
      "Epoch 3144 loss 2.928259\n",
      "Epoch 3145 loss 2.928256\n",
      "Epoch 3146 loss 2.928255\n",
      "Epoch 3147 loss 2.928252\n",
      "Epoch 3148 loss 2.928251\n",
      "Epoch 3149 loss 2.928248\n",
      "Epoch 3150 loss 2.928246\n",
      "Epoch 3151 loss 2.928245\n",
      "Epoch 3152 loss 2.928242\n",
      "Epoch 3153 loss 2.928240\n",
      "Epoch 3154 loss 2.928236\n",
      "Epoch 3155 loss 2.928236\n",
      "Epoch 3156 loss 2.928233\n",
      "Epoch 3157 loss 2.928231\n",
      "Epoch 3158 loss 2.928230\n",
      "Epoch 3159 loss 2.928227\n",
      "Epoch 3160 loss 2.928226\n",
      "Epoch 3161 loss 2.928225\n",
      "Epoch 3162 loss 2.928223\n",
      "Epoch 3163 loss 2.928219\n",
      "Epoch 3164 loss 2.928218\n",
      "Epoch 3165 loss 2.928216\n",
      "Epoch 3166 loss 2.928215\n",
      "Epoch 3167 loss 2.928212\n",
      "Epoch 3168 loss 2.928211\n",
      "Epoch 3169 loss 2.928210\n",
      "Epoch 3170 loss 2.928206\n",
      "Epoch 3171 loss 2.928205\n",
      "Epoch 3172 loss 2.928204\n",
      "Epoch 3173 loss 2.928202\n",
      "Epoch 3174 loss 2.928200\n",
      "Epoch 3175 loss 2.928196\n",
      "Epoch 3176 loss 2.928195\n",
      "Epoch 3177 loss 2.928195\n",
      "Epoch 3178 loss 2.928192\n",
      "Epoch 3179 loss 2.928190\n",
      "Epoch 3180 loss 2.928188\n",
      "Epoch 3181 loss 2.928186\n",
      "Epoch 3182 loss 2.928185\n",
      "Epoch 3183 loss 2.928184\n",
      "Epoch 3184 loss 2.928182\n",
      "Epoch 3185 loss 2.928180\n",
      "Epoch 3186 loss 2.928178\n",
      "Epoch 3187 loss 2.928175\n",
      "Epoch 3188 loss 2.928172\n",
      "Epoch 3189 loss 2.928170\n",
      "Epoch 3190 loss 2.928170\n",
      "Epoch 3191 loss 2.928169\n",
      "Epoch 3192 loss 2.928167\n",
      "Epoch 3193 loss 2.928164\n",
      "Epoch 3194 loss 2.928164\n",
      "Epoch 3195 loss 2.928162\n",
      "Epoch 3196 loss 2.928160\n",
      "Epoch 3197 loss 2.928158\n",
      "Epoch 3198 loss 2.928158\n",
      "Epoch 3199 loss 2.928154\n",
      "Epoch 3200 loss 2.928152\n",
      "Epoch 3201 loss 2.928149\n",
      "Epoch 3202 loss 2.928150\n",
      "Epoch 3203 loss 2.928147\n",
      "Epoch 3204 loss 2.928146\n",
      "Epoch 3205 loss 2.928144\n",
      "Epoch 3206 loss 2.928142\n",
      "Epoch 3207 loss 2.928140\n",
      "Epoch 3208 loss 2.928139\n",
      "Epoch 3209 loss 2.928137\n",
      "Epoch 3210 loss 2.928135\n",
      "Epoch 3211 loss 2.928135\n",
      "Epoch 3212 loss 2.928133\n",
      "Epoch 3213 loss 2.928131\n",
      "Epoch 3214 loss 2.928130\n",
      "Epoch 3215 loss 2.928125\n",
      "Epoch 3216 loss 2.928125\n",
      "Epoch 3217 loss 2.928124\n",
      "Epoch 3218 loss 2.928121\n",
      "Epoch 3219 loss 2.928121\n",
      "Epoch 3220 loss 2.928120\n",
      "Epoch 3221 loss 2.928118\n",
      "Epoch 3222 loss 2.928117\n",
      "Epoch 3223 loss 2.928115\n",
      "Epoch 3224 loss 2.928113\n",
      "Epoch 3225 loss 2.928110\n",
      "Epoch 3226 loss 2.928109\n",
      "Epoch 3227 loss 2.928108\n",
      "Epoch 3228 loss 2.928104\n",
      "Epoch 3229 loss 2.928105\n",
      "Epoch 3230 loss 2.928104\n",
      "Epoch 3231 loss 2.928102\n",
      "Epoch 3232 loss 2.928101\n",
      "Epoch 3233 loss 2.928098\n",
      "Epoch 3234 loss 2.928097\n",
      "Epoch 3235 loss 2.928095\n",
      "Epoch 3236 loss 2.928094\n",
      "Epoch 3237 loss 2.928093\n",
      "Epoch 3238 loss 2.928091\n",
      "Epoch 3239 loss 2.928090\n",
      "Epoch 3240 loss 2.928088\n",
      "Epoch 3241 loss 2.928086\n",
      "Epoch 3242 loss 2.928085\n",
      "Epoch 3243 loss 2.928084\n",
      "Epoch 3244 loss 2.928082\n",
      "Epoch 3245 loss 2.928080\n",
      "Epoch 3246 loss 2.928079\n",
      "Epoch 3247 loss 2.928076\n",
      "Epoch 3248 loss 2.928076\n",
      "Epoch 3249 loss 2.928075\n",
      "Epoch 3250 loss 2.928072\n",
      "Epoch 3251 loss 2.928072\n",
      "Epoch 3252 loss 2.928071\n",
      "Epoch 3253 loss 2.928068\n",
      "Epoch 3254 loss 2.928068\n",
      "Epoch 3255 loss 2.928066\n",
      "Epoch 3256 loss 2.928065\n",
      "Epoch 3257 loss 2.928063\n",
      "Epoch 3258 loss 2.928061\n",
      "Epoch 3259 loss 2.928061\n",
      "Epoch 3260 loss 2.928057\n",
      "Epoch 3261 loss 2.928058\n",
      "Epoch 3262 loss 2.928056\n",
      "Epoch 3263 loss 2.928055\n",
      "Epoch 3264 loss 2.928052\n",
      "Epoch 3265 loss 2.928053\n",
      "Epoch 3266 loss 2.928051\n",
      "Epoch 3267 loss 2.928050\n",
      "Epoch 3268 loss 2.928047\n",
      "Epoch 3269 loss 2.928046\n",
      "Epoch 3270 loss 2.928046\n",
      "Epoch 3271 loss 2.928044\n",
      "Epoch 3272 loss 2.928042\n",
      "Epoch 3273 loss 2.928040\n",
      "Epoch 3274 loss 2.928040\n",
      "Epoch 3275 loss 2.928037\n",
      "Epoch 3276 loss 2.928036\n",
      "Epoch 3277 loss 2.928037\n",
      "Epoch 3278 loss 2.928034\n",
      "Epoch 3279 loss 2.928034\n",
      "Epoch 3280 loss 2.928031\n",
      "Epoch 3281 loss 2.928032\n",
      "Epoch 3282 loss 2.928029\n",
      "Epoch 3283 loss 2.928027\n",
      "Epoch 3284 loss 2.928026\n",
      "Epoch 3285 loss 2.928025\n",
      "Epoch 3286 loss 2.928024\n",
      "Epoch 3287 loss 2.928023\n",
      "Epoch 3288 loss 2.928022\n",
      "Epoch 3289 loss 2.928021\n",
      "Epoch 3290 loss 2.928019\n",
      "Epoch 3291 loss 2.928018\n",
      "Epoch 3292 loss 2.928017\n",
      "Epoch 3293 loss 2.928015\n",
      "Epoch 3294 loss 2.928013\n",
      "Epoch 3295 loss 2.928012\n",
      "Epoch 3296 loss 2.928011\n",
      "Epoch 3297 loss 2.928009\n",
      "Epoch 3298 loss 2.928009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3299 loss 2.928006\n",
      "Epoch 3300 loss 2.928007\n",
      "Epoch 3301 loss 2.928007\n",
      "Epoch 3302 loss 2.928004\n",
      "Epoch 3303 loss 2.928002\n",
      "Epoch 3304 loss 2.928001\n",
      "Epoch 3305 loss 2.928000\n",
      "Epoch 3306 loss 2.928000\n",
      "Epoch 3307 loss 2.927998\n",
      "Epoch 3308 loss 2.927995\n",
      "Epoch 3309 loss 2.927995\n",
      "Epoch 3310 loss 2.927994\n",
      "Epoch 3311 loss 2.927994\n",
      "Epoch 3312 loss 2.927992\n",
      "Epoch 3313 loss 2.927992\n",
      "Epoch 3314 loss 2.927990\n",
      "Epoch 3315 loss 2.927989\n",
      "Epoch 3316 loss 2.927987\n",
      "Epoch 3317 loss 2.927986\n",
      "Epoch 3318 loss 2.927985\n",
      "Epoch 3319 loss 2.927983\n",
      "Epoch 3320 loss 2.927983\n",
      "Epoch 3321 loss 2.927981\n",
      "Epoch 3322 loss 2.927980\n",
      "Epoch 3323 loss 2.927979\n",
      "Epoch 3324 loss 2.927978\n",
      "Epoch 3325 loss 2.927977\n",
      "Epoch 3326 loss 2.927975\n",
      "Epoch 3327 loss 2.927973\n",
      "Epoch 3328 loss 2.927973\n",
      "Epoch 3329 loss 2.927974\n",
      "Epoch 3330 loss 2.927971\n",
      "Epoch 3331 loss 2.927972\n",
      "Epoch 3332 loss 2.927969\n",
      "Epoch 3333 loss 2.927969\n",
      "Epoch 3334 loss 2.927967\n",
      "Epoch 3335 loss 2.927967\n",
      "Epoch 3336 loss 2.927963\n",
      "Epoch 3337 loss 2.927963\n",
      "Epoch 3338 loss 2.927962\n",
      "Epoch 3339 loss 2.927962\n",
      "Epoch 3340 loss 2.927961\n",
      "Epoch 3341 loss 2.927960\n",
      "Epoch 3342 loss 2.927959\n",
      "Epoch 3343 loss 2.927958\n",
      "Epoch 3344 loss 2.927956\n",
      "Epoch 3345 loss 2.927955\n",
      "Epoch 3346 loss 2.927954\n",
      "Epoch 3347 loss 2.927953\n",
      "Epoch 3348 loss 2.927953\n",
      "Epoch 3349 loss 2.927950\n",
      "Epoch 3350 loss 2.927950\n",
      "Epoch 3351 loss 2.927948\n",
      "Epoch 3352 loss 2.927947\n",
      "Epoch 3353 loss 2.927948\n",
      "Epoch 3354 loss 2.927945\n",
      "Epoch 3355 loss 2.927944\n",
      "Epoch 3356 loss 2.927944\n",
      "Epoch 3357 loss 2.927944\n",
      "Epoch 3358 loss 2.927942\n",
      "Epoch 3359 loss 2.927941\n",
      "Epoch 3360 loss 2.927940\n",
      "Epoch 3361 loss 2.927938\n",
      "Epoch 3362 loss 2.927938\n",
      "Epoch 3363 loss 2.927936\n",
      "Epoch 3364 loss 2.927936\n",
      "Epoch 3365 loss 2.927937\n",
      "Epoch 3366 loss 2.927934\n",
      "Epoch 3367 loss 2.927934\n",
      "Epoch 3368 loss 2.927933\n",
      "Epoch 3369 loss 2.927930\n",
      "Epoch 3370 loss 2.927929\n",
      "Epoch 3371 loss 2.927931\n",
      "Epoch 3372 loss 2.927929\n",
      "Epoch 3373 loss 2.927927\n",
      "Epoch 3374 loss 2.927926\n",
      "Epoch 3375 loss 2.927925\n",
      "Epoch 3376 loss 2.927924\n",
      "Epoch 3377 loss 2.927922\n",
      "Epoch 3378 loss 2.927924\n",
      "Epoch 3379 loss 2.927922\n",
      "Epoch 3380 loss 2.927921\n",
      "Epoch 3381 loss 2.927920\n",
      "Epoch 3382 loss 2.927918\n",
      "Epoch 3383 loss 2.927917\n",
      "Epoch 3384 loss 2.927917\n",
      "Epoch 3385 loss 2.927915\n",
      "Epoch 3386 loss 2.927916\n",
      "Epoch 3387 loss 2.927914\n",
      "Epoch 3388 loss 2.927914\n",
      "Epoch 3389 loss 2.927912\n",
      "Epoch 3390 loss 2.927913\n",
      "Epoch 3391 loss 2.927911\n",
      "Epoch 3392 loss 2.927910\n",
      "Epoch 3393 loss 2.927909\n",
      "Epoch 3394 loss 2.927908\n",
      "Epoch 3395 loss 2.927907\n",
      "Epoch 3396 loss 2.927906\n",
      "Epoch 3397 loss 2.927905\n",
      "Epoch 3398 loss 2.927905\n",
      "Epoch 3399 loss 2.927904\n",
      "Epoch 3400 loss 2.927902\n",
      "Epoch 3401 loss 2.927902\n",
      "Epoch 3402 loss 2.927902\n",
      "Epoch 3403 loss 2.927899\n",
      "Epoch 3404 loss 2.927899\n",
      "Epoch 3405 loss 2.927898\n",
      "Epoch 3406 loss 2.927899\n",
      "Epoch 3407 loss 2.927896\n",
      "Epoch 3408 loss 2.927895\n",
      "Epoch 3409 loss 2.927896\n",
      "Epoch 3410 loss 2.927894\n",
      "Epoch 3411 loss 2.927892\n",
      "Epoch 3412 loss 2.927893\n",
      "Epoch 3413 loss 2.927891\n",
      "Epoch 3414 loss 2.927891\n",
      "Epoch 3415 loss 2.927890\n",
      "Epoch 3416 loss 2.927891\n",
      "Epoch 3417 loss 2.927888\n",
      "Epoch 3418 loss 2.927888\n",
      "Epoch 3419 loss 2.927886\n",
      "Epoch 3420 loss 2.927887\n",
      "Epoch 3421 loss 2.927885\n",
      "Epoch 3422 loss 2.927884\n",
      "Epoch 3423 loss 2.927883\n",
      "Epoch 3424 loss 2.927881\n",
      "Epoch 3425 loss 2.927881\n",
      "Epoch 3426 loss 2.927880\n",
      "Epoch 3427 loss 2.927880\n",
      "Epoch 3428 loss 2.927879\n",
      "Epoch 3429 loss 2.927878\n",
      "Epoch 3430 loss 2.927877\n",
      "Epoch 3431 loss 2.927876\n",
      "Epoch 3432 loss 2.927876\n",
      "Epoch 3433 loss 2.927875\n",
      "Epoch 3434 loss 2.927875\n",
      "Epoch 3435 loss 2.927875\n",
      "Epoch 3436 loss 2.927873\n",
      "Epoch 3437 loss 2.927872\n",
      "Epoch 3438 loss 2.927870\n",
      "Epoch 3439 loss 2.927871\n",
      "Epoch 3440 loss 2.927871\n",
      "Epoch 3441 loss 2.927869\n",
      "Epoch 3442 loss 2.927869\n",
      "Epoch 3443 loss 2.927866\n",
      "Epoch 3444 loss 2.927865\n",
      "Epoch 3445 loss 2.927866\n",
      "Epoch 3446 loss 2.927866\n",
      "Epoch 3447 loss 2.927864\n",
      "Epoch 3448 loss 2.927863\n",
      "Epoch 3449 loss 2.927863\n",
      "Epoch 3450 loss 2.927862\n",
      "Epoch 3451 loss 2.927863\n",
      "Epoch 3452 loss 2.927860\n",
      "Epoch 3453 loss 2.927860\n",
      "Epoch 3454 loss 2.927860\n",
      "Epoch 3455 loss 2.927859\n",
      "Epoch 3456 loss 2.927858\n",
      "Epoch 3457 loss 2.927858\n",
      "Epoch 3458 loss 2.927855\n",
      "Epoch 3459 loss 2.927857\n",
      "Epoch 3460 loss 2.927854\n",
      "Epoch 3461 loss 2.927855\n",
      "Epoch 3462 loss 2.927854\n",
      "Epoch 3463 loss 2.927854\n",
      "Epoch 3464 loss 2.927851\n",
      "Epoch 3465 loss 2.927853\n",
      "Epoch 3466 loss 2.927852\n",
      "Epoch 3467 loss 2.927850\n",
      "Epoch 3468 loss 2.927849\n",
      "Epoch 3469 loss 2.927849\n",
      "Epoch 3470 loss 2.927848\n",
      "Epoch 3471 loss 2.927848\n",
      "Epoch 3472 loss 2.927846\n",
      "Epoch 3473 loss 2.927846\n",
      "Epoch 3474 loss 2.927845\n",
      "Epoch 3475 loss 2.927844\n",
      "Epoch 3476 loss 2.927844\n",
      "Epoch 3477 loss 2.927844\n",
      "Epoch 3478 loss 2.927843\n",
      "Epoch 3479 loss 2.927842\n",
      "Epoch 3480 loss 2.927842\n",
      "Epoch 3481 loss 2.927840\n",
      "Epoch 3482 loss 2.927841\n",
      "Epoch 3483 loss 2.927839\n",
      "Epoch 3484 loss 2.927838\n",
      "Epoch 3485 loss 2.927839\n",
      "Epoch 3486 loss 2.927839\n",
      "Epoch 3487 loss 2.927837\n",
      "Epoch 3488 loss 2.927835\n",
      "Epoch 3489 loss 2.927837\n",
      "Epoch 3490 loss 2.927835\n",
      "Epoch 3491 loss 2.927834\n",
      "Epoch 3492 loss 2.927833\n",
      "Epoch 3493 loss 2.927833\n",
      "Epoch 3494 loss 2.927833\n",
      "Epoch 3495 loss 2.927832\n",
      "Epoch 3496 loss 2.927831\n",
      "Epoch 3497 loss 2.927830\n",
      "Epoch 3498 loss 2.927830\n",
      "Epoch 3499 loss 2.927830\n",
      "Epoch 3500 loss 2.927830\n",
      "Epoch 3501 loss 2.927828\n",
      "Epoch 3502 loss 2.927828\n",
      "Epoch 3503 loss 2.927827\n",
      "Epoch 3504 loss 2.927825\n",
      "Epoch 3505 loss 2.927827\n",
      "Epoch 3506 loss 2.927825\n",
      "Epoch 3507 loss 2.927824\n",
      "Epoch 3508 loss 2.927824\n",
      "Epoch 3509 loss 2.927824\n",
      "Epoch 3510 loss 2.927822\n",
      "Epoch 3511 loss 2.927822\n",
      "Epoch 3512 loss 2.927821\n",
      "Epoch 3513 loss 2.927820\n",
      "Epoch 3514 loss 2.927819\n",
      "Epoch 3515 loss 2.927821\n",
      "Epoch 3516 loss 2.927819\n",
      "Epoch 3517 loss 2.927819\n",
      "Epoch 3518 loss 2.927818\n",
      "Epoch 3519 loss 2.927818\n",
      "Epoch 3520 loss 2.927817\n",
      "Epoch 3521 loss 2.927816\n",
      "Epoch 3522 loss 2.927815\n",
      "Epoch 3523 loss 2.927816\n",
      "Epoch 3524 loss 2.927814\n",
      "Epoch 3525 loss 2.927813\n",
      "Epoch 3526 loss 2.927813\n",
      "Epoch 3527 loss 2.927812\n",
      "Epoch 3528 loss 2.927811\n",
      "Epoch 3529 loss 2.927811\n",
      "Epoch 3530 loss 2.927811\n",
      "Epoch 3531 loss 2.927810\n",
      "Epoch 3532 loss 2.927809\n",
      "Epoch 3533 loss 2.927810\n",
      "Epoch 3534 loss 2.927809\n",
      "Epoch 3535 loss 2.927808\n",
      "Epoch 3536 loss 2.927809\n",
      "Epoch 3537 loss 2.927806\n",
      "Epoch 3538 loss 2.927806\n",
      "Epoch 3539 loss 2.927805\n",
      "Epoch 3540 loss 2.927804\n",
      "Epoch 3541 loss 2.927804\n",
      "Epoch 3542 loss 2.927804\n",
      "Epoch 3543 loss 2.927805\n",
      "Epoch 3544 loss 2.927804\n",
      "Epoch 3545 loss 2.927804\n",
      "Epoch 3546 loss 2.927802\n",
      "Epoch 3547 loss 2.927802\n",
      "Epoch 3548 loss 2.927801\n",
      "Epoch 3549 loss 2.927801\n",
      "Epoch 3550 loss 2.927799\n",
      "Epoch 3551 loss 2.927801\n",
      "Epoch 3552 loss 2.927798\n",
      "Epoch 3553 loss 2.927798\n",
      "Epoch 3554 loss 2.927798\n",
      "Epoch 3555 loss 2.927798\n",
      "Epoch 3556 loss 2.927798\n",
      "Epoch 3557 loss 2.927796\n",
      "Epoch 3558 loss 2.927796\n",
      "Epoch 3559 loss 2.927796\n",
      "Epoch 3560 loss 2.927794\n",
      "Epoch 3561 loss 2.927796\n",
      "Epoch 3562 loss 2.927795\n",
      "Epoch 3563 loss 2.927794\n",
      "Epoch 3564 loss 2.927794\n",
      "Epoch 3565 loss 2.927791\n",
      "Epoch 3566 loss 2.927792\n",
      "Epoch 3567 loss 2.927792\n",
      "Epoch 3568 loss 2.927790\n",
      "Epoch 3569 loss 2.927790\n",
      "Epoch 3570 loss 2.927789\n",
      "Epoch 3571 loss 2.927790\n",
      "Epoch 3572 loss 2.927789\n",
      "Epoch 3573 loss 2.927790\n",
      "Epoch 3574 loss 2.927789\n",
      "Epoch 3575 loss 2.927787\n",
      "Epoch 3576 loss 2.927786\n",
      "Epoch 3577 loss 2.927788\n",
      "Epoch 3578 loss 2.927785\n",
      "Epoch 3579 loss 2.927785\n",
      "Epoch 3580 loss 2.927786\n",
      "Epoch 3581 loss 2.927785\n",
      "Epoch 3582 loss 2.927785\n",
      "Epoch 3583 loss 2.927784\n",
      "Epoch 3584 loss 2.927783\n",
      "Epoch 3585 loss 2.927783\n",
      "Epoch 3586 loss 2.927781\n",
      "Epoch 3587 loss 2.927782\n",
      "Epoch 3588 loss 2.927780\n",
      "Epoch 3589 loss 2.927781\n",
      "Epoch 3590 loss 2.927781\n",
      "Epoch 3591 loss 2.927780\n",
      "Epoch 3592 loss 2.927780\n",
      "Epoch 3593 loss 2.927778\n",
      "Epoch 3594 loss 2.927779\n",
      "Epoch 3595 loss 2.927778\n",
      "Epoch 3596 loss 2.927778\n",
      "Epoch 3597 loss 2.927778\n",
      "Epoch 3598 loss 2.927777\n",
      "Epoch 3599 loss 2.927776\n",
      "Epoch 3600 loss 2.927775\n",
      "Epoch 3601 loss 2.927775\n",
      "Epoch 3602 loss 2.927773\n",
      "Epoch 3603 loss 2.927775\n",
      "Epoch 3604 loss 2.927775\n",
      "Epoch 3605 loss 2.927774\n",
      "Epoch 3606 loss 2.927773\n",
      "Epoch 3607 loss 2.927773\n",
      "Epoch 3608 loss 2.927772\n",
      "Epoch 3609 loss 2.927772\n",
      "Epoch 3610 loss 2.927772\n",
      "Epoch 3611 loss 2.927770\n",
      "Epoch 3612 loss 2.927772\n",
      "Epoch 3613 loss 2.927772\n",
      "Epoch 3614 loss 2.927770\n",
      "Epoch 3615 loss 2.927770\n",
      "Epoch 3616 loss 2.927769\n",
      "Epoch 3617 loss 2.927768\n",
      "Epoch 3618 loss 2.927769\n",
      "Epoch 3619 loss 2.927768\n",
      "Epoch 3620 loss 2.927766\n",
      "Epoch 3621 loss 2.927767\n",
      "Epoch 3622 loss 2.927767\n",
      "Epoch 3623 loss 2.927765\n",
      "Epoch 3624 loss 2.927766\n",
      "Epoch 3625 loss 2.927765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3626 loss 2.927766\n",
      "Epoch 3627 loss 2.927764\n",
      "Epoch 3628 loss 2.927764\n",
      "Epoch 3629 loss 2.927764\n",
      "Epoch 3630 loss 2.927762\n",
      "Epoch 3631 loss 2.927763\n",
      "Epoch 3632 loss 2.927763\n",
      "Epoch 3633 loss 2.927762\n",
      "Epoch 3634 loss 2.927761\n",
      "Epoch 3635 loss 2.927762\n",
      "Epoch 3636 loss 2.927759\n",
      "Epoch 3637 loss 2.927761\n",
      "Epoch 3638 loss 2.927761\n",
      "Epoch 3639 loss 2.927760\n",
      "Epoch 3640 loss 2.927759\n",
      "Epoch 3641 loss 2.927758\n",
      "Epoch 3642 loss 2.927759\n",
      "Epoch 3643 loss 2.927757\n",
      "Epoch 3644 loss 2.927758\n",
      "Epoch 3645 loss 2.927757\n",
      "Epoch 3646 loss 2.927757\n",
      "Epoch 3647 loss 2.927757\n",
      "Epoch 3648 loss 2.927756\n",
      "Epoch 3649 loss 2.927758\n",
      "Epoch 3650 loss 2.927756\n",
      "Epoch 3651 loss 2.927756\n",
      "Epoch 3652 loss 2.927755\n",
      "Epoch 3653 loss 2.927755\n",
      "Epoch 3654 loss 2.927754\n",
      "Epoch 3655 loss 2.927754\n",
      "Epoch 3656 loss 2.927755\n",
      "Epoch 3657 loss 2.927753\n",
      "Epoch 3658 loss 2.927752\n",
      "Epoch 3659 loss 2.927754\n",
      "Epoch 3660 loss 2.927752\n",
      "Epoch 3661 loss 2.927751\n",
      "Epoch 3662 loss 2.927752\n",
      "Epoch 3663 loss 2.927750\n",
      "Epoch 3664 loss 2.927750\n",
      "Epoch 3665 loss 2.927752\n",
      "Epoch 3666 loss 2.927750\n",
      "Epoch 3667 loss 2.927750\n",
      "Epoch 3668 loss 2.927747\n",
      "Epoch 3669 loss 2.927749\n",
      "Epoch 3670 loss 2.927748\n",
      "Epoch 3671 loss 2.927748\n",
      "Epoch 3672 loss 2.927749\n",
      "Epoch 3673 loss 2.927747\n",
      "Epoch 3674 loss 2.927747\n",
      "Epoch 3675 loss 2.927748\n",
      "Epoch 3676 loss 2.927747\n",
      "Epoch 3677 loss 2.927746\n",
      "Epoch 3678 loss 2.927746\n",
      "Epoch 3679 loss 2.927745\n",
      "Epoch 3680 loss 2.927745\n",
      "Epoch 3681 loss 2.927744\n",
      "Epoch 3682 loss 2.927744\n",
      "Epoch 3683 loss 2.927743\n",
      "Epoch 3684 loss 2.927742\n",
      "Epoch 3685 loss 2.927743\n",
      "Epoch 3686 loss 2.927743\n",
      "Epoch 3687 loss 2.927744\n",
      "Epoch 3688 loss 2.927743\n",
      "Epoch 3689 loss 2.927742\n",
      "Epoch 3690 loss 2.927742\n",
      "Epoch 3691 loss 2.927742\n",
      "Epoch 3692 loss 2.927741\n",
      "Epoch 3693 loss 2.927741\n",
      "Epoch 3694 loss 2.927741\n",
      "Epoch 3695 loss 2.927742\n",
      "Epoch 3696 loss 2.927741\n",
      "Epoch 3697 loss 2.927740\n",
      "Epoch 3698 loss 2.927740\n",
      "Epoch 3699 loss 2.927738\n",
      "Epoch 3700 loss 2.927738\n",
      "Epoch 3701 loss 2.927738\n",
      "Epoch 3702 loss 2.927737\n",
      "Epoch 3703 loss 2.927737\n",
      "Epoch 3704 loss 2.927738\n",
      "Epoch 3705 loss 2.927738\n",
      "Epoch 3706 loss 2.927737\n",
      "Epoch 3707 loss 2.927737\n",
      "Epoch 3708 loss 2.927736\n",
      "Epoch 3709 loss 2.927735\n",
      "Epoch 3710 loss 2.927734\n",
      "Epoch 3711 loss 2.927735\n",
      "Epoch 3712 loss 2.927736\n",
      "Epoch 3713 loss 2.927734\n",
      "Epoch 3714 loss 2.927734\n",
      "Epoch 3715 loss 2.927733\n",
      "Epoch 3716 loss 2.927734\n",
      "Epoch 3717 loss 2.927733\n",
      "Epoch 3718 loss 2.927733\n",
      "Epoch 3719 loss 2.927733\n",
      "Epoch 3720 loss 2.927733\n",
      "Epoch 3721 loss 2.927731\n",
      "Epoch 3722 loss 2.927731\n",
      "Epoch 3723 loss 2.927732\n",
      "Epoch 3724 loss 2.927730\n",
      "Epoch 3725 loss 2.927730\n",
      "Epoch 3726 loss 2.927731\n",
      "Epoch 3727 loss 2.927730\n",
      "Epoch 3728 loss 2.927732\n",
      "Epoch 3729 loss 2.927732\n",
      "Epoch 3730 loss 2.927730\n",
      "Epoch 3731 loss 2.927728\n",
      "Epoch 3732 loss 2.927729\n",
      "Epoch 3733 loss 2.927730\n",
      "Epoch 3734 loss 2.927729\n",
      "Epoch 3735 loss 2.927728\n",
      "Epoch 3736 loss 2.927728\n",
      "Epoch 3737 loss 2.927728\n",
      "Epoch 3738 loss 2.927727\n",
      "Epoch 3739 loss 2.927728\n",
      "Epoch 3740 loss 2.927728\n",
      "Epoch 3741 loss 2.927727\n",
      "Epoch 3742 loss 2.927727\n",
      "Epoch 3743 loss 2.927726\n",
      "Epoch 3744 loss 2.927726\n",
      "Epoch 3745 loss 2.927725\n",
      "Epoch 3746 loss 2.927725\n",
      "Epoch 3747 loss 2.927725\n",
      "Epoch 3748 loss 2.927724\n",
      "Epoch 3749 loss 2.927724\n",
      "Epoch 3750 loss 2.927724\n",
      "Epoch 3751 loss 2.927725\n",
      "Epoch 3752 loss 2.927724\n",
      "Epoch 3753 loss 2.927724\n",
      "Epoch 3754 loss 2.927723\n",
      "Epoch 3755 loss 2.927723\n",
      "Epoch 3756 loss 2.927722\n",
      "Epoch 3757 loss 2.927722\n",
      "Epoch 3758 loss 2.927723\n",
      "Epoch 3759 loss 2.927722\n",
      "Epoch 3760 loss 2.927723\n",
      "Epoch 3761 loss 2.927721\n",
      "Epoch 3762 loss 2.927721\n",
      "Epoch 3763 loss 2.927720\n",
      "Epoch 3764 loss 2.927720\n",
      "Epoch 3765 loss 2.927719\n",
      "Epoch 3766 loss 2.927721\n",
      "Epoch 3767 loss 2.927719\n",
      "Epoch 3768 loss 2.927719\n",
      "Epoch 3769 loss 2.927719\n",
      "Epoch 3770 loss 2.927719\n",
      "Epoch 3771 loss 2.927718\n",
      "Epoch 3772 loss 2.927720\n",
      "Epoch 3773 loss 2.927718\n",
      "Epoch 3774 loss 2.927718\n",
      "Epoch 3775 loss 2.927717\n",
      "Epoch 3776 loss 2.927718\n",
      "Epoch 3777 loss 2.927717\n",
      "Epoch 3778 loss 2.927717\n",
      "Epoch 3779 loss 2.927716\n",
      "Epoch 3780 loss 2.927716\n",
      "Epoch 3781 loss 2.927717\n",
      "Epoch 3782 loss 2.927717\n",
      "Epoch 3783 loss 2.927716\n",
      "Epoch 3784 loss 2.927715\n",
      "Epoch 3785 loss 2.927715\n",
      "Epoch 3786 loss 2.927715\n",
      "Epoch 3787 loss 2.927715\n",
      "Epoch 3788 loss 2.927715\n",
      "Epoch 3789 loss 2.927715\n",
      "Epoch 3790 loss 2.927714\n",
      "Epoch 3791 loss 2.927714\n",
      "Epoch 3792 loss 2.927714\n",
      "Epoch 3793 loss 2.927713\n",
      "Epoch 3794 loss 2.927713\n",
      "Epoch 3795 loss 2.927714\n",
      "Epoch 3796 loss 2.927713\n",
      "Epoch 3797 loss 2.927712\n",
      "Epoch 3798 loss 2.927712\n",
      "Epoch 3799 loss 2.927712\n",
      "Epoch 3800 loss 2.927711\n",
      "Epoch 3801 loss 2.927711\n",
      "Epoch 3802 loss 2.927713\n",
      "Epoch 3803 loss 2.927711\n",
      "Epoch 3804 loss 2.927712\n",
      "Epoch 3805 loss 2.927711\n",
      "Epoch 3806 loss 2.927711\n",
      "Epoch 3807 loss 2.927711\n",
      "Epoch 3808 loss 2.927709\n",
      "Epoch 3809 loss 2.927711\n",
      "Epoch 3810 loss 2.927710\n",
      "Epoch 3811 loss 2.927708\n",
      "Epoch 3812 loss 2.927708\n",
      "Epoch 3813 loss 2.927709\n",
      "Epoch 3814 loss 2.927709\n",
      "Epoch 3815 loss 2.927710\n",
      "Epoch 3816 loss 2.927708\n",
      "Epoch 3817 loss 2.927708\n",
      "Epoch 3818 loss 2.927706\n",
      "Epoch 3819 loss 2.927707\n",
      "Epoch 3820 loss 2.927708\n",
      "Epoch 3821 loss 2.927707\n",
      "Epoch 3822 loss 2.927707\n",
      "Epoch 3823 loss 2.927707\n",
      "Epoch 3824 loss 2.927708\n",
      "Epoch 3825 loss 2.927708\n",
      "Epoch 3826 loss 2.927706\n",
      "Epoch 3827 loss 2.927707\n",
      "Epoch 3828 loss 2.927706\n",
      "Epoch 3829 loss 2.927706\n",
      "Epoch 3830 loss 2.927706\n",
      "Epoch 3831 loss 2.927705\n",
      "Epoch 3832 loss 2.927705\n",
      "Epoch 3833 loss 2.927705\n",
      "Epoch 3834 loss 2.927705\n",
      "Epoch 3835 loss 2.927705\n",
      "Epoch 3836 loss 2.927704\n",
      "Epoch 3837 loss 2.927703\n",
      "Epoch 3838 loss 2.927704\n",
      "Epoch 3839 loss 2.927704\n",
      "Epoch 3840 loss 2.927703\n",
      "Epoch 3841 loss 2.927702\n",
      "Epoch 3842 loss 2.927703\n",
      "Epoch 3843 loss 2.927702\n",
      "Epoch 3844 loss 2.927704\n",
      "Epoch 3845 loss 2.927702\n",
      "Epoch 3846 loss 2.927701\n",
      "Epoch 3847 loss 2.927703\n",
      "Epoch 3848 loss 2.927702\n",
      "Epoch 3849 loss 2.927701\n",
      "Epoch 3850 loss 2.927701\n",
      "Epoch 3851 loss 2.927703\n",
      "Epoch 3852 loss 2.927700\n",
      "Epoch 3853 loss 2.927701\n",
      "Epoch 3854 loss 2.927701\n",
      "Epoch 3855 loss 2.927700\n",
      "Epoch 3856 loss 2.927700\n",
      "Epoch 3857 loss 2.927700\n",
      "Epoch 3858 loss 2.927701\n",
      "Epoch 3859 loss 2.927700\n",
      "Epoch 3860 loss 2.927700\n",
      "Epoch 3861 loss 2.927700\n",
      "Epoch 3862 loss 2.927699\n",
      "Epoch 3863 loss 2.927698\n",
      "Epoch 3864 loss 2.927700\n",
      "Epoch 3865 loss 2.927697\n",
      "Epoch 3866 loss 2.927700\n",
      "Epoch 3867 loss 2.927700\n",
      "Epoch 3868 loss 2.927698\n",
      "Epoch 3869 loss 2.927697\n",
      "Epoch 3870 loss 2.927698\n",
      "Epoch 3871 loss 2.927696\n",
      "Epoch 3872 loss 2.927699\n",
      "Epoch 3873 loss 2.927697\n",
      "Epoch 3874 loss 2.927696\n",
      "Epoch 3875 loss 2.927699\n",
      "Epoch 3876 loss 2.927697\n",
      "Epoch 3877 loss 2.927696\n",
      "Epoch 3878 loss 2.927697\n",
      "Epoch 3879 loss 2.927696\n",
      "Epoch 3880 loss 2.927696\n",
      "Epoch 3881 loss 2.927696\n",
      "Epoch 3882 loss 2.927696\n",
      "Epoch 3883 loss 2.927695\n",
      "Epoch 3884 loss 2.927695\n",
      "Epoch 3885 loss 2.927696\n",
      "Epoch 3886 loss 2.927696\n",
      "Epoch 3887 loss 2.927695\n",
      "Epoch 3888 loss 2.927694\n",
      "Epoch 3889 loss 2.927694\n",
      "Epoch 3890 loss 2.927694\n",
      "Epoch 3891 loss 2.927693\n",
      "Epoch 3892 loss 2.927695\n",
      "Epoch 3893 loss 2.927695\n",
      "Epoch 3894 loss 2.927694\n",
      "Epoch 3895 loss 2.927695\n",
      "Epoch 3896 loss 2.927693\n",
      "Epoch 3897 loss 2.927693\n",
      "Epoch 3898 loss 2.927695\n",
      "Epoch 3899 loss 2.927693\n",
      "Epoch 3900 loss 2.927692\n",
      "Epoch 3901 loss 2.927694\n",
      "Epoch 3902 loss 2.927692\n",
      "Epoch 3903 loss 2.927693\n",
      "Epoch 3904 loss 2.927691\n",
      "Epoch 3905 loss 2.927692\n",
      "Epoch 3906 loss 2.927692\n",
      "Epoch 3907 loss 2.927692\n",
      "Epoch 3908 loss 2.927692\n",
      "Epoch 3909 loss 2.927692\n",
      "Epoch 3910 loss 2.927690\n",
      "Epoch 3911 loss 2.927692\n",
      "Epoch 3912 loss 2.927691\n",
      "Epoch 3913 loss 2.927691\n",
      "Epoch 3914 loss 2.927689\n",
      "Epoch 3915 loss 2.927691\n",
      "Epoch 3916 loss 2.927691\n",
      "Epoch 3917 loss 2.927689\n",
      "Epoch 3918 loss 2.927690\n",
      "Epoch 3919 loss 2.927690\n",
      "Epoch 3920 loss 2.927690\n",
      "Epoch 3921 loss 2.927690\n",
      "Epoch 3922 loss 2.927689\n",
      "Epoch 3923 loss 2.927688\n",
      "Epoch 3924 loss 2.927689\n",
      "Epoch 3925 loss 2.927688\n",
      "Epoch 3926 loss 2.927689\n",
      "Epoch 3927 loss 2.927689\n",
      "Epoch 3928 loss 2.927689\n",
      "Epoch 3929 loss 2.927688\n",
      "Epoch 3930 loss 2.927688\n",
      "Epoch 3931 loss 2.927688\n",
      "Epoch 3932 loss 2.927687\n",
      "Epoch 3933 loss 2.927689\n",
      "Epoch 3934 loss 2.927688\n",
      "Epoch 3935 loss 2.927687\n",
      "Epoch 3936 loss 2.927688\n",
      "Epoch 3937 loss 2.927686\n",
      "Epoch 3938 loss 2.927686\n",
      "Epoch 3939 loss 2.927686\n",
      "Epoch 3940 loss 2.927687\n",
      "Epoch 3941 loss 2.927687\n",
      "Epoch 3942 loss 2.927686\n",
      "Epoch 3943 loss 2.927687\n",
      "Epoch 3944 loss 2.927686\n",
      "Epoch 3945 loss 2.927685\n",
      "Epoch 3946 loss 2.927685\n",
      "Epoch 3947 loss 2.927686\n",
      "Epoch 3948 loss 2.927685\n",
      "Epoch 3949 loss 2.927686\n",
      "Epoch 3950 loss 2.927686\n",
      "Epoch 3951 loss 2.927686\n",
      "Epoch 3952 loss 2.927685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3953 loss 2.927686\n",
      "Epoch 3954 loss 2.927685\n",
      "Epoch 3955 loss 2.927685\n",
      "Epoch 3956 loss 2.927683\n",
      "Epoch 3957 loss 2.927684\n",
      "Epoch 3958 loss 2.927685\n",
      "Epoch 3959 loss 2.927684\n",
      "Epoch 3960 loss 2.927684\n",
      "Epoch 3961 loss 2.927684\n",
      "Epoch 3962 loss 2.927685\n",
      "Epoch 3963 loss 2.927683\n",
      "Epoch 3964 loss 2.927685\n",
      "Epoch 3965 loss 2.927684\n",
      "Epoch 3966 loss 2.927683\n",
      "Epoch 3967 loss 2.927683\n",
      "Epoch 3968 loss 2.927683\n",
      "Epoch 3969 loss 2.927682\n",
      "Epoch 3970 loss 2.927682\n",
      "Epoch 3971 loss 2.927684\n",
      "Epoch 3972 loss 2.927683\n",
      "Epoch 3973 loss 2.927684\n",
      "Epoch 3974 loss 2.927683\n",
      "Epoch 3975 loss 2.927682\n",
      "Epoch 3976 loss 2.927682\n",
      "Epoch 3977 loss 2.927682\n",
      "Epoch 3978 loss 2.927682\n",
      "Epoch 3979 loss 2.927681\n",
      "Epoch 3980 loss 2.927682\n",
      "Epoch 3981 loss 2.927681\n",
      "Epoch 3982 loss 2.927681\n",
      "Epoch 3983 loss 2.927682\n",
      "Epoch 3984 loss 2.927681\n",
      "Epoch 3985 loss 2.927681\n",
      "Epoch 3986 loss 2.927681\n",
      "Epoch 3987 loss 2.927680\n",
      "Epoch 3988 loss 2.927681\n",
      "Epoch 3989 loss 2.927681\n",
      "Epoch 3990 loss 2.927680\n",
      "Epoch 3991 loss 2.927682\n",
      "Epoch 3992 loss 2.927681\n",
      "Epoch 3993 loss 2.927680\n",
      "Epoch 3994 loss 2.927679\n",
      "Epoch 3995 loss 2.927680\n",
      "Epoch 3996 loss 2.927679\n",
      "Epoch 3997 loss 2.927680\n",
      "Epoch 3998 loss 2.927680\n",
      "Epoch 3999 loss 2.927679\n",
      "Epoch 4000 loss 2.927679\n",
      "Epoch 4001 loss 2.927679\n",
      "Epoch 4002 loss 2.927679\n",
      "Epoch 4003 loss 2.927679\n",
      "Epoch 4004 loss 2.927680\n",
      "Epoch 4005 loss 2.927681\n",
      "Epoch 4006 loss 2.927679\n",
      "Epoch 4007 loss 2.927679\n",
      "Epoch 4008 loss 2.927679\n",
      "Epoch 4009 loss 2.927679\n",
      "Epoch 4010 loss 2.927678\n",
      "Epoch 4011 loss 2.927679\n",
      "Epoch 4012 loss 2.927679\n",
      "Epoch 4013 loss 2.927677\n",
      "Epoch 4014 loss 2.927678\n",
      "Epoch 4015 loss 2.927677\n",
      "Epoch 4016 loss 2.927678\n",
      "Epoch 4017 loss 2.927677\n",
      "Epoch 4018 loss 2.927677\n",
      "Epoch 4019 loss 2.927676\n",
      "Epoch 4020 loss 2.927678\n",
      "Epoch 4021 loss 2.927677\n",
      "Epoch 4022 loss 2.927677\n",
      "Epoch 4023 loss 2.927678\n",
      "Epoch 4024 loss 2.927678\n",
      "Epoch 4025 loss 2.927677\n",
      "Epoch 4026 loss 2.927676\n",
      "Epoch 4027 loss 2.927676\n",
      "Epoch 4028 loss 2.927677\n",
      "Epoch 4029 loss 2.927676\n",
      "Epoch 4030 loss 2.927675\n",
      "Epoch 4031 loss 2.927676\n",
      "Epoch 4032 loss 2.927676\n",
      "Epoch 4033 loss 2.927675\n",
      "Epoch 4034 loss 2.927676\n",
      "Epoch 4035 loss 2.927676\n",
      "Epoch 4036 loss 2.927676\n",
      "Epoch 4037 loss 2.927677\n",
      "Epoch 4038 loss 2.927676\n",
      "Epoch 4039 loss 2.927675\n",
      "Epoch 4040 loss 2.927676\n",
      "Epoch 4041 loss 2.927675\n",
      "Epoch 4042 loss 2.927675\n",
      "Epoch 4043 loss 2.927675\n",
      "Epoch 4044 loss 2.927675\n",
      "Epoch 4045 loss 2.927675\n",
      "Epoch 4046 loss 2.927675\n",
      "Epoch 4047 loss 2.927675\n",
      "Epoch 4048 loss 2.927675\n",
      "Epoch 4049 loss 2.927673\n",
      "Epoch 4050 loss 2.927675\n",
      "Epoch 4051 loss 2.927675\n",
      "Epoch 4052 loss 2.927675\n",
      "Epoch 4053 loss 2.927673\n",
      "Epoch 4054 loss 2.927674\n",
      "Epoch 4055 loss 2.927675\n",
      "Epoch 4056 loss 2.927673\n",
      "Epoch 4057 loss 2.927673\n",
      "Epoch 4058 loss 2.927673\n",
      "Epoch 4059 loss 2.927675\n",
      "Epoch 4060 loss 2.927673\n",
      "Epoch 4061 loss 2.927673\n",
      "Epoch 4062 loss 2.927673\n",
      "Epoch 4063 loss 2.927673\n",
      "Epoch 4064 loss 2.927673\n",
      "Epoch 4065 loss 2.927672\n",
      "Epoch 4066 loss 2.927673\n",
      "Epoch 4067 loss 2.927672\n",
      "Epoch 4068 loss 2.927672\n",
      "Epoch 4069 loss 2.927673\n",
      "Epoch 4070 loss 2.927672\n",
      "Epoch 4071 loss 2.927672\n",
      "Epoch 4072 loss 2.927672\n",
      "Epoch 4073 loss 2.927672\n",
      "Epoch 4074 loss 2.927672\n",
      "Epoch 4075 loss 2.927672\n",
      "Epoch 4076 loss 2.927671\n",
      "Epoch 4077 loss 2.927671\n",
      "Epoch 4078 loss 2.927673\n",
      "Epoch 4079 loss 2.927671\n",
      "Epoch 4080 loss 2.927670\n",
      "Epoch 4081 loss 2.927672\n",
      "Epoch 4082 loss 2.927671\n",
      "Epoch 4083 loss 2.927673\n",
      "Epoch 4084 loss 2.927670\n",
      "Epoch 4085 loss 2.927670\n",
      "Epoch 4086 loss 2.927671\n",
      "Epoch 4087 loss 2.927672\n",
      "Epoch 4088 loss 2.927670\n",
      "Epoch 4089 loss 2.927670\n",
      "Epoch 4090 loss 2.927670\n",
      "Epoch 4091 loss 2.927671\n",
      "Epoch 4092 loss 2.927670\n",
      "Epoch 4093 loss 2.927671\n",
      "Epoch 4094 loss 2.927670\n",
      "Epoch 4095 loss 2.927670\n",
      "Epoch 4096 loss 2.927670\n",
      "Epoch 4097 loss 2.927670\n",
      "Epoch 4098 loss 2.927671\n",
      "Epoch 4099 loss 2.927670\n",
      "Epoch 4100 loss 2.927669\n",
      "Epoch 4101 loss 2.927669\n",
      "Epoch 4102 loss 2.927671\n",
      "Epoch 4103 loss 2.927670\n",
      "Epoch 4104 loss 2.927670\n",
      "Epoch 4105 loss 2.927670\n",
      "Epoch 4106 loss 2.927670\n",
      "Epoch 4107 loss 2.927670\n",
      "Epoch 4108 loss 2.927669\n",
      "Epoch 4109 loss 2.927668\n",
      "Epoch 4110 loss 2.927670\n",
      "Epoch 4111 loss 2.927669\n",
      "Epoch 4112 loss 2.927669\n",
      "Epoch 4113 loss 2.927669\n",
      "Epoch 4114 loss 2.927670\n",
      "Epoch 4115 loss 2.927669\n",
      "Epoch 4116 loss 2.927668\n",
      "Epoch 4117 loss 2.927667\n",
      "Epoch 4118 loss 2.927669\n",
      "Epoch 4119 loss 2.927668\n",
      "Epoch 4120 loss 2.927668\n",
      "Epoch 4121 loss 2.927669\n",
      "Epoch 4122 loss 2.927669\n",
      "Epoch 4123 loss 2.927668\n",
      "Epoch 4124 loss 2.927668\n",
      "Epoch 4125 loss 2.927668\n",
      "Epoch 4126 loss 2.927668\n",
      "Epoch 4127 loss 2.927667\n",
      "Epoch 4128 loss 2.927668\n",
      "Epoch 4129 loss 2.927667\n",
      "Epoch 4130 loss 2.927667\n",
      "Epoch 4131 loss 2.927667\n",
      "Epoch 4132 loss 2.927668\n",
      "Epoch 4133 loss 2.927666\n",
      "Epoch 4134 loss 2.927667\n",
      "Epoch 4135 loss 2.927667\n",
      "Epoch 4136 loss 2.927667\n",
      "Epoch 4137 loss 2.927667\n",
      "Epoch 4138 loss 2.927666\n",
      "Epoch 4139 loss 2.927669\n",
      "Epoch 4140 loss 2.927667\n",
      "Epoch 4141 loss 2.927666\n",
      "Epoch 4142 loss 2.927667\n",
      "Epoch 4143 loss 2.927665\n",
      "Epoch 4144 loss 2.927667\n",
      "Epoch 4145 loss 2.927666\n",
      "Epoch 4146 loss 2.927666\n",
      "Epoch 4147 loss 2.927667\n",
      "Epoch 4148 loss 2.927666\n",
      "Epoch 4149 loss 2.927667\n",
      "Epoch 4150 loss 2.927666\n",
      "Epoch 4151 loss 2.927666\n",
      "Epoch 4152 loss 2.927667\n",
      "Epoch 4153 loss 2.927666\n",
      "Epoch 4154 loss 2.927666\n",
      "Epoch 4155 loss 2.927666\n",
      "Epoch 4156 loss 2.927666\n",
      "Epoch 4157 loss 2.927666\n",
      "Epoch 4158 loss 2.927666\n",
      "Epoch 4159 loss 2.927665\n",
      "Epoch 4160 loss 2.927666\n",
      "Epoch 4161 loss 2.927665\n",
      "Epoch 4162 loss 2.927665\n",
      "Epoch 4163 loss 2.927666\n",
      "Epoch 4164 loss 2.927666\n",
      "Epoch 4165 loss 2.927665\n",
      "Epoch 4166 loss 2.927664\n",
      "Epoch 4167 loss 2.927666\n",
      "Epoch 4168 loss 2.927664\n",
      "Epoch 4169 loss 2.927665\n",
      "Epoch 4170 loss 2.927665\n",
      "Epoch 4171 loss 2.927664\n",
      "Epoch 4172 loss 2.927666\n",
      "Epoch 4173 loss 2.927665\n",
      "Epoch 4174 loss 2.927664\n",
      "Epoch 4175 loss 2.927665\n",
      "Epoch 4176 loss 2.927665\n",
      "Epoch 4177 loss 2.927665\n",
      "Epoch 4178 loss 2.927663\n",
      "Epoch 4179 loss 2.927665\n",
      "Epoch 4180 loss 2.927664\n",
      "Epoch 4181 loss 2.927664\n",
      "Epoch 4182 loss 2.927664\n",
      "Epoch 4183 loss 2.927663\n",
      "Epoch 4184 loss 2.927663\n",
      "Epoch 4185 loss 2.927665\n",
      "Epoch 4186 loss 2.927664\n",
      "Epoch 4187 loss 2.927663\n",
      "Epoch 4188 loss 2.927664\n",
      "Epoch 4189 loss 2.927664\n",
      "Epoch 4190 loss 2.927663\n",
      "Epoch 4191 loss 2.927662\n",
      "Epoch 4192 loss 2.927663\n",
      "Epoch 4193 loss 2.927663\n",
      "Epoch 4194 loss 2.927663\n",
      "Epoch 4195 loss 2.927663\n",
      "Epoch 4196 loss 2.927662\n",
      "Epoch 4197 loss 2.927663\n",
      "Epoch 4198 loss 2.927664\n",
      "Epoch 4199 loss 2.927663\n",
      "Epoch 4200 loss 2.927664\n",
      "Epoch 4201 loss 2.927663\n",
      "Epoch 4202 loss 2.927664\n",
      "Epoch 4203 loss 2.927663\n",
      "Epoch 4204 loss 2.927662\n",
      "Epoch 4205 loss 2.927662\n",
      "Epoch 4206 loss 2.927662\n",
      "Epoch 4207 loss 2.927663\n",
      "Epoch 4208 loss 2.927662\n",
      "Epoch 4209 loss 2.927663\n",
      "Epoch 4210 loss 2.927662\n",
      "Epoch 4211 loss 2.927664\n",
      "Epoch 4212 loss 2.927663\n",
      "Epoch 4213 loss 2.927662\n",
      "Epoch 4214 loss 2.927662\n",
      "Epoch 4215 loss 2.927664\n",
      "Epoch 4216 loss 2.927662\n",
      "Epoch 4217 loss 2.927660\n",
      "Epoch 4218 loss 2.927663\n",
      "Epoch 4219 loss 2.927662\n",
      "Epoch 4220 loss 2.927663\n",
      "Epoch 4221 loss 2.927662\n",
      "Epoch 4222 loss 2.927661\n",
      "Epoch 4223 loss 2.927660\n",
      "Epoch 4224 loss 2.927662\n",
      "Epoch 4225 loss 2.927663\n",
      "Epoch 4226 loss 2.927661\n",
      "Epoch 4227 loss 2.927661\n",
      "Epoch 4228 loss 2.927663\n",
      "Epoch 4229 loss 2.927662\n",
      "Epoch 4230 loss 2.927662\n",
      "Epoch 4231 loss 2.927661\n",
      "Epoch 4232 loss 2.927660\n",
      "Epoch 4233 loss 2.927661\n",
      "Epoch 4234 loss 2.927662\n",
      "Epoch 4235 loss 2.927661\n",
      "Epoch 4236 loss 2.927660\n",
      "Epoch 4237 loss 2.927662\n",
      "Epoch 4238 loss 2.927661\n",
      "Epoch 4239 loss 2.927661\n",
      "Epoch 4240 loss 2.927660\n",
      "Epoch 4241 loss 2.927660\n",
      "Epoch 4242 loss 2.927662\n",
      "Epoch 4243 loss 2.927662\n",
      "Epoch 4244 loss 2.927661\n",
      "Epoch 4245 loss 2.927661\n",
      "Epoch 4246 loss 2.927661\n",
      "Epoch 4247 loss 2.927661\n",
      "Epoch 4248 loss 2.927660\n",
      "Epoch 4249 loss 2.927661\n",
      "Epoch 4250 loss 2.927660\n",
      "Epoch 4251 loss 2.927660\n",
      "Epoch 4252 loss 2.927660\n",
      "Epoch 4253 loss 2.927659\n",
      "Epoch 4254 loss 2.927659\n",
      "Epoch 4255 loss 2.927661\n",
      "Epoch 4256 loss 2.927662\n",
      "Epoch 4257 loss 2.927661\n",
      "Epoch 4258 loss 2.927660\n",
      "Epoch 4259 loss 2.927660\n",
      "Epoch 4260 loss 2.927660\n",
      "Epoch 4261 loss 2.927658\n",
      "Epoch 4262 loss 2.927658\n",
      "Epoch 4263 loss 2.927660\n",
      "Epoch 4264 loss 2.927659\n",
      "Epoch 4265 loss 2.927660\n",
      "Epoch 4266 loss 2.927660\n",
      "Epoch 4267 loss 2.927660\n",
      "Epoch 4268 loss 2.927660\n",
      "Epoch 4269 loss 2.927660\n",
      "Epoch 4270 loss 2.927660\n",
      "Epoch 4271 loss 2.927660\n",
      "Epoch 4272 loss 2.927659\n",
      "Epoch 4273 loss 2.927659\n",
      "Epoch 4274 loss 2.927660\n",
      "Epoch 4275 loss 2.927660\n",
      "Epoch 4276 loss 2.927660\n",
      "Epoch 4277 loss 2.927659\n",
      "Epoch 4278 loss 2.927658\n",
      "Epoch 4279 loss 2.927659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4280 loss 2.927659\n",
      "Epoch 4281 loss 2.927660\n",
      "Epoch 4282 loss 2.927658\n",
      "Epoch 4283 loss 2.927659\n",
      "Epoch 4284 loss 2.927658\n",
      "Epoch 4285 loss 2.927657\n",
      "Epoch 4286 loss 2.927659\n",
      "Epoch 4287 loss 2.927659\n",
      "Epoch 4288 loss 2.927660\n",
      "Epoch 4289 loss 2.927658\n",
      "Epoch 4290 loss 2.927658\n",
      "Epoch 4291 loss 2.927658\n",
      "Epoch 4292 loss 2.927658\n",
      "Epoch 4293 loss 2.927657\n",
      "Epoch 4294 loss 2.927658\n",
      "Epoch 4295 loss 2.927659\n",
      "Epoch 4296 loss 2.927660\n",
      "Epoch 4297 loss 2.927658\n",
      "Epoch 4298 loss 2.927659\n",
      "Epoch 4299 loss 2.927658\n",
      "Epoch 4300 loss 2.927657\n",
      "Epoch 4301 loss 2.927658\n",
      "Epoch 4302 loss 2.927658\n",
      "Epoch 4303 loss 2.927658\n",
      "Epoch 4304 loss 2.927657\n",
      "Epoch 4305 loss 2.927659\n",
      "Epoch 4306 loss 2.927657\n",
      "Epoch 4307 loss 2.927658\n",
      "Epoch 4308 loss 2.927658\n",
      "Epoch 4309 loss 2.927658\n",
      "Epoch 4310 loss 2.927658\n",
      "Epoch 4311 loss 2.927657\n",
      "Epoch 4312 loss 2.927657\n",
      "Epoch 4313 loss 2.927657\n",
      "Epoch 4314 loss 2.927656\n",
      "Epoch 4315 loss 2.927657\n",
      "Epoch 4316 loss 2.927657\n",
      "Epoch 4317 loss 2.927657\n",
      "Epoch 4318 loss 2.927656\n",
      "Epoch 4319 loss 2.927657\n",
      "Epoch 4320 loss 2.927657\n",
      "Epoch 4321 loss 2.927656\n",
      "Epoch 4322 loss 2.927658\n",
      "Epoch 4323 loss 2.927658\n",
      "Epoch 4324 loss 2.927657\n",
      "Epoch 4325 loss 2.927656\n",
      "Epoch 4326 loss 2.927657\n",
      "Epoch 4327 loss 2.927658\n",
      "Epoch 4328 loss 2.927657\n",
      "Epoch 4329 loss 2.927657\n",
      "Epoch 4330 loss 2.927657\n",
      "Epoch 4331 loss 2.927658\n",
      "Epoch 4332 loss 2.927658\n",
      "Epoch 4333 loss 2.927657\n",
      "Epoch 4334 loss 2.927658\n",
      "Epoch 4335 loss 2.927657\n",
      "Epoch 4336 loss 2.927657\n",
      "Epoch 4337 loss 2.927657\n",
      "Epoch 4338 loss 2.927657\n",
      "Epoch 4339 loss 2.927657\n",
      "Epoch 4340 loss 2.927656\n",
      "Epoch 4341 loss 2.927657\n",
      "Epoch 4342 loss 2.927655\n",
      "Epoch 4343 loss 2.927656\n",
      "Epoch 4344 loss 2.927656\n",
      "Epoch 4345 loss 2.927657\n",
      "Epoch 4346 loss 2.927656\n",
      "Epoch 4347 loss 2.927657\n",
      "Epoch 4348 loss 2.927655\n",
      "Epoch 4349 loss 2.927656\n",
      "Epoch 4350 loss 2.927656\n",
      "Epoch 4351 loss 2.927655\n",
      "Epoch 4352 loss 2.927656\n",
      "Epoch 4353 loss 2.927656\n",
      "Epoch 4354 loss 2.927655\n",
      "Epoch 4355 loss 2.927655\n",
      "Epoch 4356 loss 2.927656\n",
      "Epoch 4357 loss 2.927655\n",
      "Epoch 4358 loss 2.927657\n",
      "Epoch 4359 loss 2.927656\n",
      "Epoch 4360 loss 2.927655\n",
      "Epoch 4361 loss 2.927656\n",
      "Epoch 4362 loss 2.927655\n",
      "Epoch 4363 loss 2.927656\n",
      "Epoch 4364 loss 2.927656\n",
      "Epoch 4365 loss 2.927656\n",
      "Epoch 4366 loss 2.927656\n",
      "Epoch 4367 loss 2.927655\n",
      "Epoch 4368 loss 2.927654\n",
      "Epoch 4369 loss 2.927655\n",
      "Epoch 4370 loss 2.927656\n",
      "Epoch 4371 loss 2.927655\n",
      "Epoch 4372 loss 2.927656\n",
      "Epoch 4373 loss 2.927656\n",
      "Epoch 4374 loss 2.927655\n",
      "Epoch 4375 loss 2.927656\n",
      "Epoch 4376 loss 2.927655\n",
      "Epoch 4377 loss 2.927656\n",
      "Epoch 4378 loss 2.927655\n",
      "Epoch 4379 loss 2.927655\n",
      "Epoch 4380 loss 2.927654\n",
      "Epoch 4381 loss 2.927656\n",
      "Epoch 4382 loss 2.927655\n",
      "Epoch 4383 loss 2.927656\n",
      "Epoch 4384 loss 2.927656\n",
      "Epoch 4385 loss 2.927655\n",
      "Epoch 4386 loss 2.927656\n",
      "Epoch 4387 loss 2.927654\n",
      "Epoch 4388 loss 2.927656\n",
      "Epoch 4389 loss 2.927654\n",
      "Epoch 4390 loss 2.927655\n",
      "Epoch 4391 loss 2.927654\n",
      "Epoch 4392 loss 2.927655\n",
      "Epoch 4393 loss 2.927655\n",
      "Epoch 4394 loss 2.927654\n",
      "Epoch 4395 loss 2.927654\n",
      "Epoch 4396 loss 2.927655\n",
      "Epoch 4397 loss 2.927655\n",
      "Epoch 4398 loss 2.927654\n",
      "Epoch 4399 loss 2.927655\n",
      "Epoch 4400 loss 2.927654\n",
      "Epoch 4401 loss 2.927655\n",
      "Epoch 4402 loss 2.927654\n",
      "Epoch 4403 loss 2.927654\n",
      "Epoch 4404 loss 2.927654\n",
      "Epoch 4405 loss 2.927654\n",
      "Epoch 4406 loss 2.927655\n",
      "Epoch 4407 loss 2.927654\n",
      "Epoch 4408 loss 2.927654\n",
      "Epoch 4409 loss 2.927654\n",
      "Epoch 4410 loss 2.927655\n",
      "Epoch 4411 loss 2.927655\n",
      "Epoch 4412 loss 2.927656\n",
      "Epoch 4413 loss 2.927654\n",
      "Epoch 4414 loss 2.927655\n",
      "Epoch 4415 loss 2.927654\n",
      "Epoch 4416 loss 2.927653\n",
      "Epoch 4417 loss 2.927655\n",
      "Epoch 4418 loss 2.927653\n",
      "Epoch 4419 loss 2.927655\n",
      "Epoch 4420 loss 2.927653\n",
      "Epoch 4421 loss 2.927654\n",
      "Epoch 4422 loss 2.927653\n",
      "Epoch 4423 loss 2.927655\n",
      "Epoch 4424 loss 2.927654\n",
      "Epoch 4425 loss 2.927655\n",
      "Epoch 4426 loss 2.927653\n",
      "Epoch 4427 loss 2.927654\n",
      "Epoch 4428 loss 2.927655\n",
      "Epoch 4429 loss 2.927654\n",
      "Epoch 4430 loss 2.927654\n",
      "Epoch 4431 loss 2.927653\n",
      "Epoch 4432 loss 2.927654\n",
      "Epoch 4433 loss 2.927654\n",
      "Epoch 4434 loss 2.927654\n",
      "Epoch 4435 loss 2.927655\n",
      "Epoch 4436 loss 2.927653\n",
      "Epoch 4437 loss 2.927652\n",
      "Epoch 4438 loss 2.927653\n",
      "Epoch 4439 loss 2.927654\n",
      "Epoch 4440 loss 2.927655\n",
      "Epoch 4441 loss 2.927655\n",
      "Epoch 4442 loss 2.927652\n",
      "Epoch 4443 loss 2.927653\n",
      "Epoch 4444 loss 2.927651\n",
      "Epoch 4445 loss 2.927654\n",
      "Epoch 4446 loss 2.927654\n",
      "Epoch 4447 loss 2.927653\n",
      "Epoch 4448 loss 2.927654\n",
      "Epoch 4449 loss 2.927655\n",
      "Epoch 4450 loss 2.927654\n",
      "Epoch 4451 loss 2.927654\n",
      "Epoch 4452 loss 2.927653\n",
      "Epoch 4453 loss 2.927652\n",
      "Epoch 4454 loss 2.927653\n",
      "Epoch 4455 loss 2.927653\n",
      "Epoch 4456 loss 2.927654\n",
      "Epoch 4457 loss 2.927653\n",
      "Epoch 4458 loss 2.927652\n",
      "Epoch 4459 loss 2.927653\n",
      "Epoch 4460 loss 2.927652\n",
      "Epoch 4461 loss 2.927654\n",
      "Epoch 4462 loss 2.927654\n",
      "Epoch 4463 loss 2.927654\n",
      "Epoch 4464 loss 2.927653\n",
      "Epoch 4465 loss 2.927653\n",
      "Epoch 4466 loss 2.927652\n",
      "Epoch 4467 loss 2.927654\n",
      "Epoch 4468 loss 2.927653\n",
      "Epoch 4469 loss 2.927653\n",
      "Epoch 4470 loss 2.927653\n",
      "Epoch 4471 loss 2.927653\n",
      "Epoch 4472 loss 2.927653\n",
      "Epoch 4473 loss 2.927654\n",
      "Epoch 4474 loss 2.927653\n",
      "Epoch 4475 loss 2.927653\n",
      "Epoch 4476 loss 2.927652\n",
      "Epoch 4477 loss 2.927653\n",
      "Epoch 4478 loss 2.927654\n",
      "Epoch 4479 loss 2.927653\n",
      "Epoch 4480 loss 2.927651\n",
      "Epoch 4481 loss 2.927653\n",
      "Epoch 4482 loss 2.927653\n",
      "Epoch 4483 loss 2.927654\n",
      "Epoch 4484 loss 2.927653\n",
      "Epoch 4485 loss 2.927653\n",
      "Epoch 4486 loss 2.927652\n",
      "Epoch 4487 loss 2.927651\n",
      "Epoch 4488 loss 2.927652\n",
      "Epoch 4489 loss 2.927653\n",
      "Epoch 4490 loss 2.927654\n",
      "Epoch 4491 loss 2.927653\n",
      "Epoch 4492 loss 2.927652\n",
      "Epoch 4493 loss 2.927652\n",
      "Epoch 4494 loss 2.927651\n",
      "Epoch 4495 loss 2.927652\n",
      "Epoch 4496 loss 2.927653\n",
      "Epoch 4497 loss 2.927653\n",
      "Epoch 4498 loss 2.927652\n",
      "Epoch 4499 loss 2.927652\n",
      "Epoch 4500 loss 2.927652\n",
      "Epoch 4501 loss 2.927652\n",
      "Epoch 4502 loss 2.927654\n",
      "Epoch 4503 loss 2.927651\n",
      "Epoch 4504 loss 2.927652\n",
      "Epoch 4505 loss 2.927653\n",
      "Epoch 4506 loss 2.927653\n",
      "Epoch 4507 loss 2.927651\n",
      "Epoch 4508 loss 2.927651\n",
      "Epoch 4509 loss 2.927653\n",
      "Epoch 4510 loss 2.927653\n",
      "Epoch 4511 loss 2.927651\n",
      "Epoch 4512 loss 2.927651\n",
      "Epoch 4513 loss 2.927653\n",
      "Epoch 4514 loss 2.927653\n",
      "Epoch 4515 loss 2.927652\n",
      "Epoch 4516 loss 2.927653\n",
      "Epoch 4517 loss 2.927652\n",
      "Epoch 4518 loss 2.927652\n",
      "Epoch 4519 loss 2.927653\n",
      "Epoch 4520 loss 2.927652\n",
      "Epoch 4521 loss 2.927652\n",
      "Epoch 4522 loss 2.927651\n",
      "Epoch 4523 loss 2.927651\n",
      "Epoch 4524 loss 2.927652\n",
      "Epoch 4525 loss 2.927652\n",
      "Epoch 4526 loss 2.927651\n",
      "Epoch 4527 loss 2.927651\n",
      "Epoch 4528 loss 2.927650\n",
      "Epoch 4529 loss 2.927651\n",
      "Epoch 4530 loss 2.927653\n",
      "Epoch 4531 loss 2.927651\n",
      "Epoch 4532 loss 2.927651\n",
      "Epoch 4533 loss 2.927652\n",
      "Epoch 4534 loss 2.927653\n",
      "Epoch 4535 loss 2.927651\n",
      "Epoch 4536 loss 2.927651\n",
      "Epoch 4537 loss 2.927650\n",
      "Epoch 4538 loss 2.927651\n",
      "Epoch 4539 loss 2.927650\n",
      "Epoch 4540 loss 2.927652\n",
      "Epoch 4541 loss 2.927652\n",
      "Epoch 4542 loss 2.927651\n",
      "Epoch 4543 loss 2.927652\n",
      "Epoch 4544 loss 2.927652\n",
      "Epoch 4545 loss 2.927651\n",
      "Epoch 4546 loss 2.927650\n",
      "Epoch 4547 loss 2.927651\n",
      "Epoch 4548 loss 2.927652\n",
      "Epoch 4549 loss 2.927651\n",
      "Epoch 4550 loss 2.927653\n",
      "Epoch 4551 loss 2.927651\n",
      "Epoch 4552 loss 2.927652\n",
      "Epoch 4553 loss 2.927651\n",
      "Epoch 4554 loss 2.927652\n",
      "Epoch 4555 loss 2.927650\n",
      "Epoch 4556 loss 2.927650\n",
      "Epoch 4557 loss 2.927650\n",
      "Epoch 4558 loss 2.927652\n",
      "Epoch 4559 loss 2.927650\n",
      "Epoch 4560 loss 2.927650\n",
      "Epoch 4561 loss 2.927651\n",
      "Epoch 4562 loss 2.927652\n",
      "Epoch 4563 loss 2.927650\n",
      "Epoch 4564 loss 2.927651\n",
      "Epoch 4565 loss 2.927650\n",
      "Epoch 4566 loss 2.927651\n",
      "Epoch 4567 loss 2.927650\n",
      "Epoch 4568 loss 2.927651\n",
      "Epoch 4569 loss 2.927650\n",
      "Epoch 4570 loss 2.927650\n",
      "Epoch 4571 loss 2.927650\n",
      "Epoch 4572 loss 2.927651\n",
      "Epoch 4573 loss 2.927652\n",
      "Epoch 4574 loss 2.927650\n",
      "Epoch 4575 loss 2.927651\n",
      "Epoch 4576 loss 2.927651\n",
      "Epoch 4577 loss 2.927651\n",
      "Epoch 4578 loss 2.927652\n",
      "Epoch 4579 loss 2.927650\n",
      "Epoch 4580 loss 2.927650\n",
      "Epoch 4581 loss 2.927650\n",
      "Epoch 4582 loss 2.927651\n",
      "Epoch 4583 loss 2.927650\n",
      "Epoch 4584 loss 2.927650\n",
      "Epoch 4585 loss 2.927652\n",
      "Epoch 4586 loss 2.927650\n",
      "Epoch 4587 loss 2.927651\n",
      "Epoch 4588 loss 2.927650\n",
      "Epoch 4589 loss 2.927650\n",
      "Epoch 4590 loss 2.927652\n",
      "Epoch 4591 loss 2.927650\n",
      "Epoch 4592 loss 2.927651\n",
      "Epoch 4593 loss 2.927651\n",
      "Epoch 4594 loss 2.927650\n",
      "Epoch 4595 loss 2.927650\n",
      "Epoch 4596 loss 2.927651\n",
      "Epoch 4597 loss 2.927651\n",
      "Epoch 4598 loss 2.927652\n",
      "Epoch 4599 loss 2.927649\n",
      "Epoch 4600 loss 2.927650\n",
      "Epoch 4601 loss 2.927650\n",
      "Epoch 4602 loss 2.927649\n",
      "Epoch 4603 loss 2.927649\n",
      "Epoch 4604 loss 2.927649\n",
      "Epoch 4605 loss 2.927650\n",
      "Epoch 4606 loss 2.927650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4607 loss 2.927650\n",
      "Epoch 4608 loss 2.927651\n",
      "Epoch 4609 loss 2.927650\n",
      "Epoch 4610 loss 2.927651\n",
      "Epoch 4611 loss 2.927650\n",
      "Epoch 4612 loss 2.927650\n",
      "Epoch 4613 loss 2.927650\n",
      "Epoch 4614 loss 2.927649\n",
      "Epoch 4615 loss 2.927650\n",
      "Epoch 4616 loss 2.927651\n",
      "Epoch 4617 loss 2.927650\n",
      "Epoch 4618 loss 2.927651\n",
      "Epoch 4619 loss 2.927649\n",
      "Epoch 4620 loss 2.927650\n",
      "Epoch 4621 loss 2.927650\n",
      "Epoch 4622 loss 2.927650\n",
      "Epoch 4623 loss 2.927651\n",
      "Epoch 4624 loss 2.927651\n",
      "Epoch 4625 loss 2.927649\n",
      "Epoch 4626 loss 2.927650\n",
      "Epoch 4627 loss 2.927650\n",
      "Epoch 4628 loss 2.927651\n",
      "Epoch 4629 loss 2.927650\n",
      "Epoch 4630 loss 2.927647\n",
      "Epoch 4631 loss 2.927648\n",
      "Epoch 4632 loss 2.927649\n",
      "Epoch 4633 loss 2.927649\n",
      "Epoch 4634 loss 2.927650\n",
      "Epoch 4635 loss 2.927649\n",
      "Epoch 4636 loss 2.927650\n",
      "Epoch 4637 loss 2.927650\n",
      "Epoch 4638 loss 2.927651\n",
      "Epoch 4639 loss 2.927649\n",
      "Epoch 4640 loss 2.927649\n",
      "Epoch 4641 loss 2.927650\n",
      "Epoch 4642 loss 2.927650\n",
      "Epoch 4643 loss 2.927649\n",
      "Epoch 4644 loss 2.927650\n",
      "Epoch 4645 loss 2.927650\n",
      "Epoch 4646 loss 2.927650\n",
      "Epoch 4647 loss 2.927651\n",
      "Epoch 4648 loss 2.927650\n",
      "Epoch 4649 loss 2.927650\n",
      "Epoch 4650 loss 2.927649\n",
      "Epoch 4651 loss 2.927650\n",
      "Epoch 4652 loss 2.927651\n",
      "Epoch 4653 loss 2.927650\n",
      "Epoch 4654 loss 2.927651\n",
      "Epoch 4655 loss 2.927651\n",
      "Epoch 4656 loss 2.927650\n",
      "Epoch 4657 loss 2.927649\n",
      "Epoch 4658 loss 2.927649\n",
      "Epoch 4659 loss 2.927649\n",
      "Epoch 4660 loss 2.927649\n",
      "Epoch 4661 loss 2.927648\n",
      "Epoch 4662 loss 2.927650\n",
      "Epoch 4663 loss 2.927649\n",
      "Epoch 4664 loss 2.927648\n",
      "Epoch 4665 loss 2.927649\n",
      "Epoch 4666 loss 2.927649\n",
      "Epoch 4667 loss 2.927649\n",
      "Epoch 4668 loss 2.927649\n",
      "Epoch 4669 loss 2.927649\n",
      "Epoch 4670 loss 2.927649\n",
      "Epoch 4671 loss 2.927649\n",
      "Epoch 4672 loss 2.927649\n",
      "Epoch 4673 loss 2.927650\n",
      "Epoch 4674 loss 2.927650\n",
      "Epoch 4675 loss 2.927650\n",
      "Epoch 4676 loss 2.927650\n",
      "Epoch 4677 loss 2.927650\n",
      "Epoch 4678 loss 2.927650\n",
      "Epoch 4679 loss 2.927649\n",
      "Epoch 4680 loss 2.927648\n",
      "Epoch 4681 loss 2.927649\n",
      "Epoch 4682 loss 2.927649\n",
      "Epoch 4683 loss 2.927648\n",
      "Epoch 4684 loss 2.927648\n",
      "Epoch 4685 loss 2.927649\n",
      "Epoch 4686 loss 2.927651\n",
      "Epoch 4687 loss 2.927649\n",
      "Epoch 4688 loss 2.927650\n",
      "Epoch 4689 loss 2.927649\n",
      "Epoch 4690 loss 2.927650\n",
      "Epoch 4691 loss 2.927648\n",
      "Epoch 4692 loss 2.927649\n",
      "Epoch 4693 loss 2.927649\n",
      "Epoch 4694 loss 2.927650\n",
      "Epoch 4695 loss 2.927648\n",
      "Epoch 4696 loss 2.927649\n",
      "Epoch 4697 loss 2.927648\n",
      "Epoch 4698 loss 2.927649\n",
      "Epoch 4699 loss 2.927649\n",
      "Epoch 4700 loss 2.927650\n",
      "Epoch 4701 loss 2.927650\n",
      "Epoch 4702 loss 2.927650\n",
      "Epoch 4703 loss 2.927648\n",
      "Epoch 4704 loss 2.927649\n",
      "Epoch 4705 loss 2.927649\n",
      "Epoch 4706 loss 2.927649\n",
      "Epoch 4707 loss 2.927649\n",
      "Epoch 4708 loss 2.927648\n",
      "Epoch 4709 loss 2.927650\n",
      "Epoch 4710 loss 2.927650\n",
      "Epoch 4711 loss 2.927649\n",
      "Epoch 4712 loss 2.927649\n",
      "Epoch 4713 loss 2.927649\n",
      "Epoch 4714 loss 2.927649\n",
      "Epoch 4715 loss 2.927650\n",
      "Epoch 4716 loss 2.927649\n",
      "Epoch 4717 loss 2.927649\n",
      "Epoch 4718 loss 2.927649\n",
      "Epoch 4719 loss 2.927648\n",
      "Epoch 4720 loss 2.927649\n",
      "Epoch 4721 loss 2.927649\n",
      "Epoch 4722 loss 2.927650\n",
      "Epoch 4723 loss 2.927650\n",
      "Epoch 4724 loss 2.927649\n",
      "Epoch 4725 loss 2.927650\n",
      "Epoch 4726 loss 2.927649\n",
      "Epoch 4727 loss 2.927648\n",
      "Epoch 4728 loss 2.927647\n",
      "Epoch 4729 loss 2.927649\n",
      "Epoch 4730 loss 2.927648\n",
      "Epoch 4731 loss 2.927649\n",
      "Epoch 4732 loss 2.927648\n",
      "Epoch 4733 loss 2.927649\n",
      "Epoch 4734 loss 2.927647\n",
      "Epoch 4735 loss 2.927650\n",
      "Epoch 4736 loss 2.927650\n",
      "Epoch 4737 loss 2.927650\n",
      "Epoch 4738 loss 2.927649\n",
      "Epoch 4739 loss 2.927648\n",
      "Epoch 4740 loss 2.927648\n",
      "Epoch 4741 loss 2.927648\n",
      "Epoch 4742 loss 2.927647\n",
      "Epoch 4743 loss 2.927648\n",
      "Epoch 4744 loss 2.927649\n",
      "Epoch 4745 loss 2.927648\n",
      "Epoch 4746 loss 2.927648\n",
      "Epoch 4747 loss 2.927649\n",
      "Epoch 4748 loss 2.927648\n",
      "Epoch 4749 loss 2.927649\n",
      "Epoch 4750 loss 2.927649\n",
      "Epoch 4751 loss 2.927649\n",
      "Epoch 4752 loss 2.927648\n",
      "Epoch 4753 loss 2.927648\n",
      "Epoch 4754 loss 2.927647\n",
      "Epoch 4755 loss 2.927648\n",
      "Epoch 4756 loss 2.927648\n",
      "Epoch 4757 loss 2.927648\n",
      "Epoch 4758 loss 2.927648\n",
      "Epoch 4759 loss 2.927649\n",
      "Epoch 4760 loss 2.927649\n",
      "Epoch 4761 loss 2.927649\n",
      "Epoch 4762 loss 2.927648\n",
      "Epoch 4763 loss 2.927647\n",
      "Epoch 4764 loss 2.927648\n",
      "Epoch 4765 loss 2.927648\n",
      "Epoch 4766 loss 2.927649\n",
      "Epoch 4767 loss 2.927648\n",
      "Epoch 4768 loss 2.927649\n",
      "Epoch 4769 loss 2.927648\n",
      "Epoch 4770 loss 2.927649\n",
      "Epoch 4771 loss 2.927648\n",
      "Epoch 4772 loss 2.927648\n",
      "Epoch 4773 loss 2.927649\n",
      "Epoch 4774 loss 2.927648\n",
      "Epoch 4775 loss 2.927647\n",
      "Epoch 4776 loss 2.927648\n",
      "Epoch 4777 loss 2.927648\n",
      "Epoch 4778 loss 2.927648\n",
      "Epoch 4779 loss 2.927649\n",
      "Epoch 4780 loss 2.927649\n",
      "Epoch 4781 loss 2.927649\n",
      "Epoch 4782 loss 2.927649\n",
      "Epoch 4783 loss 2.927649\n",
      "Epoch 4784 loss 2.927648\n",
      "Epoch 4785 loss 2.927648\n",
      "Epoch 4786 loss 2.927647\n",
      "Epoch 4787 loss 2.927647\n",
      "Epoch 4788 loss 2.927648\n",
      "Epoch 4789 loss 2.927648\n",
      "Epoch 4790 loss 2.927648\n",
      "Epoch 4791 loss 2.927648\n",
      "Epoch 4792 loss 2.927648\n",
      "Epoch 4793 loss 2.927648\n",
      "Epoch 4794 loss 2.927650\n",
      "Epoch 4795 loss 2.927648\n",
      "Epoch 4796 loss 2.927649\n",
      "Epoch 4797 loss 2.927649\n",
      "Epoch 4798 loss 2.927648\n",
      "Epoch 4799 loss 2.927648\n",
      "Epoch 4800 loss 2.927650\n",
      "Epoch 4801 loss 2.927647\n",
      "Epoch 4802 loss 2.927648\n",
      "Epoch 4803 loss 2.927649\n",
      "Epoch 4804 loss 2.927647\n",
      "Epoch 4805 loss 2.927649\n",
      "Epoch 4806 loss 2.927648\n",
      "Epoch 4807 loss 2.927649\n",
      "Epoch 4808 loss 2.927648\n",
      "Epoch 4809 loss 2.927649\n",
      "Epoch 4810 loss 2.927649\n",
      "Epoch 4811 loss 2.927647\n",
      "Epoch 4812 loss 2.927649\n",
      "Epoch 4813 loss 2.927648\n",
      "Epoch 4814 loss 2.927647\n",
      "Epoch 4815 loss 2.927649\n",
      "Epoch 4816 loss 2.927647\n",
      "Epoch 4817 loss 2.927648\n",
      "Epoch 4818 loss 2.927646\n",
      "Epoch 4819 loss 2.927649\n",
      "Epoch 4820 loss 2.927647\n",
      "Epoch 4821 loss 2.927649\n",
      "Epoch 4822 loss 2.927649\n",
      "Epoch 4823 loss 2.927648\n",
      "Epoch 4824 loss 2.927648\n",
      "Epoch 4825 loss 2.927649\n",
      "Epoch 4826 loss 2.927648\n",
      "Epoch 4827 loss 2.927649\n",
      "Epoch 4828 loss 2.927649\n",
      "Epoch 4829 loss 2.927648\n",
      "Epoch 4830 loss 2.927648\n",
      "Epoch 4831 loss 2.927646\n",
      "Epoch 4832 loss 2.927648\n",
      "Epoch 4833 loss 2.927647\n",
      "Epoch 4834 loss 2.927648\n",
      "Epoch 4835 loss 2.927649\n",
      "Epoch 4836 loss 2.927647\n",
      "Epoch 4837 loss 2.927648\n",
      "Epoch 4838 loss 2.927648\n",
      "Epoch 4839 loss 2.927648\n",
      "Epoch 4840 loss 2.927648\n",
      "Epoch 4841 loss 2.927648\n",
      "Epoch 4842 loss 2.927649\n",
      "Epoch 4843 loss 2.927647\n",
      "Epoch 4844 loss 2.927648\n",
      "Epoch 4845 loss 2.927647\n",
      "Epoch 4846 loss 2.927647\n",
      "Epoch 4847 loss 2.927648\n",
      "Epoch 4848 loss 2.927648\n",
      "Epoch 4849 loss 2.927649\n",
      "Epoch 4850 loss 2.927647\n",
      "Epoch 4851 loss 2.927649\n",
      "Epoch 4852 loss 2.927648\n",
      "Epoch 4853 loss 2.927647\n",
      "Epoch 4854 loss 2.927649\n",
      "Epoch 4855 loss 2.927648\n",
      "Epoch 4856 loss 2.927649\n",
      "Epoch 4857 loss 2.927649\n",
      "Epoch 4858 loss 2.927647\n",
      "Epoch 4859 loss 2.927648\n",
      "Epoch 4860 loss 2.927648\n",
      "Epoch 4861 loss 2.927649\n",
      "Epoch 4862 loss 2.927647\n",
      "Epoch 4863 loss 2.927647\n",
      "Epoch 4864 loss 2.927648\n",
      "Epoch 4865 loss 2.927646\n",
      "Epoch 4866 loss 2.927648\n",
      "Epoch 4867 loss 2.927648\n",
      "Epoch 4868 loss 2.927648\n",
      "Epoch 4869 loss 2.927648\n",
      "Epoch 4870 loss 2.927647\n",
      "Epoch 4871 loss 2.927649\n",
      "Epoch 4872 loss 2.927648\n",
      "Epoch 4873 loss 2.927647\n",
      "Epoch 4874 loss 2.927648\n",
      "Epoch 4875 loss 2.927648\n",
      "Epoch 4876 loss 2.927649\n",
      "Epoch 4877 loss 2.927648\n",
      "Epoch 4878 loss 2.927648\n",
      "Epoch 4879 loss 2.927647\n",
      "Epoch 4880 loss 2.927648\n",
      "Epoch 4881 loss 2.927648\n",
      "Epoch 4882 loss 2.927647\n",
      "Epoch 4883 loss 2.927649\n",
      "Epoch 4884 loss 2.927648\n",
      "Epoch 4885 loss 2.927647\n",
      "Epoch 4886 loss 2.927649\n",
      "Epoch 4887 loss 2.927648\n",
      "Epoch 4888 loss 2.927647\n",
      "Epoch 4889 loss 2.927646\n",
      "Epoch 4890 loss 2.927647\n",
      "Epoch 4891 loss 2.927648\n",
      "Epoch 4892 loss 2.927646\n",
      "Epoch 4893 loss 2.927649\n",
      "Epoch 4894 loss 2.927648\n",
      "Epoch 4895 loss 2.927648\n",
      "Epoch 4896 loss 2.927649\n",
      "Epoch 4897 loss 2.927647\n",
      "Epoch 4898 loss 2.927648\n",
      "Epoch 4899 loss 2.927648\n",
      "Epoch 4900 loss 2.927648\n",
      "Epoch 4901 loss 2.927649\n",
      "Epoch 4902 loss 2.927647\n",
      "Epoch 4903 loss 2.927647\n",
      "Epoch 4904 loss 2.927646\n",
      "Epoch 4905 loss 2.927647\n",
      "Epoch 4906 loss 2.927647\n",
      "Epoch 4907 loss 2.927647\n",
      "Epoch 4908 loss 2.927647\n",
      "Epoch 4909 loss 2.927648\n",
      "Epoch 4910 loss 2.927649\n",
      "Epoch 4911 loss 2.927646\n",
      "Epoch 4912 loss 2.927648\n",
      "Epoch 4913 loss 2.927648\n",
      "Epoch 4914 loss 2.927647\n",
      "Epoch 4915 loss 2.927649\n",
      "Epoch 4916 loss 2.927648\n",
      "Epoch 4917 loss 2.927647\n",
      "Epoch 4918 loss 2.927647\n",
      "Epoch 4919 loss 2.927647\n",
      "Epoch 4920 loss 2.927647\n",
      "Epoch 4921 loss 2.927646\n",
      "Epoch 4922 loss 2.927649\n",
      "Epoch 4923 loss 2.927647\n",
      "Epoch 4924 loss 2.927646\n",
      "Epoch 4925 loss 2.927647\n",
      "Epoch 4926 loss 2.927648\n",
      "Epoch 4927 loss 2.927647\n",
      "Epoch 4928 loss 2.927649\n",
      "Epoch 4929 loss 2.927648\n",
      "Epoch 4930 loss 2.927648\n",
      "Epoch 4931 loss 2.927646\n",
      "Epoch 4932 loss 2.927648\n",
      "Epoch 4933 loss 2.927646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4934 loss 2.927646\n",
      "Epoch 4935 loss 2.927649\n",
      "Epoch 4936 loss 2.927647\n",
      "Epoch 4937 loss 2.927647\n",
      "Epoch 4938 loss 2.927647\n",
      "Epoch 4939 loss 2.927647\n",
      "Epoch 4940 loss 2.927647\n",
      "Epoch 4941 loss 2.927646\n",
      "Epoch 4942 loss 2.927649\n",
      "Epoch 4943 loss 2.927647\n",
      "Epoch 4944 loss 2.927648\n",
      "Epoch 4945 loss 2.927647\n",
      "Epoch 4946 loss 2.927649\n",
      "Epoch 4947 loss 2.927646\n",
      "Epoch 4948 loss 2.927649\n",
      "Epoch 4949 loss 2.927648\n",
      "Epoch 4950 loss 2.927647\n",
      "Epoch 4951 loss 2.927648\n",
      "Epoch 4952 loss 2.927647\n",
      "Epoch 4953 loss 2.927647\n",
      "Epoch 4954 loss 2.927647\n",
      "Epoch 4955 loss 2.927648\n",
      "Epoch 4956 loss 2.927646\n",
      "Epoch 4957 loss 2.927648\n",
      "Epoch 4958 loss 2.927647\n",
      "Epoch 4959 loss 2.927647\n",
      "Epoch 4960 loss 2.927647\n",
      "Epoch 4961 loss 2.927648\n",
      "Epoch 4962 loss 2.927647\n",
      "Epoch 4963 loss 2.927648\n",
      "Epoch 4964 loss 2.927648\n",
      "Epoch 4965 loss 2.927648\n",
      "Epoch 4966 loss 2.927648\n",
      "Epoch 4967 loss 2.927648\n",
      "Epoch 4968 loss 2.927647\n",
      "Epoch 4969 loss 2.927647\n",
      "Epoch 4970 loss 2.927646\n",
      "Epoch 4971 loss 2.927647\n",
      "Epoch 4972 loss 2.927647\n",
      "Epoch 4973 loss 2.927647\n",
      "Epoch 4974 loss 2.927647\n",
      "Epoch 4975 loss 2.927647\n",
      "Epoch 4976 loss 2.927647\n",
      "Epoch 4977 loss 2.927648\n",
      "Epoch 4978 loss 2.927647\n",
      "Epoch 4979 loss 2.927648\n",
      "Epoch 4980 loss 2.927647\n",
      "Epoch 4981 loss 2.927648\n",
      "Epoch 4982 loss 2.927648\n",
      "Epoch 4983 loss 2.927646\n",
      "Epoch 4984 loss 2.927648\n",
      "Epoch 4985 loss 2.927647\n",
      "Epoch 4986 loss 2.927648\n",
      "Epoch 4987 loss 2.927648\n",
      "Epoch 4988 loss 2.927648\n",
      "Epoch 4989 loss 2.927646\n",
      "Epoch 4990 loss 2.927648\n",
      "Epoch 4991 loss 2.927647\n",
      "Epoch 4992 loss 2.927647\n",
      "Epoch 4993 loss 2.927647\n",
      "Epoch 4994 loss 2.927648\n",
      "Epoch 4995 loss 2.927647\n",
      "Epoch 4996 loss 2.927646\n",
      "Epoch 4997 loss 2.927647\n",
      "Epoch 4998 loss 2.927648\n",
      "Epoch 4999 loss 2.927647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "params = torch.tensor([1.0,0.0], requires_grad=True)\n",
    "epochs = 5000\n",
    "lr = 1e-2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    t_p = model(t_u,*params)\n",
    "    loss = loss_fn(t_p, t_c)\n",
    "    print('Epoch %d loss %f' % (epoch , float(loss)))\n",
    "    if params.grad is not None:\n",
    "        params.grad.zero_()\n",
    "    loss.backward()\n",
    "    params = (params - lr * params.grad).detach().requires_grad_()\n",
    "    \n",
    "params\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'AdamW',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'Optimizer',\n",
       " 'RMSprop',\n",
       " 'Rprop',\n",
       " 'SGD',\n",
       " 'SparseAdam',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'lr_scheduler']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0,0.0], requires_grad=True)\n",
    "lr = 1e-5\n",
    "optimizer = optim.SGD([params], lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0008e+00, 1.0640e-04], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p = model(t_u, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7761, 0.1064], requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0,0.0], requires_grad=True)\n",
    "lr = 1e-2\n",
    "optimizer = optim.SGD([params],lr=lr)\n",
    "t_p = model(t_u, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0,0.0], requires_grad=True)\n",
    "lr = 1e-2\n",
    "epochs = 5000\n",
    "optimizer = optim.SGD([params], lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 80.364342\n",
      "Epoch 1 Loss 37.574917\n",
      "Epoch 2 Loss 30.871077\n",
      "Epoch 3 Loss 29.756193\n",
      "Epoch 4 Loss 29.507149\n",
      "Epoch 5 Loss 29.392458\n",
      "Epoch 6 Loss 29.298828\n",
      "Epoch 7 Loss 29.208717\n",
      "Epoch 8 Loss 29.119417\n",
      "Epoch 9 Loss 29.030487\n",
      "Epoch 10 Loss 28.941875\n",
      "Epoch 11 Loss 28.853565\n",
      "Epoch 12 Loss 28.765556\n",
      "Epoch 13 Loss 28.677851\n",
      "Epoch 14 Loss 28.590431\n",
      "Epoch 15 Loss 28.503321\n",
      "Epoch 16 Loss 28.416496\n",
      "Epoch 17 Loss 28.329973\n",
      "Epoch 18 Loss 28.243738\n",
      "Epoch 19 Loss 28.157801\n",
      "Epoch 20 Loss 28.072151\n",
      "Epoch 21 Loss 27.986799\n",
      "Epoch 22 Loss 27.901731\n",
      "Epoch 23 Loss 27.816954\n",
      "Epoch 24 Loss 27.732460\n",
      "Epoch 25 Loss 27.648256\n",
      "Epoch 26 Loss 27.564342\n",
      "Epoch 27 Loss 27.480711\n",
      "Epoch 28 Loss 27.397358\n",
      "Epoch 29 Loss 27.314293\n",
      "Epoch 30 Loss 27.231512\n",
      "Epoch 31 Loss 27.149006\n",
      "Epoch 32 Loss 27.066790\n",
      "Epoch 33 Loss 26.984844\n",
      "Epoch 34 Loss 26.903173\n",
      "Epoch 35 Loss 26.821791\n",
      "Epoch 36 Loss 26.740675\n",
      "Epoch 37 Loss 26.659838\n",
      "Epoch 38 Loss 26.579279\n",
      "Epoch 39 Loss 26.498987\n",
      "Epoch 40 Loss 26.418974\n",
      "Epoch 41 Loss 26.339228\n",
      "Epoch 42 Loss 26.259752\n",
      "Epoch 43 Loss 26.180548\n",
      "Epoch 44 Loss 26.101616\n",
      "Epoch 45 Loss 26.022947\n",
      "Epoch 46 Loss 25.944550\n",
      "Epoch 47 Loss 25.866417\n",
      "Epoch 48 Loss 25.788549\n",
      "Epoch 49 Loss 25.710936\n",
      "Epoch 50 Loss 25.633600\n",
      "Epoch 51 Loss 25.556524\n",
      "Epoch 52 Loss 25.479700\n",
      "Epoch 53 Loss 25.403145\n",
      "Epoch 54 Loss 25.326849\n",
      "Epoch 55 Loss 25.250811\n",
      "Epoch 56 Loss 25.175035\n",
      "Epoch 57 Loss 25.099510\n",
      "Epoch 58 Loss 25.024248\n",
      "Epoch 59 Loss 24.949238\n",
      "Epoch 60 Loss 24.874483\n",
      "Epoch 61 Loss 24.799980\n",
      "Epoch 62 Loss 24.725737\n",
      "Epoch 63 Loss 24.651735\n",
      "Epoch 64 Loss 24.577990\n",
      "Epoch 65 Loss 24.504494\n",
      "Epoch 66 Loss 24.431250\n",
      "Epoch 67 Loss 24.358257\n",
      "Epoch 68 Loss 24.285503\n",
      "Epoch 69 Loss 24.212996\n",
      "Epoch 70 Loss 24.140747\n",
      "Epoch 71 Loss 24.068733\n",
      "Epoch 72 Loss 23.996967\n",
      "Epoch 73 Loss 23.925446\n",
      "Epoch 74 Loss 23.854168\n",
      "Epoch 75 Loss 23.783129\n",
      "Epoch 76 Loss 23.712328\n",
      "Epoch 77 Loss 23.641771\n",
      "Epoch 78 Loss 23.571455\n",
      "Epoch 79 Loss 23.501379\n",
      "Epoch 80 Loss 23.431538\n",
      "Epoch 81 Loss 23.361933\n",
      "Epoch 82 Loss 23.292566\n",
      "Epoch 83 Loss 23.223436\n",
      "Epoch 84 Loss 23.154539\n",
      "Epoch 85 Loss 23.085882\n",
      "Epoch 86 Loss 23.017447\n",
      "Epoch 87 Loss 22.949249\n",
      "Epoch 88 Loss 22.881281\n",
      "Epoch 89 Loss 22.813547\n",
      "Epoch 90 Loss 22.746044\n",
      "Epoch 91 Loss 22.678768\n",
      "Epoch 92 Loss 22.611719\n",
      "Epoch 93 Loss 22.544899\n",
      "Epoch 94 Loss 22.478306\n",
      "Epoch 95 Loss 22.411940\n",
      "Epoch 96 Loss 22.345793\n",
      "Epoch 97 Loss 22.279875\n",
      "Epoch 98 Loss 22.214186\n",
      "Epoch 99 Loss 22.148710\n",
      "Epoch 100 Loss 22.083464\n",
      "Epoch 101 Loss 22.018436\n",
      "Epoch 102 Loss 21.953630\n",
      "Epoch 103 Loss 21.889046\n",
      "Epoch 104 Loss 21.824677\n",
      "Epoch 105 Loss 21.760530\n",
      "Epoch 106 Loss 21.696600\n",
      "Epoch 107 Loss 21.632881\n",
      "Epoch 108 Loss 21.569389\n",
      "Epoch 109 Loss 21.506104\n",
      "Epoch 110 Loss 21.443037\n",
      "Epoch 111 Loss 21.380190\n",
      "Epoch 112 Loss 21.317547\n",
      "Epoch 113 Loss 21.255119\n",
      "Epoch 114 Loss 21.192904\n",
      "Epoch 115 Loss 21.130901\n",
      "Epoch 116 Loss 21.069105\n",
      "Epoch 117 Loss 21.007528\n",
      "Epoch 118 Loss 20.946150\n",
      "Epoch 119 Loss 20.884983\n",
      "Epoch 120 Loss 20.824026\n",
      "Epoch 121 Loss 20.763273\n",
      "Epoch 122 Loss 20.702726\n",
      "Epoch 123 Loss 20.642384\n",
      "Epoch 124 Loss 20.582251\n",
      "Epoch 125 Loss 20.522322\n",
      "Epoch 126 Loss 20.462589\n",
      "Epoch 127 Loss 20.403067\n",
      "Epoch 128 Loss 20.343746\n",
      "Epoch 129 Loss 20.284622\n",
      "Epoch 130 Loss 20.225702\n",
      "Epoch 131 Loss 20.166983\n",
      "Epoch 132 Loss 20.108461\n",
      "Epoch 133 Loss 20.050135\n",
      "Epoch 134 Loss 19.992014\n",
      "Epoch 135 Loss 19.934088\n",
      "Epoch 136 Loss 19.876352\n",
      "Epoch 137 Loss 19.818821\n",
      "Epoch 138 Loss 19.761480\n",
      "Epoch 139 Loss 19.704332\n",
      "Epoch 140 Loss 19.647387\n",
      "Epoch 141 Loss 19.590626\n",
      "Epoch 142 Loss 19.534063\n",
      "Epoch 143 Loss 19.477690\n",
      "Epoch 144 Loss 19.421507\n",
      "Epoch 145 Loss 19.365517\n",
      "Epoch 146 Loss 19.309715\n",
      "Epoch 147 Loss 19.254107\n",
      "Epoch 148 Loss 19.198685\n",
      "Epoch 149 Loss 19.143446\n",
      "Epoch 150 Loss 19.088400\n",
      "Epoch 151 Loss 19.033545\n",
      "Epoch 152 Loss 18.978868\n",
      "Epoch 153 Loss 18.924377\n",
      "Epoch 154 Loss 18.870081\n",
      "Epoch 155 Loss 18.815960\n",
      "Epoch 156 Loss 18.762022\n",
      "Epoch 157 Loss 18.708269\n",
      "Epoch 158 Loss 18.654703\n",
      "Epoch 159 Loss 18.601313\n",
      "Epoch 160 Loss 18.548111\n",
      "Epoch 161 Loss 18.495081\n",
      "Epoch 162 Loss 18.442234\n",
      "Epoch 163 Loss 18.389570\n",
      "Epoch 164 Loss 18.337080\n",
      "Epoch 165 Loss 18.284777\n",
      "Epoch 166 Loss 18.232643\n",
      "Epoch 167 Loss 18.180687\n",
      "Epoch 168 Loss 18.128904\n",
      "Epoch 169 Loss 18.077303\n",
      "Epoch 170 Loss 18.025879\n",
      "Epoch 171 Loss 17.974623\n",
      "Epoch 172 Loss 17.923546\n",
      "Epoch 173 Loss 17.872641\n",
      "Epoch 174 Loss 17.821907\n",
      "Epoch 175 Loss 17.771343\n",
      "Epoch 176 Loss 17.720955\n",
      "Epoch 177 Loss 17.670738\n",
      "Epoch 178 Loss 17.620691\n",
      "Epoch 179 Loss 17.570814\n",
      "Epoch 180 Loss 17.521105\n",
      "Epoch 181 Loss 17.471563\n",
      "Epoch 182 Loss 17.422194\n",
      "Epoch 183 Loss 17.372992\n",
      "Epoch 184 Loss 17.323954\n",
      "Epoch 185 Loss 17.275085\n",
      "Epoch 186 Loss 17.226379\n",
      "Epoch 187 Loss 17.177839\n",
      "Epoch 188 Loss 17.129467\n",
      "Epoch 189 Loss 17.081255\n",
      "Epoch 190 Loss 17.033207\n",
      "Epoch 191 Loss 16.985327\n",
      "Epoch 192 Loss 16.937605\n",
      "Epoch 193 Loss 16.890047\n",
      "Epoch 194 Loss 16.842649\n",
      "Epoch 195 Loss 16.795412\n",
      "Epoch 196 Loss 16.748339\n",
      "Epoch 197 Loss 16.701424\n",
      "Epoch 198 Loss 16.654661\n",
      "Epoch 199 Loss 16.608065\n",
      "Epoch 200 Loss 16.561625\n",
      "Epoch 201 Loss 16.515343\n",
      "Epoch 202 Loss 16.469219\n",
      "Epoch 203 Loss 16.423250\n",
      "Epoch 204 Loss 16.377434\n",
      "Epoch 205 Loss 16.331776\n",
      "Epoch 206 Loss 16.286276\n",
      "Epoch 207 Loss 16.240925\n",
      "Epoch 208 Loss 16.195734\n",
      "Epoch 209 Loss 16.150694\n",
      "Epoch 210 Loss 16.105806\n",
      "Epoch 211 Loss 16.061071\n",
      "Epoch 212 Loss 16.016487\n",
      "Epoch 213 Loss 15.972058\n",
      "Epoch 214 Loss 15.927777\n",
      "Epoch 215 Loss 15.883645\n",
      "Epoch 216 Loss 15.839664\n",
      "Epoch 217 Loss 15.795832\n",
      "Epoch 218 Loss 15.752149\n",
      "Epoch 219 Loss 15.708612\n",
      "Epoch 220 Loss 15.665228\n",
      "Epoch 221 Loss 15.621990\n",
      "Epoch 222 Loss 15.578897\n",
      "Epoch 223 Loss 15.535950\n",
      "Epoch 224 Loss 15.493152\n",
      "Epoch 225 Loss 15.450497\n",
      "Epoch 226 Loss 15.407981\n",
      "Epoch 227 Loss 15.365615\n",
      "Epoch 228 Loss 15.323395\n",
      "Epoch 229 Loss 15.281318\n",
      "Epoch 230 Loss 15.239380\n",
      "Epoch 231 Loss 15.197586\n",
      "Epoch 232 Loss 15.155931\n",
      "Epoch 233 Loss 15.114425\n",
      "Epoch 234 Loss 15.073053\n",
      "Epoch 235 Loss 15.031823\n",
      "Epoch 236 Loss 14.990737\n",
      "Epoch 237 Loss 14.949784\n",
      "Epoch 238 Loss 14.908973\n",
      "Epoch 239 Loss 14.868304\n",
      "Epoch 240 Loss 14.827767\n",
      "Epoch 241 Loss 14.787370\n",
      "Epoch 242 Loss 14.747110\n",
      "Epoch 243 Loss 14.706989\n",
      "Epoch 244 Loss 14.667002\n",
      "Epoch 245 Loss 14.627149\n",
      "Epoch 246 Loss 14.587436\n",
      "Epoch 247 Loss 14.547854\n",
      "Epoch 248 Loss 14.508408\n",
      "Epoch 249 Loss 14.469095\n",
      "Epoch 250 Loss 14.429919\n",
      "Epoch 251 Loss 14.390872\n",
      "Epoch 252 Loss 14.351956\n",
      "Epoch 253 Loss 14.313177\n",
      "Epoch 254 Loss 14.274525\n",
      "Epoch 255 Loss 14.236008\n",
      "Epoch 256 Loss 14.197620\n",
      "Epoch 257 Loss 14.159363\n",
      "Epoch 258 Loss 14.121234\n",
      "Epoch 259 Loss 14.083237\n",
      "Epoch 260 Loss 14.045368\n",
      "Epoch 261 Loss 14.007627\n",
      "Epoch 262 Loss 13.970016\n",
      "Epoch 263 Loss 13.932532\n",
      "Epoch 264 Loss 13.895172\n",
      "Epoch 265 Loss 13.857942\n",
      "Epoch 266 Loss 13.820837\n",
      "Epoch 267 Loss 13.783858\n",
      "Epoch 268 Loss 13.747006\n",
      "Epoch 269 Loss 13.710278\n",
      "Epoch 270 Loss 13.673676\n",
      "Epoch 271 Loss 13.637196\n",
      "Epoch 272 Loss 13.600842\n",
      "Epoch 273 Loss 13.564609\n",
      "Epoch 274 Loss 13.528501\n",
      "Epoch 275 Loss 13.492515\n",
      "Epoch 276 Loss 13.456651\n",
      "Epoch 277 Loss 13.420910\n",
      "Epoch 278 Loss 13.385287\n",
      "Epoch 279 Loss 13.349787\n",
      "Epoch 280 Loss 13.314410\n",
      "Epoch 281 Loss 13.279148\n",
      "Epoch 282 Loss 13.244009\n",
      "Epoch 283 Loss 13.208993\n",
      "Epoch 284 Loss 13.174088\n",
      "Epoch 285 Loss 13.139307\n",
      "Epoch 286 Loss 13.104638\n",
      "Epoch 287 Loss 13.070093\n",
      "Epoch 288 Loss 13.035663\n",
      "Epoch 289 Loss 13.001349\n",
      "Epoch 290 Loss 12.967154\n",
      "Epoch 291 Loss 12.933074\n",
      "Epoch 292 Loss 12.899109\n",
      "Epoch 293 Loss 12.865259\n",
      "Epoch 294 Loss 12.831525\n",
      "Epoch 295 Loss 12.797904\n",
      "Epoch 296 Loss 12.764399\n",
      "Epoch 297 Loss 12.731007\n",
      "Epoch 298 Loss 12.697727\n",
      "Epoch 299 Loss 12.664560\n",
      "Epoch 300 Loss 12.631507\n",
      "Epoch 301 Loss 12.598566\n",
      "Epoch 302 Loss 12.565738\n",
      "Epoch 303 Loss 12.533021\n",
      "Epoch 304 Loss 12.500415\n",
      "Epoch 305 Loss 12.467919\n",
      "Epoch 306 Loss 12.435533\n",
      "Epoch 307 Loss 12.403255\n",
      "Epoch 308 Loss 12.371088\n",
      "Epoch 309 Loss 12.339031\n",
      "Epoch 310 Loss 12.307083\n",
      "Epoch 311 Loss 12.275247\n",
      "Epoch 312 Loss 12.243509\n",
      "Epoch 313 Loss 12.211887\n",
      "Epoch 314 Loss 12.180370\n",
      "Epoch 315 Loss 12.148962\n",
      "Epoch 316 Loss 12.117655\n",
      "Epoch 317 Loss 12.086463\n",
      "Epoch 318 Loss 12.055373\n",
      "Epoch 319 Loss 12.024384\n",
      "Epoch 320 Loss 11.993508\n",
      "Epoch 321 Loss 11.962732\n",
      "Epoch 322 Loss 11.932056\n",
      "Epoch 323 Loss 11.901492\n",
      "Epoch 324 Loss 11.871029\n",
      "Epoch 325 Loss 11.840671\n",
      "Epoch 326 Loss 11.810413\n",
      "Epoch 327 Loss 11.780257\n",
      "Epoch 328 Loss 11.750208\n",
      "Epoch 329 Loss 11.720258\n",
      "Epoch 330 Loss 11.690412\n",
      "Epoch 331 Loss 11.660664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 332 Loss 11.631016\n",
      "Epoch 333 Loss 11.601473\n",
      "Epoch 334 Loss 11.572030\n",
      "Epoch 335 Loss 11.542686\n",
      "Epoch 336 Loss 11.513440\n",
      "Epoch 337 Loss 11.484293\n",
      "Epoch 338 Loss 11.455247\n",
      "Epoch 339 Loss 11.426300\n",
      "Epoch 340 Loss 11.397448\n",
      "Epoch 341 Loss 11.368696\n",
      "Epoch 342 Loss 11.340043\n",
      "Epoch 343 Loss 11.311487\n",
      "Epoch 344 Loss 11.283028\n",
      "Epoch 345 Loss 11.254662\n",
      "Epoch 346 Loss 11.226396\n",
      "Epoch 347 Loss 11.198221\n",
      "Epoch 348 Loss 11.170149\n",
      "Epoch 349 Loss 11.142170\n",
      "Epoch 350 Loss 11.114283\n",
      "Epoch 351 Loss 11.086493\n",
      "Epoch 352 Loss 11.058796\n",
      "Epoch 353 Loss 11.031192\n",
      "Epoch 354 Loss 11.003686\n",
      "Epoch 355 Loss 10.976271\n",
      "Epoch 356 Loss 10.948948\n",
      "Epoch 357 Loss 10.921718\n",
      "Epoch 358 Loss 10.894581\n",
      "Epoch 359 Loss 10.867537\n",
      "Epoch 360 Loss 10.840583\n",
      "Epoch 361 Loss 10.813720\n",
      "Epoch 362 Loss 10.786951\n",
      "Epoch 363 Loss 10.760270\n",
      "Epoch 364 Loss 10.733681\n",
      "Epoch 365 Loss 10.707183\n",
      "Epoch 366 Loss 10.680775\n",
      "Epoch 367 Loss 10.654453\n",
      "Epoch 368 Loss 10.628225\n",
      "Epoch 369 Loss 10.602084\n",
      "Epoch 370 Loss 10.576032\n",
      "Epoch 371 Loss 10.550071\n",
      "Epoch 372 Loss 10.524195\n",
      "Epoch 373 Loss 10.498408\n",
      "Epoch 374 Loss 10.472707\n",
      "Epoch 375 Loss 10.447094\n",
      "Epoch 376 Loss 10.421568\n",
      "Epoch 377 Loss 10.396132\n",
      "Epoch 378 Loss 10.370778\n",
      "Epoch 379 Loss 10.345510\n",
      "Epoch 380 Loss 10.320329\n",
      "Epoch 381 Loss 10.295236\n",
      "Epoch 382 Loss 10.270224\n",
      "Epoch 383 Loss 10.245296\n",
      "Epoch 384 Loss 10.220456\n",
      "Epoch 385 Loss 10.195701\n",
      "Epoch 386 Loss 10.171027\n",
      "Epoch 387 Loss 10.146436\n",
      "Epoch 388 Loss 10.121934\n",
      "Epoch 389 Loss 10.097512\n",
      "Epoch 390 Loss 10.073174\n",
      "Epoch 391 Loss 10.048919\n",
      "Epoch 392 Loss 10.024742\n",
      "Epoch 393 Loss 10.000652\n",
      "Epoch 394 Loss 9.976640\n",
      "Epoch 395 Loss 9.952712\n",
      "Epoch 396 Loss 9.928863\n",
      "Epoch 397 Loss 9.905092\n",
      "Epoch 398 Loss 9.881409\n",
      "Epoch 399 Loss 9.857802\n",
      "Epoch 400 Loss 9.834277\n",
      "Epoch 401 Loss 9.810832\n",
      "Epoch 402 Loss 9.787466\n",
      "Epoch 403 Loss 9.764176\n",
      "Epoch 404 Loss 9.740971\n",
      "Epoch 405 Loss 9.717843\n",
      "Epoch 406 Loss 9.694793\n",
      "Epoch 407 Loss 9.671823\n",
      "Epoch 408 Loss 9.648926\n",
      "Epoch 409 Loss 9.626110\n",
      "Epoch 410 Loss 9.603373\n",
      "Epoch 411 Loss 9.580710\n",
      "Epoch 412 Loss 9.558124\n",
      "Epoch 413 Loss 9.535618\n",
      "Epoch 414 Loss 9.513185\n",
      "Epoch 415 Loss 9.490829\n",
      "Epoch 416 Loss 9.468551\n",
      "Epoch 417 Loss 9.446347\n",
      "Epoch 418 Loss 9.424216\n",
      "Epoch 419 Loss 9.402163\n",
      "Epoch 420 Loss 9.380185\n",
      "Epoch 421 Loss 9.358281\n",
      "Epoch 422 Loss 9.336448\n",
      "Epoch 423 Loss 9.314696\n",
      "Epoch 424 Loss 9.293013\n",
      "Epoch 425 Loss 9.271402\n",
      "Epoch 426 Loss 9.249870\n",
      "Epoch 427 Loss 9.228409\n",
      "Epoch 428 Loss 9.207021\n",
      "Epoch 429 Loss 9.185704\n",
      "Epoch 430 Loss 9.164462\n",
      "Epoch 431 Loss 9.143288\n",
      "Epoch 432 Loss 9.122189\n",
      "Epoch 433 Loss 9.101160\n",
      "Epoch 434 Loss 9.080204\n",
      "Epoch 435 Loss 9.059317\n",
      "Epoch 436 Loss 9.038502\n",
      "Epoch 437 Loss 9.017757\n",
      "Epoch 438 Loss 8.997085\n",
      "Epoch 439 Loss 8.976479\n",
      "Epoch 440 Loss 8.955945\n",
      "Epoch 441 Loss 8.935481\n",
      "Epoch 442 Loss 8.915089\n",
      "Epoch 443 Loss 8.894763\n",
      "Epoch 444 Loss 8.874508\n",
      "Epoch 445 Loss 8.854318\n",
      "Epoch 446 Loss 8.834197\n",
      "Epoch 447 Loss 8.814149\n",
      "Epoch 448 Loss 8.794162\n",
      "Epoch 449 Loss 8.774252\n",
      "Epoch 450 Loss 8.754406\n",
      "Epoch 451 Loss 8.734625\n",
      "Epoch 452 Loss 8.714911\n",
      "Epoch 453 Loss 8.695266\n",
      "Epoch 454 Loss 8.675689\n",
      "Epoch 455 Loss 8.656174\n",
      "Epoch 456 Loss 8.636728\n",
      "Epoch 457 Loss 8.617346\n",
      "Epoch 458 Loss 8.598029\n",
      "Epoch 459 Loss 8.578781\n",
      "Epoch 460 Loss 8.559597\n",
      "Epoch 461 Loss 8.540478\n",
      "Epoch 462 Loss 8.521426\n",
      "Epoch 463 Loss 8.502438\n",
      "Epoch 464 Loss 8.483516\n",
      "Epoch 465 Loss 8.464652\n",
      "Epoch 466 Loss 8.445858\n",
      "Epoch 467 Loss 8.427128\n",
      "Epoch 468 Loss 8.408456\n",
      "Epoch 469 Loss 8.389848\n",
      "Epoch 470 Loss 8.371305\n",
      "Epoch 471 Loss 8.352828\n",
      "Epoch 472 Loss 8.334408\n",
      "Epoch 473 Loss 8.316055\n",
      "Epoch 474 Loss 8.297764\n",
      "Epoch 475 Loss 8.279534\n",
      "Epoch 476 Loss 8.261369\n",
      "Epoch 477 Loss 8.243261\n",
      "Epoch 478 Loss 8.225213\n",
      "Epoch 479 Loss 8.207232\n",
      "Epoch 480 Loss 8.189310\n",
      "Epoch 481 Loss 8.171450\n",
      "Epoch 482 Loss 8.153648\n",
      "Epoch 483 Loss 8.135907\n",
      "Epoch 484 Loss 8.118226\n",
      "Epoch 485 Loss 8.100607\n",
      "Epoch 486 Loss 8.083045\n",
      "Epoch 487 Loss 8.065548\n",
      "Epoch 488 Loss 8.048104\n",
      "Epoch 489 Loss 8.030723\n",
      "Epoch 490 Loss 8.013400\n",
      "Epoch 491 Loss 7.996135\n",
      "Epoch 492 Loss 7.978929\n",
      "Epoch 493 Loss 7.961784\n",
      "Epoch 494 Loss 7.944690\n",
      "Epoch 495 Loss 7.927662\n",
      "Epoch 496 Loss 7.910690\n",
      "Epoch 497 Loss 7.893775\n",
      "Epoch 498 Loss 7.876915\n",
      "Epoch 499 Loss 7.860116\n",
      "Epoch 500 Loss 7.843370\n",
      "Epoch 501 Loss 7.826681\n",
      "Epoch 502 Loss 7.810053\n",
      "Epoch 503 Loss 7.793480\n",
      "Epoch 504 Loss 7.776962\n",
      "Epoch 505 Loss 7.760498\n",
      "Epoch 506 Loss 7.744092\n",
      "Epoch 507 Loss 7.727745\n",
      "Epoch 508 Loss 7.711447\n",
      "Epoch 509 Loss 7.695212\n",
      "Epoch 510 Loss 7.679024\n",
      "Epoch 511 Loss 7.662895\n",
      "Epoch 512 Loss 7.646819\n",
      "Epoch 513 Loss 7.630803\n",
      "Epoch 514 Loss 7.614836\n",
      "Epoch 515 Loss 7.598925\n",
      "Epoch 516 Loss 7.583069\n",
      "Epoch 517 Loss 7.567266\n",
      "Epoch 518 Loss 7.551516\n",
      "Epoch 519 Loss 7.535819\n",
      "Epoch 520 Loss 7.520176\n",
      "Epoch 521 Loss 7.504588\n",
      "Epoch 522 Loss 7.489048\n",
      "Epoch 523 Loss 7.473566\n",
      "Epoch 524 Loss 7.458135\n",
      "Epoch 525 Loss 7.442751\n",
      "Epoch 526 Loss 7.427426\n",
      "Epoch 527 Loss 7.412152\n",
      "Epoch 528 Loss 7.396928\n",
      "Epoch 529 Loss 7.381756\n",
      "Epoch 530 Loss 7.366636\n",
      "Epoch 531 Loss 7.351566\n",
      "Epoch 532 Loss 7.336550\n",
      "Epoch 533 Loss 7.321585\n",
      "Epoch 534 Loss 7.306670\n",
      "Epoch 535 Loss 7.291803\n",
      "Epoch 536 Loss 7.276989\n",
      "Epoch 537 Loss 7.262227\n",
      "Epoch 538 Loss 7.247512\n",
      "Epoch 539 Loss 7.232846\n",
      "Epoch 540 Loss 7.218231\n",
      "Epoch 541 Loss 7.203666\n",
      "Epoch 542 Loss 7.189151\n",
      "Epoch 543 Loss 7.174683\n",
      "Epoch 544 Loss 7.160267\n",
      "Epoch 545 Loss 7.145897\n",
      "Epoch 546 Loss 7.131578\n",
      "Epoch 547 Loss 7.117305\n",
      "Epoch 548 Loss 7.103083\n",
      "Epoch 549 Loss 7.088911\n",
      "Epoch 550 Loss 7.074785\n",
      "Epoch 551 Loss 7.060707\n",
      "Epoch 552 Loss 7.046677\n",
      "Epoch 553 Loss 7.032695\n",
      "Epoch 554 Loss 7.018756\n",
      "Epoch 555 Loss 7.004869\n",
      "Epoch 556 Loss 6.991029\n",
      "Epoch 557 Loss 6.977232\n",
      "Epoch 558 Loss 6.963488\n",
      "Epoch 559 Loss 6.949786\n",
      "Epoch 560 Loss 6.936135\n",
      "Epoch 561 Loss 6.922528\n",
      "Epoch 562 Loss 6.908967\n",
      "Epoch 563 Loss 6.895452\n",
      "Epoch 564 Loss 6.881980\n",
      "Epoch 565 Loss 6.868558\n",
      "Epoch 566 Loss 6.855180\n",
      "Epoch 567 Loss 6.841848\n",
      "Epoch 568 Loss 6.828561\n",
      "Epoch 569 Loss 6.815319\n",
      "Epoch 570 Loss 6.802118\n",
      "Epoch 571 Loss 6.788968\n",
      "Epoch 572 Loss 6.775864\n",
      "Epoch 573 Loss 6.762798\n",
      "Epoch 574 Loss 6.749779\n",
      "Epoch 575 Loss 6.736803\n",
      "Epoch 576 Loss 6.723875\n",
      "Epoch 577 Loss 6.710986\n",
      "Epoch 578 Loss 6.698142\n",
      "Epoch 579 Loss 6.685344\n",
      "Epoch 580 Loss 6.672589\n",
      "Epoch 581 Loss 6.659874\n",
      "Epoch 582 Loss 6.647207\n",
      "Epoch 583 Loss 6.634577\n",
      "Epoch 584 Loss 6.621995\n",
      "Epoch 585 Loss 6.609454\n",
      "Epoch 586 Loss 6.596954\n",
      "Epoch 587 Loss 6.584500\n",
      "Epoch 588 Loss 6.572087\n",
      "Epoch 589 Loss 6.559712\n",
      "Epoch 590 Loss 6.547384\n",
      "Epoch 591 Loss 6.535097\n",
      "Epoch 592 Loss 6.522851\n",
      "Epoch 593 Loss 6.510646\n",
      "Epoch 594 Loss 6.498481\n",
      "Epoch 595 Loss 6.486362\n",
      "Epoch 596 Loss 6.474282\n",
      "Epoch 597 Loss 6.462242\n",
      "Epoch 598 Loss 6.450243\n",
      "Epoch 599 Loss 6.438284\n",
      "Epoch 600 Loss 6.426367\n",
      "Epoch 601 Loss 6.414490\n",
      "Epoch 602 Loss 6.402655\n",
      "Epoch 603 Loss 6.390859\n",
      "Epoch 604 Loss 6.379102\n",
      "Epoch 605 Loss 6.367384\n",
      "Epoch 606 Loss 6.355706\n",
      "Epoch 607 Loss 6.344071\n",
      "Epoch 608 Loss 6.332472\n",
      "Epoch 609 Loss 6.320912\n",
      "Epoch 610 Loss 6.309395\n",
      "Epoch 611 Loss 6.297915\n",
      "Epoch 612 Loss 6.286473\n",
      "Epoch 613 Loss 6.275074\n",
      "Epoch 614 Loss 6.263707\n",
      "Epoch 615 Loss 6.252382\n",
      "Epoch 616 Loss 6.241098\n",
      "Epoch 617 Loss 6.229849\n",
      "Epoch 618 Loss 6.218639\n",
      "Epoch 619 Loss 6.207471\n",
      "Epoch 620 Loss 6.196334\n",
      "Epoch 621 Loss 6.185240\n",
      "Epoch 622 Loss 6.174181\n",
      "Epoch 623 Loss 6.163159\n",
      "Epoch 624 Loss 6.152177\n",
      "Epoch 625 Loss 6.141229\n",
      "Epoch 626 Loss 6.130321\n",
      "Epoch 627 Loss 6.119448\n",
      "Epoch 628 Loss 6.108614\n",
      "Epoch 629 Loss 6.097815\n",
      "Epoch 630 Loss 6.087054\n",
      "Epoch 631 Loss 6.076329\n",
      "Epoch 632 Loss 6.065643\n",
      "Epoch 633 Loss 6.054988\n",
      "Epoch 634 Loss 6.044372\n",
      "Epoch 635 Loss 6.033794\n",
      "Epoch 636 Loss 6.023247\n",
      "Epoch 637 Loss 6.012738\n",
      "Epoch 638 Loss 6.002264\n",
      "Epoch 639 Loss 5.991829\n",
      "Epoch 640 Loss 5.981426\n",
      "Epoch 641 Loss 5.971057\n",
      "Epoch 642 Loss 5.960727\n",
      "Epoch 643 Loss 5.950432\n",
      "Epoch 644 Loss 5.940171\n",
      "Epoch 645 Loss 5.929944\n",
      "Epoch 646 Loss 5.919752\n",
      "Epoch 647 Loss 5.909597\n",
      "Epoch 648 Loss 5.899473\n",
      "Epoch 649 Loss 5.889384\n",
      "Epoch 650 Loss 5.879326\n",
      "Epoch 651 Loss 5.869310\n",
      "Epoch 652 Loss 5.859322\n",
      "Epoch 653 Loss 5.849374\n",
      "Epoch 654 Loss 5.839453\n",
      "Epoch 655 Loss 5.829570\n",
      "Epoch 656 Loss 5.819718\n",
      "Epoch 657 Loss 5.809900\n",
      "Epoch 658 Loss 5.800117\n",
      "Epoch 659 Loss 5.790367\n",
      "Epoch 660 Loss 5.780647\n",
      "Epoch 661 Loss 5.770962\n",
      "Epoch 662 Loss 5.761312\n",
      "Epoch 663 Loss 5.751693\n",
      "Epoch 664 Loss 5.742105\n",
      "Epoch 665 Loss 5.732550\n",
      "Epoch 666 Loss 5.723031\n",
      "Epoch 667 Loss 5.713540\n",
      "Epoch 668 Loss 5.704084\n",
      "Epoch 669 Loss 5.694658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 670 Loss 5.685265\n",
      "Epoch 671 Loss 5.675904\n",
      "Epoch 672 Loss 5.666573\n",
      "Epoch 673 Loss 5.657277\n",
      "Epoch 674 Loss 5.648010\n",
      "Epoch 675 Loss 5.638776\n",
      "Epoch 676 Loss 5.629575\n",
      "Epoch 677 Loss 5.620402\n",
      "Epoch 678 Loss 5.611260\n",
      "Epoch 679 Loss 5.602148\n",
      "Epoch 680 Loss 5.593071\n",
      "Epoch 681 Loss 5.584022\n",
      "Epoch 682 Loss 5.575005\n",
      "Epoch 683 Loss 5.566019\n",
      "Epoch 684 Loss 5.557063\n",
      "Epoch 685 Loss 5.548136\n",
      "Epoch 686 Loss 5.539241\n",
      "Epoch 687 Loss 5.530376\n",
      "Epoch 688 Loss 5.521540\n",
      "Epoch 689 Loss 5.512733\n",
      "Epoch 690 Loss 5.503958\n",
      "Epoch 691 Loss 5.495212\n",
      "Epoch 692 Loss 5.486496\n",
      "Epoch 693 Loss 5.477808\n",
      "Epoch 694 Loss 5.469152\n",
      "Epoch 695 Loss 5.460525\n",
      "Epoch 696 Loss 5.451928\n",
      "Epoch 697 Loss 5.443359\n",
      "Epoch 698 Loss 5.434820\n",
      "Epoch 699 Loss 5.426310\n",
      "Epoch 700 Loss 5.417827\n",
      "Epoch 701 Loss 5.409373\n",
      "Epoch 702 Loss 5.400949\n",
      "Epoch 703 Loss 5.392551\n",
      "Epoch 704 Loss 5.384184\n",
      "Epoch 705 Loss 5.375845\n",
      "Epoch 706 Loss 5.367537\n",
      "Epoch 707 Loss 5.359253\n",
      "Epoch 708 Loss 5.350998\n",
      "Epoch 709 Loss 5.342771\n",
      "Epoch 710 Loss 5.334575\n",
      "Epoch 711 Loss 5.326403\n",
      "Epoch 712 Loss 5.318259\n",
      "Epoch 713 Loss 5.310144\n",
      "Epoch 714 Loss 5.302055\n",
      "Epoch 715 Loss 5.293994\n",
      "Epoch 716 Loss 5.285964\n",
      "Epoch 717 Loss 5.277958\n",
      "Epoch 718 Loss 5.269979\n",
      "Epoch 719 Loss 5.262026\n",
      "Epoch 720 Loss 5.254103\n",
      "Epoch 721 Loss 5.246205\n",
      "Epoch 722 Loss 5.238335\n",
      "Epoch 723 Loss 5.230491\n",
      "Epoch 724 Loss 5.222673\n",
      "Epoch 725 Loss 5.214881\n",
      "Epoch 726 Loss 5.207120\n",
      "Epoch 727 Loss 5.199381\n",
      "Epoch 728 Loss 5.191670\n",
      "Epoch 729 Loss 5.183984\n",
      "Epoch 730 Loss 5.176324\n",
      "Epoch 731 Loss 5.168688\n",
      "Epoch 732 Loss 5.161084\n",
      "Epoch 733 Loss 5.153500\n",
      "Epoch 734 Loss 5.145943\n",
      "Epoch 735 Loss 5.138412\n",
      "Epoch 736 Loss 5.130910\n",
      "Epoch 737 Loss 5.123428\n",
      "Epoch 738 Loss 5.115977\n",
      "Epoch 739 Loss 5.108547\n",
      "Epoch 740 Loss 5.101144\n",
      "Epoch 741 Loss 5.093765\n",
      "Epoch 742 Loss 5.086413\n",
      "Epoch 743 Loss 5.079085\n",
      "Epoch 744 Loss 5.071782\n",
      "Epoch 745 Loss 5.064505\n",
      "Epoch 746 Loss 5.057247\n",
      "Epoch 747 Loss 5.050022\n",
      "Epoch 748 Loss 5.042817\n",
      "Epoch 749 Loss 5.035636\n",
      "Epoch 750 Loss 5.028476\n",
      "Epoch 751 Loss 5.021346\n",
      "Epoch 752 Loss 5.014239\n",
      "Epoch 753 Loss 5.007157\n",
      "Epoch 754 Loss 5.000099\n",
      "Epoch 755 Loss 4.993064\n",
      "Epoch 756 Loss 4.986051\n",
      "Epoch 757 Loss 4.979065\n",
      "Epoch 758 Loss 4.972100\n",
      "Epoch 759 Loss 4.965159\n",
      "Epoch 760 Loss 4.958245\n",
      "Epoch 761 Loss 4.951350\n",
      "Epoch 762 Loss 4.944479\n",
      "Epoch 763 Loss 4.937633\n",
      "Epoch 764 Loss 4.930812\n",
      "Epoch 765 Loss 4.924009\n",
      "Epoch 766 Loss 4.917234\n",
      "Epoch 767 Loss 4.910480\n",
      "Epoch 768 Loss 4.903749\n",
      "Epoch 769 Loss 4.897040\n",
      "Epoch 770 Loss 4.890356\n",
      "Epoch 771 Loss 4.883691\n",
      "Epoch 772 Loss 4.877052\n",
      "Epoch 773 Loss 4.870436\n",
      "Epoch 774 Loss 4.863839\n",
      "Epoch 775 Loss 4.857267\n",
      "Epoch 776 Loss 4.850717\n",
      "Epoch 777 Loss 4.844189\n",
      "Epoch 778 Loss 4.837683\n",
      "Epoch 779 Loss 4.831196\n",
      "Epoch 780 Loss 4.824737\n",
      "Epoch 781 Loss 4.818298\n",
      "Epoch 782 Loss 4.811880\n",
      "Epoch 783 Loss 4.805481\n",
      "Epoch 784 Loss 4.799106\n",
      "Epoch 785 Loss 4.792755\n",
      "Epoch 786 Loss 4.786422\n",
      "Epoch 787 Loss 4.780112\n",
      "Epoch 788 Loss 4.773824\n",
      "Epoch 789 Loss 4.767559\n",
      "Epoch 790 Loss 4.761311\n",
      "Epoch 791 Loss 4.755087\n",
      "Epoch 792 Loss 4.748885\n",
      "Epoch 793 Loss 4.742701\n",
      "Epoch 794 Loss 4.736537\n",
      "Epoch 795 Loss 4.730397\n",
      "Epoch 796 Loss 4.724279\n",
      "Epoch 797 Loss 4.718181\n",
      "Epoch 798 Loss 4.712101\n",
      "Epoch 799 Loss 4.706046\n",
      "Epoch 800 Loss 4.700009\n",
      "Epoch 801 Loss 4.693989\n",
      "Epoch 802 Loss 4.687995\n",
      "Epoch 803 Loss 4.682020\n",
      "Epoch 804 Loss 4.676063\n",
      "Epoch 805 Loss 4.670130\n",
      "Epoch 806 Loss 4.664214\n",
      "Epoch 807 Loss 4.658320\n",
      "Epoch 808 Loss 4.652445\n",
      "Epoch 809 Loss 4.646592\n",
      "Epoch 810 Loss 4.640753\n",
      "Epoch 811 Loss 4.634938\n",
      "Epoch 812 Loss 4.629142\n",
      "Epoch 813 Loss 4.623368\n",
      "Epoch 814 Loss 4.617611\n",
      "Epoch 815 Loss 4.611873\n",
      "Epoch 816 Loss 4.606156\n",
      "Epoch 817 Loss 4.600458\n",
      "Epoch 818 Loss 4.594780\n",
      "Epoch 819 Loss 4.589119\n",
      "Epoch 820 Loss 4.583479\n",
      "Epoch 821 Loss 4.577857\n",
      "Epoch 822 Loss 4.572256\n",
      "Epoch 823 Loss 4.566675\n",
      "Epoch 824 Loss 4.561109\n",
      "Epoch 825 Loss 4.555565\n",
      "Epoch 826 Loss 4.550039\n",
      "Epoch 827 Loss 4.544533\n",
      "Epoch 828 Loss 4.539044\n",
      "Epoch 829 Loss 4.533575\n",
      "Epoch 830 Loss 4.528122\n",
      "Epoch 831 Loss 4.522691\n",
      "Epoch 832 Loss 4.517276\n",
      "Epoch 833 Loss 4.511879\n",
      "Epoch 834 Loss 4.506504\n",
      "Epoch 835 Loss 4.501141\n",
      "Epoch 836 Loss 4.495801\n",
      "Epoch 837 Loss 4.490474\n",
      "Epoch 838 Loss 4.485170\n",
      "Epoch 839 Loss 4.479884\n",
      "Epoch 840 Loss 4.474614\n",
      "Epoch 841 Loss 4.469364\n",
      "Epoch 842 Loss 4.464129\n",
      "Epoch 843 Loss 4.458913\n",
      "Epoch 844 Loss 4.453716\n",
      "Epoch 845 Loss 4.448534\n",
      "Epoch 846 Loss 4.443372\n",
      "Epoch 847 Loss 4.438227\n",
      "Epoch 848 Loss 4.433099\n",
      "Epoch 849 Loss 4.427989\n",
      "Epoch 850 Loss 4.422897\n",
      "Epoch 851 Loss 4.417819\n",
      "Epoch 852 Loss 4.412762\n",
      "Epoch 853 Loss 4.407720\n",
      "Epoch 854 Loss 4.402697\n",
      "Epoch 855 Loss 4.397688\n",
      "Epoch 856 Loss 4.392697\n",
      "Epoch 857 Loss 4.387725\n",
      "Epoch 858 Loss 4.382769\n",
      "Epoch 859 Loss 4.377828\n",
      "Epoch 860 Loss 4.372905\n",
      "Epoch 861 Loss 4.368000\n",
      "Epoch 862 Loss 4.363111\n",
      "Epoch 863 Loss 4.358238\n",
      "Epoch 864 Loss 4.353383\n",
      "Epoch 865 Loss 4.348542\n",
      "Epoch 866 Loss 4.343716\n",
      "Epoch 867 Loss 4.338911\n",
      "Epoch 868 Loss 4.334121\n",
      "Epoch 869 Loss 4.329345\n",
      "Epoch 870 Loss 4.324588\n",
      "Epoch 871 Loss 4.319845\n",
      "Epoch 872 Loss 4.315118\n",
      "Epoch 873 Loss 4.310409\n",
      "Epoch 874 Loss 4.305714\n",
      "Epoch 875 Loss 4.301035\n",
      "Epoch 876 Loss 4.296376\n",
      "Epoch 877 Loss 4.291727\n",
      "Epoch 878 Loss 4.287097\n",
      "Epoch 879 Loss 4.282482\n",
      "Epoch 880 Loss 4.277882\n",
      "Epoch 881 Loss 4.273299\n",
      "Epoch 882 Loss 4.268732\n",
      "Epoch 883 Loss 4.264178\n",
      "Epoch 884 Loss 4.259643\n",
      "Epoch 885 Loss 4.255120\n",
      "Epoch 886 Loss 4.250613\n",
      "Epoch 887 Loss 4.246124\n",
      "Epoch 888 Loss 4.241648\n",
      "Epoch 889 Loss 4.237185\n",
      "Epoch 890 Loss 4.232740\n",
      "Epoch 891 Loss 4.228308\n",
      "Epoch 892 Loss 4.223895\n",
      "Epoch 893 Loss 4.219494\n",
      "Epoch 894 Loss 4.215109\n",
      "Epoch 895 Loss 4.210737\n",
      "Epoch 896 Loss 4.206383\n",
      "Epoch 897 Loss 4.202042\n",
      "Epoch 898 Loss 4.197715\n",
      "Epoch 899 Loss 4.193405\n",
      "Epoch 900 Loss 4.189108\n",
      "Epoch 901 Loss 4.184825\n",
      "Epoch 902 Loss 4.180559\n",
      "Epoch 903 Loss 4.176305\n",
      "Epoch 904 Loss 4.172065\n",
      "Epoch 905 Loss 4.167842\n",
      "Epoch 906 Loss 4.163631\n",
      "Epoch 907 Loss 4.159436\n",
      "Epoch 908 Loss 4.155253\n",
      "Epoch 909 Loss 4.151086\n",
      "Epoch 910 Loss 4.146934\n",
      "Epoch 911 Loss 4.142795\n",
      "Epoch 912 Loss 4.138669\n",
      "Epoch 913 Loss 4.134559\n",
      "Epoch 914 Loss 4.130464\n",
      "Epoch 915 Loss 4.126378\n",
      "Epoch 916 Loss 4.122310\n",
      "Epoch 917 Loss 4.118254\n",
      "Epoch 918 Loss 4.114213\n",
      "Epoch 919 Loss 4.110184\n",
      "Epoch 920 Loss 4.106169\n",
      "Epoch 921 Loss 4.102170\n",
      "Epoch 922 Loss 4.098181\n",
      "Epoch 923 Loss 4.094210\n",
      "Epoch 924 Loss 4.090249\n",
      "Epoch 925 Loss 4.086300\n",
      "Epoch 926 Loss 4.082366\n",
      "Epoch 927 Loss 4.078448\n",
      "Epoch 928 Loss 4.074541\n",
      "Epoch 929 Loss 4.070649\n",
      "Epoch 930 Loss 4.066768\n",
      "Epoch 931 Loss 4.062900\n",
      "Epoch 932 Loss 4.059047\n",
      "Epoch 933 Loss 4.055204\n",
      "Epoch 934 Loss 4.051379\n",
      "Epoch 935 Loss 4.047564\n",
      "Epoch 936 Loss 4.043762\n",
      "Epoch 937 Loss 4.039972\n",
      "Epoch 938 Loss 4.036197\n",
      "Epoch 939 Loss 4.032434\n",
      "Epoch 940 Loss 4.028686\n",
      "Epoch 941 Loss 4.024947\n",
      "Epoch 942 Loss 4.021224\n",
      "Epoch 943 Loss 4.017509\n",
      "Epoch 944 Loss 4.013810\n",
      "Epoch 945 Loss 4.010122\n",
      "Epoch 946 Loss 4.006450\n",
      "Epoch 947 Loss 4.002785\n",
      "Epoch 948 Loss 3.999137\n",
      "Epoch 949 Loss 3.995498\n",
      "Epoch 950 Loss 3.991874\n",
      "Epoch 951 Loss 3.988262\n",
      "Epoch 952 Loss 3.984659\n",
      "Epoch 953 Loss 3.981071\n",
      "Epoch 954 Loss 3.977497\n",
      "Epoch 955 Loss 3.973931\n",
      "Epoch 956 Loss 3.970381\n",
      "Epoch 957 Loss 3.966840\n",
      "Epoch 958 Loss 3.963312\n",
      "Epoch 959 Loss 3.959797\n",
      "Epoch 960 Loss 3.956295\n",
      "Epoch 961 Loss 3.952801\n",
      "Epoch 962 Loss 3.949323\n",
      "Epoch 963 Loss 3.945855\n",
      "Epoch 964 Loss 3.942398\n",
      "Epoch 965 Loss 3.938953\n",
      "Epoch 966 Loss 3.935521\n",
      "Epoch 967 Loss 3.932096\n",
      "Epoch 968 Loss 3.928688\n",
      "Epoch 969 Loss 3.925292\n",
      "Epoch 970 Loss 3.921906\n",
      "Epoch 971 Loss 3.918528\n",
      "Epoch 972 Loss 3.915166\n",
      "Epoch 973 Loss 3.911815\n",
      "Epoch 974 Loss 3.908474\n",
      "Epoch 975 Loss 3.905144\n",
      "Epoch 976 Loss 3.901824\n",
      "Epoch 977 Loss 3.898517\n",
      "Epoch 978 Loss 3.895222\n",
      "Epoch 979 Loss 3.891935\n",
      "Epoch 980 Loss 3.888664\n",
      "Epoch 981 Loss 3.885400\n",
      "Epoch 982 Loss 3.882150\n",
      "Epoch 983 Loss 3.878911\n",
      "Epoch 984 Loss 3.875680\n",
      "Epoch 985 Loss 3.872463\n",
      "Epoch 986 Loss 3.869256\n",
      "Epoch 987 Loss 3.866060\n",
      "Epoch 988 Loss 3.862873\n",
      "Epoch 989 Loss 3.859699\n",
      "Epoch 990 Loss 3.856535\n",
      "Epoch 991 Loss 3.853381\n",
      "Epoch 992 Loss 3.850237\n",
      "Epoch 993 Loss 3.847109\n",
      "Epoch 994 Loss 3.843984\n",
      "Epoch 995 Loss 3.840876\n",
      "Epoch 996 Loss 3.837775\n",
      "Epoch 997 Loss 3.834686\n",
      "Epoch 998 Loss 3.831606\n",
      "Epoch 999 Loss 3.828538\n",
      "Epoch 1000 Loss 3.825484\n",
      "Epoch 1001 Loss 3.822433\n",
      "Epoch 1002 Loss 3.819398\n",
      "Epoch 1003 Loss 3.816369\n",
      "Epoch 1004 Loss 3.813350\n",
      "Epoch 1005 Loss 3.810344\n",
      "Epoch 1006 Loss 3.807348\n",
      "Epoch 1007 Loss 3.804360\n",
      "Epoch 1008 Loss 3.801384\n",
      "Epoch 1009 Loss 3.798421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1010 Loss 3.795465\n",
      "Epoch 1011 Loss 3.792518\n",
      "Epoch 1012 Loss 3.789584\n",
      "Epoch 1013 Loss 3.786658\n",
      "Epoch 1014 Loss 3.783740\n",
      "Epoch 1015 Loss 3.780832\n",
      "Epoch 1016 Loss 3.777939\n",
      "Epoch 1017 Loss 3.775053\n",
      "Epoch 1018 Loss 3.772173\n",
      "Epoch 1019 Loss 3.769310\n",
      "Epoch 1020 Loss 3.766451\n",
      "Epoch 1021 Loss 3.763602\n",
      "Epoch 1022 Loss 3.760766\n",
      "Epoch 1023 Loss 3.757936\n",
      "Epoch 1024 Loss 3.755118\n",
      "Epoch 1025 Loss 3.752309\n",
      "Epoch 1026 Loss 3.749511\n",
      "Epoch 1027 Loss 3.746722\n",
      "Epoch 1028 Loss 3.743940\n",
      "Epoch 1029 Loss 3.741169\n",
      "Epoch 1030 Loss 3.738407\n",
      "Epoch 1031 Loss 3.735656\n",
      "Epoch 1032 Loss 3.732914\n",
      "Epoch 1033 Loss 3.730181\n",
      "Epoch 1034 Loss 3.727456\n",
      "Epoch 1035 Loss 3.724741\n",
      "Epoch 1036 Loss 3.722034\n",
      "Epoch 1037 Loss 3.719337\n",
      "Epoch 1038 Loss 3.716650\n",
      "Epoch 1039 Loss 3.713972\n",
      "Epoch 1040 Loss 3.711302\n",
      "Epoch 1041 Loss 3.708643\n",
      "Epoch 1042 Loss 3.705990\n",
      "Epoch 1043 Loss 3.703351\n",
      "Epoch 1044 Loss 3.700716\n",
      "Epoch 1045 Loss 3.698092\n",
      "Epoch 1046 Loss 3.695476\n",
      "Epoch 1047 Loss 3.692869\n",
      "Epoch 1048 Loss 3.690273\n",
      "Epoch 1049 Loss 3.687683\n",
      "Epoch 1050 Loss 3.685103\n",
      "Epoch 1051 Loss 3.682532\n",
      "Epoch 1052 Loss 3.679969\n",
      "Epoch 1053 Loss 3.677417\n",
      "Epoch 1054 Loss 3.674871\n",
      "Epoch 1055 Loss 3.672334\n",
      "Epoch 1056 Loss 3.669805\n",
      "Epoch 1057 Loss 3.667287\n",
      "Epoch 1058 Loss 3.664775\n",
      "Epoch 1059 Loss 3.662273\n",
      "Epoch 1060 Loss 3.659778\n",
      "Epoch 1061 Loss 3.657295\n",
      "Epoch 1062 Loss 3.654816\n",
      "Epoch 1063 Loss 3.652350\n",
      "Epoch 1064 Loss 3.649889\n",
      "Epoch 1065 Loss 3.647437\n",
      "Epoch 1066 Loss 3.644991\n",
      "Epoch 1067 Loss 3.642559\n",
      "Epoch 1068 Loss 3.640131\n",
      "Epoch 1069 Loss 3.637711\n",
      "Epoch 1070 Loss 3.635302\n",
      "Epoch 1071 Loss 3.632902\n",
      "Epoch 1072 Loss 3.630508\n",
      "Epoch 1073 Loss 3.628119\n",
      "Epoch 1074 Loss 3.625741\n",
      "Epoch 1075 Loss 3.623374\n",
      "Epoch 1076 Loss 3.621010\n",
      "Epoch 1077 Loss 3.618659\n",
      "Epoch 1078 Loss 3.616311\n",
      "Epoch 1079 Loss 3.613973\n",
      "Epoch 1080 Loss 3.611643\n",
      "Epoch 1081 Loss 3.609322\n",
      "Epoch 1082 Loss 3.607007\n",
      "Epoch 1083 Loss 3.604701\n",
      "Epoch 1084 Loss 3.602404\n",
      "Epoch 1085 Loss 3.600114\n",
      "Epoch 1086 Loss 3.597830\n",
      "Epoch 1087 Loss 3.595553\n",
      "Epoch 1088 Loss 3.593287\n",
      "Epoch 1089 Loss 3.591030\n",
      "Epoch 1090 Loss 3.588776\n",
      "Epoch 1091 Loss 3.586534\n",
      "Epoch 1092 Loss 3.584295\n",
      "Epoch 1093 Loss 3.582067\n",
      "Epoch 1094 Loss 3.579846\n",
      "Epoch 1095 Loss 3.577631\n",
      "Epoch 1096 Loss 3.575424\n",
      "Epoch 1097 Loss 3.573225\n",
      "Epoch 1098 Loss 3.571034\n",
      "Epoch 1099 Loss 3.568848\n",
      "Epoch 1100 Loss 3.566673\n",
      "Epoch 1101 Loss 3.564506\n",
      "Epoch 1102 Loss 3.562340\n",
      "Epoch 1103 Loss 3.560185\n",
      "Epoch 1104 Loss 3.558040\n",
      "Epoch 1105 Loss 3.555901\n",
      "Epoch 1106 Loss 3.553767\n",
      "Epoch 1107 Loss 3.551641\n",
      "Epoch 1108 Loss 3.549524\n",
      "Epoch 1109 Loss 3.547411\n",
      "Epoch 1110 Loss 3.545309\n",
      "Epoch 1111 Loss 3.543211\n",
      "Epoch 1112 Loss 3.541124\n",
      "Epoch 1113 Loss 3.539041\n",
      "Epoch 1114 Loss 3.536966\n",
      "Epoch 1115 Loss 3.534897\n",
      "Epoch 1116 Loss 3.532835\n",
      "Epoch 1117 Loss 3.530781\n",
      "Epoch 1118 Loss 3.528734\n",
      "Epoch 1119 Loss 3.526694\n",
      "Epoch 1120 Loss 3.524662\n",
      "Epoch 1121 Loss 3.522633\n",
      "Epoch 1122 Loss 3.520614\n",
      "Epoch 1123 Loss 3.518601\n",
      "Epoch 1124 Loss 3.516594\n",
      "Epoch 1125 Loss 3.514594\n",
      "Epoch 1126 Loss 3.512602\n",
      "Epoch 1127 Loss 3.510619\n",
      "Epoch 1128 Loss 3.508637\n",
      "Epoch 1129 Loss 3.506665\n",
      "Epoch 1130 Loss 3.504700\n",
      "Epoch 1131 Loss 3.502740\n",
      "Epoch 1132 Loss 3.500789\n",
      "Epoch 1133 Loss 3.498843\n",
      "Epoch 1134 Loss 3.496905\n",
      "Epoch 1135 Loss 3.494972\n",
      "Epoch 1136 Loss 3.493046\n",
      "Epoch 1137 Loss 3.491127\n",
      "Epoch 1138 Loss 3.489213\n",
      "Epoch 1139 Loss 3.487308\n",
      "Epoch 1140 Loss 3.485410\n",
      "Epoch 1141 Loss 3.483515\n",
      "Epoch 1142 Loss 3.481627\n",
      "Epoch 1143 Loss 3.479746\n",
      "Epoch 1144 Loss 3.477872\n",
      "Epoch 1145 Loss 3.476005\n",
      "Epoch 1146 Loss 3.474143\n",
      "Epoch 1147 Loss 3.472288\n",
      "Epoch 1148 Loss 3.470441\n",
      "Epoch 1149 Loss 3.468597\n",
      "Epoch 1150 Loss 3.466762\n",
      "Epoch 1151 Loss 3.464930\n",
      "Epoch 1152 Loss 3.463105\n",
      "Epoch 1153 Loss 3.461289\n",
      "Epoch 1154 Loss 3.459477\n",
      "Epoch 1155 Loss 3.457672\n",
      "Epoch 1156 Loss 3.455873\n",
      "Epoch 1157 Loss 3.454080\n",
      "Epoch 1158 Loss 3.452293\n",
      "Epoch 1159 Loss 3.450512\n",
      "Epoch 1160 Loss 3.448736\n",
      "Epoch 1161 Loss 3.446968\n",
      "Epoch 1162 Loss 3.445203\n",
      "Epoch 1163 Loss 3.443449\n",
      "Epoch 1164 Loss 3.441696\n",
      "Epoch 1165 Loss 3.439952\n",
      "Epoch 1166 Loss 3.438210\n",
      "Epoch 1167 Loss 3.436478\n",
      "Epoch 1168 Loss 3.434753\n",
      "Epoch 1169 Loss 3.433029\n",
      "Epoch 1170 Loss 3.431314\n",
      "Epoch 1171 Loss 3.429608\n",
      "Epoch 1172 Loss 3.427903\n",
      "Epoch 1173 Loss 3.426204\n",
      "Epoch 1174 Loss 3.424509\n",
      "Epoch 1175 Loss 3.422824\n",
      "Epoch 1176 Loss 3.421144\n",
      "Epoch 1177 Loss 3.419468\n",
      "Epoch 1178 Loss 3.417798\n",
      "Epoch 1179 Loss 3.416134\n",
      "Epoch 1180 Loss 3.414477\n",
      "Epoch 1181 Loss 3.412824\n",
      "Epoch 1182 Loss 3.411176\n",
      "Epoch 1183 Loss 3.409534\n",
      "Epoch 1184 Loss 3.407899\n",
      "Epoch 1185 Loss 3.406272\n",
      "Epoch 1186 Loss 3.404645\n",
      "Epoch 1187 Loss 3.403024\n",
      "Epoch 1188 Loss 3.401413\n",
      "Epoch 1189 Loss 3.399802\n",
      "Epoch 1190 Loss 3.398200\n",
      "Epoch 1191 Loss 3.396602\n",
      "Epoch 1192 Loss 3.395011\n",
      "Epoch 1193 Loss 3.393425\n",
      "Epoch 1194 Loss 3.391845\n",
      "Epoch 1195 Loss 3.390267\n",
      "Epoch 1196 Loss 3.388697\n",
      "Epoch 1197 Loss 3.387132\n",
      "Epoch 1198 Loss 3.385571\n",
      "Epoch 1199 Loss 3.384017\n",
      "Epoch 1200 Loss 3.382467\n",
      "Epoch 1201 Loss 3.380925\n",
      "Epoch 1202 Loss 3.379386\n",
      "Epoch 1203 Loss 3.377852\n",
      "Epoch 1204 Loss 3.376323\n",
      "Epoch 1205 Loss 3.374800\n",
      "Epoch 1206 Loss 3.373284\n",
      "Epoch 1207 Loss 3.371769\n",
      "Epoch 1208 Loss 3.370261\n",
      "Epoch 1209 Loss 3.368759\n",
      "Epoch 1210 Loss 3.367262\n",
      "Epoch 1211 Loss 3.365771\n",
      "Epoch 1212 Loss 3.364282\n",
      "Epoch 1213 Loss 3.362800\n",
      "Epoch 1214 Loss 3.361325\n",
      "Epoch 1215 Loss 3.359851\n",
      "Epoch 1216 Loss 3.358383\n",
      "Epoch 1217 Loss 3.356921\n",
      "Epoch 1218 Loss 3.355464\n",
      "Epoch 1219 Loss 3.354013\n",
      "Epoch 1220 Loss 3.352564\n",
      "Epoch 1221 Loss 3.351122\n",
      "Epoch 1222 Loss 3.349685\n",
      "Epoch 1223 Loss 3.348251\n",
      "Epoch 1224 Loss 3.346825\n",
      "Epoch 1225 Loss 3.345403\n",
      "Epoch 1226 Loss 3.343982\n",
      "Epoch 1227 Loss 3.342571\n",
      "Epoch 1228 Loss 3.341161\n",
      "Epoch 1229 Loss 3.339758\n",
      "Epoch 1230 Loss 3.338359\n",
      "Epoch 1231 Loss 3.336965\n",
      "Epoch 1232 Loss 3.335577\n",
      "Epoch 1233 Loss 3.334191\n",
      "Epoch 1234 Loss 3.332811\n",
      "Epoch 1235 Loss 3.331435\n",
      "Epoch 1236 Loss 3.330065\n",
      "Epoch 1237 Loss 3.328699\n",
      "Epoch 1238 Loss 3.327338\n",
      "Epoch 1239 Loss 3.325980\n",
      "Epoch 1240 Loss 3.324628\n",
      "Epoch 1241 Loss 3.323280\n",
      "Epoch 1242 Loss 3.321935\n",
      "Epoch 1243 Loss 3.320599\n",
      "Epoch 1244 Loss 3.319264\n",
      "Epoch 1245 Loss 3.317935\n",
      "Epoch 1246 Loss 3.316610\n",
      "Epoch 1247 Loss 3.315289\n",
      "Epoch 1248 Loss 3.313974\n",
      "Epoch 1249 Loss 3.312663\n",
      "Epoch 1250 Loss 3.311353\n",
      "Epoch 1251 Loss 3.310053\n",
      "Epoch 1252 Loss 3.308756\n",
      "Epoch 1253 Loss 3.307462\n",
      "Epoch 1254 Loss 3.306170\n",
      "Epoch 1255 Loss 3.304887\n",
      "Epoch 1256 Loss 3.303605\n",
      "Epoch 1257 Loss 3.302329\n",
      "Epoch 1258 Loss 3.301058\n",
      "Epoch 1259 Loss 3.299791\n",
      "Epoch 1260 Loss 3.298527\n",
      "Epoch 1261 Loss 3.297266\n",
      "Epoch 1262 Loss 3.296014\n",
      "Epoch 1263 Loss 3.294762\n",
      "Epoch 1264 Loss 3.293517\n",
      "Epoch 1265 Loss 3.292275\n",
      "Epoch 1266 Loss 3.291036\n",
      "Epoch 1267 Loss 3.289804\n",
      "Epoch 1268 Loss 3.288573\n",
      "Epoch 1269 Loss 3.287347\n",
      "Epoch 1270 Loss 3.286129\n",
      "Epoch 1271 Loss 3.284911\n",
      "Epoch 1272 Loss 3.283698\n",
      "Epoch 1273 Loss 3.282488\n",
      "Epoch 1274 Loss 3.281284\n",
      "Epoch 1275 Loss 3.280086\n",
      "Epoch 1276 Loss 3.278888\n",
      "Epoch 1277 Loss 3.277696\n",
      "Epoch 1278 Loss 3.276506\n",
      "Epoch 1279 Loss 3.275322\n",
      "Epoch 1280 Loss 3.274142\n",
      "Epoch 1281 Loss 3.272967\n",
      "Epoch 1282 Loss 3.271793\n",
      "Epoch 1283 Loss 3.270625\n",
      "Epoch 1284 Loss 3.269460\n",
      "Epoch 1285 Loss 3.268301\n",
      "Epoch 1286 Loss 3.267143\n",
      "Epoch 1287 Loss 3.265991\n",
      "Epoch 1288 Loss 3.264842\n",
      "Epoch 1289 Loss 3.263700\n",
      "Epoch 1290 Loss 3.262556\n",
      "Epoch 1291 Loss 3.261421\n",
      "Epoch 1292 Loss 3.260288\n",
      "Epoch 1293 Loss 3.259161\n",
      "Epoch 1294 Loss 3.258033\n",
      "Epoch 1295 Loss 3.256912\n",
      "Epoch 1296 Loss 3.255795\n",
      "Epoch 1297 Loss 3.254681\n",
      "Epoch 1298 Loss 3.253569\n",
      "Epoch 1299 Loss 3.252462\n",
      "Epoch 1300 Loss 3.251362\n",
      "Epoch 1301 Loss 3.250264\n",
      "Epoch 1302 Loss 3.249168\n",
      "Epoch 1303 Loss 3.248077\n",
      "Epoch 1304 Loss 3.246989\n",
      "Epoch 1305 Loss 3.245904\n",
      "Epoch 1306 Loss 3.244824\n",
      "Epoch 1307 Loss 3.243747\n",
      "Epoch 1308 Loss 3.242674\n",
      "Epoch 1309 Loss 3.241606\n",
      "Epoch 1310 Loss 3.240538\n",
      "Epoch 1311 Loss 3.239475\n",
      "Epoch 1312 Loss 3.238420\n",
      "Epoch 1313 Loss 3.237364\n",
      "Epoch 1314 Loss 3.236314\n",
      "Epoch 1315 Loss 3.235264\n",
      "Epoch 1316 Loss 3.234218\n",
      "Epoch 1317 Loss 3.233179\n",
      "Epoch 1318 Loss 3.232143\n",
      "Epoch 1319 Loss 3.231108\n",
      "Epoch 1320 Loss 3.230078\n",
      "Epoch 1321 Loss 3.229051\n",
      "Epoch 1322 Loss 3.228027\n",
      "Epoch 1323 Loss 3.227010\n",
      "Epoch 1324 Loss 3.225993\n",
      "Epoch 1325 Loss 3.224979\n",
      "Epoch 1326 Loss 3.223971\n",
      "Epoch 1327 Loss 3.222965\n",
      "Epoch 1328 Loss 3.221961\n",
      "Epoch 1329 Loss 3.220962\n",
      "Epoch 1330 Loss 3.219967\n",
      "Epoch 1331 Loss 3.218975\n",
      "Epoch 1332 Loss 3.217986\n",
      "Epoch 1333 Loss 3.217000\n",
      "Epoch 1334 Loss 3.216017\n",
      "Epoch 1335 Loss 3.215039\n",
      "Epoch 1336 Loss 3.214062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1337 Loss 3.213092\n",
      "Epoch 1338 Loss 3.212122\n",
      "Epoch 1339 Loss 3.211157\n",
      "Epoch 1340 Loss 3.210193\n",
      "Epoch 1341 Loss 3.209235\n",
      "Epoch 1342 Loss 3.208279\n",
      "Epoch 1343 Loss 3.207326\n",
      "Epoch 1344 Loss 3.206376\n",
      "Epoch 1345 Loss 3.205430\n",
      "Epoch 1346 Loss 3.204488\n",
      "Epoch 1347 Loss 3.203547\n",
      "Epoch 1348 Loss 3.202611\n",
      "Epoch 1349 Loss 3.201678\n",
      "Epoch 1350 Loss 3.200747\n",
      "Epoch 1351 Loss 3.199820\n",
      "Epoch 1352 Loss 3.198897\n",
      "Epoch 1353 Loss 3.197976\n",
      "Epoch 1354 Loss 3.197060\n",
      "Epoch 1355 Loss 3.196144\n",
      "Epoch 1356 Loss 3.195231\n",
      "Epoch 1357 Loss 3.194324\n",
      "Epoch 1358 Loss 3.193419\n",
      "Epoch 1359 Loss 3.192517\n",
      "Epoch 1360 Loss 3.191616\n",
      "Epoch 1361 Loss 3.190720\n",
      "Epoch 1362 Loss 3.189829\n",
      "Epoch 1363 Loss 3.188937\n",
      "Epoch 1364 Loss 3.188051\n",
      "Epoch 1365 Loss 3.187166\n",
      "Epoch 1366 Loss 3.186288\n",
      "Epoch 1367 Loss 3.185409\n",
      "Epoch 1368 Loss 3.184535\n",
      "Epoch 1369 Loss 3.183662\n",
      "Epoch 1370 Loss 3.182791\n",
      "Epoch 1371 Loss 3.181925\n",
      "Epoch 1372 Loss 3.181063\n",
      "Epoch 1373 Loss 3.180201\n",
      "Epoch 1374 Loss 3.179347\n",
      "Epoch 1375 Loss 3.178490\n",
      "Epoch 1376 Loss 3.177638\n",
      "Epoch 1377 Loss 3.176789\n",
      "Epoch 1378 Loss 3.175945\n",
      "Epoch 1379 Loss 3.175101\n",
      "Epoch 1380 Loss 3.174262\n",
      "Epoch 1381 Loss 3.173424\n",
      "Epoch 1382 Loss 3.172590\n",
      "Epoch 1383 Loss 3.171759\n",
      "Epoch 1384 Loss 3.170929\n",
      "Epoch 1385 Loss 3.170103\n",
      "Epoch 1386 Loss 3.169280\n",
      "Epoch 1387 Loss 3.168462\n",
      "Epoch 1388 Loss 3.167644\n",
      "Epoch 1389 Loss 3.166827\n",
      "Epoch 1390 Loss 3.166017\n",
      "Epoch 1391 Loss 3.165206\n",
      "Epoch 1392 Loss 3.164401\n",
      "Epoch 1393 Loss 3.163594\n",
      "Epoch 1394 Loss 3.162795\n",
      "Epoch 1395 Loss 3.161996\n",
      "Epoch 1396 Loss 3.161201\n",
      "Epoch 1397 Loss 3.160410\n",
      "Epoch 1398 Loss 3.159618\n",
      "Epoch 1399 Loss 3.158831\n",
      "Epoch 1400 Loss 3.158046\n",
      "Epoch 1401 Loss 3.157263\n",
      "Epoch 1402 Loss 3.156484\n",
      "Epoch 1403 Loss 3.155708\n",
      "Epoch 1404 Loss 3.154933\n",
      "Epoch 1405 Loss 3.154162\n",
      "Epoch 1406 Loss 3.153393\n",
      "Epoch 1407 Loss 3.152627\n",
      "Epoch 1408 Loss 3.151864\n",
      "Epoch 1409 Loss 3.151101\n",
      "Epoch 1410 Loss 3.150343\n",
      "Epoch 1411 Loss 3.149587\n",
      "Epoch 1412 Loss 3.148833\n",
      "Epoch 1413 Loss 3.148082\n",
      "Epoch 1414 Loss 3.147335\n",
      "Epoch 1415 Loss 3.146588\n",
      "Epoch 1416 Loss 3.145845\n",
      "Epoch 1417 Loss 3.145105\n",
      "Epoch 1418 Loss 3.144367\n",
      "Epoch 1419 Loss 3.143630\n",
      "Epoch 1420 Loss 3.142899\n",
      "Epoch 1421 Loss 3.142166\n",
      "Epoch 1422 Loss 3.141439\n",
      "Epoch 1423 Loss 3.140712\n",
      "Epoch 1424 Loss 3.139989\n",
      "Epoch 1425 Loss 3.139271\n",
      "Epoch 1426 Loss 3.138551\n",
      "Epoch 1427 Loss 3.137834\n",
      "Epoch 1428 Loss 3.137121\n",
      "Epoch 1429 Loss 3.136410\n",
      "Epoch 1430 Loss 3.135700\n",
      "Epoch 1431 Loss 3.134995\n",
      "Epoch 1432 Loss 3.134291\n",
      "Epoch 1433 Loss 3.133590\n",
      "Epoch 1434 Loss 3.132889\n",
      "Epoch 1435 Loss 3.132194\n",
      "Epoch 1436 Loss 3.131500\n",
      "Epoch 1437 Loss 3.130810\n",
      "Epoch 1438 Loss 3.130119\n",
      "Epoch 1439 Loss 3.129432\n",
      "Epoch 1440 Loss 3.128746\n",
      "Epoch 1441 Loss 3.128064\n",
      "Epoch 1442 Loss 3.127381\n",
      "Epoch 1443 Loss 3.126705\n",
      "Epoch 1444 Loss 3.126031\n",
      "Epoch 1445 Loss 3.125356\n",
      "Epoch 1446 Loss 3.124683\n",
      "Epoch 1447 Loss 3.124017\n",
      "Epoch 1448 Loss 3.123348\n",
      "Epoch 1449 Loss 3.122685\n",
      "Epoch 1450 Loss 3.122022\n",
      "Epoch 1451 Loss 3.121362\n",
      "Epoch 1452 Loss 3.120706\n",
      "Epoch 1453 Loss 3.120049\n",
      "Epoch 1454 Loss 3.119396\n",
      "Epoch 1455 Loss 3.118746\n",
      "Epoch 1456 Loss 3.118098\n",
      "Epoch 1457 Loss 3.117452\n",
      "Epoch 1458 Loss 3.116805\n",
      "Epoch 1459 Loss 3.116164\n",
      "Epoch 1460 Loss 3.115525\n",
      "Epoch 1461 Loss 3.114886\n",
      "Epoch 1462 Loss 3.114250\n",
      "Epoch 1463 Loss 3.113617\n",
      "Epoch 1464 Loss 3.112984\n",
      "Epoch 1465 Loss 3.112358\n",
      "Epoch 1466 Loss 3.111731\n",
      "Epoch 1467 Loss 3.111103\n",
      "Epoch 1468 Loss 3.110484\n",
      "Epoch 1469 Loss 3.109860\n",
      "Epoch 1470 Loss 3.109242\n",
      "Epoch 1471 Loss 3.108627\n",
      "Epoch 1472 Loss 3.108011\n",
      "Epoch 1473 Loss 3.107401\n",
      "Epoch 1474 Loss 3.106791\n",
      "Epoch 1475 Loss 3.106180\n",
      "Epoch 1476 Loss 3.105575\n",
      "Epoch 1477 Loss 3.104972\n",
      "Epoch 1478 Loss 3.104370\n",
      "Epoch 1479 Loss 3.103770\n",
      "Epoch 1480 Loss 3.103172\n",
      "Epoch 1481 Loss 3.102576\n",
      "Epoch 1482 Loss 3.101982\n",
      "Epoch 1483 Loss 3.101390\n",
      "Epoch 1484 Loss 3.100802\n",
      "Epoch 1485 Loss 3.100213\n",
      "Epoch 1486 Loss 3.099627\n",
      "Epoch 1487 Loss 3.099044\n",
      "Epoch 1488 Loss 3.098462\n",
      "Epoch 1489 Loss 3.097883\n",
      "Epoch 1490 Loss 3.097302\n",
      "Epoch 1491 Loss 3.096727\n",
      "Epoch 1492 Loss 3.096153\n",
      "Epoch 1493 Loss 3.095583\n",
      "Epoch 1494 Loss 3.095011\n",
      "Epoch 1495 Loss 3.094444\n",
      "Epoch 1496 Loss 3.093876\n",
      "Epoch 1497 Loss 3.093314\n",
      "Epoch 1498 Loss 3.092751\n",
      "Epoch 1499 Loss 3.092191\n",
      "Epoch 1500 Loss 3.091631\n",
      "Epoch 1501 Loss 3.091074\n",
      "Epoch 1502 Loss 3.090520\n",
      "Epoch 1503 Loss 3.089969\n",
      "Epoch 1504 Loss 3.089417\n",
      "Epoch 1505 Loss 3.088867\n",
      "Epoch 1506 Loss 3.088320\n",
      "Epoch 1507 Loss 3.087775\n",
      "Epoch 1508 Loss 3.087232\n",
      "Epoch 1509 Loss 3.086690\n",
      "Epoch 1510 Loss 3.086150\n",
      "Epoch 1511 Loss 3.085612\n",
      "Epoch 1512 Loss 3.085075\n",
      "Epoch 1513 Loss 3.084542\n",
      "Epoch 1514 Loss 3.084009\n",
      "Epoch 1515 Loss 3.083478\n",
      "Epoch 1516 Loss 3.082948\n",
      "Epoch 1517 Loss 3.082422\n",
      "Epoch 1518 Loss 3.081897\n",
      "Epoch 1519 Loss 3.081373\n",
      "Epoch 1520 Loss 3.080850\n",
      "Epoch 1521 Loss 3.080331\n",
      "Epoch 1522 Loss 3.079811\n",
      "Epoch 1523 Loss 3.079297\n",
      "Epoch 1524 Loss 3.078781\n",
      "Epoch 1525 Loss 3.078268\n",
      "Epoch 1526 Loss 3.077757\n",
      "Epoch 1527 Loss 3.077247\n",
      "Epoch 1528 Loss 3.076739\n",
      "Epoch 1529 Loss 3.076232\n",
      "Epoch 1530 Loss 3.075729\n",
      "Epoch 1531 Loss 3.075225\n",
      "Epoch 1532 Loss 3.074724\n",
      "Epoch 1533 Loss 3.074227\n",
      "Epoch 1534 Loss 3.073726\n",
      "Epoch 1535 Loss 3.073232\n",
      "Epoch 1536 Loss 3.072739\n",
      "Epoch 1537 Loss 3.072245\n",
      "Epoch 1538 Loss 3.071753\n",
      "Epoch 1539 Loss 3.071265\n",
      "Epoch 1540 Loss 3.070778\n",
      "Epoch 1541 Loss 3.070293\n",
      "Epoch 1542 Loss 3.069808\n",
      "Epoch 1543 Loss 3.069326\n",
      "Epoch 1544 Loss 3.068845\n",
      "Epoch 1545 Loss 3.068366\n",
      "Epoch 1546 Loss 3.067887\n",
      "Epoch 1547 Loss 3.067412\n",
      "Epoch 1548 Loss 3.066937\n",
      "Epoch 1549 Loss 3.066464\n",
      "Epoch 1550 Loss 3.065993\n",
      "Epoch 1551 Loss 3.065524\n",
      "Epoch 1552 Loss 3.065055\n",
      "Epoch 1553 Loss 3.064588\n",
      "Epoch 1554 Loss 3.064123\n",
      "Epoch 1555 Loss 3.063660\n",
      "Epoch 1556 Loss 3.063199\n",
      "Epoch 1557 Loss 3.062738\n",
      "Epoch 1558 Loss 3.062280\n",
      "Epoch 1559 Loss 3.061822\n",
      "Epoch 1560 Loss 3.061368\n",
      "Epoch 1561 Loss 3.060913\n",
      "Epoch 1562 Loss 3.060461\n",
      "Epoch 1563 Loss 3.060011\n",
      "Epoch 1564 Loss 3.059561\n",
      "Epoch 1565 Loss 3.059114\n",
      "Epoch 1566 Loss 3.058668\n",
      "Epoch 1567 Loss 3.058221\n",
      "Epoch 1568 Loss 3.057780\n",
      "Epoch 1569 Loss 3.057338\n",
      "Epoch 1570 Loss 3.056898\n",
      "Epoch 1571 Loss 3.056458\n",
      "Epoch 1572 Loss 3.056019\n",
      "Epoch 1573 Loss 3.055585\n",
      "Epoch 1574 Loss 3.055151\n",
      "Epoch 1575 Loss 3.054717\n",
      "Epoch 1576 Loss 3.054286\n",
      "Epoch 1577 Loss 3.053857\n",
      "Epoch 1578 Loss 3.053428\n",
      "Epoch 1579 Loss 3.053001\n",
      "Epoch 1580 Loss 3.052576\n",
      "Epoch 1581 Loss 3.052152\n",
      "Epoch 1582 Loss 3.051730\n",
      "Epoch 1583 Loss 3.051307\n",
      "Epoch 1584 Loss 3.050888\n",
      "Epoch 1585 Loss 3.050471\n",
      "Epoch 1586 Loss 3.050052\n",
      "Epoch 1587 Loss 3.049639\n",
      "Epoch 1588 Loss 3.049223\n",
      "Epoch 1589 Loss 3.048811\n",
      "Epoch 1590 Loss 3.048398\n",
      "Epoch 1591 Loss 3.047991\n",
      "Epoch 1592 Loss 3.047581\n",
      "Epoch 1593 Loss 3.047173\n",
      "Epoch 1594 Loss 3.046768\n",
      "Epoch 1595 Loss 3.046363\n",
      "Epoch 1596 Loss 3.045960\n",
      "Epoch 1597 Loss 3.045559\n",
      "Epoch 1598 Loss 3.045160\n",
      "Epoch 1599 Loss 3.044759\n",
      "Epoch 1600 Loss 3.044361\n",
      "Epoch 1601 Loss 3.043966\n",
      "Epoch 1602 Loss 3.043571\n",
      "Epoch 1603 Loss 3.043176\n",
      "Epoch 1604 Loss 3.042785\n",
      "Epoch 1605 Loss 3.042395\n",
      "Epoch 1606 Loss 3.042004\n",
      "Epoch 1607 Loss 3.041615\n",
      "Epoch 1608 Loss 3.041230\n",
      "Epoch 1609 Loss 3.040844\n",
      "Epoch 1610 Loss 3.040460\n",
      "Epoch 1611 Loss 3.040077\n",
      "Epoch 1612 Loss 3.039695\n",
      "Epoch 1613 Loss 3.039314\n",
      "Epoch 1614 Loss 3.038934\n",
      "Epoch 1615 Loss 3.038557\n",
      "Epoch 1616 Loss 3.038182\n",
      "Epoch 1617 Loss 3.037806\n",
      "Epoch 1618 Loss 3.037431\n",
      "Epoch 1619 Loss 3.037059\n",
      "Epoch 1620 Loss 3.036689\n",
      "Epoch 1621 Loss 3.036319\n",
      "Epoch 1622 Loss 3.035949\n",
      "Epoch 1623 Loss 3.035583\n",
      "Epoch 1624 Loss 3.035215\n",
      "Epoch 1625 Loss 3.034849\n",
      "Epoch 1626 Loss 3.034485\n",
      "Epoch 1627 Loss 3.034122\n",
      "Epoch 1628 Loss 3.033762\n",
      "Epoch 1629 Loss 3.033402\n",
      "Epoch 1630 Loss 3.033042\n",
      "Epoch 1631 Loss 3.032685\n",
      "Epoch 1632 Loss 3.032329\n",
      "Epoch 1633 Loss 3.031973\n",
      "Epoch 1634 Loss 3.031619\n",
      "Epoch 1635 Loss 3.031265\n",
      "Epoch 1636 Loss 3.030913\n",
      "Epoch 1637 Loss 3.030564\n",
      "Epoch 1638 Loss 3.030215\n",
      "Epoch 1639 Loss 3.029866\n",
      "Epoch 1640 Loss 3.029518\n",
      "Epoch 1641 Loss 3.029172\n",
      "Epoch 1642 Loss 3.028829\n",
      "Epoch 1643 Loss 3.028486\n",
      "Epoch 1644 Loss 3.028142\n",
      "Epoch 1645 Loss 3.027802\n",
      "Epoch 1646 Loss 3.027462\n",
      "Epoch 1647 Loss 3.027122\n",
      "Epoch 1648 Loss 3.026784\n",
      "Epoch 1649 Loss 3.026447\n",
      "Epoch 1650 Loss 3.026111\n",
      "Epoch 1651 Loss 3.025780\n",
      "Epoch 1652 Loss 3.025446\n",
      "Epoch 1653 Loss 3.025114\n",
      "Epoch 1654 Loss 3.024782\n",
      "Epoch 1655 Loss 3.024452\n",
      "Epoch 1656 Loss 3.024125\n",
      "Epoch 1657 Loss 3.023797\n",
      "Epoch 1658 Loss 3.023471\n",
      "Epoch 1659 Loss 3.023146\n",
      "Epoch 1660 Loss 3.022821\n",
      "Epoch 1661 Loss 3.022498\n",
      "Epoch 1662 Loss 3.022177\n",
      "Epoch 1663 Loss 3.021855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1664 Loss 3.021534\n",
      "Epoch 1665 Loss 3.021217\n",
      "Epoch 1666 Loss 3.020898\n",
      "Epoch 1667 Loss 3.020582\n",
      "Epoch 1668 Loss 3.020266\n",
      "Epoch 1669 Loss 3.019952\n",
      "Epoch 1670 Loss 3.019639\n",
      "Epoch 1671 Loss 3.019325\n",
      "Epoch 1672 Loss 3.019016\n",
      "Epoch 1673 Loss 3.018706\n",
      "Epoch 1674 Loss 3.018395\n",
      "Epoch 1675 Loss 3.018089\n",
      "Epoch 1676 Loss 3.017780\n",
      "Epoch 1677 Loss 3.017475\n",
      "Epoch 1678 Loss 3.017170\n",
      "Epoch 1679 Loss 3.016867\n",
      "Epoch 1680 Loss 3.016564\n",
      "Epoch 1681 Loss 3.016262\n",
      "Epoch 1682 Loss 3.015959\n",
      "Epoch 1683 Loss 3.015661\n",
      "Epoch 1684 Loss 3.015362\n",
      "Epoch 1685 Loss 3.015064\n",
      "Epoch 1686 Loss 3.014768\n",
      "Epoch 1687 Loss 3.014472\n",
      "Epoch 1688 Loss 3.014179\n",
      "Epoch 1689 Loss 3.013884\n",
      "Epoch 1690 Loss 3.013591\n",
      "Epoch 1691 Loss 3.013299\n",
      "Epoch 1692 Loss 3.013008\n",
      "Epoch 1693 Loss 3.012719\n",
      "Epoch 1694 Loss 3.012431\n",
      "Epoch 1695 Loss 3.012141\n",
      "Epoch 1696 Loss 3.011855\n",
      "Epoch 1697 Loss 3.011570\n",
      "Epoch 1698 Loss 3.011284\n",
      "Epoch 1699 Loss 3.011001\n",
      "Epoch 1700 Loss 3.010718\n",
      "Epoch 1701 Loss 3.010436\n",
      "Epoch 1702 Loss 3.010156\n",
      "Epoch 1703 Loss 3.009876\n",
      "Epoch 1704 Loss 3.009595\n",
      "Epoch 1705 Loss 3.009319\n",
      "Epoch 1706 Loss 3.009040\n",
      "Epoch 1707 Loss 3.008763\n",
      "Epoch 1708 Loss 3.008487\n",
      "Epoch 1709 Loss 3.008214\n",
      "Epoch 1710 Loss 3.007941\n",
      "Epoch 1711 Loss 3.007668\n",
      "Epoch 1712 Loss 3.007396\n",
      "Epoch 1713 Loss 3.007126\n",
      "Epoch 1714 Loss 3.006856\n",
      "Epoch 1715 Loss 3.006586\n",
      "Epoch 1716 Loss 3.006318\n",
      "Epoch 1717 Loss 3.006052\n",
      "Epoch 1718 Loss 3.005785\n",
      "Epoch 1719 Loss 3.005520\n",
      "Epoch 1720 Loss 3.005256\n",
      "Epoch 1721 Loss 3.004993\n",
      "Epoch 1722 Loss 3.004729\n",
      "Epoch 1723 Loss 3.004467\n",
      "Epoch 1724 Loss 3.004207\n",
      "Epoch 1725 Loss 3.003947\n",
      "Epoch 1726 Loss 3.003690\n",
      "Epoch 1727 Loss 3.003430\n",
      "Epoch 1728 Loss 3.003174\n",
      "Epoch 1729 Loss 3.002918\n",
      "Epoch 1730 Loss 3.002661\n",
      "Epoch 1731 Loss 3.002406\n",
      "Epoch 1732 Loss 3.002152\n",
      "Epoch 1733 Loss 3.001901\n",
      "Epoch 1734 Loss 3.001649\n",
      "Epoch 1735 Loss 3.001395\n",
      "Epoch 1736 Loss 3.001145\n",
      "Epoch 1737 Loss 3.000898\n",
      "Epoch 1738 Loss 3.000648\n",
      "Epoch 1739 Loss 3.000400\n",
      "Epoch 1740 Loss 3.000154\n",
      "Epoch 1741 Loss 2.999907\n",
      "Epoch 1742 Loss 2.999662\n",
      "Epoch 1743 Loss 2.999417\n",
      "Epoch 1744 Loss 2.999174\n",
      "Epoch 1745 Loss 2.998930\n",
      "Epoch 1746 Loss 2.998688\n",
      "Epoch 1747 Loss 2.998448\n",
      "Epoch 1748 Loss 2.998208\n",
      "Epoch 1749 Loss 2.997968\n",
      "Epoch 1750 Loss 2.997730\n",
      "Epoch 1751 Loss 2.997490\n",
      "Epoch 1752 Loss 2.997254\n",
      "Epoch 1753 Loss 2.997018\n",
      "Epoch 1754 Loss 2.996783\n",
      "Epoch 1755 Loss 2.996548\n",
      "Epoch 1756 Loss 2.996313\n",
      "Epoch 1757 Loss 2.996081\n",
      "Epoch 1758 Loss 2.995847\n",
      "Epoch 1759 Loss 2.995615\n",
      "Epoch 1760 Loss 2.995387\n",
      "Epoch 1761 Loss 2.995156\n",
      "Epoch 1762 Loss 2.994929\n",
      "Epoch 1763 Loss 2.994699\n",
      "Epoch 1764 Loss 2.994472\n",
      "Epoch 1765 Loss 2.994245\n",
      "Epoch 1766 Loss 2.994018\n",
      "Epoch 1767 Loss 2.993793\n",
      "Epoch 1768 Loss 2.993569\n",
      "Epoch 1769 Loss 2.993344\n",
      "Epoch 1770 Loss 2.993122\n",
      "Epoch 1771 Loss 2.992900\n",
      "Epoch 1772 Loss 2.992678\n",
      "Epoch 1773 Loss 2.992457\n",
      "Epoch 1774 Loss 2.992238\n",
      "Epoch 1775 Loss 2.992017\n",
      "Epoch 1776 Loss 2.991798\n",
      "Epoch 1777 Loss 2.991583\n",
      "Epoch 1778 Loss 2.991366\n",
      "Epoch 1779 Loss 2.991146\n",
      "Epoch 1780 Loss 2.990932\n",
      "Epoch 1781 Loss 2.990719\n",
      "Epoch 1782 Loss 2.990503\n",
      "Epoch 1783 Loss 2.990289\n",
      "Epoch 1784 Loss 2.990078\n",
      "Epoch 1785 Loss 2.989866\n",
      "Epoch 1786 Loss 2.989654\n",
      "Epoch 1787 Loss 2.989443\n",
      "Epoch 1788 Loss 2.989233\n",
      "Epoch 1789 Loss 2.989025\n",
      "Epoch 1790 Loss 2.988817\n",
      "Epoch 1791 Loss 2.988609\n",
      "Epoch 1792 Loss 2.988401\n",
      "Epoch 1793 Loss 2.988195\n",
      "Epoch 1794 Loss 2.987989\n",
      "Epoch 1795 Loss 2.987784\n",
      "Epoch 1796 Loss 2.987581\n",
      "Epoch 1797 Loss 2.987377\n",
      "Epoch 1798 Loss 2.987174\n",
      "Epoch 1799 Loss 2.986974\n",
      "Epoch 1800 Loss 2.986771\n",
      "Epoch 1801 Loss 2.986570\n",
      "Epoch 1802 Loss 2.986371\n",
      "Epoch 1803 Loss 2.986171\n",
      "Epoch 1804 Loss 2.985972\n",
      "Epoch 1805 Loss 2.985774\n",
      "Epoch 1806 Loss 2.985578\n",
      "Epoch 1807 Loss 2.985381\n",
      "Epoch 1808 Loss 2.985184\n",
      "Epoch 1809 Loss 2.984989\n",
      "Epoch 1810 Loss 2.984793\n",
      "Epoch 1811 Loss 2.984601\n",
      "Epoch 1812 Loss 2.984406\n",
      "Epoch 1813 Loss 2.984215\n",
      "Epoch 1814 Loss 2.984022\n",
      "Epoch 1815 Loss 2.983830\n",
      "Epoch 1816 Loss 2.983639\n",
      "Epoch 1817 Loss 2.983449\n",
      "Epoch 1818 Loss 2.983260\n",
      "Epoch 1819 Loss 2.983073\n",
      "Epoch 1820 Loss 2.982884\n",
      "Epoch 1821 Loss 2.982697\n",
      "Epoch 1822 Loss 2.982510\n",
      "Epoch 1823 Loss 2.982322\n",
      "Epoch 1824 Loss 2.982137\n",
      "Epoch 1825 Loss 2.981953\n",
      "Epoch 1826 Loss 2.981769\n",
      "Epoch 1827 Loss 2.981586\n",
      "Epoch 1828 Loss 2.981402\n",
      "Epoch 1829 Loss 2.981219\n",
      "Epoch 1830 Loss 2.981037\n",
      "Epoch 1831 Loss 2.980856\n",
      "Epoch 1832 Loss 2.980676\n",
      "Epoch 1833 Loss 2.980495\n",
      "Epoch 1834 Loss 2.980316\n",
      "Epoch 1835 Loss 2.980137\n",
      "Epoch 1836 Loss 2.979958\n",
      "Epoch 1837 Loss 2.979782\n",
      "Epoch 1838 Loss 2.979604\n",
      "Epoch 1839 Loss 2.979428\n",
      "Epoch 1840 Loss 2.979253\n",
      "Epoch 1841 Loss 2.979078\n",
      "Epoch 1842 Loss 2.978902\n",
      "Epoch 1843 Loss 2.978729\n",
      "Epoch 1844 Loss 2.978555\n",
      "Epoch 1845 Loss 2.978382\n",
      "Epoch 1846 Loss 2.978211\n",
      "Epoch 1847 Loss 2.978039\n",
      "Epoch 1848 Loss 2.977867\n",
      "Epoch 1849 Loss 2.977696\n",
      "Epoch 1850 Loss 2.977527\n",
      "Epoch 1851 Loss 2.977357\n",
      "Epoch 1852 Loss 2.977188\n",
      "Epoch 1853 Loss 2.977021\n",
      "Epoch 1854 Loss 2.976853\n",
      "Epoch 1855 Loss 2.976687\n",
      "Epoch 1856 Loss 2.976520\n",
      "Epoch 1857 Loss 2.976354\n",
      "Epoch 1858 Loss 2.976189\n",
      "Epoch 1859 Loss 2.976023\n",
      "Epoch 1860 Loss 2.975860\n",
      "Epoch 1861 Loss 2.975697\n",
      "Epoch 1862 Loss 2.975532\n",
      "Epoch 1863 Loss 2.975369\n",
      "Epoch 1864 Loss 2.975208\n",
      "Epoch 1865 Loss 2.975046\n",
      "Epoch 1866 Loss 2.974886\n",
      "Epoch 1867 Loss 2.974725\n",
      "Epoch 1868 Loss 2.974565\n",
      "Epoch 1869 Loss 2.974406\n",
      "Epoch 1870 Loss 2.974248\n",
      "Epoch 1871 Loss 2.974088\n",
      "Epoch 1872 Loss 2.973930\n",
      "Epoch 1873 Loss 2.973776\n",
      "Epoch 1874 Loss 2.973618\n",
      "Epoch 1875 Loss 2.973463\n",
      "Epoch 1876 Loss 2.973307\n",
      "Epoch 1877 Loss 2.973150\n",
      "Epoch 1878 Loss 2.972996\n",
      "Epoch 1879 Loss 2.972844\n",
      "Epoch 1880 Loss 2.972690\n",
      "Epoch 1881 Loss 2.972536\n",
      "Epoch 1882 Loss 2.972383\n",
      "Epoch 1883 Loss 2.972232\n",
      "Epoch 1884 Loss 2.972081\n",
      "Epoch 1885 Loss 2.971931\n",
      "Epoch 1886 Loss 2.971780\n",
      "Epoch 1887 Loss 2.971629\n",
      "Epoch 1888 Loss 2.971481\n",
      "Epoch 1889 Loss 2.971332\n",
      "Epoch 1890 Loss 2.971185\n",
      "Epoch 1891 Loss 2.971035\n",
      "Epoch 1892 Loss 2.970888\n",
      "Epoch 1893 Loss 2.970741\n",
      "Epoch 1894 Loss 2.970596\n",
      "Epoch 1895 Loss 2.970449\n",
      "Epoch 1896 Loss 2.970304\n",
      "Epoch 1897 Loss 2.970159\n",
      "Epoch 1898 Loss 2.970015\n",
      "Epoch 1899 Loss 2.969871\n",
      "Epoch 1900 Loss 2.969727\n",
      "Epoch 1901 Loss 2.969586\n",
      "Epoch 1902 Loss 2.969443\n",
      "Epoch 1903 Loss 2.969302\n",
      "Epoch 1904 Loss 2.969160\n",
      "Epoch 1905 Loss 2.969017\n",
      "Epoch 1906 Loss 2.968879\n",
      "Epoch 1907 Loss 2.968739\n",
      "Epoch 1908 Loss 2.968599\n",
      "Epoch 1909 Loss 2.968460\n",
      "Epoch 1910 Loss 2.968322\n",
      "Epoch 1911 Loss 2.968184\n",
      "Epoch 1912 Loss 2.968046\n",
      "Epoch 1913 Loss 2.967908\n",
      "Epoch 1914 Loss 2.967772\n",
      "Epoch 1915 Loss 2.967636\n",
      "Epoch 1916 Loss 2.967499\n",
      "Epoch 1917 Loss 2.967365\n",
      "Epoch 1918 Loss 2.967230\n",
      "Epoch 1919 Loss 2.967095\n",
      "Epoch 1920 Loss 2.966961\n",
      "Epoch 1921 Loss 2.966827\n",
      "Epoch 1922 Loss 2.966693\n",
      "Epoch 1923 Loss 2.966561\n",
      "Epoch 1924 Loss 2.966429\n",
      "Epoch 1925 Loss 2.966297\n",
      "Epoch 1926 Loss 2.966167\n",
      "Epoch 1927 Loss 2.966036\n",
      "Epoch 1928 Loss 2.965904\n",
      "Epoch 1929 Loss 2.965776\n",
      "Epoch 1930 Loss 2.965646\n",
      "Epoch 1931 Loss 2.965517\n",
      "Epoch 1932 Loss 2.965387\n",
      "Epoch 1933 Loss 2.965261\n",
      "Epoch 1934 Loss 2.965131\n",
      "Epoch 1935 Loss 2.965005\n",
      "Epoch 1936 Loss 2.964878\n",
      "Epoch 1937 Loss 2.964751\n",
      "Epoch 1938 Loss 2.964625\n",
      "Epoch 1939 Loss 2.964500\n",
      "Epoch 1940 Loss 2.964375\n",
      "Epoch 1941 Loss 2.964250\n",
      "Epoch 1942 Loss 2.964126\n",
      "Epoch 1943 Loss 2.964001\n",
      "Epoch 1944 Loss 2.963879\n",
      "Epoch 1945 Loss 2.963756\n",
      "Epoch 1946 Loss 2.963632\n",
      "Epoch 1947 Loss 2.963511\n",
      "Epoch 1948 Loss 2.963388\n",
      "Epoch 1949 Loss 2.963266\n",
      "Epoch 1950 Loss 2.963149\n",
      "Epoch 1951 Loss 2.963026\n",
      "Epoch 1952 Loss 2.962907\n",
      "Epoch 1953 Loss 2.962788\n",
      "Epoch 1954 Loss 2.962666\n",
      "Epoch 1955 Loss 2.962547\n",
      "Epoch 1956 Loss 2.962429\n",
      "Epoch 1957 Loss 2.962312\n",
      "Epoch 1958 Loss 2.962195\n",
      "Epoch 1959 Loss 2.962078\n",
      "Epoch 1960 Loss 2.961959\n",
      "Epoch 1961 Loss 2.961843\n",
      "Epoch 1962 Loss 2.961728\n",
      "Epoch 1963 Loss 2.961611\n",
      "Epoch 1964 Loss 2.961497\n",
      "Epoch 1965 Loss 2.961382\n",
      "Epoch 1966 Loss 2.961268\n",
      "Epoch 1967 Loss 2.961153\n",
      "Epoch 1968 Loss 2.961038\n",
      "Epoch 1969 Loss 2.960926\n",
      "Epoch 1970 Loss 2.960814\n",
      "Epoch 1971 Loss 2.960699\n",
      "Epoch 1972 Loss 2.960587\n",
      "Epoch 1973 Loss 2.960475\n",
      "Epoch 1974 Loss 2.960365\n",
      "Epoch 1975 Loss 2.960254\n",
      "Epoch 1976 Loss 2.960143\n",
      "Epoch 1977 Loss 2.960033\n",
      "Epoch 1978 Loss 2.959923\n",
      "Epoch 1979 Loss 2.959812\n",
      "Epoch 1980 Loss 2.959703\n",
      "Epoch 1981 Loss 2.959594\n",
      "Epoch 1982 Loss 2.959486\n",
      "Epoch 1983 Loss 2.959378\n",
      "Epoch 1984 Loss 2.959271\n",
      "Epoch 1985 Loss 2.959163\n",
      "Epoch 1986 Loss 2.959055\n",
      "Epoch 1987 Loss 2.958950\n",
      "Epoch 1988 Loss 2.958842\n",
      "Epoch 1989 Loss 2.958738\n",
      "Epoch 1990 Loss 2.958632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1991 Loss 2.958526\n",
      "Epoch 1992 Loss 2.958421\n",
      "Epoch 1993 Loss 2.958317\n",
      "Epoch 1994 Loss 2.958212\n",
      "Epoch 1995 Loss 2.958109\n",
      "Epoch 1996 Loss 2.958006\n",
      "Epoch 1997 Loss 2.957903\n",
      "Epoch 1998 Loss 2.957801\n",
      "Epoch 1999 Loss 2.957697\n",
      "Epoch 2000 Loss 2.957596\n",
      "Epoch 2001 Loss 2.957494\n",
      "Epoch 2002 Loss 2.957393\n",
      "Epoch 2003 Loss 2.957293\n",
      "Epoch 2004 Loss 2.957193\n",
      "Epoch 2005 Loss 2.957091\n",
      "Epoch 2006 Loss 2.956991\n",
      "Epoch 2007 Loss 2.956892\n",
      "Epoch 2008 Loss 2.956792\n",
      "Epoch 2009 Loss 2.956694\n",
      "Epoch 2010 Loss 2.956595\n",
      "Epoch 2011 Loss 2.956496\n",
      "Epoch 2012 Loss 2.956397\n",
      "Epoch 2013 Loss 2.956300\n",
      "Epoch 2014 Loss 2.956204\n",
      "Epoch 2015 Loss 2.956108\n",
      "Epoch 2016 Loss 2.956010\n",
      "Epoch 2017 Loss 2.955914\n",
      "Epoch 2018 Loss 2.955817\n",
      "Epoch 2019 Loss 2.955722\n",
      "Epoch 2020 Loss 2.955627\n",
      "Epoch 2021 Loss 2.955533\n",
      "Epoch 2022 Loss 2.955436\n",
      "Epoch 2023 Loss 2.955343\n",
      "Epoch 2024 Loss 2.955250\n",
      "Epoch 2025 Loss 2.955154\n",
      "Epoch 2026 Loss 2.955062\n",
      "Epoch 2027 Loss 2.954968\n",
      "Epoch 2028 Loss 2.954875\n",
      "Epoch 2029 Loss 2.954783\n",
      "Epoch 2030 Loss 2.954691\n",
      "Epoch 2031 Loss 2.954600\n",
      "Epoch 2032 Loss 2.954507\n",
      "Epoch 2033 Loss 2.954417\n",
      "Epoch 2034 Loss 2.954326\n",
      "Epoch 2035 Loss 2.954235\n",
      "Epoch 2036 Loss 2.954145\n",
      "Epoch 2037 Loss 2.954055\n",
      "Epoch 2038 Loss 2.953966\n",
      "Epoch 2039 Loss 2.953876\n",
      "Epoch 2040 Loss 2.953787\n",
      "Epoch 2041 Loss 2.953698\n",
      "Epoch 2042 Loss 2.953610\n",
      "Epoch 2043 Loss 2.953521\n",
      "Epoch 2044 Loss 2.953434\n",
      "Epoch 2045 Loss 2.953346\n",
      "Epoch 2046 Loss 2.953259\n",
      "Epoch 2047 Loss 2.953172\n",
      "Epoch 2048 Loss 2.953085\n",
      "Epoch 2049 Loss 2.953000\n",
      "Epoch 2050 Loss 2.952913\n",
      "Epoch 2051 Loss 2.952828\n",
      "Epoch 2052 Loss 2.952742\n",
      "Epoch 2053 Loss 2.952657\n",
      "Epoch 2054 Loss 2.952571\n",
      "Epoch 2055 Loss 2.952487\n",
      "Epoch 2056 Loss 2.952403\n",
      "Epoch 2057 Loss 2.952318\n",
      "Epoch 2058 Loss 2.952235\n",
      "Epoch 2059 Loss 2.952152\n",
      "Epoch 2060 Loss 2.952068\n",
      "Epoch 2061 Loss 2.951985\n",
      "Epoch 2062 Loss 2.951903\n",
      "Epoch 2063 Loss 2.951820\n",
      "Epoch 2064 Loss 2.951738\n",
      "Epoch 2065 Loss 2.951656\n",
      "Epoch 2066 Loss 2.951575\n",
      "Epoch 2067 Loss 2.951494\n",
      "Epoch 2068 Loss 2.951413\n",
      "Epoch 2069 Loss 2.951333\n",
      "Epoch 2070 Loss 2.951252\n",
      "Epoch 2071 Loss 2.951172\n",
      "Epoch 2072 Loss 2.951093\n",
      "Epoch 2073 Loss 2.951012\n",
      "Epoch 2074 Loss 2.950932\n",
      "Epoch 2075 Loss 2.950853\n",
      "Epoch 2076 Loss 2.950774\n",
      "Epoch 2077 Loss 2.950698\n",
      "Epoch 2078 Loss 2.950618\n",
      "Epoch 2079 Loss 2.950540\n",
      "Epoch 2080 Loss 2.950463\n",
      "Epoch 2081 Loss 2.950385\n",
      "Epoch 2082 Loss 2.950308\n",
      "Epoch 2083 Loss 2.950231\n",
      "Epoch 2084 Loss 2.950155\n",
      "Epoch 2085 Loss 2.950079\n",
      "Epoch 2086 Loss 2.950003\n",
      "Epoch 2087 Loss 2.949925\n",
      "Epoch 2088 Loss 2.949850\n",
      "Epoch 2089 Loss 2.949776\n",
      "Epoch 2090 Loss 2.949699\n",
      "Epoch 2091 Loss 2.949626\n",
      "Epoch 2092 Loss 2.949551\n",
      "Epoch 2093 Loss 2.949476\n",
      "Epoch 2094 Loss 2.949401\n",
      "Epoch 2095 Loss 2.949328\n",
      "Epoch 2096 Loss 2.949254\n",
      "Epoch 2097 Loss 2.949182\n",
      "Epoch 2098 Loss 2.949108\n",
      "Epoch 2099 Loss 2.949036\n",
      "Epoch 2100 Loss 2.948962\n",
      "Epoch 2101 Loss 2.948890\n",
      "Epoch 2102 Loss 2.948818\n",
      "Epoch 2103 Loss 2.948746\n",
      "Epoch 2104 Loss 2.948675\n",
      "Epoch 2105 Loss 2.948602\n",
      "Epoch 2106 Loss 2.948532\n",
      "Epoch 2107 Loss 2.948462\n",
      "Epoch 2108 Loss 2.948391\n",
      "Epoch 2109 Loss 2.948321\n",
      "Epoch 2110 Loss 2.948250\n",
      "Epoch 2111 Loss 2.948180\n",
      "Epoch 2112 Loss 2.948109\n",
      "Epoch 2113 Loss 2.948040\n",
      "Epoch 2114 Loss 2.947971\n",
      "Epoch 2115 Loss 2.947902\n",
      "Epoch 2116 Loss 2.947833\n",
      "Epoch 2117 Loss 2.947765\n",
      "Epoch 2118 Loss 2.947696\n",
      "Epoch 2119 Loss 2.947628\n",
      "Epoch 2120 Loss 2.947560\n",
      "Epoch 2121 Loss 2.947494\n",
      "Epoch 2122 Loss 2.947426\n",
      "Epoch 2123 Loss 2.947358\n",
      "Epoch 2124 Loss 2.947294\n",
      "Epoch 2125 Loss 2.947226\n",
      "Epoch 2126 Loss 2.947158\n",
      "Epoch 2127 Loss 2.947091\n",
      "Epoch 2128 Loss 2.947026\n",
      "Epoch 2129 Loss 2.946960\n",
      "Epoch 2130 Loss 2.946895\n",
      "Epoch 2131 Loss 2.946830\n",
      "Epoch 2132 Loss 2.946764\n",
      "Epoch 2133 Loss 2.946700\n",
      "Epoch 2134 Loss 2.946635\n",
      "Epoch 2135 Loss 2.946571\n",
      "Epoch 2136 Loss 2.946507\n",
      "Epoch 2137 Loss 2.946442\n",
      "Epoch 2138 Loss 2.946378\n",
      "Epoch 2139 Loss 2.946314\n",
      "Epoch 2140 Loss 2.946251\n",
      "Epoch 2141 Loss 2.946189\n",
      "Epoch 2142 Loss 2.946125\n",
      "Epoch 2143 Loss 2.946063\n",
      "Epoch 2144 Loss 2.946001\n",
      "Epoch 2145 Loss 2.945937\n",
      "Epoch 2146 Loss 2.945876\n",
      "Epoch 2147 Loss 2.945815\n",
      "Epoch 2148 Loss 2.945753\n",
      "Epoch 2149 Loss 2.945690\n",
      "Epoch 2150 Loss 2.945630\n",
      "Epoch 2151 Loss 2.945567\n",
      "Epoch 2152 Loss 2.945509\n",
      "Epoch 2153 Loss 2.945448\n",
      "Epoch 2154 Loss 2.945386\n",
      "Epoch 2155 Loss 2.945326\n",
      "Epoch 2156 Loss 2.945267\n",
      "Epoch 2157 Loss 2.945207\n",
      "Epoch 2158 Loss 2.945146\n",
      "Epoch 2159 Loss 2.945088\n",
      "Epoch 2160 Loss 2.945028\n",
      "Epoch 2161 Loss 2.944969\n",
      "Epoch 2162 Loss 2.944911\n",
      "Epoch 2163 Loss 2.944852\n",
      "Epoch 2164 Loss 2.944792\n",
      "Epoch 2165 Loss 2.944736\n",
      "Epoch 2166 Loss 2.944678\n",
      "Epoch 2167 Loss 2.944619\n",
      "Epoch 2168 Loss 2.944562\n",
      "Epoch 2169 Loss 2.944504\n",
      "Epoch 2170 Loss 2.944447\n",
      "Epoch 2171 Loss 2.944391\n",
      "Epoch 2172 Loss 2.944332\n",
      "Epoch 2173 Loss 2.944276\n",
      "Epoch 2174 Loss 2.944220\n",
      "Epoch 2175 Loss 2.944164\n",
      "Epoch 2176 Loss 2.944108\n",
      "Epoch 2177 Loss 2.944052\n",
      "Epoch 2178 Loss 2.943996\n",
      "Epoch 2179 Loss 2.943941\n",
      "Epoch 2180 Loss 2.943886\n",
      "Epoch 2181 Loss 2.943831\n",
      "Epoch 2182 Loss 2.943775\n",
      "Epoch 2183 Loss 2.943721\n",
      "Epoch 2184 Loss 2.943666\n",
      "Epoch 2185 Loss 2.943613\n",
      "Epoch 2186 Loss 2.943558\n",
      "Epoch 2187 Loss 2.943504\n",
      "Epoch 2188 Loss 2.943451\n",
      "Epoch 2189 Loss 2.943395\n",
      "Epoch 2190 Loss 2.943343\n",
      "Epoch 2191 Loss 2.943290\n",
      "Epoch 2192 Loss 2.943235\n",
      "Epoch 2193 Loss 2.943183\n",
      "Epoch 2194 Loss 2.943130\n",
      "Epoch 2195 Loss 2.943079\n",
      "Epoch 2196 Loss 2.943026\n",
      "Epoch 2197 Loss 2.942974\n",
      "Epoch 2198 Loss 2.942922\n",
      "Epoch 2199 Loss 2.942870\n",
      "Epoch 2200 Loss 2.942818\n",
      "Epoch 2201 Loss 2.942765\n",
      "Epoch 2202 Loss 2.942714\n",
      "Epoch 2203 Loss 2.942664\n",
      "Epoch 2204 Loss 2.942612\n",
      "Epoch 2205 Loss 2.942563\n",
      "Epoch 2206 Loss 2.942510\n",
      "Epoch 2207 Loss 2.942461\n",
      "Epoch 2208 Loss 2.942411\n",
      "Epoch 2209 Loss 2.942361\n",
      "Epoch 2210 Loss 2.942310\n",
      "Epoch 2211 Loss 2.942261\n",
      "Epoch 2212 Loss 2.942211\n",
      "Epoch 2213 Loss 2.942162\n",
      "Epoch 2214 Loss 2.942112\n",
      "Epoch 2215 Loss 2.942062\n",
      "Epoch 2216 Loss 2.942014\n",
      "Epoch 2217 Loss 2.941965\n",
      "Epoch 2218 Loss 2.941918\n",
      "Epoch 2219 Loss 2.941868\n",
      "Epoch 2220 Loss 2.941821\n",
      "Epoch 2221 Loss 2.941773\n",
      "Epoch 2222 Loss 2.941725\n",
      "Epoch 2223 Loss 2.941677\n",
      "Epoch 2224 Loss 2.941629\n",
      "Epoch 2225 Loss 2.941582\n",
      "Epoch 2226 Loss 2.941534\n",
      "Epoch 2227 Loss 2.941488\n",
      "Epoch 2228 Loss 2.941440\n",
      "Epoch 2229 Loss 2.941393\n",
      "Epoch 2230 Loss 2.941346\n",
      "Epoch 2231 Loss 2.941299\n",
      "Epoch 2232 Loss 2.941252\n",
      "Epoch 2233 Loss 2.941206\n",
      "Epoch 2234 Loss 2.941163\n",
      "Epoch 2235 Loss 2.941115\n",
      "Epoch 2236 Loss 2.941070\n",
      "Epoch 2237 Loss 2.941025\n",
      "Epoch 2238 Loss 2.940979\n",
      "Epoch 2239 Loss 2.940933\n",
      "Epoch 2240 Loss 2.940890\n",
      "Epoch 2241 Loss 2.940844\n",
      "Epoch 2242 Loss 2.940798\n",
      "Epoch 2243 Loss 2.940753\n",
      "Epoch 2244 Loss 2.940711\n",
      "Epoch 2245 Loss 2.940666\n",
      "Epoch 2246 Loss 2.940621\n",
      "Epoch 2247 Loss 2.940576\n",
      "Epoch 2248 Loss 2.940533\n",
      "Epoch 2249 Loss 2.940489\n",
      "Epoch 2250 Loss 2.940446\n",
      "Epoch 2251 Loss 2.940403\n",
      "Epoch 2252 Loss 2.940358\n",
      "Epoch 2253 Loss 2.940316\n",
      "Epoch 2254 Loss 2.940274\n",
      "Epoch 2255 Loss 2.940229\n",
      "Epoch 2256 Loss 2.940187\n",
      "Epoch 2257 Loss 2.940144\n",
      "Epoch 2258 Loss 2.940102\n",
      "Epoch 2259 Loss 2.940060\n",
      "Epoch 2260 Loss 2.940018\n",
      "Epoch 2261 Loss 2.939977\n",
      "Epoch 2262 Loss 2.939934\n",
      "Epoch 2263 Loss 2.939892\n",
      "Epoch 2264 Loss 2.939851\n",
      "Epoch 2265 Loss 2.939809\n",
      "Epoch 2266 Loss 2.939769\n",
      "Epoch 2267 Loss 2.939727\n",
      "Epoch 2268 Loss 2.939686\n",
      "Epoch 2269 Loss 2.939646\n",
      "Epoch 2270 Loss 2.939605\n",
      "Epoch 2271 Loss 2.939565\n",
      "Epoch 2272 Loss 2.939522\n",
      "Epoch 2273 Loss 2.939483\n",
      "Epoch 2274 Loss 2.939443\n",
      "Epoch 2275 Loss 2.939403\n",
      "Epoch 2276 Loss 2.939361\n",
      "Epoch 2277 Loss 2.939323\n",
      "Epoch 2278 Loss 2.939282\n",
      "Epoch 2279 Loss 2.939243\n",
      "Epoch 2280 Loss 2.939205\n",
      "Epoch 2281 Loss 2.939165\n",
      "Epoch 2282 Loss 2.939127\n",
      "Epoch 2283 Loss 2.939087\n",
      "Epoch 2284 Loss 2.939049\n",
      "Epoch 2285 Loss 2.939011\n",
      "Epoch 2286 Loss 2.938971\n",
      "Epoch 2287 Loss 2.938933\n",
      "Epoch 2288 Loss 2.938893\n",
      "Epoch 2289 Loss 2.938857\n",
      "Epoch 2290 Loss 2.938820\n",
      "Epoch 2291 Loss 2.938779\n",
      "Epoch 2292 Loss 2.938743\n",
      "Epoch 2293 Loss 2.938705\n",
      "Epoch 2294 Loss 2.938667\n",
      "Epoch 2295 Loss 2.938629\n",
      "Epoch 2296 Loss 2.938593\n",
      "Epoch 2297 Loss 2.938555\n",
      "Epoch 2298 Loss 2.938519\n",
      "Epoch 2299 Loss 2.938481\n",
      "Epoch 2300 Loss 2.938444\n",
      "Epoch 2301 Loss 2.938408\n",
      "Epoch 2302 Loss 2.938371\n",
      "Epoch 2303 Loss 2.938335\n",
      "Epoch 2304 Loss 2.938299\n",
      "Epoch 2305 Loss 2.938262\n",
      "Epoch 2306 Loss 2.938227\n",
      "Epoch 2307 Loss 2.938191\n",
      "Epoch 2308 Loss 2.938155\n",
      "Epoch 2309 Loss 2.938118\n",
      "Epoch 2310 Loss 2.938084\n",
      "Epoch 2311 Loss 2.938049\n",
      "Epoch 2312 Loss 2.938014\n",
      "Epoch 2313 Loss 2.937977\n",
      "Epoch 2314 Loss 2.937943\n",
      "Epoch 2315 Loss 2.937908\n",
      "Epoch 2316 Loss 2.937872\n",
      "Epoch 2317 Loss 2.937839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2318 Loss 2.937804\n",
      "Epoch 2319 Loss 2.937768\n",
      "Epoch 2320 Loss 2.937734\n",
      "Epoch 2321 Loss 2.937700\n",
      "Epoch 2322 Loss 2.937665\n",
      "Epoch 2323 Loss 2.937632\n",
      "Epoch 2324 Loss 2.937598\n",
      "Epoch 2325 Loss 2.937565\n",
      "Epoch 2326 Loss 2.937531\n",
      "Epoch 2327 Loss 2.937499\n",
      "Epoch 2328 Loss 2.937464\n",
      "Epoch 2329 Loss 2.937430\n",
      "Epoch 2330 Loss 2.937398\n",
      "Epoch 2331 Loss 2.937364\n",
      "Epoch 2332 Loss 2.937332\n",
      "Epoch 2333 Loss 2.937299\n",
      "Epoch 2334 Loss 2.937265\n",
      "Epoch 2335 Loss 2.937232\n",
      "Epoch 2336 Loss 2.937201\n",
      "Epoch 2337 Loss 2.937168\n",
      "Epoch 2338 Loss 2.937134\n",
      "Epoch 2339 Loss 2.937104\n",
      "Epoch 2340 Loss 2.937071\n",
      "Epoch 2341 Loss 2.937039\n",
      "Epoch 2342 Loss 2.937008\n",
      "Epoch 2343 Loss 2.936976\n",
      "Epoch 2344 Loss 2.936945\n",
      "Epoch 2345 Loss 2.936912\n",
      "Epoch 2346 Loss 2.936882\n",
      "Epoch 2347 Loss 2.936851\n",
      "Epoch 2348 Loss 2.936819\n",
      "Epoch 2349 Loss 2.936788\n",
      "Epoch 2350 Loss 2.936757\n",
      "Epoch 2351 Loss 2.936725\n",
      "Epoch 2352 Loss 2.936694\n",
      "Epoch 2353 Loss 2.936665\n",
      "Epoch 2354 Loss 2.936633\n",
      "Epoch 2355 Loss 2.936602\n",
      "Epoch 2356 Loss 2.936572\n",
      "Epoch 2357 Loss 2.936542\n",
      "Epoch 2358 Loss 2.936511\n",
      "Epoch 2359 Loss 2.936482\n",
      "Epoch 2360 Loss 2.936451\n",
      "Epoch 2361 Loss 2.936421\n",
      "Epoch 2362 Loss 2.936392\n",
      "Epoch 2363 Loss 2.936362\n",
      "Epoch 2364 Loss 2.936332\n",
      "Epoch 2365 Loss 2.936304\n",
      "Epoch 2366 Loss 2.936274\n",
      "Epoch 2367 Loss 2.936244\n",
      "Epoch 2368 Loss 2.936215\n",
      "Epoch 2369 Loss 2.936188\n",
      "Epoch 2370 Loss 2.936156\n",
      "Epoch 2371 Loss 2.936128\n",
      "Epoch 2372 Loss 2.936100\n",
      "Epoch 2373 Loss 2.936071\n",
      "Epoch 2374 Loss 2.936043\n",
      "Epoch 2375 Loss 2.936014\n",
      "Epoch 2376 Loss 2.935986\n",
      "Epoch 2377 Loss 2.935957\n",
      "Epoch 2378 Loss 2.935928\n",
      "Epoch 2379 Loss 2.935901\n",
      "Epoch 2380 Loss 2.935873\n",
      "Epoch 2381 Loss 2.935845\n",
      "Epoch 2382 Loss 2.935817\n",
      "Epoch 2383 Loss 2.935789\n",
      "Epoch 2384 Loss 2.935761\n",
      "Epoch 2385 Loss 2.935734\n",
      "Epoch 2386 Loss 2.935707\n",
      "Epoch 2387 Loss 2.935679\n",
      "Epoch 2388 Loss 2.935650\n",
      "Epoch 2389 Loss 2.935626\n",
      "Epoch 2390 Loss 2.935596\n",
      "Epoch 2391 Loss 2.935571\n",
      "Epoch 2392 Loss 2.935544\n",
      "Epoch 2393 Loss 2.935516\n",
      "Epoch 2394 Loss 2.935489\n",
      "Epoch 2395 Loss 2.935464\n",
      "Epoch 2396 Loss 2.935436\n",
      "Epoch 2397 Loss 2.935412\n",
      "Epoch 2398 Loss 2.935385\n",
      "Epoch 2399 Loss 2.935357\n",
      "Epoch 2400 Loss 2.935332\n",
      "Epoch 2401 Loss 2.935304\n",
      "Epoch 2402 Loss 2.935281\n",
      "Epoch 2403 Loss 2.935252\n",
      "Epoch 2404 Loss 2.935229\n",
      "Epoch 2405 Loss 2.935203\n",
      "Epoch 2406 Loss 2.935177\n",
      "Epoch 2407 Loss 2.935152\n",
      "Epoch 2408 Loss 2.935126\n",
      "Epoch 2409 Loss 2.935099\n",
      "Epoch 2410 Loss 2.935075\n",
      "Epoch 2411 Loss 2.935049\n",
      "Epoch 2412 Loss 2.935024\n",
      "Epoch 2413 Loss 2.935001\n",
      "Epoch 2414 Loss 2.934973\n",
      "Epoch 2415 Loss 2.934949\n",
      "Epoch 2416 Loss 2.934925\n",
      "Epoch 2417 Loss 2.934899\n",
      "Epoch 2418 Loss 2.934876\n",
      "Epoch 2419 Loss 2.934852\n",
      "Epoch 2420 Loss 2.934827\n",
      "Epoch 2421 Loss 2.934802\n",
      "Epoch 2422 Loss 2.934777\n",
      "Epoch 2423 Loss 2.934753\n",
      "Epoch 2424 Loss 2.934730\n",
      "Epoch 2425 Loss 2.934705\n",
      "Epoch 2426 Loss 2.934681\n",
      "Epoch 2427 Loss 2.934658\n",
      "Epoch 2428 Loss 2.934635\n",
      "Epoch 2429 Loss 2.934609\n",
      "Epoch 2430 Loss 2.934585\n",
      "Epoch 2431 Loss 2.934564\n",
      "Epoch 2432 Loss 2.934541\n",
      "Epoch 2433 Loss 2.934516\n",
      "Epoch 2434 Loss 2.934493\n",
      "Epoch 2435 Loss 2.934469\n",
      "Epoch 2436 Loss 2.934446\n",
      "Epoch 2437 Loss 2.934423\n",
      "Epoch 2438 Loss 2.934400\n",
      "Epoch 2439 Loss 2.934377\n",
      "Epoch 2440 Loss 2.934355\n",
      "Epoch 2441 Loss 2.934331\n",
      "Epoch 2442 Loss 2.934309\n",
      "Epoch 2443 Loss 2.934287\n",
      "Epoch 2444 Loss 2.934264\n",
      "Epoch 2445 Loss 2.934242\n",
      "Epoch 2446 Loss 2.934219\n",
      "Epoch 2447 Loss 2.934198\n",
      "Epoch 2448 Loss 2.934175\n",
      "Epoch 2449 Loss 2.934151\n",
      "Epoch 2450 Loss 2.934129\n",
      "Epoch 2451 Loss 2.934108\n",
      "Epoch 2452 Loss 2.934084\n",
      "Epoch 2453 Loss 2.934064\n",
      "Epoch 2454 Loss 2.934043\n",
      "Epoch 2455 Loss 2.934020\n",
      "Epoch 2456 Loss 2.934000\n",
      "Epoch 2457 Loss 2.933978\n",
      "Epoch 2458 Loss 2.933956\n",
      "Epoch 2459 Loss 2.933935\n",
      "Epoch 2460 Loss 2.933913\n",
      "Epoch 2461 Loss 2.933893\n",
      "Epoch 2462 Loss 2.933871\n",
      "Epoch 2463 Loss 2.933849\n",
      "Epoch 2464 Loss 2.933828\n",
      "Epoch 2465 Loss 2.933807\n",
      "Epoch 2466 Loss 2.933787\n",
      "Epoch 2467 Loss 2.933766\n",
      "Epoch 2468 Loss 2.933745\n",
      "Epoch 2469 Loss 2.933723\n",
      "Epoch 2470 Loss 2.933704\n",
      "Epoch 2471 Loss 2.933682\n",
      "Epoch 2472 Loss 2.933662\n",
      "Epoch 2473 Loss 2.933643\n",
      "Epoch 2474 Loss 2.933622\n",
      "Epoch 2475 Loss 2.933602\n",
      "Epoch 2476 Loss 2.933583\n",
      "Epoch 2477 Loss 2.933561\n",
      "Epoch 2478 Loss 2.933541\n",
      "Epoch 2479 Loss 2.933521\n",
      "Epoch 2480 Loss 2.933501\n",
      "Epoch 2481 Loss 2.933480\n",
      "Epoch 2482 Loss 2.933463\n",
      "Epoch 2483 Loss 2.933442\n",
      "Epoch 2484 Loss 2.933423\n",
      "Epoch 2485 Loss 2.933403\n",
      "Epoch 2486 Loss 2.933382\n",
      "Epoch 2487 Loss 2.933365\n",
      "Epoch 2488 Loss 2.933345\n",
      "Epoch 2489 Loss 2.933325\n",
      "Epoch 2490 Loss 2.933306\n",
      "Epoch 2491 Loss 2.933287\n",
      "Epoch 2492 Loss 2.933266\n",
      "Epoch 2493 Loss 2.933249\n",
      "Epoch 2494 Loss 2.933228\n",
      "Epoch 2495 Loss 2.933209\n",
      "Epoch 2496 Loss 2.933190\n",
      "Epoch 2497 Loss 2.933172\n",
      "Epoch 2498 Loss 2.933154\n",
      "Epoch 2499 Loss 2.933134\n",
      "Epoch 2500 Loss 2.933116\n",
      "Epoch 2501 Loss 2.933097\n",
      "Epoch 2502 Loss 2.933079\n",
      "Epoch 2503 Loss 2.933060\n",
      "Epoch 2504 Loss 2.933043\n",
      "Epoch 2505 Loss 2.933025\n",
      "Epoch 2506 Loss 2.933007\n",
      "Epoch 2507 Loss 2.932988\n",
      "Epoch 2508 Loss 2.932970\n",
      "Epoch 2509 Loss 2.932953\n",
      "Epoch 2510 Loss 2.932932\n",
      "Epoch 2511 Loss 2.932915\n",
      "Epoch 2512 Loss 2.932898\n",
      "Epoch 2513 Loss 2.932880\n",
      "Epoch 2514 Loss 2.932862\n",
      "Epoch 2515 Loss 2.932846\n",
      "Epoch 2516 Loss 2.932826\n",
      "Epoch 2517 Loss 2.932810\n",
      "Epoch 2518 Loss 2.932791\n",
      "Epoch 2519 Loss 2.932774\n",
      "Epoch 2520 Loss 2.932758\n",
      "Epoch 2521 Loss 2.932739\n",
      "Epoch 2522 Loss 2.932723\n",
      "Epoch 2523 Loss 2.932706\n",
      "Epoch 2524 Loss 2.932689\n",
      "Epoch 2525 Loss 2.932671\n",
      "Epoch 2526 Loss 2.932654\n",
      "Epoch 2527 Loss 2.932637\n",
      "Epoch 2528 Loss 2.932619\n",
      "Epoch 2529 Loss 2.932603\n",
      "Epoch 2530 Loss 2.932585\n",
      "Epoch 2531 Loss 2.932569\n",
      "Epoch 2532 Loss 2.932553\n",
      "Epoch 2533 Loss 2.932535\n",
      "Epoch 2534 Loss 2.932520\n",
      "Epoch 2535 Loss 2.932502\n",
      "Epoch 2536 Loss 2.932487\n",
      "Epoch 2537 Loss 2.932469\n",
      "Epoch 2538 Loss 2.932455\n",
      "Epoch 2539 Loss 2.932438\n",
      "Epoch 2540 Loss 2.932421\n",
      "Epoch 2541 Loss 2.932404\n",
      "Epoch 2542 Loss 2.932387\n",
      "Epoch 2543 Loss 2.932370\n",
      "Epoch 2544 Loss 2.932358\n",
      "Epoch 2545 Loss 2.932340\n",
      "Epoch 2546 Loss 2.932324\n",
      "Epoch 2547 Loss 2.932310\n",
      "Epoch 2548 Loss 2.932293\n",
      "Epoch 2549 Loss 2.932278\n",
      "Epoch 2550 Loss 2.932261\n",
      "Epoch 2551 Loss 2.932246\n",
      "Epoch 2552 Loss 2.932229\n",
      "Epoch 2553 Loss 2.932215\n",
      "Epoch 2554 Loss 2.932198\n",
      "Epoch 2555 Loss 2.932184\n",
      "Epoch 2556 Loss 2.932168\n",
      "Epoch 2557 Loss 2.932153\n",
      "Epoch 2558 Loss 2.932137\n",
      "Epoch 2559 Loss 2.932122\n",
      "Epoch 2560 Loss 2.932107\n",
      "Epoch 2561 Loss 2.932092\n",
      "Epoch 2562 Loss 2.932076\n",
      "Epoch 2563 Loss 2.932061\n",
      "Epoch 2564 Loss 2.932047\n",
      "Epoch 2565 Loss 2.932031\n",
      "Epoch 2566 Loss 2.932017\n",
      "Epoch 2567 Loss 2.932002\n",
      "Epoch 2568 Loss 2.931986\n",
      "Epoch 2569 Loss 2.931972\n",
      "Epoch 2570 Loss 2.931957\n",
      "Epoch 2571 Loss 2.931941\n",
      "Epoch 2572 Loss 2.931929\n",
      "Epoch 2573 Loss 2.931914\n",
      "Epoch 2574 Loss 2.931900\n",
      "Epoch 2575 Loss 2.931885\n",
      "Epoch 2576 Loss 2.931870\n",
      "Epoch 2577 Loss 2.931855\n",
      "Epoch 2578 Loss 2.931843\n",
      "Epoch 2579 Loss 2.931828\n",
      "Epoch 2580 Loss 2.931813\n",
      "Epoch 2581 Loss 2.931799\n",
      "Epoch 2582 Loss 2.931786\n",
      "Epoch 2583 Loss 2.931771\n",
      "Epoch 2584 Loss 2.931759\n",
      "Epoch 2585 Loss 2.931742\n",
      "Epoch 2586 Loss 2.931729\n",
      "Epoch 2587 Loss 2.931717\n",
      "Epoch 2588 Loss 2.931701\n",
      "Epoch 2589 Loss 2.931687\n",
      "Epoch 2590 Loss 2.931674\n",
      "Epoch 2591 Loss 2.931660\n",
      "Epoch 2592 Loss 2.931647\n",
      "Epoch 2593 Loss 2.931632\n",
      "Epoch 2594 Loss 2.931619\n",
      "Epoch 2595 Loss 2.931606\n",
      "Epoch 2596 Loss 2.931594\n",
      "Epoch 2597 Loss 2.931580\n",
      "Epoch 2598 Loss 2.931566\n",
      "Epoch 2599 Loss 2.931554\n",
      "Epoch 2600 Loss 2.931539\n",
      "Epoch 2601 Loss 2.931526\n",
      "Epoch 2602 Loss 2.931512\n",
      "Epoch 2603 Loss 2.931500\n",
      "Epoch 2604 Loss 2.931488\n",
      "Epoch 2605 Loss 2.931474\n",
      "Epoch 2606 Loss 2.931462\n",
      "Epoch 2607 Loss 2.931448\n",
      "Epoch 2608 Loss 2.931436\n",
      "Epoch 2609 Loss 2.931422\n",
      "Epoch 2610 Loss 2.931411\n",
      "Epoch 2611 Loss 2.931398\n",
      "Epoch 2612 Loss 2.931384\n",
      "Epoch 2613 Loss 2.931370\n",
      "Epoch 2614 Loss 2.931358\n",
      "Epoch 2615 Loss 2.931346\n",
      "Epoch 2616 Loss 2.931334\n",
      "Epoch 2617 Loss 2.931322\n",
      "Epoch 2618 Loss 2.931309\n",
      "Epoch 2619 Loss 2.931296\n",
      "Epoch 2620 Loss 2.931282\n",
      "Epoch 2621 Loss 2.931272\n",
      "Epoch 2622 Loss 2.931258\n",
      "Epoch 2623 Loss 2.931245\n",
      "Epoch 2624 Loss 2.931235\n",
      "Epoch 2625 Loss 2.931222\n",
      "Epoch 2626 Loss 2.931211\n",
      "Epoch 2627 Loss 2.931196\n",
      "Epoch 2628 Loss 2.931185\n",
      "Epoch 2629 Loss 2.931173\n",
      "Epoch 2630 Loss 2.931162\n",
      "Epoch 2631 Loss 2.931149\n",
      "Epoch 2632 Loss 2.931139\n",
      "Epoch 2633 Loss 2.931126\n",
      "Epoch 2634 Loss 2.931114\n",
      "Epoch 2635 Loss 2.931101\n",
      "Epoch 2636 Loss 2.931090\n",
      "Epoch 2637 Loss 2.931079\n",
      "Epoch 2638 Loss 2.931067\n",
      "Epoch 2639 Loss 2.931054\n",
      "Epoch 2640 Loss 2.931044\n",
      "Epoch 2641 Loss 2.931034\n",
      "Epoch 2642 Loss 2.931021\n",
      "Epoch 2643 Loss 2.931010\n",
      "Epoch 2644 Loss 2.930999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2645 Loss 2.930987\n",
      "Epoch 2646 Loss 2.930976\n",
      "Epoch 2647 Loss 2.930964\n",
      "Epoch 2648 Loss 2.930953\n",
      "Epoch 2649 Loss 2.930941\n",
      "Epoch 2650 Loss 2.930932\n",
      "Epoch 2651 Loss 2.930920\n",
      "Epoch 2652 Loss 2.930908\n",
      "Epoch 2653 Loss 2.930899\n",
      "Epoch 2654 Loss 2.930885\n",
      "Epoch 2655 Loss 2.930876\n",
      "Epoch 2656 Loss 2.930864\n",
      "Epoch 2657 Loss 2.930854\n",
      "Epoch 2658 Loss 2.930841\n",
      "Epoch 2659 Loss 2.930833\n",
      "Epoch 2660 Loss 2.930821\n",
      "Epoch 2661 Loss 2.930811\n",
      "Epoch 2662 Loss 2.930801\n",
      "Epoch 2663 Loss 2.930788\n",
      "Epoch 2664 Loss 2.930778\n",
      "Epoch 2665 Loss 2.930766\n",
      "Epoch 2666 Loss 2.930757\n",
      "Epoch 2667 Loss 2.930746\n",
      "Epoch 2668 Loss 2.930735\n",
      "Epoch 2669 Loss 2.930724\n",
      "Epoch 2670 Loss 2.930715\n",
      "Epoch 2671 Loss 2.930704\n",
      "Epoch 2672 Loss 2.930694\n",
      "Epoch 2673 Loss 2.930685\n",
      "Epoch 2674 Loss 2.930674\n",
      "Epoch 2675 Loss 2.930663\n",
      "Epoch 2676 Loss 2.930654\n",
      "Epoch 2677 Loss 2.930644\n",
      "Epoch 2678 Loss 2.930631\n",
      "Epoch 2679 Loss 2.930622\n",
      "Epoch 2680 Loss 2.930614\n",
      "Epoch 2681 Loss 2.930603\n",
      "Epoch 2682 Loss 2.930592\n",
      "Epoch 2683 Loss 2.930582\n",
      "Epoch 2684 Loss 2.930572\n",
      "Epoch 2685 Loss 2.930562\n",
      "Epoch 2686 Loss 2.930552\n",
      "Epoch 2687 Loss 2.930543\n",
      "Epoch 2688 Loss 2.930534\n",
      "Epoch 2689 Loss 2.930524\n",
      "Epoch 2690 Loss 2.930514\n",
      "Epoch 2691 Loss 2.930502\n",
      "Epoch 2692 Loss 2.930493\n",
      "Epoch 2693 Loss 2.930482\n",
      "Epoch 2694 Loss 2.930474\n",
      "Epoch 2695 Loss 2.930465\n",
      "Epoch 2696 Loss 2.930454\n",
      "Epoch 2697 Loss 2.930446\n",
      "Epoch 2698 Loss 2.930436\n",
      "Epoch 2699 Loss 2.930426\n",
      "Epoch 2700 Loss 2.930417\n",
      "Epoch 2701 Loss 2.930408\n",
      "Epoch 2702 Loss 2.930398\n",
      "Epoch 2703 Loss 2.930388\n",
      "Epoch 2704 Loss 2.930380\n",
      "Epoch 2705 Loss 2.930370\n",
      "Epoch 2706 Loss 2.930360\n",
      "Epoch 2707 Loss 2.930352\n",
      "Epoch 2708 Loss 2.930342\n",
      "Epoch 2709 Loss 2.930334\n",
      "Epoch 2710 Loss 2.930325\n",
      "Epoch 2711 Loss 2.930315\n",
      "Epoch 2712 Loss 2.930306\n",
      "Epoch 2713 Loss 2.930298\n",
      "Epoch 2714 Loss 2.930288\n",
      "Epoch 2715 Loss 2.930279\n",
      "Epoch 2716 Loss 2.930270\n",
      "Epoch 2717 Loss 2.930262\n",
      "Epoch 2718 Loss 2.930254\n",
      "Epoch 2719 Loss 2.930244\n",
      "Epoch 2720 Loss 2.930235\n",
      "Epoch 2721 Loss 2.930226\n",
      "Epoch 2722 Loss 2.930218\n",
      "Epoch 2723 Loss 2.930209\n",
      "Epoch 2724 Loss 2.930201\n",
      "Epoch 2725 Loss 2.930190\n",
      "Epoch 2726 Loss 2.930182\n",
      "Epoch 2727 Loss 2.930173\n",
      "Epoch 2728 Loss 2.930167\n",
      "Epoch 2729 Loss 2.930156\n",
      "Epoch 2730 Loss 2.930149\n",
      "Epoch 2731 Loss 2.930139\n",
      "Epoch 2732 Loss 2.930131\n",
      "Epoch 2733 Loss 2.930123\n",
      "Epoch 2734 Loss 2.930113\n",
      "Epoch 2735 Loss 2.930107\n",
      "Epoch 2736 Loss 2.930099\n",
      "Epoch 2737 Loss 2.930090\n",
      "Epoch 2738 Loss 2.930081\n",
      "Epoch 2739 Loss 2.930073\n",
      "Epoch 2740 Loss 2.930064\n",
      "Epoch 2741 Loss 2.930056\n",
      "Epoch 2742 Loss 2.930048\n",
      "Epoch 2743 Loss 2.930041\n",
      "Epoch 2744 Loss 2.930032\n",
      "Epoch 2745 Loss 2.930022\n",
      "Epoch 2746 Loss 2.930016\n",
      "Epoch 2747 Loss 2.930008\n",
      "Epoch 2748 Loss 2.930000\n",
      "Epoch 2749 Loss 2.929992\n",
      "Epoch 2750 Loss 2.929983\n",
      "Epoch 2751 Loss 2.929975\n",
      "Epoch 2752 Loss 2.929968\n",
      "Epoch 2753 Loss 2.929960\n",
      "Epoch 2754 Loss 2.929953\n",
      "Epoch 2755 Loss 2.929945\n",
      "Epoch 2756 Loss 2.929937\n",
      "Epoch 2757 Loss 2.929929\n",
      "Epoch 2758 Loss 2.929921\n",
      "Epoch 2759 Loss 2.929914\n",
      "Epoch 2760 Loss 2.929905\n",
      "Epoch 2761 Loss 2.929896\n",
      "Epoch 2762 Loss 2.929891\n",
      "Epoch 2763 Loss 2.929882\n",
      "Epoch 2764 Loss 2.929874\n",
      "Epoch 2765 Loss 2.929869\n",
      "Epoch 2766 Loss 2.929859\n",
      "Epoch 2767 Loss 2.929852\n",
      "Epoch 2768 Loss 2.929845\n",
      "Epoch 2769 Loss 2.929838\n",
      "Epoch 2770 Loss 2.929830\n",
      "Epoch 2771 Loss 2.929822\n",
      "Epoch 2772 Loss 2.929816\n",
      "Epoch 2773 Loss 2.929806\n",
      "Epoch 2774 Loss 2.929799\n",
      "Epoch 2775 Loss 2.929793\n",
      "Epoch 2776 Loss 2.929786\n",
      "Epoch 2777 Loss 2.929778\n",
      "Epoch 2778 Loss 2.929771\n",
      "Epoch 2779 Loss 2.929765\n",
      "Epoch 2780 Loss 2.929757\n",
      "Epoch 2781 Loss 2.929750\n",
      "Epoch 2782 Loss 2.929743\n",
      "Epoch 2783 Loss 2.929735\n",
      "Epoch 2784 Loss 2.929729\n",
      "Epoch 2785 Loss 2.929722\n",
      "Epoch 2786 Loss 2.929714\n",
      "Epoch 2787 Loss 2.929707\n",
      "Epoch 2788 Loss 2.929701\n",
      "Epoch 2789 Loss 2.929692\n",
      "Epoch 2790 Loss 2.929685\n",
      "Epoch 2791 Loss 2.929680\n",
      "Epoch 2792 Loss 2.929672\n",
      "Epoch 2793 Loss 2.929666\n",
      "Epoch 2794 Loss 2.929658\n",
      "Epoch 2795 Loss 2.929652\n",
      "Epoch 2796 Loss 2.929646\n",
      "Epoch 2797 Loss 2.929638\n",
      "Epoch 2798 Loss 2.929632\n",
      "Epoch 2799 Loss 2.929626\n",
      "Epoch 2800 Loss 2.929620\n",
      "Epoch 2801 Loss 2.929611\n",
      "Epoch 2802 Loss 2.929605\n",
      "Epoch 2803 Loss 2.929600\n",
      "Epoch 2804 Loss 2.929593\n",
      "Epoch 2805 Loss 2.929586\n",
      "Epoch 2806 Loss 2.929579\n",
      "Epoch 2807 Loss 2.929572\n",
      "Epoch 2808 Loss 2.929566\n",
      "Epoch 2809 Loss 2.929559\n",
      "Epoch 2810 Loss 2.929552\n",
      "Epoch 2811 Loss 2.929545\n",
      "Epoch 2812 Loss 2.929540\n",
      "Epoch 2813 Loss 2.929533\n",
      "Epoch 2814 Loss 2.929527\n",
      "Epoch 2815 Loss 2.929520\n",
      "Epoch 2816 Loss 2.929513\n",
      "Epoch 2817 Loss 2.929507\n",
      "Epoch 2818 Loss 2.929501\n",
      "Epoch 2819 Loss 2.929496\n",
      "Epoch 2820 Loss 2.929489\n",
      "Epoch 2821 Loss 2.929482\n",
      "Epoch 2822 Loss 2.929476\n",
      "Epoch 2823 Loss 2.929471\n",
      "Epoch 2824 Loss 2.929463\n",
      "Epoch 2825 Loss 2.929457\n",
      "Epoch 2826 Loss 2.929452\n",
      "Epoch 2827 Loss 2.929445\n",
      "Epoch 2828 Loss 2.929439\n",
      "Epoch 2829 Loss 2.929433\n",
      "Epoch 2830 Loss 2.929427\n",
      "Epoch 2831 Loss 2.929421\n",
      "Epoch 2832 Loss 2.929415\n",
      "Epoch 2833 Loss 2.929409\n",
      "Epoch 2834 Loss 2.929404\n",
      "Epoch 2835 Loss 2.929396\n",
      "Epoch 2836 Loss 2.929391\n",
      "Epoch 2837 Loss 2.929383\n",
      "Epoch 2838 Loss 2.929380\n",
      "Epoch 2839 Loss 2.929373\n",
      "Epoch 2840 Loss 2.929368\n",
      "Epoch 2841 Loss 2.929362\n",
      "Epoch 2842 Loss 2.929356\n",
      "Epoch 2843 Loss 2.929351\n",
      "Epoch 2844 Loss 2.929344\n",
      "Epoch 2845 Loss 2.929338\n",
      "Epoch 2846 Loss 2.929332\n",
      "Epoch 2847 Loss 2.929328\n",
      "Epoch 2848 Loss 2.929321\n",
      "Epoch 2849 Loss 2.929316\n",
      "Epoch 2850 Loss 2.929309\n",
      "Epoch 2851 Loss 2.929304\n",
      "Epoch 2852 Loss 2.929300\n",
      "Epoch 2853 Loss 2.929293\n",
      "Epoch 2854 Loss 2.929288\n",
      "Epoch 2855 Loss 2.929282\n",
      "Epoch 2856 Loss 2.929277\n",
      "Epoch 2857 Loss 2.929271\n",
      "Epoch 2858 Loss 2.929266\n",
      "Epoch 2859 Loss 2.929260\n",
      "Epoch 2860 Loss 2.929255\n",
      "Epoch 2861 Loss 2.929250\n",
      "Epoch 2862 Loss 2.929244\n",
      "Epoch 2863 Loss 2.929237\n",
      "Epoch 2864 Loss 2.929234\n",
      "Epoch 2865 Loss 2.929227\n",
      "Epoch 2866 Loss 2.929222\n",
      "Epoch 2867 Loss 2.929216\n",
      "Epoch 2868 Loss 2.929212\n",
      "Epoch 2869 Loss 2.929207\n",
      "Epoch 2870 Loss 2.929202\n",
      "Epoch 2871 Loss 2.929195\n",
      "Epoch 2872 Loss 2.929191\n",
      "Epoch 2873 Loss 2.929184\n",
      "Epoch 2874 Loss 2.929180\n",
      "Epoch 2875 Loss 2.929175\n",
      "Epoch 2876 Loss 2.929170\n",
      "Epoch 2877 Loss 2.929166\n",
      "Epoch 2878 Loss 2.929160\n",
      "Epoch 2879 Loss 2.929155\n",
      "Epoch 2880 Loss 2.929150\n",
      "Epoch 2881 Loss 2.929144\n",
      "Epoch 2882 Loss 2.929138\n",
      "Epoch 2883 Loss 2.929133\n",
      "Epoch 2884 Loss 2.929127\n",
      "Epoch 2885 Loss 2.929122\n",
      "Epoch 2886 Loss 2.929118\n",
      "Epoch 2887 Loss 2.929113\n",
      "Epoch 2888 Loss 2.929108\n",
      "Epoch 2889 Loss 2.929103\n",
      "Epoch 2890 Loss 2.929099\n",
      "Epoch 2891 Loss 2.929093\n",
      "Epoch 2892 Loss 2.929090\n",
      "Epoch 2893 Loss 2.929084\n",
      "Epoch 2894 Loss 2.929079\n",
      "Epoch 2895 Loss 2.929075\n",
      "Epoch 2896 Loss 2.929069\n",
      "Epoch 2897 Loss 2.929065\n",
      "Epoch 2898 Loss 2.929059\n",
      "Epoch 2899 Loss 2.929054\n",
      "Epoch 2900 Loss 2.929050\n",
      "Epoch 2901 Loss 2.929044\n",
      "Epoch 2902 Loss 2.929040\n",
      "Epoch 2903 Loss 2.929036\n",
      "Epoch 2904 Loss 2.929031\n",
      "Epoch 2905 Loss 2.929025\n",
      "Epoch 2906 Loss 2.929021\n",
      "Epoch 2907 Loss 2.929017\n",
      "Epoch 2908 Loss 2.929012\n",
      "Epoch 2909 Loss 2.929006\n",
      "Epoch 2910 Loss 2.929002\n",
      "Epoch 2911 Loss 2.928999\n",
      "Epoch 2912 Loss 2.928994\n",
      "Epoch 2913 Loss 2.928988\n",
      "Epoch 2914 Loss 2.928984\n",
      "Epoch 2915 Loss 2.928980\n",
      "Epoch 2916 Loss 2.928976\n",
      "Epoch 2917 Loss 2.928971\n",
      "Epoch 2918 Loss 2.928967\n",
      "Epoch 2919 Loss 2.928962\n",
      "Epoch 2920 Loss 2.928958\n",
      "Epoch 2921 Loss 2.928953\n",
      "Epoch 2922 Loss 2.928947\n",
      "Epoch 2923 Loss 2.928946\n",
      "Epoch 2924 Loss 2.928941\n",
      "Epoch 2925 Loss 2.928935\n",
      "Epoch 2926 Loss 2.928932\n",
      "Epoch 2927 Loss 2.928926\n",
      "Epoch 2928 Loss 2.928923\n",
      "Epoch 2929 Loss 2.928919\n",
      "Epoch 2930 Loss 2.928915\n",
      "Epoch 2931 Loss 2.928909\n",
      "Epoch 2932 Loss 2.928904\n",
      "Epoch 2933 Loss 2.928902\n",
      "Epoch 2934 Loss 2.928897\n",
      "Epoch 2935 Loss 2.928893\n",
      "Epoch 2936 Loss 2.928887\n",
      "Epoch 2937 Loss 2.928883\n",
      "Epoch 2938 Loss 2.928880\n",
      "Epoch 2939 Loss 2.928877\n",
      "Epoch 2940 Loss 2.928871\n",
      "Epoch 2941 Loss 2.928867\n",
      "Epoch 2942 Loss 2.928864\n",
      "Epoch 2943 Loss 2.928860\n",
      "Epoch 2944 Loss 2.928855\n",
      "Epoch 2945 Loss 2.928850\n",
      "Epoch 2946 Loss 2.928845\n",
      "Epoch 2947 Loss 2.928843\n",
      "Epoch 2948 Loss 2.928838\n",
      "Epoch 2949 Loss 2.928833\n",
      "Epoch 2950 Loss 2.928830\n",
      "Epoch 2951 Loss 2.928826\n",
      "Epoch 2952 Loss 2.928822\n",
      "Epoch 2953 Loss 2.928818\n",
      "Epoch 2954 Loss 2.928815\n",
      "Epoch 2955 Loss 2.928811\n",
      "Epoch 2956 Loss 2.928805\n",
      "Epoch 2957 Loss 2.928801\n",
      "Epoch 2958 Loss 2.928799\n",
      "Epoch 2959 Loss 2.928795\n",
      "Epoch 2960 Loss 2.928789\n",
      "Epoch 2961 Loss 2.928789\n",
      "Epoch 2962 Loss 2.928783\n",
      "Epoch 2963 Loss 2.928779\n",
      "Epoch 2964 Loss 2.928775\n",
      "Epoch 2965 Loss 2.928771\n",
      "Epoch 2966 Loss 2.928767\n",
      "Epoch 2967 Loss 2.928765\n",
      "Epoch 2968 Loss 2.928761\n",
      "Epoch 2969 Loss 2.928758\n",
      "Epoch 2970 Loss 2.928752\n",
      "Epoch 2971 Loss 2.928750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2972 Loss 2.928745\n",
      "Epoch 2973 Loss 2.928741\n",
      "Epoch 2974 Loss 2.928737\n",
      "Epoch 2975 Loss 2.928735\n",
      "Epoch 2976 Loss 2.928730\n",
      "Epoch 2977 Loss 2.928727\n",
      "Epoch 2978 Loss 2.928723\n",
      "Epoch 2979 Loss 2.928719\n",
      "Epoch 2980 Loss 2.928716\n",
      "Epoch 2981 Loss 2.928712\n",
      "Epoch 2982 Loss 2.928708\n",
      "Epoch 2983 Loss 2.928705\n",
      "Epoch 2984 Loss 2.928700\n",
      "Epoch 2985 Loss 2.928698\n",
      "Epoch 2986 Loss 2.928695\n",
      "Epoch 2987 Loss 2.928690\n",
      "Epoch 2988 Loss 2.928687\n",
      "Epoch 2989 Loss 2.928684\n",
      "Epoch 2990 Loss 2.928679\n",
      "Epoch 2991 Loss 2.928677\n",
      "Epoch 2992 Loss 2.928673\n",
      "Epoch 2993 Loss 2.928669\n",
      "Epoch 2994 Loss 2.928666\n",
      "Epoch 2995 Loss 2.928662\n",
      "Epoch 2996 Loss 2.928660\n",
      "Epoch 2997 Loss 2.928656\n",
      "Epoch 2998 Loss 2.928651\n",
      "Epoch 2999 Loss 2.928648\n",
      "Epoch 3000 Loss 2.928646\n",
      "Epoch 3001 Loss 2.928643\n",
      "Epoch 3002 Loss 2.928638\n",
      "Epoch 3003 Loss 2.928635\n",
      "Epoch 3004 Loss 2.928632\n",
      "Epoch 3005 Loss 2.928629\n",
      "Epoch 3006 Loss 2.928625\n",
      "Epoch 3007 Loss 2.928621\n",
      "Epoch 3008 Loss 2.928617\n",
      "Epoch 3009 Loss 2.928616\n",
      "Epoch 3010 Loss 2.928612\n",
      "Epoch 3011 Loss 2.928608\n",
      "Epoch 3012 Loss 2.928604\n",
      "Epoch 3013 Loss 2.928601\n",
      "Epoch 3014 Loss 2.928599\n",
      "Epoch 3015 Loss 2.928595\n",
      "Epoch 3016 Loss 2.928592\n",
      "Epoch 3017 Loss 2.928588\n",
      "Epoch 3018 Loss 2.928586\n",
      "Epoch 3019 Loss 2.928583\n",
      "Epoch 3020 Loss 2.928580\n",
      "Epoch 3021 Loss 2.928576\n",
      "Epoch 3022 Loss 2.928574\n",
      "Epoch 3023 Loss 2.928569\n",
      "Epoch 3024 Loss 2.928567\n",
      "Epoch 3025 Loss 2.928564\n",
      "Epoch 3026 Loss 2.928560\n",
      "Epoch 3027 Loss 2.928557\n",
      "Epoch 3028 Loss 2.928555\n",
      "Epoch 3029 Loss 2.928551\n",
      "Epoch 3030 Loss 2.928548\n",
      "Epoch 3031 Loss 2.928545\n",
      "Epoch 3032 Loss 2.928543\n",
      "Epoch 3033 Loss 2.928539\n",
      "Epoch 3034 Loss 2.928536\n",
      "Epoch 3035 Loss 2.928532\n",
      "Epoch 3036 Loss 2.928531\n",
      "Epoch 3037 Loss 2.928528\n",
      "Epoch 3038 Loss 2.928524\n",
      "Epoch 3039 Loss 2.928521\n",
      "Epoch 3040 Loss 2.928519\n",
      "Epoch 3041 Loss 2.928514\n",
      "Epoch 3042 Loss 2.928512\n",
      "Epoch 3043 Loss 2.928509\n",
      "Epoch 3044 Loss 2.928505\n",
      "Epoch 3045 Loss 2.928503\n",
      "Epoch 3046 Loss 2.928500\n",
      "Epoch 3047 Loss 2.928498\n",
      "Epoch 3048 Loss 2.928495\n",
      "Epoch 3049 Loss 2.928491\n",
      "Epoch 3050 Loss 2.928488\n",
      "Epoch 3051 Loss 2.928486\n",
      "Epoch 3052 Loss 2.928484\n",
      "Epoch 3053 Loss 2.928480\n",
      "Epoch 3054 Loss 2.928477\n",
      "Epoch 3055 Loss 2.928475\n",
      "Epoch 3056 Loss 2.928473\n",
      "Epoch 3057 Loss 2.928469\n",
      "Epoch 3058 Loss 2.928468\n",
      "Epoch 3059 Loss 2.928463\n",
      "Epoch 3060 Loss 2.928460\n",
      "Epoch 3061 Loss 2.928458\n",
      "Epoch 3062 Loss 2.928456\n",
      "Epoch 3063 Loss 2.928452\n",
      "Epoch 3064 Loss 2.928450\n",
      "Epoch 3065 Loss 2.928447\n",
      "Epoch 3066 Loss 2.928443\n",
      "Epoch 3067 Loss 2.928443\n",
      "Epoch 3068 Loss 2.928440\n",
      "Epoch 3069 Loss 2.928435\n",
      "Epoch 3070 Loss 2.928436\n",
      "Epoch 3071 Loss 2.928430\n",
      "Epoch 3072 Loss 2.928428\n",
      "Epoch 3073 Loss 2.928426\n",
      "Epoch 3074 Loss 2.928423\n",
      "Epoch 3075 Loss 2.928421\n",
      "Epoch 3076 Loss 2.928417\n",
      "Epoch 3077 Loss 2.928415\n",
      "Epoch 3078 Loss 2.928411\n",
      "Epoch 3079 Loss 2.928410\n",
      "Epoch 3080 Loss 2.928407\n",
      "Epoch 3081 Loss 2.928404\n",
      "Epoch 3082 Loss 2.928402\n",
      "Epoch 3083 Loss 2.928399\n",
      "Epoch 3084 Loss 2.928396\n",
      "Epoch 3085 Loss 2.928396\n",
      "Epoch 3086 Loss 2.928392\n",
      "Epoch 3087 Loss 2.928389\n",
      "Epoch 3088 Loss 2.928386\n",
      "Epoch 3089 Loss 2.928383\n",
      "Epoch 3090 Loss 2.928383\n",
      "Epoch 3091 Loss 2.928379\n",
      "Epoch 3092 Loss 2.928378\n",
      "Epoch 3093 Loss 2.928375\n",
      "Epoch 3094 Loss 2.928372\n",
      "Epoch 3095 Loss 2.928370\n",
      "Epoch 3096 Loss 2.928368\n",
      "Epoch 3097 Loss 2.928364\n",
      "Epoch 3098 Loss 2.928362\n",
      "Epoch 3099 Loss 2.928361\n",
      "Epoch 3100 Loss 2.928357\n",
      "Epoch 3101 Loss 2.928355\n",
      "Epoch 3102 Loss 2.928353\n",
      "Epoch 3103 Loss 2.928349\n",
      "Epoch 3104 Loss 2.928348\n",
      "Epoch 3105 Loss 2.928345\n",
      "Epoch 3106 Loss 2.928343\n",
      "Epoch 3107 Loss 2.928340\n",
      "Epoch 3108 Loss 2.928339\n",
      "Epoch 3109 Loss 2.928337\n",
      "Epoch 3110 Loss 2.928333\n",
      "Epoch 3111 Loss 2.928332\n",
      "Epoch 3112 Loss 2.928328\n",
      "Epoch 3113 Loss 2.928329\n",
      "Epoch 3114 Loss 2.928324\n",
      "Epoch 3115 Loss 2.928323\n",
      "Epoch 3116 Loss 2.928320\n",
      "Epoch 3117 Loss 2.928318\n",
      "Epoch 3118 Loss 2.928315\n",
      "Epoch 3119 Loss 2.928313\n",
      "Epoch 3120 Loss 2.928311\n",
      "Epoch 3121 Loss 2.928308\n",
      "Epoch 3122 Loss 2.928306\n",
      "Epoch 3123 Loss 2.928304\n",
      "Epoch 3124 Loss 2.928303\n",
      "Epoch 3125 Loss 2.928300\n",
      "Epoch 3126 Loss 2.928296\n",
      "Epoch 3127 Loss 2.928295\n",
      "Epoch 3128 Loss 2.928292\n",
      "Epoch 3129 Loss 2.928292\n",
      "Epoch 3130 Loss 2.928288\n",
      "Epoch 3131 Loss 2.928287\n",
      "Epoch 3132 Loss 2.928285\n",
      "Epoch 3133 Loss 2.928282\n",
      "Epoch 3134 Loss 2.928279\n",
      "Epoch 3135 Loss 2.928276\n",
      "Epoch 3136 Loss 2.928275\n",
      "Epoch 3137 Loss 2.928273\n",
      "Epoch 3138 Loss 2.928272\n",
      "Epoch 3139 Loss 2.928268\n",
      "Epoch 3140 Loss 2.928267\n",
      "Epoch 3141 Loss 2.928265\n",
      "Epoch 3142 Loss 2.928263\n",
      "Epoch 3143 Loss 2.928260\n",
      "Epoch 3144 Loss 2.928259\n",
      "Epoch 3145 Loss 2.928256\n",
      "Epoch 3146 Loss 2.928255\n",
      "Epoch 3147 Loss 2.928252\n",
      "Epoch 3148 Loss 2.928251\n",
      "Epoch 3149 Loss 2.928248\n",
      "Epoch 3150 Loss 2.928246\n",
      "Epoch 3151 Loss 2.928245\n",
      "Epoch 3152 Loss 2.928242\n",
      "Epoch 3153 Loss 2.928240\n",
      "Epoch 3154 Loss 2.928236\n",
      "Epoch 3155 Loss 2.928236\n",
      "Epoch 3156 Loss 2.928233\n",
      "Epoch 3157 Loss 2.928231\n",
      "Epoch 3158 Loss 2.928230\n",
      "Epoch 3159 Loss 2.928227\n",
      "Epoch 3160 Loss 2.928226\n",
      "Epoch 3161 Loss 2.928225\n",
      "Epoch 3162 Loss 2.928223\n",
      "Epoch 3163 Loss 2.928219\n",
      "Epoch 3164 Loss 2.928218\n",
      "Epoch 3165 Loss 2.928216\n",
      "Epoch 3166 Loss 2.928215\n",
      "Epoch 3167 Loss 2.928212\n",
      "Epoch 3168 Loss 2.928211\n",
      "Epoch 3169 Loss 2.928210\n",
      "Epoch 3170 Loss 2.928206\n",
      "Epoch 3171 Loss 2.928205\n",
      "Epoch 3172 Loss 2.928204\n",
      "Epoch 3173 Loss 2.928202\n",
      "Epoch 3174 Loss 2.928200\n",
      "Epoch 3175 Loss 2.928196\n",
      "Epoch 3176 Loss 2.928195\n",
      "Epoch 3177 Loss 2.928195\n",
      "Epoch 3178 Loss 2.928192\n",
      "Epoch 3179 Loss 2.928190\n",
      "Epoch 3180 Loss 2.928188\n",
      "Epoch 3181 Loss 2.928186\n",
      "Epoch 3182 Loss 2.928185\n",
      "Epoch 3183 Loss 2.928184\n",
      "Epoch 3184 Loss 2.928182\n",
      "Epoch 3185 Loss 2.928180\n",
      "Epoch 3186 Loss 2.928178\n",
      "Epoch 3187 Loss 2.928175\n",
      "Epoch 3188 Loss 2.928172\n",
      "Epoch 3189 Loss 2.928170\n",
      "Epoch 3190 Loss 2.928170\n",
      "Epoch 3191 Loss 2.928169\n",
      "Epoch 3192 Loss 2.928167\n",
      "Epoch 3193 Loss 2.928164\n",
      "Epoch 3194 Loss 2.928164\n",
      "Epoch 3195 Loss 2.928162\n",
      "Epoch 3196 Loss 2.928160\n",
      "Epoch 3197 Loss 2.928158\n",
      "Epoch 3198 Loss 2.928158\n",
      "Epoch 3199 Loss 2.928154\n",
      "Epoch 3200 Loss 2.928152\n",
      "Epoch 3201 Loss 2.928149\n",
      "Epoch 3202 Loss 2.928150\n",
      "Epoch 3203 Loss 2.928147\n",
      "Epoch 3204 Loss 2.928146\n",
      "Epoch 3205 Loss 2.928144\n",
      "Epoch 3206 Loss 2.928142\n",
      "Epoch 3207 Loss 2.928140\n",
      "Epoch 3208 Loss 2.928139\n",
      "Epoch 3209 Loss 2.928137\n",
      "Epoch 3210 Loss 2.928135\n",
      "Epoch 3211 Loss 2.928135\n",
      "Epoch 3212 Loss 2.928133\n",
      "Epoch 3213 Loss 2.928131\n",
      "Epoch 3214 Loss 2.928130\n",
      "Epoch 3215 Loss 2.928125\n",
      "Epoch 3216 Loss 2.928125\n",
      "Epoch 3217 Loss 2.928124\n",
      "Epoch 3218 Loss 2.928121\n",
      "Epoch 3219 Loss 2.928121\n",
      "Epoch 3220 Loss 2.928120\n",
      "Epoch 3221 Loss 2.928118\n",
      "Epoch 3222 Loss 2.928117\n",
      "Epoch 3223 Loss 2.928115\n",
      "Epoch 3224 Loss 2.928113\n",
      "Epoch 3225 Loss 2.928110\n",
      "Epoch 3226 Loss 2.928109\n",
      "Epoch 3227 Loss 2.928108\n",
      "Epoch 3228 Loss 2.928104\n",
      "Epoch 3229 Loss 2.928105\n",
      "Epoch 3230 Loss 2.928104\n",
      "Epoch 3231 Loss 2.928102\n",
      "Epoch 3232 Loss 2.928101\n",
      "Epoch 3233 Loss 2.928098\n",
      "Epoch 3234 Loss 2.928097\n",
      "Epoch 3235 Loss 2.928095\n",
      "Epoch 3236 Loss 2.928094\n",
      "Epoch 3237 Loss 2.928093\n",
      "Epoch 3238 Loss 2.928091\n",
      "Epoch 3239 Loss 2.928090\n",
      "Epoch 3240 Loss 2.928088\n",
      "Epoch 3241 Loss 2.928086\n",
      "Epoch 3242 Loss 2.928085\n",
      "Epoch 3243 Loss 2.928084\n",
      "Epoch 3244 Loss 2.928082\n",
      "Epoch 3245 Loss 2.928080\n",
      "Epoch 3246 Loss 2.928079\n",
      "Epoch 3247 Loss 2.928076\n",
      "Epoch 3248 Loss 2.928076\n",
      "Epoch 3249 Loss 2.928075\n",
      "Epoch 3250 Loss 2.928072\n",
      "Epoch 3251 Loss 2.928072\n",
      "Epoch 3252 Loss 2.928071\n",
      "Epoch 3253 Loss 2.928068\n",
      "Epoch 3254 Loss 2.928068\n",
      "Epoch 3255 Loss 2.928066\n",
      "Epoch 3256 Loss 2.928065\n",
      "Epoch 3257 Loss 2.928063\n",
      "Epoch 3258 Loss 2.928061\n",
      "Epoch 3259 Loss 2.928061\n",
      "Epoch 3260 Loss 2.928057\n",
      "Epoch 3261 Loss 2.928058\n",
      "Epoch 3262 Loss 2.928056\n",
      "Epoch 3263 Loss 2.928055\n",
      "Epoch 3264 Loss 2.928052\n",
      "Epoch 3265 Loss 2.928053\n",
      "Epoch 3266 Loss 2.928051\n",
      "Epoch 3267 Loss 2.928050\n",
      "Epoch 3268 Loss 2.928047\n",
      "Epoch 3269 Loss 2.928046\n",
      "Epoch 3270 Loss 2.928046\n",
      "Epoch 3271 Loss 2.928044\n",
      "Epoch 3272 Loss 2.928042\n",
      "Epoch 3273 Loss 2.928040\n",
      "Epoch 3274 Loss 2.928040\n",
      "Epoch 3275 Loss 2.928037\n",
      "Epoch 3276 Loss 2.928036\n",
      "Epoch 3277 Loss 2.928037\n",
      "Epoch 3278 Loss 2.928034\n",
      "Epoch 3279 Loss 2.928034\n",
      "Epoch 3280 Loss 2.928031\n",
      "Epoch 3281 Loss 2.928032\n",
      "Epoch 3282 Loss 2.928029\n",
      "Epoch 3283 Loss 2.928027\n",
      "Epoch 3284 Loss 2.928026\n",
      "Epoch 3285 Loss 2.928025\n",
      "Epoch 3286 Loss 2.928024\n",
      "Epoch 3287 Loss 2.928023\n",
      "Epoch 3288 Loss 2.928022\n",
      "Epoch 3289 Loss 2.928021\n",
      "Epoch 3290 Loss 2.928019\n",
      "Epoch 3291 Loss 2.928018\n",
      "Epoch 3292 Loss 2.928017\n",
      "Epoch 3293 Loss 2.928015\n",
      "Epoch 3294 Loss 2.928013\n",
      "Epoch 3295 Loss 2.928012\n",
      "Epoch 3296 Loss 2.928011\n",
      "Epoch 3297 Loss 2.928009\n",
      "Epoch 3298 Loss 2.928009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3299 Loss 2.928006\n",
      "Epoch 3300 Loss 2.928007\n",
      "Epoch 3301 Loss 2.928007\n",
      "Epoch 3302 Loss 2.928004\n",
      "Epoch 3303 Loss 2.928002\n",
      "Epoch 3304 Loss 2.928001\n",
      "Epoch 3305 Loss 2.928000\n",
      "Epoch 3306 Loss 2.928000\n",
      "Epoch 3307 Loss 2.927998\n",
      "Epoch 3308 Loss 2.927995\n",
      "Epoch 3309 Loss 2.927995\n",
      "Epoch 3310 Loss 2.927994\n",
      "Epoch 3311 Loss 2.927994\n",
      "Epoch 3312 Loss 2.927992\n",
      "Epoch 3313 Loss 2.927992\n",
      "Epoch 3314 Loss 2.927990\n",
      "Epoch 3315 Loss 2.927989\n",
      "Epoch 3316 Loss 2.927987\n",
      "Epoch 3317 Loss 2.927986\n",
      "Epoch 3318 Loss 2.927985\n",
      "Epoch 3319 Loss 2.927983\n",
      "Epoch 3320 Loss 2.927983\n",
      "Epoch 3321 Loss 2.927981\n",
      "Epoch 3322 Loss 2.927980\n",
      "Epoch 3323 Loss 2.927979\n",
      "Epoch 3324 Loss 2.927978\n",
      "Epoch 3325 Loss 2.927977\n",
      "Epoch 3326 Loss 2.927975\n",
      "Epoch 3327 Loss 2.927973\n",
      "Epoch 3328 Loss 2.927973\n",
      "Epoch 3329 Loss 2.927974\n",
      "Epoch 3330 Loss 2.927971\n",
      "Epoch 3331 Loss 2.927972\n",
      "Epoch 3332 Loss 2.927969\n",
      "Epoch 3333 Loss 2.927969\n",
      "Epoch 3334 Loss 2.927967\n",
      "Epoch 3335 Loss 2.927967\n",
      "Epoch 3336 Loss 2.927963\n",
      "Epoch 3337 Loss 2.927963\n",
      "Epoch 3338 Loss 2.927962\n",
      "Epoch 3339 Loss 2.927962\n",
      "Epoch 3340 Loss 2.927961\n",
      "Epoch 3341 Loss 2.927960\n",
      "Epoch 3342 Loss 2.927959\n",
      "Epoch 3343 Loss 2.927958\n",
      "Epoch 3344 Loss 2.927956\n",
      "Epoch 3345 Loss 2.927955\n",
      "Epoch 3346 Loss 2.927954\n",
      "Epoch 3347 Loss 2.927953\n",
      "Epoch 3348 Loss 2.927953\n",
      "Epoch 3349 Loss 2.927950\n",
      "Epoch 3350 Loss 2.927950\n",
      "Epoch 3351 Loss 2.927948\n",
      "Epoch 3352 Loss 2.927947\n",
      "Epoch 3353 Loss 2.927948\n",
      "Epoch 3354 Loss 2.927945\n",
      "Epoch 3355 Loss 2.927944\n",
      "Epoch 3356 Loss 2.927944\n",
      "Epoch 3357 Loss 2.927944\n",
      "Epoch 3358 Loss 2.927942\n",
      "Epoch 3359 Loss 2.927941\n",
      "Epoch 3360 Loss 2.927940\n",
      "Epoch 3361 Loss 2.927938\n",
      "Epoch 3362 Loss 2.927938\n",
      "Epoch 3363 Loss 2.927936\n",
      "Epoch 3364 Loss 2.927936\n",
      "Epoch 3365 Loss 2.927937\n",
      "Epoch 3366 Loss 2.927934\n",
      "Epoch 3367 Loss 2.927934\n",
      "Epoch 3368 Loss 2.927933\n",
      "Epoch 3369 Loss 2.927930\n",
      "Epoch 3370 Loss 2.927929\n",
      "Epoch 3371 Loss 2.927931\n",
      "Epoch 3372 Loss 2.927929\n",
      "Epoch 3373 Loss 2.927927\n",
      "Epoch 3374 Loss 2.927926\n",
      "Epoch 3375 Loss 2.927925\n",
      "Epoch 3376 Loss 2.927924\n",
      "Epoch 3377 Loss 2.927922\n",
      "Epoch 3378 Loss 2.927924\n",
      "Epoch 3379 Loss 2.927922\n",
      "Epoch 3380 Loss 2.927921\n",
      "Epoch 3381 Loss 2.927920\n",
      "Epoch 3382 Loss 2.927918\n",
      "Epoch 3383 Loss 2.927917\n",
      "Epoch 3384 Loss 2.927917\n",
      "Epoch 3385 Loss 2.927915\n",
      "Epoch 3386 Loss 2.927916\n",
      "Epoch 3387 Loss 2.927914\n",
      "Epoch 3388 Loss 2.927914\n",
      "Epoch 3389 Loss 2.927912\n",
      "Epoch 3390 Loss 2.927913\n",
      "Epoch 3391 Loss 2.927911\n",
      "Epoch 3392 Loss 2.927910\n",
      "Epoch 3393 Loss 2.927909\n",
      "Epoch 3394 Loss 2.927908\n",
      "Epoch 3395 Loss 2.927907\n",
      "Epoch 3396 Loss 2.927906\n",
      "Epoch 3397 Loss 2.927905\n",
      "Epoch 3398 Loss 2.927905\n",
      "Epoch 3399 Loss 2.927904\n",
      "Epoch 3400 Loss 2.927902\n",
      "Epoch 3401 Loss 2.927902\n",
      "Epoch 3402 Loss 2.927902\n",
      "Epoch 3403 Loss 2.927899\n",
      "Epoch 3404 Loss 2.927899\n",
      "Epoch 3405 Loss 2.927898\n",
      "Epoch 3406 Loss 2.927899\n",
      "Epoch 3407 Loss 2.927896\n",
      "Epoch 3408 Loss 2.927895\n",
      "Epoch 3409 Loss 2.927896\n",
      "Epoch 3410 Loss 2.927894\n",
      "Epoch 3411 Loss 2.927892\n",
      "Epoch 3412 Loss 2.927893\n",
      "Epoch 3413 Loss 2.927891\n",
      "Epoch 3414 Loss 2.927891\n",
      "Epoch 3415 Loss 2.927890\n",
      "Epoch 3416 Loss 2.927891\n",
      "Epoch 3417 Loss 2.927888\n",
      "Epoch 3418 Loss 2.927888\n",
      "Epoch 3419 Loss 2.927886\n",
      "Epoch 3420 Loss 2.927887\n",
      "Epoch 3421 Loss 2.927885\n",
      "Epoch 3422 Loss 2.927884\n",
      "Epoch 3423 Loss 2.927883\n",
      "Epoch 3424 Loss 2.927881\n",
      "Epoch 3425 Loss 2.927881\n",
      "Epoch 3426 Loss 2.927880\n",
      "Epoch 3427 Loss 2.927880\n",
      "Epoch 3428 Loss 2.927879\n",
      "Epoch 3429 Loss 2.927878\n",
      "Epoch 3430 Loss 2.927877\n",
      "Epoch 3431 Loss 2.927876\n",
      "Epoch 3432 Loss 2.927876\n",
      "Epoch 3433 Loss 2.927875\n",
      "Epoch 3434 Loss 2.927875\n",
      "Epoch 3435 Loss 2.927875\n",
      "Epoch 3436 Loss 2.927873\n",
      "Epoch 3437 Loss 2.927872\n",
      "Epoch 3438 Loss 2.927870\n",
      "Epoch 3439 Loss 2.927871\n",
      "Epoch 3440 Loss 2.927871\n",
      "Epoch 3441 Loss 2.927869\n",
      "Epoch 3442 Loss 2.927869\n",
      "Epoch 3443 Loss 2.927866\n",
      "Epoch 3444 Loss 2.927865\n",
      "Epoch 3445 Loss 2.927866\n",
      "Epoch 3446 Loss 2.927866\n",
      "Epoch 3447 Loss 2.927864\n",
      "Epoch 3448 Loss 2.927863\n",
      "Epoch 3449 Loss 2.927863\n",
      "Epoch 3450 Loss 2.927862\n",
      "Epoch 3451 Loss 2.927863\n",
      "Epoch 3452 Loss 2.927860\n",
      "Epoch 3453 Loss 2.927860\n",
      "Epoch 3454 Loss 2.927860\n",
      "Epoch 3455 Loss 2.927859\n",
      "Epoch 3456 Loss 2.927858\n",
      "Epoch 3457 Loss 2.927858\n",
      "Epoch 3458 Loss 2.927855\n",
      "Epoch 3459 Loss 2.927857\n",
      "Epoch 3460 Loss 2.927854\n",
      "Epoch 3461 Loss 2.927855\n",
      "Epoch 3462 Loss 2.927854\n",
      "Epoch 3463 Loss 2.927854\n",
      "Epoch 3464 Loss 2.927851\n",
      "Epoch 3465 Loss 2.927853\n",
      "Epoch 3466 Loss 2.927852\n",
      "Epoch 3467 Loss 2.927850\n",
      "Epoch 3468 Loss 2.927849\n",
      "Epoch 3469 Loss 2.927849\n",
      "Epoch 3470 Loss 2.927848\n",
      "Epoch 3471 Loss 2.927848\n",
      "Epoch 3472 Loss 2.927846\n",
      "Epoch 3473 Loss 2.927846\n",
      "Epoch 3474 Loss 2.927845\n",
      "Epoch 3475 Loss 2.927844\n",
      "Epoch 3476 Loss 2.927844\n",
      "Epoch 3477 Loss 2.927844\n",
      "Epoch 3478 Loss 2.927843\n",
      "Epoch 3479 Loss 2.927842\n",
      "Epoch 3480 Loss 2.927842\n",
      "Epoch 3481 Loss 2.927840\n",
      "Epoch 3482 Loss 2.927841\n",
      "Epoch 3483 Loss 2.927839\n",
      "Epoch 3484 Loss 2.927838\n",
      "Epoch 3485 Loss 2.927839\n",
      "Epoch 3486 Loss 2.927839\n",
      "Epoch 3487 Loss 2.927837\n",
      "Epoch 3488 Loss 2.927835\n",
      "Epoch 3489 Loss 2.927837\n",
      "Epoch 3490 Loss 2.927835\n",
      "Epoch 3491 Loss 2.927834\n",
      "Epoch 3492 Loss 2.927833\n",
      "Epoch 3493 Loss 2.927833\n",
      "Epoch 3494 Loss 2.927833\n",
      "Epoch 3495 Loss 2.927832\n",
      "Epoch 3496 Loss 2.927831\n",
      "Epoch 3497 Loss 2.927830\n",
      "Epoch 3498 Loss 2.927830\n",
      "Epoch 3499 Loss 2.927830\n",
      "Epoch 3500 Loss 2.927830\n",
      "Epoch 3501 Loss 2.927828\n",
      "Epoch 3502 Loss 2.927828\n",
      "Epoch 3503 Loss 2.927827\n",
      "Epoch 3504 Loss 2.927825\n",
      "Epoch 3505 Loss 2.927827\n",
      "Epoch 3506 Loss 2.927825\n",
      "Epoch 3507 Loss 2.927824\n",
      "Epoch 3508 Loss 2.927824\n",
      "Epoch 3509 Loss 2.927824\n",
      "Epoch 3510 Loss 2.927822\n",
      "Epoch 3511 Loss 2.927822\n",
      "Epoch 3512 Loss 2.927821\n",
      "Epoch 3513 Loss 2.927820\n",
      "Epoch 3514 Loss 2.927819\n",
      "Epoch 3515 Loss 2.927821\n",
      "Epoch 3516 Loss 2.927819\n",
      "Epoch 3517 Loss 2.927819\n",
      "Epoch 3518 Loss 2.927818\n",
      "Epoch 3519 Loss 2.927818\n",
      "Epoch 3520 Loss 2.927817\n",
      "Epoch 3521 Loss 2.927816\n",
      "Epoch 3522 Loss 2.927815\n",
      "Epoch 3523 Loss 2.927816\n",
      "Epoch 3524 Loss 2.927814\n",
      "Epoch 3525 Loss 2.927813\n",
      "Epoch 3526 Loss 2.927813\n",
      "Epoch 3527 Loss 2.927812\n",
      "Epoch 3528 Loss 2.927811\n",
      "Epoch 3529 Loss 2.927811\n",
      "Epoch 3530 Loss 2.927811\n",
      "Epoch 3531 Loss 2.927810\n",
      "Epoch 3532 Loss 2.927809\n",
      "Epoch 3533 Loss 2.927810\n",
      "Epoch 3534 Loss 2.927809\n",
      "Epoch 3535 Loss 2.927808\n",
      "Epoch 3536 Loss 2.927809\n",
      "Epoch 3537 Loss 2.927806\n",
      "Epoch 3538 Loss 2.927806\n",
      "Epoch 3539 Loss 2.927805\n",
      "Epoch 3540 Loss 2.927804\n",
      "Epoch 3541 Loss 2.927804\n",
      "Epoch 3542 Loss 2.927804\n",
      "Epoch 3543 Loss 2.927805\n",
      "Epoch 3544 Loss 2.927804\n",
      "Epoch 3545 Loss 2.927804\n",
      "Epoch 3546 Loss 2.927802\n",
      "Epoch 3547 Loss 2.927802\n",
      "Epoch 3548 Loss 2.927801\n",
      "Epoch 3549 Loss 2.927801\n",
      "Epoch 3550 Loss 2.927799\n",
      "Epoch 3551 Loss 2.927801\n",
      "Epoch 3552 Loss 2.927798\n",
      "Epoch 3553 Loss 2.927798\n",
      "Epoch 3554 Loss 2.927798\n",
      "Epoch 3555 Loss 2.927798\n",
      "Epoch 3556 Loss 2.927798\n",
      "Epoch 3557 Loss 2.927796\n",
      "Epoch 3558 Loss 2.927796\n",
      "Epoch 3559 Loss 2.927796\n",
      "Epoch 3560 Loss 2.927794\n",
      "Epoch 3561 Loss 2.927796\n",
      "Epoch 3562 Loss 2.927795\n",
      "Epoch 3563 Loss 2.927794\n",
      "Epoch 3564 Loss 2.927794\n",
      "Epoch 3565 Loss 2.927791\n",
      "Epoch 3566 Loss 2.927792\n",
      "Epoch 3567 Loss 2.927792\n",
      "Epoch 3568 Loss 2.927790\n",
      "Epoch 3569 Loss 2.927790\n",
      "Epoch 3570 Loss 2.927789\n",
      "Epoch 3571 Loss 2.927790\n",
      "Epoch 3572 Loss 2.927789\n",
      "Epoch 3573 Loss 2.927790\n",
      "Epoch 3574 Loss 2.927789\n",
      "Epoch 3575 Loss 2.927787\n",
      "Epoch 3576 Loss 2.927786\n",
      "Epoch 3577 Loss 2.927788\n",
      "Epoch 3578 Loss 2.927785\n",
      "Epoch 3579 Loss 2.927785\n",
      "Epoch 3580 Loss 2.927786\n",
      "Epoch 3581 Loss 2.927785\n",
      "Epoch 3582 Loss 2.927785\n",
      "Epoch 3583 Loss 2.927784\n",
      "Epoch 3584 Loss 2.927783\n",
      "Epoch 3585 Loss 2.927783\n",
      "Epoch 3586 Loss 2.927781\n",
      "Epoch 3587 Loss 2.927782\n",
      "Epoch 3588 Loss 2.927780\n",
      "Epoch 3589 Loss 2.927781\n",
      "Epoch 3590 Loss 2.927781\n",
      "Epoch 3591 Loss 2.927780\n",
      "Epoch 3592 Loss 2.927780\n",
      "Epoch 3593 Loss 2.927778\n",
      "Epoch 3594 Loss 2.927779\n",
      "Epoch 3595 Loss 2.927778\n",
      "Epoch 3596 Loss 2.927778\n",
      "Epoch 3597 Loss 2.927778\n",
      "Epoch 3598 Loss 2.927777\n",
      "Epoch 3599 Loss 2.927776\n",
      "Epoch 3600 Loss 2.927775\n",
      "Epoch 3601 Loss 2.927775\n",
      "Epoch 3602 Loss 2.927773\n",
      "Epoch 3603 Loss 2.927775\n",
      "Epoch 3604 Loss 2.927775\n",
      "Epoch 3605 Loss 2.927774\n",
      "Epoch 3606 Loss 2.927773\n",
      "Epoch 3607 Loss 2.927773\n",
      "Epoch 3608 Loss 2.927772\n",
      "Epoch 3609 Loss 2.927772\n",
      "Epoch 3610 Loss 2.927772\n",
      "Epoch 3611 Loss 2.927770\n",
      "Epoch 3612 Loss 2.927772\n",
      "Epoch 3613 Loss 2.927772\n",
      "Epoch 3614 Loss 2.927770\n",
      "Epoch 3615 Loss 2.927770\n",
      "Epoch 3616 Loss 2.927769\n",
      "Epoch 3617 Loss 2.927768\n",
      "Epoch 3618 Loss 2.927769\n",
      "Epoch 3619 Loss 2.927768\n",
      "Epoch 3620 Loss 2.927766\n",
      "Epoch 3621 Loss 2.927767\n",
      "Epoch 3622 Loss 2.927767\n",
      "Epoch 3623 Loss 2.927765\n",
      "Epoch 3624 Loss 2.927766\n",
      "Epoch 3625 Loss 2.927765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3626 Loss 2.927766\n",
      "Epoch 3627 Loss 2.927764\n",
      "Epoch 3628 Loss 2.927764\n",
      "Epoch 3629 Loss 2.927764\n",
      "Epoch 3630 Loss 2.927762\n",
      "Epoch 3631 Loss 2.927763\n",
      "Epoch 3632 Loss 2.927763\n",
      "Epoch 3633 Loss 2.927762\n",
      "Epoch 3634 Loss 2.927761\n",
      "Epoch 3635 Loss 2.927762\n",
      "Epoch 3636 Loss 2.927759\n",
      "Epoch 3637 Loss 2.927761\n",
      "Epoch 3638 Loss 2.927761\n",
      "Epoch 3639 Loss 2.927760\n",
      "Epoch 3640 Loss 2.927759\n",
      "Epoch 3641 Loss 2.927758\n",
      "Epoch 3642 Loss 2.927759\n",
      "Epoch 3643 Loss 2.927757\n",
      "Epoch 3644 Loss 2.927758\n",
      "Epoch 3645 Loss 2.927757\n",
      "Epoch 3646 Loss 2.927757\n",
      "Epoch 3647 Loss 2.927757\n",
      "Epoch 3648 Loss 2.927756\n",
      "Epoch 3649 Loss 2.927758\n",
      "Epoch 3650 Loss 2.927756\n",
      "Epoch 3651 Loss 2.927756\n",
      "Epoch 3652 Loss 2.927755\n",
      "Epoch 3653 Loss 2.927755\n",
      "Epoch 3654 Loss 2.927754\n",
      "Epoch 3655 Loss 2.927754\n",
      "Epoch 3656 Loss 2.927755\n",
      "Epoch 3657 Loss 2.927753\n",
      "Epoch 3658 Loss 2.927752\n",
      "Epoch 3659 Loss 2.927754\n",
      "Epoch 3660 Loss 2.927752\n",
      "Epoch 3661 Loss 2.927751\n",
      "Epoch 3662 Loss 2.927752\n",
      "Epoch 3663 Loss 2.927750\n",
      "Epoch 3664 Loss 2.927750\n",
      "Epoch 3665 Loss 2.927752\n",
      "Epoch 3666 Loss 2.927750\n",
      "Epoch 3667 Loss 2.927750\n",
      "Epoch 3668 Loss 2.927747\n",
      "Epoch 3669 Loss 2.927749\n",
      "Epoch 3670 Loss 2.927748\n",
      "Epoch 3671 Loss 2.927748\n",
      "Epoch 3672 Loss 2.927749\n",
      "Epoch 3673 Loss 2.927747\n",
      "Epoch 3674 Loss 2.927747\n",
      "Epoch 3675 Loss 2.927748\n",
      "Epoch 3676 Loss 2.927747\n",
      "Epoch 3677 Loss 2.927746\n",
      "Epoch 3678 Loss 2.927746\n",
      "Epoch 3679 Loss 2.927745\n",
      "Epoch 3680 Loss 2.927745\n",
      "Epoch 3681 Loss 2.927744\n",
      "Epoch 3682 Loss 2.927744\n",
      "Epoch 3683 Loss 2.927743\n",
      "Epoch 3684 Loss 2.927742\n",
      "Epoch 3685 Loss 2.927743\n",
      "Epoch 3686 Loss 2.927743\n",
      "Epoch 3687 Loss 2.927744\n",
      "Epoch 3688 Loss 2.927743\n",
      "Epoch 3689 Loss 2.927742\n",
      "Epoch 3690 Loss 2.927742\n",
      "Epoch 3691 Loss 2.927742\n",
      "Epoch 3692 Loss 2.927741\n",
      "Epoch 3693 Loss 2.927741\n",
      "Epoch 3694 Loss 2.927741\n",
      "Epoch 3695 Loss 2.927742\n",
      "Epoch 3696 Loss 2.927741\n",
      "Epoch 3697 Loss 2.927740\n",
      "Epoch 3698 Loss 2.927740\n",
      "Epoch 3699 Loss 2.927738\n",
      "Epoch 3700 Loss 2.927738\n",
      "Epoch 3701 Loss 2.927738\n",
      "Epoch 3702 Loss 2.927737\n",
      "Epoch 3703 Loss 2.927737\n",
      "Epoch 3704 Loss 2.927738\n",
      "Epoch 3705 Loss 2.927738\n",
      "Epoch 3706 Loss 2.927737\n",
      "Epoch 3707 Loss 2.927737\n",
      "Epoch 3708 Loss 2.927736\n",
      "Epoch 3709 Loss 2.927735\n",
      "Epoch 3710 Loss 2.927734\n",
      "Epoch 3711 Loss 2.927735\n",
      "Epoch 3712 Loss 2.927736\n",
      "Epoch 3713 Loss 2.927734\n",
      "Epoch 3714 Loss 2.927734\n",
      "Epoch 3715 Loss 2.927733\n",
      "Epoch 3716 Loss 2.927734\n",
      "Epoch 3717 Loss 2.927733\n",
      "Epoch 3718 Loss 2.927733\n",
      "Epoch 3719 Loss 2.927733\n",
      "Epoch 3720 Loss 2.927733\n",
      "Epoch 3721 Loss 2.927731\n",
      "Epoch 3722 Loss 2.927731\n",
      "Epoch 3723 Loss 2.927732\n",
      "Epoch 3724 Loss 2.927730\n",
      "Epoch 3725 Loss 2.927730\n",
      "Epoch 3726 Loss 2.927731\n",
      "Epoch 3727 Loss 2.927730\n",
      "Epoch 3728 Loss 2.927732\n",
      "Epoch 3729 Loss 2.927732\n",
      "Epoch 3730 Loss 2.927730\n",
      "Epoch 3731 Loss 2.927728\n",
      "Epoch 3732 Loss 2.927729\n",
      "Epoch 3733 Loss 2.927730\n",
      "Epoch 3734 Loss 2.927729\n",
      "Epoch 3735 Loss 2.927728\n",
      "Epoch 3736 Loss 2.927728\n",
      "Epoch 3737 Loss 2.927728\n",
      "Epoch 3738 Loss 2.927727\n",
      "Epoch 3739 Loss 2.927728\n",
      "Epoch 3740 Loss 2.927728\n",
      "Epoch 3741 Loss 2.927727\n",
      "Epoch 3742 Loss 2.927727\n",
      "Epoch 3743 Loss 2.927726\n",
      "Epoch 3744 Loss 2.927726\n",
      "Epoch 3745 Loss 2.927725\n",
      "Epoch 3746 Loss 2.927725\n",
      "Epoch 3747 Loss 2.927725\n",
      "Epoch 3748 Loss 2.927724\n",
      "Epoch 3749 Loss 2.927724\n",
      "Epoch 3750 Loss 2.927724\n",
      "Epoch 3751 Loss 2.927725\n",
      "Epoch 3752 Loss 2.927724\n",
      "Epoch 3753 Loss 2.927724\n",
      "Epoch 3754 Loss 2.927723\n",
      "Epoch 3755 Loss 2.927723\n",
      "Epoch 3756 Loss 2.927722\n",
      "Epoch 3757 Loss 2.927722\n",
      "Epoch 3758 Loss 2.927723\n",
      "Epoch 3759 Loss 2.927722\n",
      "Epoch 3760 Loss 2.927723\n",
      "Epoch 3761 Loss 2.927721\n",
      "Epoch 3762 Loss 2.927721\n",
      "Epoch 3763 Loss 2.927720\n",
      "Epoch 3764 Loss 2.927720\n",
      "Epoch 3765 Loss 2.927719\n",
      "Epoch 3766 Loss 2.927721\n",
      "Epoch 3767 Loss 2.927719\n",
      "Epoch 3768 Loss 2.927719\n",
      "Epoch 3769 Loss 2.927719\n",
      "Epoch 3770 Loss 2.927719\n",
      "Epoch 3771 Loss 2.927718\n",
      "Epoch 3772 Loss 2.927720\n",
      "Epoch 3773 Loss 2.927718\n",
      "Epoch 3774 Loss 2.927718\n",
      "Epoch 3775 Loss 2.927717\n",
      "Epoch 3776 Loss 2.927718\n",
      "Epoch 3777 Loss 2.927717\n",
      "Epoch 3778 Loss 2.927717\n",
      "Epoch 3779 Loss 2.927716\n",
      "Epoch 3780 Loss 2.927716\n",
      "Epoch 3781 Loss 2.927717\n",
      "Epoch 3782 Loss 2.927717\n",
      "Epoch 3783 Loss 2.927716\n",
      "Epoch 3784 Loss 2.927715\n",
      "Epoch 3785 Loss 2.927715\n",
      "Epoch 3786 Loss 2.927715\n",
      "Epoch 3787 Loss 2.927715\n",
      "Epoch 3788 Loss 2.927715\n",
      "Epoch 3789 Loss 2.927715\n",
      "Epoch 3790 Loss 2.927714\n",
      "Epoch 3791 Loss 2.927714\n",
      "Epoch 3792 Loss 2.927714\n",
      "Epoch 3793 Loss 2.927713\n",
      "Epoch 3794 Loss 2.927713\n",
      "Epoch 3795 Loss 2.927714\n",
      "Epoch 3796 Loss 2.927713\n",
      "Epoch 3797 Loss 2.927712\n",
      "Epoch 3798 Loss 2.927712\n",
      "Epoch 3799 Loss 2.927712\n",
      "Epoch 3800 Loss 2.927711\n",
      "Epoch 3801 Loss 2.927711\n",
      "Epoch 3802 Loss 2.927713\n",
      "Epoch 3803 Loss 2.927711\n",
      "Epoch 3804 Loss 2.927712\n",
      "Epoch 3805 Loss 2.927711\n",
      "Epoch 3806 Loss 2.927711\n",
      "Epoch 3807 Loss 2.927711\n",
      "Epoch 3808 Loss 2.927709\n",
      "Epoch 3809 Loss 2.927711\n",
      "Epoch 3810 Loss 2.927710\n",
      "Epoch 3811 Loss 2.927708\n",
      "Epoch 3812 Loss 2.927708\n",
      "Epoch 3813 Loss 2.927709\n",
      "Epoch 3814 Loss 2.927709\n",
      "Epoch 3815 Loss 2.927710\n",
      "Epoch 3816 Loss 2.927708\n",
      "Epoch 3817 Loss 2.927708\n",
      "Epoch 3818 Loss 2.927706\n",
      "Epoch 3819 Loss 2.927707\n",
      "Epoch 3820 Loss 2.927708\n",
      "Epoch 3821 Loss 2.927707\n",
      "Epoch 3822 Loss 2.927707\n",
      "Epoch 3823 Loss 2.927707\n",
      "Epoch 3824 Loss 2.927708\n",
      "Epoch 3825 Loss 2.927708\n",
      "Epoch 3826 Loss 2.927706\n",
      "Epoch 3827 Loss 2.927707\n",
      "Epoch 3828 Loss 2.927706\n",
      "Epoch 3829 Loss 2.927706\n",
      "Epoch 3830 Loss 2.927706\n",
      "Epoch 3831 Loss 2.927705\n",
      "Epoch 3832 Loss 2.927705\n",
      "Epoch 3833 Loss 2.927705\n",
      "Epoch 3834 Loss 2.927705\n",
      "Epoch 3835 Loss 2.927705\n",
      "Epoch 3836 Loss 2.927704\n",
      "Epoch 3837 Loss 2.927703\n",
      "Epoch 3838 Loss 2.927704\n",
      "Epoch 3839 Loss 2.927704\n",
      "Epoch 3840 Loss 2.927703\n",
      "Epoch 3841 Loss 2.927702\n",
      "Epoch 3842 Loss 2.927703\n",
      "Epoch 3843 Loss 2.927702\n",
      "Epoch 3844 Loss 2.927704\n",
      "Epoch 3845 Loss 2.927702\n",
      "Epoch 3846 Loss 2.927701\n",
      "Epoch 3847 Loss 2.927703\n",
      "Epoch 3848 Loss 2.927702\n",
      "Epoch 3849 Loss 2.927701\n",
      "Epoch 3850 Loss 2.927701\n",
      "Epoch 3851 Loss 2.927703\n",
      "Epoch 3852 Loss 2.927700\n",
      "Epoch 3853 Loss 2.927701\n",
      "Epoch 3854 Loss 2.927701\n",
      "Epoch 3855 Loss 2.927700\n",
      "Epoch 3856 Loss 2.927700\n",
      "Epoch 3857 Loss 2.927700\n",
      "Epoch 3858 Loss 2.927701\n",
      "Epoch 3859 Loss 2.927700\n",
      "Epoch 3860 Loss 2.927700\n",
      "Epoch 3861 Loss 2.927700\n",
      "Epoch 3862 Loss 2.927699\n",
      "Epoch 3863 Loss 2.927698\n",
      "Epoch 3864 Loss 2.927700\n",
      "Epoch 3865 Loss 2.927697\n",
      "Epoch 3866 Loss 2.927700\n",
      "Epoch 3867 Loss 2.927700\n",
      "Epoch 3868 Loss 2.927698\n",
      "Epoch 3869 Loss 2.927697\n",
      "Epoch 3870 Loss 2.927698\n",
      "Epoch 3871 Loss 2.927696\n",
      "Epoch 3872 Loss 2.927699\n",
      "Epoch 3873 Loss 2.927697\n",
      "Epoch 3874 Loss 2.927696\n",
      "Epoch 3875 Loss 2.927699\n",
      "Epoch 3876 Loss 2.927697\n",
      "Epoch 3877 Loss 2.927696\n",
      "Epoch 3878 Loss 2.927697\n",
      "Epoch 3879 Loss 2.927696\n",
      "Epoch 3880 Loss 2.927696\n",
      "Epoch 3881 Loss 2.927696\n",
      "Epoch 3882 Loss 2.927696\n",
      "Epoch 3883 Loss 2.927695\n",
      "Epoch 3884 Loss 2.927695\n",
      "Epoch 3885 Loss 2.927696\n",
      "Epoch 3886 Loss 2.927696\n",
      "Epoch 3887 Loss 2.927695\n",
      "Epoch 3888 Loss 2.927694\n",
      "Epoch 3889 Loss 2.927694\n",
      "Epoch 3890 Loss 2.927694\n",
      "Epoch 3891 Loss 2.927693\n",
      "Epoch 3892 Loss 2.927695\n",
      "Epoch 3893 Loss 2.927695\n",
      "Epoch 3894 Loss 2.927694\n",
      "Epoch 3895 Loss 2.927695\n",
      "Epoch 3896 Loss 2.927693\n",
      "Epoch 3897 Loss 2.927693\n",
      "Epoch 3898 Loss 2.927695\n",
      "Epoch 3899 Loss 2.927693\n",
      "Epoch 3900 Loss 2.927692\n",
      "Epoch 3901 Loss 2.927694\n",
      "Epoch 3902 Loss 2.927692\n",
      "Epoch 3903 Loss 2.927693\n",
      "Epoch 3904 Loss 2.927691\n",
      "Epoch 3905 Loss 2.927692\n",
      "Epoch 3906 Loss 2.927692\n",
      "Epoch 3907 Loss 2.927692\n",
      "Epoch 3908 Loss 2.927692\n",
      "Epoch 3909 Loss 2.927692\n",
      "Epoch 3910 Loss 2.927690\n",
      "Epoch 3911 Loss 2.927692\n",
      "Epoch 3912 Loss 2.927691\n",
      "Epoch 3913 Loss 2.927691\n",
      "Epoch 3914 Loss 2.927689\n",
      "Epoch 3915 Loss 2.927691\n",
      "Epoch 3916 Loss 2.927691\n",
      "Epoch 3917 Loss 2.927689\n",
      "Epoch 3918 Loss 2.927690\n",
      "Epoch 3919 Loss 2.927690\n",
      "Epoch 3920 Loss 2.927690\n",
      "Epoch 3921 Loss 2.927690\n",
      "Epoch 3922 Loss 2.927689\n",
      "Epoch 3923 Loss 2.927688\n",
      "Epoch 3924 Loss 2.927689\n",
      "Epoch 3925 Loss 2.927688\n",
      "Epoch 3926 Loss 2.927689\n",
      "Epoch 3927 Loss 2.927689\n",
      "Epoch 3928 Loss 2.927689\n",
      "Epoch 3929 Loss 2.927688\n",
      "Epoch 3930 Loss 2.927688\n",
      "Epoch 3931 Loss 2.927688\n",
      "Epoch 3932 Loss 2.927687\n",
      "Epoch 3933 Loss 2.927689\n",
      "Epoch 3934 Loss 2.927688\n",
      "Epoch 3935 Loss 2.927687\n",
      "Epoch 3936 Loss 2.927688\n",
      "Epoch 3937 Loss 2.927686\n",
      "Epoch 3938 Loss 2.927686\n",
      "Epoch 3939 Loss 2.927686\n",
      "Epoch 3940 Loss 2.927687\n",
      "Epoch 3941 Loss 2.927687\n",
      "Epoch 3942 Loss 2.927686\n",
      "Epoch 3943 Loss 2.927687\n",
      "Epoch 3944 Loss 2.927686\n",
      "Epoch 3945 Loss 2.927685\n",
      "Epoch 3946 Loss 2.927685\n",
      "Epoch 3947 Loss 2.927686\n",
      "Epoch 3948 Loss 2.927685\n",
      "Epoch 3949 Loss 2.927686\n",
      "Epoch 3950 Loss 2.927686\n",
      "Epoch 3951 Loss 2.927686\n",
      "Epoch 3952 Loss 2.927685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3953 Loss 2.927686\n",
      "Epoch 3954 Loss 2.927685\n",
      "Epoch 3955 Loss 2.927685\n",
      "Epoch 3956 Loss 2.927683\n",
      "Epoch 3957 Loss 2.927684\n",
      "Epoch 3958 Loss 2.927685\n",
      "Epoch 3959 Loss 2.927684\n",
      "Epoch 3960 Loss 2.927684\n",
      "Epoch 3961 Loss 2.927684\n",
      "Epoch 3962 Loss 2.927685\n",
      "Epoch 3963 Loss 2.927683\n",
      "Epoch 3964 Loss 2.927685\n",
      "Epoch 3965 Loss 2.927684\n",
      "Epoch 3966 Loss 2.927683\n",
      "Epoch 3967 Loss 2.927683\n",
      "Epoch 3968 Loss 2.927683\n",
      "Epoch 3969 Loss 2.927682\n",
      "Epoch 3970 Loss 2.927682\n",
      "Epoch 3971 Loss 2.927684\n",
      "Epoch 3972 Loss 2.927683\n",
      "Epoch 3973 Loss 2.927684\n",
      "Epoch 3974 Loss 2.927683\n",
      "Epoch 3975 Loss 2.927682\n",
      "Epoch 3976 Loss 2.927682\n",
      "Epoch 3977 Loss 2.927682\n",
      "Epoch 3978 Loss 2.927682\n",
      "Epoch 3979 Loss 2.927681\n",
      "Epoch 3980 Loss 2.927682\n",
      "Epoch 3981 Loss 2.927681\n",
      "Epoch 3982 Loss 2.927681\n",
      "Epoch 3983 Loss 2.927682\n",
      "Epoch 3984 Loss 2.927681\n",
      "Epoch 3985 Loss 2.927681\n",
      "Epoch 3986 Loss 2.927681\n",
      "Epoch 3987 Loss 2.927680\n",
      "Epoch 3988 Loss 2.927681\n",
      "Epoch 3989 Loss 2.927681\n",
      "Epoch 3990 Loss 2.927680\n",
      "Epoch 3991 Loss 2.927682\n",
      "Epoch 3992 Loss 2.927681\n",
      "Epoch 3993 Loss 2.927680\n",
      "Epoch 3994 Loss 2.927679\n",
      "Epoch 3995 Loss 2.927680\n",
      "Epoch 3996 Loss 2.927679\n",
      "Epoch 3997 Loss 2.927680\n",
      "Epoch 3998 Loss 2.927680\n",
      "Epoch 3999 Loss 2.927679\n",
      "Epoch 4000 Loss 2.927679\n",
      "Epoch 4001 Loss 2.927679\n",
      "Epoch 4002 Loss 2.927679\n",
      "Epoch 4003 Loss 2.927679\n",
      "Epoch 4004 Loss 2.927680\n",
      "Epoch 4005 Loss 2.927681\n",
      "Epoch 4006 Loss 2.927679\n",
      "Epoch 4007 Loss 2.927679\n",
      "Epoch 4008 Loss 2.927679\n",
      "Epoch 4009 Loss 2.927679\n",
      "Epoch 4010 Loss 2.927678\n",
      "Epoch 4011 Loss 2.927679\n",
      "Epoch 4012 Loss 2.927679\n",
      "Epoch 4013 Loss 2.927677\n",
      "Epoch 4014 Loss 2.927678\n",
      "Epoch 4015 Loss 2.927677\n",
      "Epoch 4016 Loss 2.927678\n",
      "Epoch 4017 Loss 2.927677\n",
      "Epoch 4018 Loss 2.927677\n",
      "Epoch 4019 Loss 2.927676\n",
      "Epoch 4020 Loss 2.927678\n",
      "Epoch 4021 Loss 2.927677\n",
      "Epoch 4022 Loss 2.927677\n",
      "Epoch 4023 Loss 2.927678\n",
      "Epoch 4024 Loss 2.927678\n",
      "Epoch 4025 Loss 2.927677\n",
      "Epoch 4026 Loss 2.927676\n",
      "Epoch 4027 Loss 2.927676\n",
      "Epoch 4028 Loss 2.927677\n",
      "Epoch 4029 Loss 2.927676\n",
      "Epoch 4030 Loss 2.927675\n",
      "Epoch 4031 Loss 2.927676\n",
      "Epoch 4032 Loss 2.927676\n",
      "Epoch 4033 Loss 2.927675\n",
      "Epoch 4034 Loss 2.927676\n",
      "Epoch 4035 Loss 2.927676\n",
      "Epoch 4036 Loss 2.927676\n",
      "Epoch 4037 Loss 2.927677\n",
      "Epoch 4038 Loss 2.927676\n",
      "Epoch 4039 Loss 2.927675\n",
      "Epoch 4040 Loss 2.927676\n",
      "Epoch 4041 Loss 2.927675\n",
      "Epoch 4042 Loss 2.927675\n",
      "Epoch 4043 Loss 2.927675\n",
      "Epoch 4044 Loss 2.927675\n",
      "Epoch 4045 Loss 2.927675\n",
      "Epoch 4046 Loss 2.927675\n",
      "Epoch 4047 Loss 2.927675\n",
      "Epoch 4048 Loss 2.927675\n",
      "Epoch 4049 Loss 2.927673\n",
      "Epoch 4050 Loss 2.927675\n",
      "Epoch 4051 Loss 2.927675\n",
      "Epoch 4052 Loss 2.927675\n",
      "Epoch 4053 Loss 2.927673\n",
      "Epoch 4054 Loss 2.927674\n",
      "Epoch 4055 Loss 2.927675\n",
      "Epoch 4056 Loss 2.927673\n",
      "Epoch 4057 Loss 2.927673\n",
      "Epoch 4058 Loss 2.927673\n",
      "Epoch 4059 Loss 2.927675\n",
      "Epoch 4060 Loss 2.927673\n",
      "Epoch 4061 Loss 2.927673\n",
      "Epoch 4062 Loss 2.927673\n",
      "Epoch 4063 Loss 2.927673\n",
      "Epoch 4064 Loss 2.927673\n",
      "Epoch 4065 Loss 2.927672\n",
      "Epoch 4066 Loss 2.927673\n",
      "Epoch 4067 Loss 2.927672\n",
      "Epoch 4068 Loss 2.927672\n",
      "Epoch 4069 Loss 2.927673\n",
      "Epoch 4070 Loss 2.927672\n",
      "Epoch 4071 Loss 2.927672\n",
      "Epoch 4072 Loss 2.927672\n",
      "Epoch 4073 Loss 2.927672\n",
      "Epoch 4074 Loss 2.927672\n",
      "Epoch 4075 Loss 2.927672\n",
      "Epoch 4076 Loss 2.927671\n",
      "Epoch 4077 Loss 2.927671\n",
      "Epoch 4078 Loss 2.927673\n",
      "Epoch 4079 Loss 2.927671\n",
      "Epoch 4080 Loss 2.927670\n",
      "Epoch 4081 Loss 2.927672\n",
      "Epoch 4082 Loss 2.927671\n",
      "Epoch 4083 Loss 2.927673\n",
      "Epoch 4084 Loss 2.927670\n",
      "Epoch 4085 Loss 2.927670\n",
      "Epoch 4086 Loss 2.927671\n",
      "Epoch 4087 Loss 2.927672\n",
      "Epoch 4088 Loss 2.927670\n",
      "Epoch 4089 Loss 2.927670\n",
      "Epoch 4090 Loss 2.927670\n",
      "Epoch 4091 Loss 2.927671\n",
      "Epoch 4092 Loss 2.927670\n",
      "Epoch 4093 Loss 2.927671\n",
      "Epoch 4094 Loss 2.927670\n",
      "Epoch 4095 Loss 2.927670\n",
      "Epoch 4096 Loss 2.927670\n",
      "Epoch 4097 Loss 2.927670\n",
      "Epoch 4098 Loss 2.927671\n",
      "Epoch 4099 Loss 2.927670\n",
      "Epoch 4100 Loss 2.927669\n",
      "Epoch 4101 Loss 2.927669\n",
      "Epoch 4102 Loss 2.927671\n",
      "Epoch 4103 Loss 2.927670\n",
      "Epoch 4104 Loss 2.927670\n",
      "Epoch 4105 Loss 2.927670\n",
      "Epoch 4106 Loss 2.927670\n",
      "Epoch 4107 Loss 2.927670\n",
      "Epoch 4108 Loss 2.927669\n",
      "Epoch 4109 Loss 2.927668\n",
      "Epoch 4110 Loss 2.927670\n",
      "Epoch 4111 Loss 2.927669\n",
      "Epoch 4112 Loss 2.927669\n",
      "Epoch 4113 Loss 2.927669\n",
      "Epoch 4114 Loss 2.927670\n",
      "Epoch 4115 Loss 2.927669\n",
      "Epoch 4116 Loss 2.927668\n",
      "Epoch 4117 Loss 2.927667\n",
      "Epoch 4118 Loss 2.927669\n",
      "Epoch 4119 Loss 2.927668\n",
      "Epoch 4120 Loss 2.927668\n",
      "Epoch 4121 Loss 2.927669\n",
      "Epoch 4122 Loss 2.927669\n",
      "Epoch 4123 Loss 2.927668\n",
      "Epoch 4124 Loss 2.927668\n",
      "Epoch 4125 Loss 2.927668\n",
      "Epoch 4126 Loss 2.927668\n",
      "Epoch 4127 Loss 2.927667\n",
      "Epoch 4128 Loss 2.927668\n",
      "Epoch 4129 Loss 2.927667\n",
      "Epoch 4130 Loss 2.927667\n",
      "Epoch 4131 Loss 2.927667\n",
      "Epoch 4132 Loss 2.927668\n",
      "Epoch 4133 Loss 2.927666\n",
      "Epoch 4134 Loss 2.927667\n",
      "Epoch 4135 Loss 2.927667\n",
      "Epoch 4136 Loss 2.927667\n",
      "Epoch 4137 Loss 2.927667\n",
      "Epoch 4138 Loss 2.927666\n",
      "Epoch 4139 Loss 2.927669\n",
      "Epoch 4140 Loss 2.927667\n",
      "Epoch 4141 Loss 2.927666\n",
      "Epoch 4142 Loss 2.927667\n",
      "Epoch 4143 Loss 2.927665\n",
      "Epoch 4144 Loss 2.927667\n",
      "Epoch 4145 Loss 2.927666\n",
      "Epoch 4146 Loss 2.927666\n",
      "Epoch 4147 Loss 2.927667\n",
      "Epoch 4148 Loss 2.927666\n",
      "Epoch 4149 Loss 2.927667\n",
      "Epoch 4150 Loss 2.927666\n",
      "Epoch 4151 Loss 2.927666\n",
      "Epoch 4152 Loss 2.927667\n",
      "Epoch 4153 Loss 2.927666\n",
      "Epoch 4154 Loss 2.927666\n",
      "Epoch 4155 Loss 2.927666\n",
      "Epoch 4156 Loss 2.927666\n",
      "Epoch 4157 Loss 2.927666\n",
      "Epoch 4158 Loss 2.927666\n",
      "Epoch 4159 Loss 2.927665\n",
      "Epoch 4160 Loss 2.927666\n",
      "Epoch 4161 Loss 2.927665\n",
      "Epoch 4162 Loss 2.927665\n",
      "Epoch 4163 Loss 2.927666\n",
      "Epoch 4164 Loss 2.927666\n",
      "Epoch 4165 Loss 2.927665\n",
      "Epoch 4166 Loss 2.927664\n",
      "Epoch 4167 Loss 2.927666\n",
      "Epoch 4168 Loss 2.927664\n",
      "Epoch 4169 Loss 2.927665\n",
      "Epoch 4170 Loss 2.927665\n",
      "Epoch 4171 Loss 2.927664\n",
      "Epoch 4172 Loss 2.927666\n",
      "Epoch 4173 Loss 2.927665\n",
      "Epoch 4174 Loss 2.927664\n",
      "Epoch 4175 Loss 2.927665\n",
      "Epoch 4176 Loss 2.927665\n",
      "Epoch 4177 Loss 2.927665\n",
      "Epoch 4178 Loss 2.927663\n",
      "Epoch 4179 Loss 2.927665\n",
      "Epoch 4180 Loss 2.927664\n",
      "Epoch 4181 Loss 2.927664\n",
      "Epoch 4182 Loss 2.927664\n",
      "Epoch 4183 Loss 2.927663\n",
      "Epoch 4184 Loss 2.927663\n",
      "Epoch 4185 Loss 2.927665\n",
      "Epoch 4186 Loss 2.927664\n",
      "Epoch 4187 Loss 2.927663\n",
      "Epoch 4188 Loss 2.927664\n",
      "Epoch 4189 Loss 2.927664\n",
      "Epoch 4190 Loss 2.927663\n",
      "Epoch 4191 Loss 2.927662\n",
      "Epoch 4192 Loss 2.927663\n",
      "Epoch 4193 Loss 2.927663\n",
      "Epoch 4194 Loss 2.927663\n",
      "Epoch 4195 Loss 2.927663\n",
      "Epoch 4196 Loss 2.927662\n",
      "Epoch 4197 Loss 2.927663\n",
      "Epoch 4198 Loss 2.927664\n",
      "Epoch 4199 Loss 2.927663\n",
      "Epoch 4200 Loss 2.927664\n",
      "Epoch 4201 Loss 2.927663\n",
      "Epoch 4202 Loss 2.927664\n",
      "Epoch 4203 Loss 2.927663\n",
      "Epoch 4204 Loss 2.927662\n",
      "Epoch 4205 Loss 2.927662\n",
      "Epoch 4206 Loss 2.927662\n",
      "Epoch 4207 Loss 2.927663\n",
      "Epoch 4208 Loss 2.927662\n",
      "Epoch 4209 Loss 2.927663\n",
      "Epoch 4210 Loss 2.927662\n",
      "Epoch 4211 Loss 2.927664\n",
      "Epoch 4212 Loss 2.927663\n",
      "Epoch 4213 Loss 2.927662\n",
      "Epoch 4214 Loss 2.927662\n",
      "Epoch 4215 Loss 2.927664\n",
      "Epoch 4216 Loss 2.927662\n",
      "Epoch 4217 Loss 2.927660\n",
      "Epoch 4218 Loss 2.927663\n",
      "Epoch 4219 Loss 2.927662\n",
      "Epoch 4220 Loss 2.927663\n",
      "Epoch 4221 Loss 2.927662\n",
      "Epoch 4222 Loss 2.927661\n",
      "Epoch 4223 Loss 2.927660\n",
      "Epoch 4224 Loss 2.927662\n",
      "Epoch 4225 Loss 2.927663\n",
      "Epoch 4226 Loss 2.927661\n",
      "Epoch 4227 Loss 2.927661\n",
      "Epoch 4228 Loss 2.927663\n",
      "Epoch 4229 Loss 2.927662\n",
      "Epoch 4230 Loss 2.927662\n",
      "Epoch 4231 Loss 2.927661\n",
      "Epoch 4232 Loss 2.927660\n",
      "Epoch 4233 Loss 2.927661\n",
      "Epoch 4234 Loss 2.927662\n",
      "Epoch 4235 Loss 2.927661\n",
      "Epoch 4236 Loss 2.927660\n",
      "Epoch 4237 Loss 2.927662\n",
      "Epoch 4238 Loss 2.927661\n",
      "Epoch 4239 Loss 2.927661\n",
      "Epoch 4240 Loss 2.927660\n",
      "Epoch 4241 Loss 2.927660\n",
      "Epoch 4242 Loss 2.927662\n",
      "Epoch 4243 Loss 2.927662\n",
      "Epoch 4244 Loss 2.927661\n",
      "Epoch 4245 Loss 2.927661\n",
      "Epoch 4246 Loss 2.927661\n",
      "Epoch 4247 Loss 2.927661\n",
      "Epoch 4248 Loss 2.927660\n",
      "Epoch 4249 Loss 2.927661\n",
      "Epoch 4250 Loss 2.927660\n",
      "Epoch 4251 Loss 2.927660\n",
      "Epoch 4252 Loss 2.927660\n",
      "Epoch 4253 Loss 2.927659\n",
      "Epoch 4254 Loss 2.927659\n",
      "Epoch 4255 Loss 2.927661\n",
      "Epoch 4256 Loss 2.927662\n",
      "Epoch 4257 Loss 2.927661\n",
      "Epoch 4258 Loss 2.927660\n",
      "Epoch 4259 Loss 2.927660\n",
      "Epoch 4260 Loss 2.927660\n",
      "Epoch 4261 Loss 2.927658\n",
      "Epoch 4262 Loss 2.927658\n",
      "Epoch 4263 Loss 2.927660\n",
      "Epoch 4264 Loss 2.927659\n",
      "Epoch 4265 Loss 2.927660\n",
      "Epoch 4266 Loss 2.927660\n",
      "Epoch 4267 Loss 2.927660\n",
      "Epoch 4268 Loss 2.927660\n",
      "Epoch 4269 Loss 2.927660\n",
      "Epoch 4270 Loss 2.927660\n",
      "Epoch 4271 Loss 2.927660\n",
      "Epoch 4272 Loss 2.927659\n",
      "Epoch 4273 Loss 2.927659\n",
      "Epoch 4274 Loss 2.927660\n",
      "Epoch 4275 Loss 2.927660\n",
      "Epoch 4276 Loss 2.927660\n",
      "Epoch 4277 Loss 2.927659\n",
      "Epoch 4278 Loss 2.927658\n",
      "Epoch 4279 Loss 2.927659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4280 Loss 2.927659\n",
      "Epoch 4281 Loss 2.927660\n",
      "Epoch 4282 Loss 2.927658\n",
      "Epoch 4283 Loss 2.927659\n",
      "Epoch 4284 Loss 2.927658\n",
      "Epoch 4285 Loss 2.927657\n",
      "Epoch 4286 Loss 2.927659\n",
      "Epoch 4287 Loss 2.927659\n",
      "Epoch 4288 Loss 2.927660\n",
      "Epoch 4289 Loss 2.927658\n",
      "Epoch 4290 Loss 2.927658\n",
      "Epoch 4291 Loss 2.927658\n",
      "Epoch 4292 Loss 2.927658\n",
      "Epoch 4293 Loss 2.927657\n",
      "Epoch 4294 Loss 2.927658\n",
      "Epoch 4295 Loss 2.927659\n",
      "Epoch 4296 Loss 2.927660\n",
      "Epoch 4297 Loss 2.927658\n",
      "Epoch 4298 Loss 2.927659\n",
      "Epoch 4299 Loss 2.927658\n",
      "Epoch 4300 Loss 2.927657\n",
      "Epoch 4301 Loss 2.927658\n",
      "Epoch 4302 Loss 2.927658\n",
      "Epoch 4303 Loss 2.927658\n",
      "Epoch 4304 Loss 2.927657\n",
      "Epoch 4305 Loss 2.927659\n",
      "Epoch 4306 Loss 2.927657\n",
      "Epoch 4307 Loss 2.927658\n",
      "Epoch 4308 Loss 2.927658\n",
      "Epoch 4309 Loss 2.927658\n",
      "Epoch 4310 Loss 2.927658\n",
      "Epoch 4311 Loss 2.927657\n",
      "Epoch 4312 Loss 2.927657\n",
      "Epoch 4313 Loss 2.927657\n",
      "Epoch 4314 Loss 2.927656\n",
      "Epoch 4315 Loss 2.927657\n",
      "Epoch 4316 Loss 2.927657\n",
      "Epoch 4317 Loss 2.927657\n",
      "Epoch 4318 Loss 2.927656\n",
      "Epoch 4319 Loss 2.927657\n",
      "Epoch 4320 Loss 2.927657\n",
      "Epoch 4321 Loss 2.927656\n",
      "Epoch 4322 Loss 2.927658\n",
      "Epoch 4323 Loss 2.927658\n",
      "Epoch 4324 Loss 2.927657\n",
      "Epoch 4325 Loss 2.927656\n",
      "Epoch 4326 Loss 2.927657\n",
      "Epoch 4327 Loss 2.927658\n",
      "Epoch 4328 Loss 2.927657\n",
      "Epoch 4329 Loss 2.927657\n",
      "Epoch 4330 Loss 2.927657\n",
      "Epoch 4331 Loss 2.927658\n",
      "Epoch 4332 Loss 2.927658\n",
      "Epoch 4333 Loss 2.927657\n",
      "Epoch 4334 Loss 2.927658\n",
      "Epoch 4335 Loss 2.927657\n",
      "Epoch 4336 Loss 2.927657\n",
      "Epoch 4337 Loss 2.927657\n",
      "Epoch 4338 Loss 2.927657\n",
      "Epoch 4339 Loss 2.927657\n",
      "Epoch 4340 Loss 2.927656\n",
      "Epoch 4341 Loss 2.927657\n",
      "Epoch 4342 Loss 2.927655\n",
      "Epoch 4343 Loss 2.927656\n",
      "Epoch 4344 Loss 2.927656\n",
      "Epoch 4345 Loss 2.927657\n",
      "Epoch 4346 Loss 2.927656\n",
      "Epoch 4347 Loss 2.927657\n",
      "Epoch 4348 Loss 2.927655\n",
      "Epoch 4349 Loss 2.927656\n",
      "Epoch 4350 Loss 2.927656\n",
      "Epoch 4351 Loss 2.927655\n",
      "Epoch 4352 Loss 2.927656\n",
      "Epoch 4353 Loss 2.927656\n",
      "Epoch 4354 Loss 2.927655\n",
      "Epoch 4355 Loss 2.927655\n",
      "Epoch 4356 Loss 2.927656\n",
      "Epoch 4357 Loss 2.927655\n",
      "Epoch 4358 Loss 2.927657\n",
      "Epoch 4359 Loss 2.927656\n",
      "Epoch 4360 Loss 2.927655\n",
      "Epoch 4361 Loss 2.927656\n",
      "Epoch 4362 Loss 2.927655\n",
      "Epoch 4363 Loss 2.927656\n",
      "Epoch 4364 Loss 2.927656\n",
      "Epoch 4365 Loss 2.927656\n",
      "Epoch 4366 Loss 2.927656\n",
      "Epoch 4367 Loss 2.927655\n",
      "Epoch 4368 Loss 2.927654\n",
      "Epoch 4369 Loss 2.927655\n",
      "Epoch 4370 Loss 2.927656\n",
      "Epoch 4371 Loss 2.927655\n",
      "Epoch 4372 Loss 2.927656\n",
      "Epoch 4373 Loss 2.927656\n",
      "Epoch 4374 Loss 2.927655\n",
      "Epoch 4375 Loss 2.927656\n",
      "Epoch 4376 Loss 2.927655\n",
      "Epoch 4377 Loss 2.927656\n",
      "Epoch 4378 Loss 2.927655\n",
      "Epoch 4379 Loss 2.927655\n",
      "Epoch 4380 Loss 2.927654\n",
      "Epoch 4381 Loss 2.927656\n",
      "Epoch 4382 Loss 2.927655\n",
      "Epoch 4383 Loss 2.927656\n",
      "Epoch 4384 Loss 2.927656\n",
      "Epoch 4385 Loss 2.927655\n",
      "Epoch 4386 Loss 2.927656\n",
      "Epoch 4387 Loss 2.927654\n",
      "Epoch 4388 Loss 2.927656\n",
      "Epoch 4389 Loss 2.927654\n",
      "Epoch 4390 Loss 2.927655\n",
      "Epoch 4391 Loss 2.927654\n",
      "Epoch 4392 Loss 2.927655\n",
      "Epoch 4393 Loss 2.927655\n",
      "Epoch 4394 Loss 2.927654\n",
      "Epoch 4395 Loss 2.927654\n",
      "Epoch 4396 Loss 2.927655\n",
      "Epoch 4397 Loss 2.927655\n",
      "Epoch 4398 Loss 2.927654\n",
      "Epoch 4399 Loss 2.927655\n",
      "Epoch 4400 Loss 2.927654\n",
      "Epoch 4401 Loss 2.927655\n",
      "Epoch 4402 Loss 2.927654\n",
      "Epoch 4403 Loss 2.927654\n",
      "Epoch 4404 Loss 2.927654\n",
      "Epoch 4405 Loss 2.927654\n",
      "Epoch 4406 Loss 2.927655\n",
      "Epoch 4407 Loss 2.927654\n",
      "Epoch 4408 Loss 2.927654\n",
      "Epoch 4409 Loss 2.927654\n",
      "Epoch 4410 Loss 2.927655\n",
      "Epoch 4411 Loss 2.927655\n",
      "Epoch 4412 Loss 2.927656\n",
      "Epoch 4413 Loss 2.927654\n",
      "Epoch 4414 Loss 2.927655\n",
      "Epoch 4415 Loss 2.927654\n",
      "Epoch 4416 Loss 2.927653\n",
      "Epoch 4417 Loss 2.927655\n",
      "Epoch 4418 Loss 2.927653\n",
      "Epoch 4419 Loss 2.927655\n",
      "Epoch 4420 Loss 2.927653\n",
      "Epoch 4421 Loss 2.927654\n",
      "Epoch 4422 Loss 2.927653\n",
      "Epoch 4423 Loss 2.927655\n",
      "Epoch 4424 Loss 2.927654\n",
      "Epoch 4425 Loss 2.927655\n",
      "Epoch 4426 Loss 2.927653\n",
      "Epoch 4427 Loss 2.927654\n",
      "Epoch 4428 Loss 2.927655\n",
      "Epoch 4429 Loss 2.927654\n",
      "Epoch 4430 Loss 2.927654\n",
      "Epoch 4431 Loss 2.927653\n",
      "Epoch 4432 Loss 2.927654\n",
      "Epoch 4433 Loss 2.927654\n",
      "Epoch 4434 Loss 2.927654\n",
      "Epoch 4435 Loss 2.927655\n",
      "Epoch 4436 Loss 2.927653\n",
      "Epoch 4437 Loss 2.927652\n",
      "Epoch 4438 Loss 2.927653\n",
      "Epoch 4439 Loss 2.927654\n",
      "Epoch 4440 Loss 2.927655\n",
      "Epoch 4441 Loss 2.927655\n",
      "Epoch 4442 Loss 2.927652\n",
      "Epoch 4443 Loss 2.927653\n",
      "Epoch 4444 Loss 2.927651\n",
      "Epoch 4445 Loss 2.927654\n",
      "Epoch 4446 Loss 2.927654\n",
      "Epoch 4447 Loss 2.927653\n",
      "Epoch 4448 Loss 2.927654\n",
      "Epoch 4449 Loss 2.927655\n",
      "Epoch 4450 Loss 2.927654\n",
      "Epoch 4451 Loss 2.927654\n",
      "Epoch 4452 Loss 2.927653\n",
      "Epoch 4453 Loss 2.927652\n",
      "Epoch 4454 Loss 2.927653\n",
      "Epoch 4455 Loss 2.927653\n",
      "Epoch 4456 Loss 2.927654\n",
      "Epoch 4457 Loss 2.927653\n",
      "Epoch 4458 Loss 2.927652\n",
      "Epoch 4459 Loss 2.927653\n",
      "Epoch 4460 Loss 2.927652\n",
      "Epoch 4461 Loss 2.927654\n",
      "Epoch 4462 Loss 2.927654\n",
      "Epoch 4463 Loss 2.927654\n",
      "Epoch 4464 Loss 2.927653\n",
      "Epoch 4465 Loss 2.927653\n",
      "Epoch 4466 Loss 2.927652\n",
      "Epoch 4467 Loss 2.927654\n",
      "Epoch 4468 Loss 2.927653\n",
      "Epoch 4469 Loss 2.927653\n",
      "Epoch 4470 Loss 2.927653\n",
      "Epoch 4471 Loss 2.927653\n",
      "Epoch 4472 Loss 2.927653\n",
      "Epoch 4473 Loss 2.927654\n",
      "Epoch 4474 Loss 2.927653\n",
      "Epoch 4475 Loss 2.927653\n",
      "Epoch 4476 Loss 2.927652\n",
      "Epoch 4477 Loss 2.927653\n",
      "Epoch 4478 Loss 2.927654\n",
      "Epoch 4479 Loss 2.927653\n",
      "Epoch 4480 Loss 2.927651\n",
      "Epoch 4481 Loss 2.927653\n",
      "Epoch 4482 Loss 2.927653\n",
      "Epoch 4483 Loss 2.927654\n",
      "Epoch 4484 Loss 2.927653\n",
      "Epoch 4485 Loss 2.927653\n",
      "Epoch 4486 Loss 2.927652\n",
      "Epoch 4487 Loss 2.927651\n",
      "Epoch 4488 Loss 2.927652\n",
      "Epoch 4489 Loss 2.927653\n",
      "Epoch 4490 Loss 2.927654\n",
      "Epoch 4491 Loss 2.927653\n",
      "Epoch 4492 Loss 2.927652\n",
      "Epoch 4493 Loss 2.927652\n",
      "Epoch 4494 Loss 2.927651\n",
      "Epoch 4495 Loss 2.927652\n",
      "Epoch 4496 Loss 2.927653\n",
      "Epoch 4497 Loss 2.927653\n",
      "Epoch 4498 Loss 2.927652\n",
      "Epoch 4499 Loss 2.927652\n",
      "Epoch 4500 Loss 2.927652\n",
      "Epoch 4501 Loss 2.927652\n",
      "Epoch 4502 Loss 2.927654\n",
      "Epoch 4503 Loss 2.927651\n",
      "Epoch 4504 Loss 2.927652\n",
      "Epoch 4505 Loss 2.927653\n",
      "Epoch 4506 Loss 2.927653\n",
      "Epoch 4507 Loss 2.927651\n",
      "Epoch 4508 Loss 2.927651\n",
      "Epoch 4509 Loss 2.927653\n",
      "Epoch 4510 Loss 2.927653\n",
      "Epoch 4511 Loss 2.927651\n",
      "Epoch 4512 Loss 2.927651\n",
      "Epoch 4513 Loss 2.927653\n",
      "Epoch 4514 Loss 2.927653\n",
      "Epoch 4515 Loss 2.927652\n",
      "Epoch 4516 Loss 2.927653\n",
      "Epoch 4517 Loss 2.927652\n",
      "Epoch 4518 Loss 2.927652\n",
      "Epoch 4519 Loss 2.927653\n",
      "Epoch 4520 Loss 2.927652\n",
      "Epoch 4521 Loss 2.927652\n",
      "Epoch 4522 Loss 2.927651\n",
      "Epoch 4523 Loss 2.927651\n",
      "Epoch 4524 Loss 2.927652\n",
      "Epoch 4525 Loss 2.927652\n",
      "Epoch 4526 Loss 2.927651\n",
      "Epoch 4527 Loss 2.927651\n",
      "Epoch 4528 Loss 2.927650\n",
      "Epoch 4529 Loss 2.927651\n",
      "Epoch 4530 Loss 2.927653\n",
      "Epoch 4531 Loss 2.927651\n",
      "Epoch 4532 Loss 2.927651\n",
      "Epoch 4533 Loss 2.927652\n",
      "Epoch 4534 Loss 2.927653\n",
      "Epoch 4535 Loss 2.927651\n",
      "Epoch 4536 Loss 2.927651\n",
      "Epoch 4537 Loss 2.927650\n",
      "Epoch 4538 Loss 2.927651\n",
      "Epoch 4539 Loss 2.927650\n",
      "Epoch 4540 Loss 2.927652\n",
      "Epoch 4541 Loss 2.927652\n",
      "Epoch 4542 Loss 2.927651\n",
      "Epoch 4543 Loss 2.927652\n",
      "Epoch 4544 Loss 2.927652\n",
      "Epoch 4545 Loss 2.927651\n",
      "Epoch 4546 Loss 2.927650\n",
      "Epoch 4547 Loss 2.927651\n",
      "Epoch 4548 Loss 2.927652\n",
      "Epoch 4549 Loss 2.927651\n",
      "Epoch 4550 Loss 2.927653\n",
      "Epoch 4551 Loss 2.927651\n",
      "Epoch 4552 Loss 2.927652\n",
      "Epoch 4553 Loss 2.927651\n",
      "Epoch 4554 Loss 2.927652\n",
      "Epoch 4555 Loss 2.927650\n",
      "Epoch 4556 Loss 2.927650\n",
      "Epoch 4557 Loss 2.927650\n",
      "Epoch 4558 Loss 2.927652\n",
      "Epoch 4559 Loss 2.927650\n",
      "Epoch 4560 Loss 2.927650\n",
      "Epoch 4561 Loss 2.927651\n",
      "Epoch 4562 Loss 2.927652\n",
      "Epoch 4563 Loss 2.927650\n",
      "Epoch 4564 Loss 2.927651\n",
      "Epoch 4565 Loss 2.927650\n",
      "Epoch 4566 Loss 2.927651\n",
      "Epoch 4567 Loss 2.927650\n",
      "Epoch 4568 Loss 2.927651\n",
      "Epoch 4569 Loss 2.927650\n",
      "Epoch 4570 Loss 2.927650\n",
      "Epoch 4571 Loss 2.927650\n",
      "Epoch 4572 Loss 2.927651\n",
      "Epoch 4573 Loss 2.927652\n",
      "Epoch 4574 Loss 2.927650\n",
      "Epoch 4575 Loss 2.927651\n",
      "Epoch 4576 Loss 2.927651\n",
      "Epoch 4577 Loss 2.927651\n",
      "Epoch 4578 Loss 2.927652\n",
      "Epoch 4579 Loss 2.927650\n",
      "Epoch 4580 Loss 2.927650\n",
      "Epoch 4581 Loss 2.927650\n",
      "Epoch 4582 Loss 2.927651\n",
      "Epoch 4583 Loss 2.927650\n",
      "Epoch 4584 Loss 2.927650\n",
      "Epoch 4585 Loss 2.927652\n",
      "Epoch 4586 Loss 2.927650\n",
      "Epoch 4587 Loss 2.927651\n",
      "Epoch 4588 Loss 2.927650\n",
      "Epoch 4589 Loss 2.927650\n",
      "Epoch 4590 Loss 2.927652\n",
      "Epoch 4591 Loss 2.927650\n",
      "Epoch 4592 Loss 2.927651\n",
      "Epoch 4593 Loss 2.927651\n",
      "Epoch 4594 Loss 2.927650\n",
      "Epoch 4595 Loss 2.927650\n",
      "Epoch 4596 Loss 2.927651\n",
      "Epoch 4597 Loss 2.927651\n",
      "Epoch 4598 Loss 2.927652\n",
      "Epoch 4599 Loss 2.927649\n",
      "Epoch 4600 Loss 2.927650\n",
      "Epoch 4601 Loss 2.927650\n",
      "Epoch 4602 Loss 2.927649\n",
      "Epoch 4603 Loss 2.927649\n",
      "Epoch 4604 Loss 2.927649\n",
      "Epoch 4605 Loss 2.927650\n",
      "Epoch 4606 Loss 2.927650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4607 Loss 2.927650\n",
      "Epoch 4608 Loss 2.927651\n",
      "Epoch 4609 Loss 2.927650\n",
      "Epoch 4610 Loss 2.927651\n",
      "Epoch 4611 Loss 2.927650\n",
      "Epoch 4612 Loss 2.927650\n",
      "Epoch 4613 Loss 2.927650\n",
      "Epoch 4614 Loss 2.927649\n",
      "Epoch 4615 Loss 2.927650\n",
      "Epoch 4616 Loss 2.927651\n",
      "Epoch 4617 Loss 2.927650\n",
      "Epoch 4618 Loss 2.927651\n",
      "Epoch 4619 Loss 2.927649\n",
      "Epoch 4620 Loss 2.927650\n",
      "Epoch 4621 Loss 2.927650\n",
      "Epoch 4622 Loss 2.927650\n",
      "Epoch 4623 Loss 2.927651\n",
      "Epoch 4624 Loss 2.927651\n",
      "Epoch 4625 Loss 2.927649\n",
      "Epoch 4626 Loss 2.927650\n",
      "Epoch 4627 Loss 2.927650\n",
      "Epoch 4628 Loss 2.927651\n",
      "Epoch 4629 Loss 2.927650\n",
      "Epoch 4630 Loss 2.927647\n",
      "Epoch 4631 Loss 2.927648\n",
      "Epoch 4632 Loss 2.927649\n",
      "Epoch 4633 Loss 2.927649\n",
      "Epoch 4634 Loss 2.927650\n",
      "Epoch 4635 Loss 2.927649\n",
      "Epoch 4636 Loss 2.927650\n",
      "Epoch 4637 Loss 2.927650\n",
      "Epoch 4638 Loss 2.927651\n",
      "Epoch 4639 Loss 2.927649\n",
      "Epoch 4640 Loss 2.927649\n",
      "Epoch 4641 Loss 2.927650\n",
      "Epoch 4642 Loss 2.927650\n",
      "Epoch 4643 Loss 2.927649\n",
      "Epoch 4644 Loss 2.927650\n",
      "Epoch 4645 Loss 2.927650\n",
      "Epoch 4646 Loss 2.927650\n",
      "Epoch 4647 Loss 2.927651\n",
      "Epoch 4648 Loss 2.927650\n",
      "Epoch 4649 Loss 2.927650\n",
      "Epoch 4650 Loss 2.927649\n",
      "Epoch 4651 Loss 2.927650\n",
      "Epoch 4652 Loss 2.927651\n",
      "Epoch 4653 Loss 2.927650\n",
      "Epoch 4654 Loss 2.927651\n",
      "Epoch 4655 Loss 2.927651\n",
      "Epoch 4656 Loss 2.927650\n",
      "Epoch 4657 Loss 2.927649\n",
      "Epoch 4658 Loss 2.927649\n",
      "Epoch 4659 Loss 2.927649\n",
      "Epoch 4660 Loss 2.927649\n",
      "Epoch 4661 Loss 2.927648\n",
      "Epoch 4662 Loss 2.927650\n",
      "Epoch 4663 Loss 2.927649\n",
      "Epoch 4664 Loss 2.927648\n",
      "Epoch 4665 Loss 2.927649\n",
      "Epoch 4666 Loss 2.927649\n",
      "Epoch 4667 Loss 2.927649\n",
      "Epoch 4668 Loss 2.927649\n",
      "Epoch 4669 Loss 2.927649\n",
      "Epoch 4670 Loss 2.927649\n",
      "Epoch 4671 Loss 2.927649\n",
      "Epoch 4672 Loss 2.927649\n",
      "Epoch 4673 Loss 2.927650\n",
      "Epoch 4674 Loss 2.927650\n",
      "Epoch 4675 Loss 2.927650\n",
      "Epoch 4676 Loss 2.927650\n",
      "Epoch 4677 Loss 2.927650\n",
      "Epoch 4678 Loss 2.927650\n",
      "Epoch 4679 Loss 2.927649\n",
      "Epoch 4680 Loss 2.927648\n",
      "Epoch 4681 Loss 2.927649\n",
      "Epoch 4682 Loss 2.927649\n",
      "Epoch 4683 Loss 2.927648\n",
      "Epoch 4684 Loss 2.927648\n",
      "Epoch 4685 Loss 2.927649\n",
      "Epoch 4686 Loss 2.927651\n",
      "Epoch 4687 Loss 2.927649\n",
      "Epoch 4688 Loss 2.927650\n",
      "Epoch 4689 Loss 2.927649\n",
      "Epoch 4690 Loss 2.927650\n",
      "Epoch 4691 Loss 2.927648\n",
      "Epoch 4692 Loss 2.927649\n",
      "Epoch 4693 Loss 2.927649\n",
      "Epoch 4694 Loss 2.927650\n",
      "Epoch 4695 Loss 2.927648\n",
      "Epoch 4696 Loss 2.927649\n",
      "Epoch 4697 Loss 2.927648\n",
      "Epoch 4698 Loss 2.927649\n",
      "Epoch 4699 Loss 2.927649\n",
      "Epoch 4700 Loss 2.927650\n",
      "Epoch 4701 Loss 2.927650\n",
      "Epoch 4702 Loss 2.927650\n",
      "Epoch 4703 Loss 2.927648\n",
      "Epoch 4704 Loss 2.927649\n",
      "Epoch 4705 Loss 2.927649\n",
      "Epoch 4706 Loss 2.927649\n",
      "Epoch 4707 Loss 2.927649\n",
      "Epoch 4708 Loss 2.927648\n",
      "Epoch 4709 Loss 2.927650\n",
      "Epoch 4710 Loss 2.927650\n",
      "Epoch 4711 Loss 2.927649\n",
      "Epoch 4712 Loss 2.927649\n",
      "Epoch 4713 Loss 2.927649\n",
      "Epoch 4714 Loss 2.927649\n",
      "Epoch 4715 Loss 2.927650\n",
      "Epoch 4716 Loss 2.927649\n",
      "Epoch 4717 Loss 2.927649\n",
      "Epoch 4718 Loss 2.927649\n",
      "Epoch 4719 Loss 2.927648\n",
      "Epoch 4720 Loss 2.927649\n",
      "Epoch 4721 Loss 2.927649\n",
      "Epoch 4722 Loss 2.927650\n",
      "Epoch 4723 Loss 2.927650\n",
      "Epoch 4724 Loss 2.927649\n",
      "Epoch 4725 Loss 2.927650\n",
      "Epoch 4726 Loss 2.927649\n",
      "Epoch 4727 Loss 2.927648\n",
      "Epoch 4728 Loss 2.927647\n",
      "Epoch 4729 Loss 2.927649\n",
      "Epoch 4730 Loss 2.927648\n",
      "Epoch 4731 Loss 2.927649\n",
      "Epoch 4732 Loss 2.927648\n",
      "Epoch 4733 Loss 2.927649\n",
      "Epoch 4734 Loss 2.927647\n",
      "Epoch 4735 Loss 2.927650\n",
      "Epoch 4736 Loss 2.927650\n",
      "Epoch 4737 Loss 2.927650\n",
      "Epoch 4738 Loss 2.927649\n",
      "Epoch 4739 Loss 2.927648\n",
      "Epoch 4740 Loss 2.927648\n",
      "Epoch 4741 Loss 2.927648\n",
      "Epoch 4742 Loss 2.927647\n",
      "Epoch 4743 Loss 2.927648\n",
      "Epoch 4744 Loss 2.927649\n",
      "Epoch 4745 Loss 2.927648\n",
      "Epoch 4746 Loss 2.927648\n",
      "Epoch 4747 Loss 2.927649\n",
      "Epoch 4748 Loss 2.927648\n",
      "Epoch 4749 Loss 2.927649\n",
      "Epoch 4750 Loss 2.927649\n",
      "Epoch 4751 Loss 2.927649\n",
      "Epoch 4752 Loss 2.927648\n",
      "Epoch 4753 Loss 2.927648\n",
      "Epoch 4754 Loss 2.927647\n",
      "Epoch 4755 Loss 2.927648\n",
      "Epoch 4756 Loss 2.927648\n",
      "Epoch 4757 Loss 2.927648\n",
      "Epoch 4758 Loss 2.927648\n",
      "Epoch 4759 Loss 2.927649\n",
      "Epoch 4760 Loss 2.927649\n",
      "Epoch 4761 Loss 2.927649\n",
      "Epoch 4762 Loss 2.927648\n",
      "Epoch 4763 Loss 2.927647\n",
      "Epoch 4764 Loss 2.927648\n",
      "Epoch 4765 Loss 2.927648\n",
      "Epoch 4766 Loss 2.927649\n",
      "Epoch 4767 Loss 2.927648\n",
      "Epoch 4768 Loss 2.927649\n",
      "Epoch 4769 Loss 2.927648\n",
      "Epoch 4770 Loss 2.927649\n",
      "Epoch 4771 Loss 2.927648\n",
      "Epoch 4772 Loss 2.927648\n",
      "Epoch 4773 Loss 2.927649\n",
      "Epoch 4774 Loss 2.927648\n",
      "Epoch 4775 Loss 2.927647\n",
      "Epoch 4776 Loss 2.927648\n",
      "Epoch 4777 Loss 2.927648\n",
      "Epoch 4778 Loss 2.927648\n",
      "Epoch 4779 Loss 2.927649\n",
      "Epoch 4780 Loss 2.927649\n",
      "Epoch 4781 Loss 2.927649\n",
      "Epoch 4782 Loss 2.927649\n",
      "Epoch 4783 Loss 2.927649\n",
      "Epoch 4784 Loss 2.927648\n",
      "Epoch 4785 Loss 2.927648\n",
      "Epoch 4786 Loss 2.927647\n",
      "Epoch 4787 Loss 2.927647\n",
      "Epoch 4788 Loss 2.927648\n",
      "Epoch 4789 Loss 2.927648\n",
      "Epoch 4790 Loss 2.927648\n",
      "Epoch 4791 Loss 2.927648\n",
      "Epoch 4792 Loss 2.927648\n",
      "Epoch 4793 Loss 2.927648\n",
      "Epoch 4794 Loss 2.927650\n",
      "Epoch 4795 Loss 2.927648\n",
      "Epoch 4796 Loss 2.927649\n",
      "Epoch 4797 Loss 2.927649\n",
      "Epoch 4798 Loss 2.927648\n",
      "Epoch 4799 Loss 2.927648\n",
      "Epoch 4800 Loss 2.927650\n",
      "Epoch 4801 Loss 2.927647\n",
      "Epoch 4802 Loss 2.927648\n",
      "Epoch 4803 Loss 2.927649\n",
      "Epoch 4804 Loss 2.927647\n",
      "Epoch 4805 Loss 2.927649\n",
      "Epoch 4806 Loss 2.927648\n",
      "Epoch 4807 Loss 2.927649\n",
      "Epoch 4808 Loss 2.927648\n",
      "Epoch 4809 Loss 2.927649\n",
      "Epoch 4810 Loss 2.927649\n",
      "Epoch 4811 Loss 2.927647\n",
      "Epoch 4812 Loss 2.927649\n",
      "Epoch 4813 Loss 2.927648\n",
      "Epoch 4814 Loss 2.927647\n",
      "Epoch 4815 Loss 2.927649\n",
      "Epoch 4816 Loss 2.927647\n",
      "Epoch 4817 Loss 2.927648\n",
      "Epoch 4818 Loss 2.927646\n",
      "Epoch 4819 Loss 2.927649\n",
      "Epoch 4820 Loss 2.927647\n",
      "Epoch 4821 Loss 2.927649\n",
      "Epoch 4822 Loss 2.927649\n",
      "Epoch 4823 Loss 2.927648\n",
      "Epoch 4824 Loss 2.927648\n",
      "Epoch 4825 Loss 2.927649\n",
      "Epoch 4826 Loss 2.927648\n",
      "Epoch 4827 Loss 2.927649\n",
      "Epoch 4828 Loss 2.927649\n",
      "Epoch 4829 Loss 2.927648\n",
      "Epoch 4830 Loss 2.927648\n",
      "Epoch 4831 Loss 2.927646\n",
      "Epoch 4832 Loss 2.927648\n",
      "Epoch 4833 Loss 2.927647\n",
      "Epoch 4834 Loss 2.927648\n",
      "Epoch 4835 Loss 2.927649\n",
      "Epoch 4836 Loss 2.927647\n",
      "Epoch 4837 Loss 2.927648\n",
      "Epoch 4838 Loss 2.927648\n",
      "Epoch 4839 Loss 2.927648\n",
      "Epoch 4840 Loss 2.927648\n",
      "Epoch 4841 Loss 2.927648\n",
      "Epoch 4842 Loss 2.927649\n",
      "Epoch 4843 Loss 2.927647\n",
      "Epoch 4844 Loss 2.927648\n",
      "Epoch 4845 Loss 2.927647\n",
      "Epoch 4846 Loss 2.927647\n",
      "Epoch 4847 Loss 2.927648\n",
      "Epoch 4848 Loss 2.927648\n",
      "Epoch 4849 Loss 2.927649\n",
      "Epoch 4850 Loss 2.927647\n",
      "Epoch 4851 Loss 2.927649\n",
      "Epoch 4852 Loss 2.927648\n",
      "Epoch 4853 Loss 2.927647\n",
      "Epoch 4854 Loss 2.927649\n",
      "Epoch 4855 Loss 2.927648\n",
      "Epoch 4856 Loss 2.927649\n",
      "Epoch 4857 Loss 2.927649\n",
      "Epoch 4858 Loss 2.927647\n",
      "Epoch 4859 Loss 2.927648\n",
      "Epoch 4860 Loss 2.927648\n",
      "Epoch 4861 Loss 2.927649\n",
      "Epoch 4862 Loss 2.927647\n",
      "Epoch 4863 Loss 2.927647\n",
      "Epoch 4864 Loss 2.927648\n",
      "Epoch 4865 Loss 2.927646\n",
      "Epoch 4866 Loss 2.927648\n",
      "Epoch 4867 Loss 2.927648\n",
      "Epoch 4868 Loss 2.927648\n",
      "Epoch 4869 Loss 2.927648\n",
      "Epoch 4870 Loss 2.927647\n",
      "Epoch 4871 Loss 2.927649\n",
      "Epoch 4872 Loss 2.927648\n",
      "Epoch 4873 Loss 2.927647\n",
      "Epoch 4874 Loss 2.927648\n",
      "Epoch 4875 Loss 2.927648\n",
      "Epoch 4876 Loss 2.927649\n",
      "Epoch 4877 Loss 2.927648\n",
      "Epoch 4878 Loss 2.927648\n",
      "Epoch 4879 Loss 2.927647\n",
      "Epoch 4880 Loss 2.927648\n",
      "Epoch 4881 Loss 2.927648\n",
      "Epoch 4882 Loss 2.927647\n",
      "Epoch 4883 Loss 2.927649\n",
      "Epoch 4884 Loss 2.927648\n",
      "Epoch 4885 Loss 2.927647\n",
      "Epoch 4886 Loss 2.927649\n",
      "Epoch 4887 Loss 2.927648\n",
      "Epoch 4888 Loss 2.927647\n",
      "Epoch 4889 Loss 2.927646\n",
      "Epoch 4890 Loss 2.927647\n",
      "Epoch 4891 Loss 2.927648\n",
      "Epoch 4892 Loss 2.927646\n",
      "Epoch 4893 Loss 2.927649\n",
      "Epoch 4894 Loss 2.927648\n",
      "Epoch 4895 Loss 2.927648\n",
      "Epoch 4896 Loss 2.927649\n",
      "Epoch 4897 Loss 2.927647\n",
      "Epoch 4898 Loss 2.927648\n",
      "Epoch 4899 Loss 2.927648\n",
      "Epoch 4900 Loss 2.927648\n",
      "Epoch 4901 Loss 2.927649\n",
      "Epoch 4902 Loss 2.927647\n",
      "Epoch 4903 Loss 2.927647\n",
      "Epoch 4904 Loss 2.927646\n",
      "Epoch 4905 Loss 2.927647\n",
      "Epoch 4906 Loss 2.927647\n",
      "Epoch 4907 Loss 2.927647\n",
      "Epoch 4908 Loss 2.927647\n",
      "Epoch 4909 Loss 2.927648\n",
      "Epoch 4910 Loss 2.927649\n",
      "Epoch 4911 Loss 2.927646\n",
      "Epoch 4912 Loss 2.927648\n",
      "Epoch 4913 Loss 2.927648\n",
      "Epoch 4914 Loss 2.927647\n",
      "Epoch 4915 Loss 2.927649\n",
      "Epoch 4916 Loss 2.927648\n",
      "Epoch 4917 Loss 2.927647\n",
      "Epoch 4918 Loss 2.927647\n",
      "Epoch 4919 Loss 2.927647\n",
      "Epoch 4920 Loss 2.927647\n",
      "Epoch 4921 Loss 2.927646\n",
      "Epoch 4922 Loss 2.927649\n",
      "Epoch 4923 Loss 2.927647\n",
      "Epoch 4924 Loss 2.927646\n",
      "Epoch 4925 Loss 2.927647\n",
      "Epoch 4926 Loss 2.927648\n",
      "Epoch 4927 Loss 2.927647\n",
      "Epoch 4928 Loss 2.927649\n",
      "Epoch 4929 Loss 2.927648\n",
      "Epoch 4930 Loss 2.927648\n",
      "Epoch 4931 Loss 2.927646\n",
      "Epoch 4932 Loss 2.927648\n",
      "Epoch 4933 Loss 2.927646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4934 Loss 2.927646\n",
      "Epoch 4935 Loss 2.927649\n",
      "Epoch 4936 Loss 2.927647\n",
      "Epoch 4937 Loss 2.927647\n",
      "Epoch 4938 Loss 2.927647\n",
      "Epoch 4939 Loss 2.927647\n",
      "Epoch 4940 Loss 2.927647\n",
      "Epoch 4941 Loss 2.927646\n",
      "Epoch 4942 Loss 2.927649\n",
      "Epoch 4943 Loss 2.927647\n",
      "Epoch 4944 Loss 2.927648\n",
      "Epoch 4945 Loss 2.927647\n",
      "Epoch 4946 Loss 2.927649\n",
      "Epoch 4947 Loss 2.927646\n",
      "Epoch 4948 Loss 2.927649\n",
      "Epoch 4949 Loss 2.927648\n",
      "Epoch 4950 Loss 2.927647\n",
      "Epoch 4951 Loss 2.927648\n",
      "Epoch 4952 Loss 2.927647\n",
      "Epoch 4953 Loss 2.927647\n",
      "Epoch 4954 Loss 2.927647\n",
      "Epoch 4955 Loss 2.927648\n",
      "Epoch 4956 Loss 2.927646\n",
      "Epoch 4957 Loss 2.927648\n",
      "Epoch 4958 Loss 2.927647\n",
      "Epoch 4959 Loss 2.927647\n",
      "Epoch 4960 Loss 2.927647\n",
      "Epoch 4961 Loss 2.927648\n",
      "Epoch 4962 Loss 2.927647\n",
      "Epoch 4963 Loss 2.927648\n",
      "Epoch 4964 Loss 2.927648\n",
      "Epoch 4965 Loss 2.927648\n",
      "Epoch 4966 Loss 2.927648\n",
      "Epoch 4967 Loss 2.927648\n",
      "Epoch 4968 Loss 2.927647\n",
      "Epoch 4969 Loss 2.927647\n",
      "Epoch 4970 Loss 2.927646\n",
      "Epoch 4971 Loss 2.927647\n",
      "Epoch 4972 Loss 2.927647\n",
      "Epoch 4973 Loss 2.927647\n",
      "Epoch 4974 Loss 2.927647\n",
      "Epoch 4975 Loss 2.927647\n",
      "Epoch 4976 Loss 2.927647\n",
      "Epoch 4977 Loss 2.927648\n",
      "Epoch 4978 Loss 2.927647\n",
      "Epoch 4979 Loss 2.927648\n",
      "Epoch 4980 Loss 2.927647\n",
      "Epoch 4981 Loss 2.927648\n",
      "Epoch 4982 Loss 2.927648\n",
      "Epoch 4983 Loss 2.927646\n",
      "Epoch 4984 Loss 2.927648\n",
      "Epoch 4985 Loss 2.927647\n",
      "Epoch 4986 Loss 2.927648\n",
      "Epoch 4987 Loss 2.927648\n",
      "Epoch 4988 Loss 2.927648\n",
      "Epoch 4989 Loss 2.927646\n",
      "Epoch 4990 Loss 2.927648\n",
      "Epoch 4991 Loss 2.927647\n",
      "Epoch 4992 Loss 2.927647\n",
      "Epoch 4993 Loss 2.927647\n",
      "Epoch 4994 Loss 2.927648\n",
      "Epoch 4995 Loss 2.927647\n",
      "Epoch 4996 Loss 2.927646\n",
      "Epoch 4997 Loss 2.927647\n",
      "Epoch 4998 Loss 2.927648\n",
      "Epoch 4999 Loss 2.927647\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    t_p = model(t_u, *params)\n",
    "    loss = loss_fn(t_p, t_c)\n",
    "    print('Epoch %d Loss %f' % (epoch, float(loss)))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "t_p = model(t_u, *params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 80.364342\n",
      "Epoch 1 Loss 79.485870\n",
      "Epoch 2 Loss 78.615791\n",
      "Epoch 3 Loss 77.754211\n",
      "Epoch 4 Loss 76.901276\n",
      "Epoch 5 Loss 76.057137\n",
      "Epoch 6 Loss 75.221901\n",
      "Epoch 7 Loss 74.395691\n",
      "Epoch 8 Loss 73.578621\n",
      "Epoch 9 Loss 72.770805\n",
      "Epoch 10 Loss 71.972336\n",
      "Epoch 11 Loss 71.183327\n",
      "Epoch 12 Loss 70.403862\n",
      "Epoch 13 Loss 69.634026\n",
      "Epoch 14 Loss 68.873856\n",
      "Epoch 15 Loss 68.123489\n",
      "Epoch 16 Loss 67.382927\n",
      "Epoch 17 Loss 66.652237\n",
      "Epoch 18 Loss 65.931488\n",
      "Epoch 19 Loss 65.220711\n",
      "Epoch 20 Loss 64.519913\n",
      "Epoch 21 Loss 63.829163\n",
      "Epoch 22 Loss 63.148441\n",
      "Epoch 23 Loss 62.477783\n",
      "Epoch 24 Loss 61.817177\n",
      "Epoch 25 Loss 61.166626\n",
      "Epoch 26 Loss 60.526123\n",
      "Epoch 27 Loss 59.895653\n",
      "Epoch 28 Loss 59.275185\n",
      "Epoch 29 Loss 58.664711\n",
      "Epoch 30 Loss 58.064182\n",
      "Epoch 31 Loss 57.473587\n",
      "Epoch 32 Loss 56.892834\n",
      "Epoch 33 Loss 56.321915\n",
      "Epoch 34 Loss 55.760777\n",
      "Epoch 35 Loss 55.209347\n",
      "Epoch 36 Loss 54.667557\n",
      "Epoch 37 Loss 54.135365\n",
      "Epoch 38 Loss 53.612686\n",
      "Epoch 39 Loss 53.099449\n",
      "Epoch 40 Loss 52.595592\n",
      "Epoch 41 Loss 52.101013\n",
      "Epoch 42 Loss 51.615646\n",
      "Epoch 43 Loss 51.139381\n",
      "Epoch 44 Loss 50.672157\n",
      "Epoch 45 Loss 50.213890\n",
      "Epoch 46 Loss 49.764462\n",
      "Epoch 47 Loss 49.323795\n",
      "Epoch 48 Loss 48.891788\n",
      "Epoch 49 Loss 48.468346\n",
      "Epoch 50 Loss 48.053360\n",
      "Epoch 51 Loss 47.646740\n",
      "Epoch 52 Loss 47.248379\n",
      "Epoch 53 Loss 46.858170\n",
      "Epoch 54 Loss 46.476028\n",
      "Epoch 55 Loss 46.101810\n",
      "Epoch 56 Loss 45.735447\n",
      "Epoch 57 Loss 45.376808\n",
      "Epoch 58 Loss 45.025803\n",
      "Epoch 59 Loss 44.682312\n",
      "Epoch 60 Loss 44.346230\n",
      "Epoch 61 Loss 44.017456\n",
      "Epoch 62 Loss 43.695866\n",
      "Epoch 63 Loss 43.381359\n",
      "Epoch 64 Loss 43.073833\n",
      "Epoch 65 Loss 42.773167\n",
      "Epoch 66 Loss 42.479244\n",
      "Epoch 67 Loss 42.191986\n",
      "Epoch 68 Loss 41.911259\n",
      "Epoch 69 Loss 41.636967\n",
      "Epoch 70 Loss 41.368988\n",
      "Epoch 71 Loss 41.107227\n",
      "Epoch 72 Loss 40.851566\n",
      "Epoch 73 Loss 40.601913\n",
      "Epoch 74 Loss 40.358139\n",
      "Epoch 75 Loss 40.120155\n",
      "Epoch 76 Loss 39.887859\n",
      "Epoch 77 Loss 39.661129\n",
      "Epoch 78 Loss 39.439880\n",
      "Epoch 79 Loss 39.223999\n",
      "Epoch 80 Loss 39.013378\n",
      "Epoch 81 Loss 38.807922\n",
      "Epoch 82 Loss 38.607529\n",
      "Epoch 83 Loss 38.412102\n",
      "Epoch 84 Loss 38.221535\n",
      "Epoch 85 Loss 38.035736\n",
      "Epoch 86 Loss 37.854603\n",
      "Epoch 87 Loss 37.678036\n",
      "Epoch 88 Loss 37.505943\n",
      "Epoch 89 Loss 37.338230\n",
      "Epoch 90 Loss 37.174809\n",
      "Epoch 91 Loss 37.015568\n",
      "Epoch 92 Loss 36.860439\n",
      "Epoch 93 Loss 36.709312\n",
      "Epoch 94 Loss 36.562111\n",
      "Epoch 95 Loss 36.418736\n",
      "Epoch 96 Loss 36.279110\n",
      "Epoch 97 Loss 36.143131\n",
      "Epoch 98 Loss 36.010731\n",
      "Epoch 99 Loss 35.881824\n",
      "Epoch 100 Loss 35.756313\n",
      "Epoch 101 Loss 35.634132\n",
      "Epoch 102 Loss 35.515186\n",
      "Epoch 103 Loss 35.399403\n",
      "Epoch 104 Loss 35.286709\n",
      "Epoch 105 Loss 35.177017\n",
      "Epoch 106 Loss 35.070259\n",
      "Epoch 107 Loss 34.966354\n",
      "Epoch 108 Loss 34.865238\n",
      "Epoch 109 Loss 34.766830\n",
      "Epoch 110 Loss 34.671059\n",
      "Epoch 111 Loss 34.577854\n",
      "Epoch 112 Loss 34.487152\n",
      "Epoch 113 Loss 34.398884\n",
      "Epoch 114 Loss 34.312981\n",
      "Epoch 115 Loss 34.229374\n",
      "Epoch 116 Loss 34.148010\n",
      "Epoch 117 Loss 34.068817\n",
      "Epoch 118 Loss 33.991737\n",
      "Epoch 119 Loss 33.916710\n",
      "Epoch 120 Loss 33.843674\n",
      "Epoch 121 Loss 33.772579\n",
      "Epoch 122 Loss 33.703354\n",
      "Epoch 123 Loss 33.635956\n",
      "Epoch 124 Loss 33.570320\n",
      "Epoch 125 Loss 33.506413\n",
      "Epoch 126 Loss 33.444157\n",
      "Epoch 127 Loss 33.383518\n",
      "Epoch 128 Loss 33.324440\n",
      "Epoch 129 Loss 33.266876\n",
      "Epoch 130 Loss 33.210773\n",
      "Epoch 131 Loss 33.156097\n",
      "Epoch 132 Loss 33.102787\n",
      "Epoch 133 Loss 33.050816\n",
      "Epoch 134 Loss 33.000118\n",
      "Epoch 135 Loss 32.950672\n",
      "Epoch 136 Loss 32.902424\n",
      "Epoch 137 Loss 32.855343\n",
      "Epoch 138 Loss 32.809383\n",
      "Epoch 139 Loss 32.764507\n",
      "Epoch 140 Loss 32.720684\n",
      "Epoch 141 Loss 32.677864\n",
      "Epoch 142 Loss 32.636032\n",
      "Epoch 143 Loss 32.595135\n",
      "Epoch 144 Loss 32.555149\n",
      "Epoch 145 Loss 32.516045\n",
      "Epoch 146 Loss 32.477776\n",
      "Epoch 147 Loss 32.440327\n",
      "Epoch 148 Loss 32.403660\n",
      "Epoch 149 Loss 32.367748\n",
      "Epoch 150 Loss 32.332573\n",
      "Epoch 151 Loss 32.298092\n",
      "Epoch 152 Loss 32.264282\n",
      "Epoch 153 Loss 32.231125\n",
      "Epoch 154 Loss 32.198593\n",
      "Epoch 155 Loss 32.166660\n",
      "Epoch 156 Loss 32.135303\n",
      "Epoch 157 Loss 32.104496\n",
      "Epoch 158 Loss 32.074226\n",
      "Epoch 159 Loss 32.044468\n",
      "Epoch 160 Loss 32.015198\n",
      "Epoch 161 Loss 31.986395\n",
      "Epoch 162 Loss 31.958050\n",
      "Epoch 163 Loss 31.930126\n",
      "Epoch 164 Loss 31.902630\n",
      "Epoch 165 Loss 31.875519\n",
      "Epoch 166 Loss 31.848793\n",
      "Epoch 167 Loss 31.822435\n",
      "Epoch 168 Loss 31.796429\n",
      "Epoch 169 Loss 31.770746\n",
      "Epoch 170 Loss 31.745390\n",
      "Epoch 171 Loss 31.720335\n",
      "Epoch 172 Loss 31.695568\n",
      "Epoch 173 Loss 31.671082\n",
      "Epoch 174 Loss 31.646864\n",
      "Epoch 175 Loss 31.622900\n",
      "Epoch 176 Loss 31.599182\n",
      "Epoch 177 Loss 31.575689\n",
      "Epoch 178 Loss 31.552416\n",
      "Epoch 179 Loss 31.529356\n",
      "Epoch 180 Loss 31.506493\n",
      "Epoch 181 Loss 31.483820\n",
      "Epoch 182 Loss 31.461323\n",
      "Epoch 183 Loss 31.439003\n",
      "Epoch 184 Loss 31.416845\n",
      "Epoch 185 Loss 31.394842\n",
      "Epoch 186 Loss 31.372988\n",
      "Epoch 187 Loss 31.351274\n",
      "Epoch 188 Loss 31.329689\n",
      "Epoch 189 Loss 31.308233\n",
      "Epoch 190 Loss 31.286896\n",
      "Epoch 191 Loss 31.265669\n",
      "Epoch 192 Loss 31.244545\n",
      "Epoch 193 Loss 31.223528\n",
      "Epoch 194 Loss 31.202604\n",
      "Epoch 195 Loss 31.181768\n",
      "Epoch 196 Loss 31.161016\n",
      "Epoch 197 Loss 31.140345\n",
      "Epoch 198 Loss 31.119743\n",
      "Epoch 199 Loss 31.099218\n",
      "Epoch 200 Loss 31.078760\n",
      "Epoch 201 Loss 31.058357\n",
      "Epoch 202 Loss 31.038019\n",
      "Epoch 203 Loss 31.017731\n",
      "Epoch 204 Loss 30.997486\n",
      "Epoch 205 Loss 30.977297\n",
      "Epoch 206 Loss 30.957153\n",
      "Epoch 207 Loss 30.937050\n",
      "Epoch 208 Loss 30.916981\n",
      "Epoch 209 Loss 30.896950\n",
      "Epoch 210 Loss 30.876951\n",
      "Epoch 211 Loss 30.856977\n",
      "Epoch 212 Loss 30.837036\n",
      "Epoch 213 Loss 30.817116\n",
      "Epoch 214 Loss 30.797222\n",
      "Epoch 215 Loss 30.777344\n",
      "Epoch 216 Loss 30.757490\n",
      "Epoch 217 Loss 30.737654\n",
      "Epoch 218 Loss 30.717827\n",
      "Epoch 219 Loss 30.698015\n",
      "Epoch 220 Loss 30.678211\n",
      "Epoch 221 Loss 30.658428\n",
      "Epoch 222 Loss 30.638647\n",
      "Epoch 223 Loss 30.618874\n",
      "Epoch 224 Loss 30.599108\n",
      "Epoch 225 Loss 30.579346\n",
      "Epoch 226 Loss 30.559593\n",
      "Epoch 227 Loss 30.539833\n",
      "Epoch 228 Loss 30.520086\n",
      "Epoch 229 Loss 30.500330\n",
      "Epoch 230 Loss 30.480577\n",
      "Epoch 231 Loss 30.460821\n",
      "Epoch 232 Loss 30.441071\n",
      "Epoch 233 Loss 30.421309\n",
      "Epoch 234 Loss 30.401562\n",
      "Epoch 235 Loss 30.381786\n",
      "Epoch 236 Loss 30.362019\n",
      "Epoch 237 Loss 30.342247\n",
      "Epoch 238 Loss 30.322462\n",
      "Epoch 239 Loss 30.302675\n",
      "Epoch 240 Loss 30.282881\n",
      "Epoch 241 Loss 30.263086\n",
      "Epoch 242 Loss 30.243275\n",
      "Epoch 243 Loss 30.223455\n",
      "Epoch 244 Loss 30.203638\n",
      "Epoch 245 Loss 30.183802\n",
      "Epoch 246 Loss 30.163965\n",
      "Epoch 247 Loss 30.144112\n",
      "Epoch 248 Loss 30.124250\n",
      "Epoch 249 Loss 30.104383\n",
      "Epoch 250 Loss 30.084501\n",
      "Epoch 251 Loss 30.064611\n",
      "Epoch 252 Loss 30.044710\n",
      "Epoch 253 Loss 30.024809\n",
      "Epoch 254 Loss 30.004877\n",
      "Epoch 255 Loss 29.984947\n",
      "Epoch 256 Loss 29.965002\n",
      "Epoch 257 Loss 29.945051\n",
      "Epoch 258 Loss 29.925093\n",
      "Epoch 259 Loss 29.905109\n",
      "Epoch 260 Loss 29.885124\n",
      "Epoch 261 Loss 29.865120\n",
      "Epoch 262 Loss 29.845110\n",
      "Epoch 263 Loss 29.825090\n",
      "Epoch 264 Loss 29.805056\n",
      "Epoch 265 Loss 29.785011\n",
      "Epoch 266 Loss 29.764956\n",
      "Epoch 267 Loss 29.744890\n",
      "Epoch 268 Loss 29.724812\n",
      "Epoch 269 Loss 29.704720\n",
      "Epoch 270 Loss 29.684618\n",
      "Epoch 271 Loss 29.664507\n",
      "Epoch 272 Loss 29.644381\n",
      "Epoch 273 Loss 29.624250\n",
      "Epoch 274 Loss 29.604103\n",
      "Epoch 275 Loss 29.583942\n",
      "Epoch 276 Loss 29.563780\n",
      "Epoch 277 Loss 29.543598\n",
      "Epoch 278 Loss 29.523407\n",
      "Epoch 279 Loss 29.503206\n",
      "Epoch 280 Loss 29.482996\n",
      "Epoch 281 Loss 29.462769\n",
      "Epoch 282 Loss 29.442535\n",
      "Epoch 283 Loss 29.422291\n",
      "Epoch 284 Loss 29.402040\n",
      "Epoch 285 Loss 29.381773\n",
      "Epoch 286 Loss 29.361494\n",
      "Epoch 287 Loss 29.341206\n",
      "Epoch 288 Loss 29.320910\n",
      "Epoch 289 Loss 29.300604\n",
      "Epoch 290 Loss 29.280291\n",
      "Epoch 291 Loss 29.259954\n",
      "Epoch 292 Loss 29.239622\n",
      "Epoch 293 Loss 29.219271\n",
      "Epoch 294 Loss 29.198921\n",
      "Epoch 295 Loss 29.178551\n",
      "Epoch 296 Loss 29.158178\n",
      "Epoch 297 Loss 29.137785\n",
      "Epoch 298 Loss 29.117393\n",
      "Epoch 299 Loss 29.096991\n",
      "Epoch 300 Loss 29.076576\n",
      "Epoch 301 Loss 29.056152\n",
      "Epoch 302 Loss 29.035719\n",
      "Epoch 303 Loss 29.015278\n",
      "Epoch 304 Loss 28.994829\n",
      "Epoch 305 Loss 28.974377\n",
      "Epoch 306 Loss 28.953911\n",
      "Epoch 307 Loss 28.933435\n",
      "Epoch 308 Loss 28.912951\n",
      "Epoch 309 Loss 28.892458\n",
      "Epoch 310 Loss 28.871956\n",
      "Epoch 311 Loss 28.851452\n",
      "Epoch 312 Loss 28.830935\n",
      "Epoch 313 Loss 28.810408\n",
      "Epoch 314 Loss 28.789881\n",
      "Epoch 315 Loss 28.769342\n",
      "Epoch 316 Loss 28.748802\n",
      "Epoch 317 Loss 28.728243\n",
      "Epoch 318 Loss 28.707678\n",
      "Epoch 319 Loss 28.687109\n",
      "Epoch 320 Loss 28.666534\n",
      "Epoch 321 Loss 28.645952\n",
      "Epoch 322 Loss 28.625359\n",
      "Epoch 323 Loss 28.604765\n",
      "Epoch 324 Loss 28.584160\n",
      "Epoch 325 Loss 28.563549\n",
      "Epoch 326 Loss 28.542925\n",
      "Epoch 327 Loss 28.522306\n",
      "Epoch 328 Loss 28.501677\n",
      "Epoch 329 Loss 28.481041\n",
      "Epoch 330 Loss 28.460396\n",
      "Epoch 331 Loss 28.439753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 332 Loss 28.419090\n",
      "Epoch 333 Loss 28.398436\n",
      "Epoch 334 Loss 28.377764\n",
      "Epoch 335 Loss 28.357088\n",
      "Epoch 336 Loss 28.336414\n",
      "Epoch 337 Loss 28.315727\n",
      "Epoch 338 Loss 28.295036\n",
      "Epoch 339 Loss 28.274345\n",
      "Epoch 340 Loss 28.253643\n",
      "Epoch 341 Loss 28.232933\n",
      "Epoch 342 Loss 28.212221\n",
      "Epoch 343 Loss 28.191504\n",
      "Epoch 344 Loss 28.170784\n",
      "Epoch 345 Loss 28.150057\n",
      "Epoch 346 Loss 28.129328\n",
      "Epoch 347 Loss 28.108595\n",
      "Epoch 348 Loss 28.087860\n",
      "Epoch 349 Loss 28.067114\n",
      "Epoch 350 Loss 28.046364\n",
      "Epoch 351 Loss 28.025612\n",
      "Epoch 352 Loss 28.004850\n",
      "Epoch 353 Loss 27.984093\n",
      "Epoch 354 Loss 27.963329\n",
      "Epoch 355 Loss 27.942554\n",
      "Epoch 356 Loss 27.921785\n",
      "Epoch 357 Loss 27.901007\n",
      "Epoch 358 Loss 27.880220\n",
      "Epoch 359 Loss 27.859438\n",
      "Epoch 360 Loss 27.838652\n",
      "Epoch 361 Loss 27.817860\n",
      "Epoch 362 Loss 27.797064\n",
      "Epoch 363 Loss 27.776262\n",
      "Epoch 364 Loss 27.755463\n",
      "Epoch 365 Loss 27.734655\n",
      "Epoch 366 Loss 27.713848\n",
      "Epoch 367 Loss 27.693041\n",
      "Epoch 368 Loss 27.672228\n",
      "Epoch 369 Loss 27.651411\n",
      "Epoch 370 Loss 27.630587\n",
      "Epoch 371 Loss 27.609770\n",
      "Epoch 372 Loss 27.588942\n",
      "Epoch 373 Loss 27.568110\n",
      "Epoch 374 Loss 27.547289\n",
      "Epoch 375 Loss 27.526453\n",
      "Epoch 376 Loss 27.505617\n",
      "Epoch 377 Loss 27.484783\n",
      "Epoch 378 Loss 27.463945\n",
      "Epoch 379 Loss 27.443104\n",
      "Epoch 380 Loss 27.422264\n",
      "Epoch 381 Loss 27.401411\n",
      "Epoch 382 Loss 27.380568\n",
      "Epoch 383 Loss 27.359722\n",
      "Epoch 384 Loss 27.338869\n",
      "Epoch 385 Loss 27.318016\n",
      "Epoch 386 Loss 27.297163\n",
      "Epoch 387 Loss 27.276312\n",
      "Epoch 388 Loss 27.255451\n",
      "Epoch 389 Loss 27.234589\n",
      "Epoch 390 Loss 27.213734\n",
      "Epoch 391 Loss 27.192877\n",
      "Epoch 392 Loss 27.172014\n",
      "Epoch 393 Loss 27.151148\n",
      "Epoch 394 Loss 27.130287\n",
      "Epoch 395 Loss 27.109425\n",
      "Epoch 396 Loss 27.088560\n",
      "Epoch 397 Loss 27.067690\n",
      "Epoch 398 Loss 27.046831\n",
      "Epoch 399 Loss 27.025959\n",
      "Epoch 400 Loss 27.005091\n",
      "Epoch 401 Loss 26.984224\n",
      "Epoch 402 Loss 26.963350\n",
      "Epoch 403 Loss 26.942488\n",
      "Epoch 404 Loss 26.921614\n",
      "Epoch 405 Loss 26.900745\n",
      "Epoch 406 Loss 26.879881\n",
      "Epoch 407 Loss 26.859009\n",
      "Epoch 408 Loss 26.838137\n",
      "Epoch 409 Loss 26.817266\n",
      "Epoch 410 Loss 26.796392\n",
      "Epoch 411 Loss 26.775530\n",
      "Epoch 412 Loss 26.754656\n",
      "Epoch 413 Loss 26.733789\n",
      "Epoch 414 Loss 26.712919\n",
      "Epoch 415 Loss 26.692053\n",
      "Epoch 416 Loss 26.671181\n",
      "Epoch 417 Loss 26.650316\n",
      "Epoch 418 Loss 26.629450\n",
      "Epoch 419 Loss 26.608582\n",
      "Epoch 420 Loss 26.587719\n",
      "Epoch 421 Loss 26.566851\n",
      "Epoch 422 Loss 26.545984\n",
      "Epoch 423 Loss 26.525127\n",
      "Epoch 424 Loss 26.504267\n",
      "Epoch 425 Loss 26.483400\n",
      "Epoch 426 Loss 26.462543\n",
      "Epoch 427 Loss 26.441692\n",
      "Epoch 428 Loss 26.420830\n",
      "Epoch 429 Loss 26.399971\n",
      "Epoch 430 Loss 26.379118\n",
      "Epoch 431 Loss 26.358265\n",
      "Epoch 432 Loss 26.337414\n",
      "Epoch 433 Loss 26.316565\n",
      "Epoch 434 Loss 26.295717\n",
      "Epoch 435 Loss 26.274874\n",
      "Epoch 436 Loss 26.254023\n",
      "Epoch 437 Loss 26.233183\n",
      "Epoch 438 Loss 26.212339\n",
      "Epoch 439 Loss 26.191498\n",
      "Epoch 440 Loss 26.170654\n",
      "Epoch 441 Loss 26.149824\n",
      "Epoch 442 Loss 26.128990\n",
      "Epoch 443 Loss 26.108160\n",
      "Epoch 444 Loss 26.087328\n",
      "Epoch 445 Loss 26.066496\n",
      "Epoch 446 Loss 26.045671\n",
      "Epoch 447 Loss 26.024843\n",
      "Epoch 448 Loss 26.004034\n",
      "Epoch 449 Loss 25.983213\n",
      "Epoch 450 Loss 25.962395\n",
      "Epoch 451 Loss 25.941576\n",
      "Epoch 452 Loss 25.920769\n",
      "Epoch 453 Loss 25.899958\n",
      "Epoch 454 Loss 25.879162\n",
      "Epoch 455 Loss 25.858351\n",
      "Epoch 456 Loss 25.837555\n",
      "Epoch 457 Loss 25.816759\n",
      "Epoch 458 Loss 25.795959\n",
      "Epoch 459 Loss 25.775169\n",
      "Epoch 460 Loss 25.754377\n",
      "Epoch 461 Loss 25.733595\n",
      "Epoch 462 Loss 25.712812\n",
      "Epoch 463 Loss 25.692030\n",
      "Epoch 464 Loss 25.671257\n",
      "Epoch 465 Loss 25.650480\n",
      "Epoch 466 Loss 25.629711\n",
      "Epoch 467 Loss 25.608942\n",
      "Epoch 468 Loss 25.588184\n",
      "Epoch 469 Loss 25.567421\n",
      "Epoch 470 Loss 25.546669\n",
      "Epoch 471 Loss 25.525915\n",
      "Epoch 472 Loss 25.505165\n",
      "Epoch 473 Loss 25.484417\n",
      "Epoch 474 Loss 25.463678\n",
      "Epoch 475 Loss 25.442940\n",
      "Epoch 476 Loss 25.422209\n",
      "Epoch 477 Loss 25.401476\n",
      "Epoch 478 Loss 25.380751\n",
      "Epoch 479 Loss 25.360023\n",
      "Epoch 480 Loss 25.339312\n",
      "Epoch 481 Loss 25.318596\n",
      "Epoch 482 Loss 25.297880\n",
      "Epoch 483 Loss 25.277178\n",
      "Epoch 484 Loss 25.256477\n",
      "Epoch 485 Loss 25.235779\n",
      "Epoch 486 Loss 25.215080\n",
      "Epoch 487 Loss 25.194395\n",
      "Epoch 488 Loss 25.173704\n",
      "Epoch 489 Loss 25.153021\n",
      "Epoch 490 Loss 25.132349\n",
      "Epoch 491 Loss 25.111671\n",
      "Epoch 492 Loss 25.091009\n",
      "Epoch 493 Loss 25.070343\n",
      "Epoch 494 Loss 25.049685\n",
      "Epoch 495 Loss 25.029030\n",
      "Epoch 496 Loss 25.008379\n",
      "Epoch 497 Loss 24.987734\n",
      "Epoch 498 Loss 24.967100\n",
      "Epoch 499 Loss 24.946461\n",
      "Epoch 500 Loss 24.925835\n",
      "Epoch 501 Loss 24.905205\n",
      "Epoch 502 Loss 24.884584\n",
      "Epoch 503 Loss 24.863970\n",
      "Epoch 504 Loss 24.843355\n",
      "Epoch 505 Loss 24.822748\n",
      "Epoch 506 Loss 24.802153\n",
      "Epoch 507 Loss 24.781555\n",
      "Epoch 508 Loss 24.760967\n",
      "Epoch 509 Loss 24.740379\n",
      "Epoch 510 Loss 24.719801\n",
      "Epoch 511 Loss 24.699221\n",
      "Epoch 512 Loss 24.678652\n",
      "Epoch 513 Loss 24.658087\n",
      "Epoch 514 Loss 24.637531\n",
      "Epoch 515 Loss 24.616980\n",
      "Epoch 516 Loss 24.596430\n",
      "Epoch 517 Loss 24.575890\n",
      "Epoch 518 Loss 24.555353\n",
      "Epoch 519 Loss 24.534821\n",
      "Epoch 520 Loss 24.514294\n",
      "Epoch 521 Loss 24.493773\n",
      "Epoch 522 Loss 24.473261\n",
      "Epoch 523 Loss 24.452747\n",
      "Epoch 524 Loss 24.432251\n",
      "Epoch 525 Loss 24.411755\n",
      "Epoch 526 Loss 24.391260\n",
      "Epoch 527 Loss 24.370775\n",
      "Epoch 528 Loss 24.350294\n",
      "Epoch 529 Loss 24.329821\n",
      "Epoch 530 Loss 24.309357\n",
      "Epoch 531 Loss 24.288897\n",
      "Epoch 532 Loss 24.268438\n",
      "Epoch 533 Loss 24.247992\n",
      "Epoch 534 Loss 24.227552\n",
      "Epoch 535 Loss 24.207106\n",
      "Epoch 536 Loss 24.186682\n",
      "Epoch 537 Loss 24.166258\n",
      "Epoch 538 Loss 24.145838\n",
      "Epoch 539 Loss 24.125422\n",
      "Epoch 540 Loss 24.105024\n",
      "Epoch 541 Loss 24.084623\n",
      "Epoch 542 Loss 24.064234\n",
      "Epoch 543 Loss 24.043842\n",
      "Epoch 544 Loss 24.023460\n",
      "Epoch 545 Loss 24.003090\n",
      "Epoch 546 Loss 23.982721\n",
      "Epoch 547 Loss 23.962366\n",
      "Epoch 548 Loss 23.942009\n",
      "Epoch 549 Loss 23.921663\n",
      "Epoch 550 Loss 23.901320\n",
      "Epoch 551 Loss 23.880993\n",
      "Epoch 552 Loss 23.860664\n",
      "Epoch 553 Loss 23.840338\n",
      "Epoch 554 Loss 23.820030\n",
      "Epoch 555 Loss 23.799725\n",
      "Epoch 556 Loss 23.779427\n",
      "Epoch 557 Loss 23.759132\n",
      "Epoch 558 Loss 23.738848\n",
      "Epoch 559 Loss 23.718569\n",
      "Epoch 560 Loss 23.698301\n",
      "Epoch 561 Loss 23.678032\n",
      "Epoch 562 Loss 23.657778\n",
      "Epoch 563 Loss 23.637526\n",
      "Epoch 564 Loss 23.617285\n",
      "Epoch 565 Loss 23.597046\n",
      "Epoch 566 Loss 23.576815\n",
      "Epoch 567 Loss 23.556599\n",
      "Epoch 568 Loss 23.536383\n",
      "Epoch 569 Loss 23.516174\n",
      "Epoch 570 Loss 23.495977\n",
      "Epoch 571 Loss 23.475786\n",
      "Epoch 572 Loss 23.455603\n",
      "Epoch 573 Loss 23.435423\n",
      "Epoch 574 Loss 23.415255\n",
      "Epoch 575 Loss 23.395092\n",
      "Epoch 576 Loss 23.374939\n",
      "Epoch 577 Loss 23.354786\n",
      "Epoch 578 Loss 23.334648\n",
      "Epoch 579 Loss 23.314514\n",
      "Epoch 580 Loss 23.294392\n",
      "Epoch 581 Loss 23.274273\n",
      "Epoch 582 Loss 23.254166\n",
      "Epoch 583 Loss 23.234060\n",
      "Epoch 584 Loss 23.213970\n",
      "Epoch 585 Loss 23.193880\n",
      "Epoch 586 Loss 23.173800\n",
      "Epoch 587 Loss 23.153730\n",
      "Epoch 588 Loss 23.133665\n",
      "Epoch 589 Loss 23.113615\n",
      "Epoch 590 Loss 23.093565\n",
      "Epoch 591 Loss 23.073530\n",
      "Epoch 592 Loss 23.053495\n",
      "Epoch 593 Loss 23.033472\n",
      "Epoch 594 Loss 23.013456\n",
      "Epoch 595 Loss 22.993452\n",
      "Epoch 596 Loss 22.973452\n",
      "Epoch 597 Loss 22.953459\n",
      "Epoch 598 Loss 22.933477\n",
      "Epoch 599 Loss 22.913498\n",
      "Epoch 600 Loss 22.893536\n",
      "Epoch 601 Loss 22.873577\n",
      "Epoch 602 Loss 22.853624\n",
      "Epoch 603 Loss 22.833683\n",
      "Epoch 604 Loss 22.813745\n",
      "Epoch 605 Loss 22.793823\n",
      "Epoch 606 Loss 22.773909\n",
      "Epoch 607 Loss 22.754000\n",
      "Epoch 608 Loss 22.734098\n",
      "Epoch 609 Loss 22.714205\n",
      "Epoch 610 Loss 22.694323\n",
      "Epoch 611 Loss 22.674442\n",
      "Epoch 612 Loss 22.654575\n",
      "Epoch 613 Loss 22.634718\n",
      "Epoch 614 Loss 22.614868\n",
      "Epoch 615 Loss 22.595026\n",
      "Epoch 616 Loss 22.575193\n",
      "Epoch 617 Loss 22.555367\n",
      "Epoch 618 Loss 22.535555\n",
      "Epoch 619 Loss 22.515745\n",
      "Epoch 620 Loss 22.495943\n",
      "Epoch 621 Loss 22.476156\n",
      "Epoch 622 Loss 22.456373\n",
      "Epoch 623 Loss 22.436604\n",
      "Epoch 624 Loss 22.416842\n",
      "Epoch 625 Loss 22.397087\n",
      "Epoch 626 Loss 22.377337\n",
      "Epoch 627 Loss 22.357601\n",
      "Epoch 628 Loss 22.337868\n",
      "Epoch 629 Loss 22.318153\n",
      "Epoch 630 Loss 22.298437\n",
      "Epoch 631 Loss 22.278740\n",
      "Epoch 632 Loss 22.259045\n",
      "Epoch 633 Loss 22.239363\n",
      "Epoch 634 Loss 22.219685\n",
      "Epoch 635 Loss 22.200024\n",
      "Epoch 636 Loss 22.180361\n",
      "Epoch 637 Loss 22.160719\n",
      "Epoch 638 Loss 22.141075\n",
      "Epoch 639 Loss 22.121449\n",
      "Epoch 640 Loss 22.101824\n",
      "Epoch 641 Loss 22.082214\n",
      "Epoch 642 Loss 22.062609\n",
      "Epoch 643 Loss 22.043016\n",
      "Epoch 644 Loss 22.023430\n",
      "Epoch 645 Loss 22.003859\n",
      "Epoch 646 Loss 21.984287\n",
      "Epoch 647 Loss 21.964735\n",
      "Epoch 648 Loss 21.945189\n",
      "Epoch 649 Loss 21.925650\n",
      "Epoch 650 Loss 21.906118\n",
      "Epoch 651 Loss 21.886599\n",
      "Epoch 652 Loss 21.867088\n",
      "Epoch 653 Loss 21.847591\n",
      "Epoch 654 Loss 21.828098\n",
      "Epoch 655 Loss 21.808613\n",
      "Epoch 656 Loss 21.789146\n",
      "Epoch 657 Loss 21.769680\n",
      "Epoch 658 Loss 21.750223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 659 Loss 21.730778\n",
      "Epoch 660 Loss 21.711340\n",
      "Epoch 661 Loss 21.691917\n",
      "Epoch 662 Loss 21.672504\n",
      "Epoch 663 Loss 21.653097\n",
      "Epoch 664 Loss 21.633699\n",
      "Epoch 665 Loss 21.614313\n",
      "Epoch 666 Loss 21.594933\n",
      "Epoch 667 Loss 21.575562\n",
      "Epoch 668 Loss 21.556210\n",
      "Epoch 669 Loss 21.536861\n",
      "Epoch 670 Loss 21.517525\n",
      "Epoch 671 Loss 21.498196\n",
      "Epoch 672 Loss 21.478868\n",
      "Epoch 673 Loss 21.459557\n",
      "Epoch 674 Loss 21.440258\n",
      "Epoch 675 Loss 21.420973\n",
      "Epoch 676 Loss 21.401691\n",
      "Epoch 677 Loss 21.382418\n",
      "Epoch 678 Loss 21.363157\n",
      "Epoch 679 Loss 21.343910\n",
      "Epoch 680 Loss 21.324661\n",
      "Epoch 681 Loss 21.305433\n",
      "Epoch 682 Loss 21.286213\n",
      "Epoch 683 Loss 21.266996\n",
      "Epoch 684 Loss 21.247799\n",
      "Epoch 685 Loss 21.228605\n",
      "Epoch 686 Loss 21.209427\n",
      "Epoch 687 Loss 21.190250\n",
      "Epoch 688 Loss 21.171089\n",
      "Epoch 689 Loss 21.151939\n",
      "Epoch 690 Loss 21.132793\n",
      "Epoch 691 Loss 21.113665\n",
      "Epoch 692 Loss 21.094545\n",
      "Epoch 693 Loss 21.075434\n",
      "Epoch 694 Loss 21.056332\n",
      "Epoch 695 Loss 21.037239\n",
      "Epoch 696 Loss 21.018160\n",
      "Epoch 697 Loss 20.999084\n",
      "Epoch 698 Loss 20.980024\n",
      "Epoch 699 Loss 20.960974\n",
      "Epoch 700 Loss 20.941929\n",
      "Epoch 701 Loss 20.922911\n",
      "Epoch 702 Loss 20.903883\n",
      "Epoch 703 Loss 20.884872\n",
      "Epoch 704 Loss 20.865875\n",
      "Epoch 705 Loss 20.846886\n",
      "Epoch 706 Loss 20.827904\n",
      "Epoch 707 Loss 20.808937\n",
      "Epoch 708 Loss 20.789972\n",
      "Epoch 709 Loss 20.771029\n",
      "Epoch 710 Loss 20.752089\n",
      "Epoch 711 Loss 20.733158\n",
      "Epoch 712 Loss 20.714247\n",
      "Epoch 713 Loss 20.695341\n",
      "Epoch 714 Loss 20.676441\n",
      "Epoch 715 Loss 20.657558\n",
      "Epoch 716 Loss 20.638680\n",
      "Epoch 717 Loss 20.619814\n",
      "Epoch 718 Loss 20.600960\n",
      "Epoch 719 Loss 20.582115\n",
      "Epoch 720 Loss 20.563284\n",
      "Epoch 721 Loss 20.544456\n",
      "Epoch 722 Loss 20.525642\n",
      "Epoch 723 Loss 20.506844\n",
      "Epoch 724 Loss 20.488050\n",
      "Epoch 725 Loss 20.469267\n",
      "Epoch 726 Loss 20.450499\n",
      "Epoch 727 Loss 20.431738\n",
      "Epoch 728 Loss 20.412991\n",
      "Epoch 729 Loss 20.394251\n",
      "Epoch 730 Loss 20.375517\n",
      "Epoch 731 Loss 20.356802\n",
      "Epoch 732 Loss 20.338097\n",
      "Epoch 733 Loss 20.319401\n",
      "Epoch 734 Loss 20.300713\n",
      "Epoch 735 Loss 20.282038\n",
      "Epoch 736 Loss 20.263371\n",
      "Epoch 737 Loss 20.244722\n",
      "Epoch 738 Loss 20.226076\n",
      "Epoch 739 Loss 20.207447\n",
      "Epoch 740 Loss 20.188826\n",
      "Epoch 741 Loss 20.170210\n",
      "Epoch 742 Loss 20.151617\n",
      "Epoch 743 Loss 20.133026\n",
      "Epoch 744 Loss 20.114449\n",
      "Epoch 745 Loss 20.095881\n",
      "Epoch 746 Loss 20.077316\n",
      "Epoch 747 Loss 20.058775\n",
      "Epoch 748 Loss 20.040243\n",
      "Epoch 749 Loss 20.021713\n",
      "Epoch 750 Loss 20.003202\n",
      "Epoch 751 Loss 19.984697\n",
      "Epoch 752 Loss 19.966208\n",
      "Epoch 753 Loss 19.947723\n",
      "Epoch 754 Loss 19.929255\n",
      "Epoch 755 Loss 19.910795\n",
      "Epoch 756 Loss 19.892342\n",
      "Epoch 757 Loss 19.873907\n",
      "Epoch 758 Loss 19.855482\n",
      "Epoch 759 Loss 19.837072\n",
      "Epoch 760 Loss 19.818666\n",
      "Epoch 761 Loss 19.800272\n",
      "Epoch 762 Loss 19.781889\n",
      "Epoch 763 Loss 19.763519\n",
      "Epoch 764 Loss 19.745161\n",
      "Epoch 765 Loss 19.726805\n",
      "Epoch 766 Loss 19.708473\n",
      "Epoch 767 Loss 19.690142\n",
      "Epoch 768 Loss 19.671829\n",
      "Epoch 769 Loss 19.653522\n",
      "Epoch 770 Loss 19.635231\n",
      "Epoch 771 Loss 19.616945\n",
      "Epoch 772 Loss 19.598673\n",
      "Epoch 773 Loss 19.580418\n",
      "Epoch 774 Loss 19.562162\n",
      "Epoch 775 Loss 19.543928\n",
      "Epoch 776 Loss 19.525698\n",
      "Epoch 777 Loss 19.507484\n",
      "Epoch 778 Loss 19.489275\n",
      "Epoch 779 Loss 19.471085\n",
      "Epoch 780 Loss 19.452900\n",
      "Epoch 781 Loss 19.434731\n",
      "Epoch 782 Loss 19.416573\n",
      "Epoch 783 Loss 19.398420\n",
      "Epoch 784 Loss 19.380287\n",
      "Epoch 785 Loss 19.362154\n",
      "Epoch 786 Loss 19.344042\n",
      "Epoch 787 Loss 19.325939\n",
      "Epoch 788 Loss 19.307844\n",
      "Epoch 789 Loss 19.289764\n",
      "Epoch 790 Loss 19.271692\n",
      "Epoch 791 Loss 19.253637\n",
      "Epoch 792 Loss 19.235584\n",
      "Epoch 793 Loss 19.217554\n",
      "Epoch 794 Loss 19.199524\n",
      "Epoch 795 Loss 19.181513\n",
      "Epoch 796 Loss 19.163511\n",
      "Epoch 797 Loss 19.145521\n",
      "Epoch 798 Loss 19.127535\n",
      "Epoch 799 Loss 19.109575\n",
      "Epoch 800 Loss 19.091614\n",
      "Epoch 801 Loss 19.073671\n",
      "Epoch 802 Loss 19.055735\n",
      "Epoch 803 Loss 19.037811\n",
      "Epoch 804 Loss 19.019901\n",
      "Epoch 805 Loss 19.002003\n",
      "Epoch 806 Loss 18.984114\n",
      "Epoch 807 Loss 18.966236\n",
      "Epoch 808 Loss 18.948368\n",
      "Epoch 809 Loss 18.930513\n",
      "Epoch 810 Loss 18.912674\n",
      "Epoch 811 Loss 18.894842\n",
      "Epoch 812 Loss 18.877022\n",
      "Epoch 813 Loss 18.859211\n",
      "Epoch 814 Loss 18.841417\n",
      "Epoch 815 Loss 18.823629\n",
      "Epoch 816 Loss 18.805857\n",
      "Epoch 817 Loss 18.788095\n",
      "Epoch 818 Loss 18.770344\n",
      "Epoch 819 Loss 18.752607\n",
      "Epoch 820 Loss 18.734877\n",
      "Epoch 821 Loss 18.717161\n",
      "Epoch 822 Loss 18.699457\n",
      "Epoch 823 Loss 18.681765\n",
      "Epoch 824 Loss 18.664082\n",
      "Epoch 825 Loss 18.646410\n",
      "Epoch 826 Loss 18.628756\n",
      "Epoch 827 Loss 18.611107\n",
      "Epoch 828 Loss 18.593473\n",
      "Epoch 829 Loss 18.575848\n",
      "Epoch 830 Loss 18.558237\n",
      "Epoch 831 Loss 18.540636\n",
      "Epoch 832 Loss 18.523048\n",
      "Epoch 833 Loss 18.505470\n",
      "Epoch 834 Loss 18.487907\n",
      "Epoch 835 Loss 18.470350\n",
      "Epoch 836 Loss 18.452806\n",
      "Epoch 837 Loss 18.435276\n",
      "Epoch 838 Loss 18.417759\n",
      "Epoch 839 Loss 18.400255\n",
      "Epoch 840 Loss 18.382759\n",
      "Epoch 841 Loss 18.365273\n",
      "Epoch 842 Loss 18.347801\n",
      "Epoch 843 Loss 18.330339\n",
      "Epoch 844 Loss 18.312891\n",
      "Epoch 845 Loss 18.295454\n",
      "Epoch 846 Loss 18.278030\n",
      "Epoch 847 Loss 18.260618\n",
      "Epoch 848 Loss 18.243214\n",
      "Epoch 849 Loss 18.225824\n",
      "Epoch 850 Loss 18.208443\n",
      "Epoch 851 Loss 18.191072\n",
      "Epoch 852 Loss 18.173721\n",
      "Epoch 853 Loss 18.156380\n",
      "Epoch 854 Loss 18.139050\n",
      "Epoch 855 Loss 18.121729\n",
      "Epoch 856 Loss 18.104418\n",
      "Epoch 857 Loss 18.087126\n",
      "Epoch 858 Loss 18.069839\n",
      "Epoch 859 Loss 18.052567\n",
      "Epoch 860 Loss 18.035301\n",
      "Epoch 861 Loss 18.018055\n",
      "Epoch 862 Loss 18.000820\n",
      "Epoch 863 Loss 17.983595\n",
      "Epoch 864 Loss 17.966385\n",
      "Epoch 865 Loss 17.949179\n",
      "Epoch 866 Loss 17.931993\n",
      "Epoch 867 Loss 17.914812\n",
      "Epoch 868 Loss 17.897646\n",
      "Epoch 869 Loss 17.880493\n",
      "Epoch 870 Loss 17.863344\n",
      "Epoch 871 Loss 17.846220\n",
      "Epoch 872 Loss 17.829100\n",
      "Epoch 873 Loss 17.811996\n",
      "Epoch 874 Loss 17.794897\n",
      "Epoch 875 Loss 17.777815\n",
      "Epoch 876 Loss 17.760742\n",
      "Epoch 877 Loss 17.743685\n",
      "Epoch 878 Loss 17.726635\n",
      "Epoch 879 Loss 17.709599\n",
      "Epoch 880 Loss 17.692577\n",
      "Epoch 881 Loss 17.675566\n",
      "Epoch 882 Loss 17.658566\n",
      "Epoch 883 Loss 17.641575\n",
      "Epoch 884 Loss 17.624603\n",
      "Epoch 885 Loss 17.607637\n",
      "Epoch 886 Loss 17.590685\n",
      "Epoch 887 Loss 17.573744\n",
      "Epoch 888 Loss 17.556816\n",
      "Epoch 889 Loss 17.539900\n",
      "Epoch 890 Loss 17.522993\n",
      "Epoch 891 Loss 17.506104\n",
      "Epoch 892 Loss 17.489220\n",
      "Epoch 893 Loss 17.472353\n",
      "Epoch 894 Loss 17.455498\n",
      "Epoch 895 Loss 17.438656\n",
      "Epoch 896 Loss 17.421822\n",
      "Epoch 897 Loss 17.404997\n",
      "Epoch 898 Loss 17.388191\n",
      "Epoch 899 Loss 17.371397\n",
      "Epoch 900 Loss 17.354609\n",
      "Epoch 901 Loss 17.337837\n",
      "Epoch 902 Loss 17.321072\n",
      "Epoch 903 Loss 17.304331\n",
      "Epoch 904 Loss 17.287590\n",
      "Epoch 905 Loss 17.270863\n",
      "Epoch 906 Loss 17.254156\n",
      "Epoch 907 Loss 17.237450\n",
      "Epoch 908 Loss 17.220764\n",
      "Epoch 909 Loss 17.204086\n",
      "Epoch 910 Loss 17.187422\n",
      "Epoch 911 Loss 17.170765\n",
      "Epoch 912 Loss 17.154129\n",
      "Epoch 913 Loss 17.137499\n",
      "Epoch 914 Loss 17.120888\n",
      "Epoch 915 Loss 17.104280\n",
      "Epoch 916 Loss 17.087688\n",
      "Epoch 917 Loss 17.071102\n",
      "Epoch 918 Loss 17.054533\n",
      "Epoch 919 Loss 17.037981\n",
      "Epoch 920 Loss 17.021437\n",
      "Epoch 921 Loss 17.004902\n",
      "Epoch 922 Loss 16.988386\n",
      "Epoch 923 Loss 16.971876\n",
      "Epoch 924 Loss 16.955379\n",
      "Epoch 925 Loss 16.938898\n",
      "Epoch 926 Loss 16.922424\n",
      "Epoch 927 Loss 16.905966\n",
      "Epoch 928 Loss 16.889517\n",
      "Epoch 929 Loss 16.873083\n",
      "Epoch 930 Loss 16.856659\n",
      "Epoch 931 Loss 16.840250\n",
      "Epoch 932 Loss 16.823847\n",
      "Epoch 933 Loss 16.807463\n",
      "Epoch 934 Loss 16.791086\n",
      "Epoch 935 Loss 16.774721\n",
      "Epoch 936 Loss 16.758369\n",
      "Epoch 937 Loss 16.742029\n",
      "Epoch 938 Loss 16.725706\n",
      "Epoch 939 Loss 16.709389\n",
      "Epoch 940 Loss 16.693089\n",
      "Epoch 941 Loss 16.676800\n",
      "Epoch 942 Loss 16.660522\n",
      "Epoch 943 Loss 16.644255\n",
      "Epoch 944 Loss 16.628004\n",
      "Epoch 945 Loss 16.611759\n",
      "Epoch 946 Loss 16.595531\n",
      "Epoch 947 Loss 16.579313\n",
      "Epoch 948 Loss 16.563110\n",
      "Epoch 949 Loss 16.546913\n",
      "Epoch 950 Loss 16.530733\n",
      "Epoch 951 Loss 16.514565\n",
      "Epoch 952 Loss 16.498411\n",
      "Epoch 953 Loss 16.482262\n",
      "Epoch 954 Loss 16.466129\n",
      "Epoch 955 Loss 16.450010\n",
      "Epoch 956 Loss 16.433901\n",
      "Epoch 957 Loss 16.417809\n",
      "Epoch 958 Loss 16.401722\n",
      "Epoch 959 Loss 16.385649\n",
      "Epoch 960 Loss 16.369591\n",
      "Epoch 961 Loss 16.353544\n",
      "Epoch 962 Loss 16.337507\n",
      "Epoch 963 Loss 16.321484\n",
      "Epoch 964 Loss 16.305473\n",
      "Epoch 965 Loss 16.289473\n",
      "Epoch 966 Loss 16.273491\n",
      "Epoch 967 Loss 16.257509\n",
      "Epoch 968 Loss 16.241550\n",
      "Epoch 969 Loss 16.225603\n",
      "Epoch 970 Loss 16.209663\n",
      "Epoch 971 Loss 16.193735\n",
      "Epoch 972 Loss 16.177822\n",
      "Epoch 973 Loss 16.161922\n",
      "Epoch 974 Loss 16.146032\n",
      "Epoch 975 Loss 16.130157\n",
      "Epoch 976 Loss 16.114290\n",
      "Epoch 977 Loss 16.098438\n",
      "Epoch 978 Loss 16.082598\n",
      "Epoch 979 Loss 16.066771\n",
      "Epoch 980 Loss 16.050957\n",
      "Epoch 981 Loss 16.035151\n",
      "Epoch 982 Loss 16.019361\n",
      "Epoch 983 Loss 16.003584\n",
      "Epoch 984 Loss 15.987815\n",
      "Epoch 985 Loss 15.972058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 986 Loss 15.956317\n",
      "Epoch 987 Loss 15.940585\n",
      "Epoch 988 Loss 15.924871\n",
      "Epoch 989 Loss 15.909164\n",
      "Epoch 990 Loss 15.893467\n",
      "Epoch 991 Loss 15.877789\n",
      "Epoch 992 Loss 15.862118\n",
      "Epoch 993 Loss 15.846459\n",
      "Epoch 994 Loss 15.830816\n",
      "Epoch 995 Loss 15.815182\n",
      "Epoch 996 Loss 15.799563\n",
      "Epoch 997 Loss 15.783954\n",
      "Epoch 998 Loss 15.768360\n",
      "Epoch 999 Loss 15.752773\n",
      "Epoch 1000 Loss 15.737202\n",
      "Epoch 1001 Loss 15.721640\n",
      "Epoch 1002 Loss 15.706097\n",
      "Epoch 1003 Loss 15.690558\n",
      "Epoch 1004 Loss 15.675036\n",
      "Epoch 1005 Loss 15.659528\n",
      "Epoch 1006 Loss 15.644027\n",
      "Epoch 1007 Loss 15.628541\n",
      "Epoch 1008 Loss 15.613070\n",
      "Epoch 1009 Loss 15.597608\n",
      "Epoch 1010 Loss 15.582157\n",
      "Epoch 1011 Loss 15.566723\n",
      "Epoch 1012 Loss 15.551291\n",
      "Epoch 1013 Loss 15.535880\n",
      "Epoch 1014 Loss 15.520480\n",
      "Epoch 1015 Loss 15.505093\n",
      "Epoch 1016 Loss 15.489718\n",
      "Epoch 1017 Loss 15.474351\n",
      "Epoch 1018 Loss 15.459000\n",
      "Epoch 1019 Loss 15.443662\n",
      "Epoch 1020 Loss 15.428335\n",
      "Epoch 1021 Loss 15.413019\n",
      "Epoch 1022 Loss 15.397717\n",
      "Epoch 1023 Loss 15.382425\n",
      "Epoch 1024 Loss 15.367150\n",
      "Epoch 1025 Loss 15.351879\n",
      "Epoch 1026 Loss 15.336628\n",
      "Epoch 1027 Loss 15.321386\n",
      "Epoch 1028 Loss 15.306158\n",
      "Epoch 1029 Loss 15.290936\n",
      "Epoch 1030 Loss 15.275735\n",
      "Epoch 1031 Loss 15.260541\n",
      "Epoch 1032 Loss 15.245361\n",
      "Epoch 1033 Loss 15.230193\n",
      "Epoch 1034 Loss 15.215038\n",
      "Epoch 1035 Loss 15.199893\n",
      "Epoch 1036 Loss 15.184762\n",
      "Epoch 1037 Loss 15.169643\n",
      "Epoch 1038 Loss 15.154537\n",
      "Epoch 1039 Loss 15.139441\n",
      "Epoch 1040 Loss 15.124359\n",
      "Epoch 1041 Loss 15.109290\n",
      "Epoch 1042 Loss 15.094230\n",
      "Epoch 1043 Loss 15.079185\n",
      "Epoch 1044 Loss 15.064147\n",
      "Epoch 1045 Loss 15.049127\n",
      "Epoch 1046 Loss 15.034116\n",
      "Epoch 1047 Loss 15.019123\n",
      "Epoch 1048 Loss 15.004136\n",
      "Epoch 1049 Loss 14.989163\n",
      "Epoch 1050 Loss 14.974204\n",
      "Epoch 1051 Loss 14.959255\n",
      "Epoch 1052 Loss 14.944315\n",
      "Epoch 1053 Loss 14.929395\n",
      "Epoch 1054 Loss 14.914484\n",
      "Epoch 1055 Loss 14.899585\n",
      "Epoch 1056 Loss 14.884698\n",
      "Epoch 1057 Loss 14.869821\n",
      "Epoch 1058 Loss 14.854961\n",
      "Epoch 1059 Loss 14.840110\n",
      "Epoch 1060 Loss 14.825275\n",
      "Epoch 1061 Loss 14.810452\n",
      "Epoch 1062 Loss 14.795632\n",
      "Epoch 1063 Loss 14.780834\n",
      "Epoch 1064 Loss 14.766044\n",
      "Epoch 1065 Loss 14.751266\n",
      "Epoch 1066 Loss 14.736503\n",
      "Epoch 1067 Loss 14.721746\n",
      "Epoch 1068 Loss 14.707012\n",
      "Epoch 1069 Loss 14.692283\n",
      "Epoch 1070 Loss 14.677564\n",
      "Epoch 1071 Loss 14.662861\n",
      "Epoch 1072 Loss 14.648169\n",
      "Epoch 1073 Loss 14.633488\n",
      "Epoch 1074 Loss 14.618823\n",
      "Epoch 1075 Loss 14.604166\n",
      "Epoch 1076 Loss 14.589526\n",
      "Epoch 1077 Loss 14.574893\n",
      "Epoch 1078 Loss 14.560272\n",
      "Epoch 1079 Loss 14.545671\n",
      "Epoch 1080 Loss 14.531075\n",
      "Epoch 1081 Loss 14.516494\n",
      "Epoch 1082 Loss 14.501923\n",
      "Epoch 1083 Loss 14.487366\n",
      "Epoch 1084 Loss 14.472823\n",
      "Epoch 1085 Loss 14.458286\n",
      "Epoch 1086 Loss 14.443763\n",
      "Epoch 1087 Loss 14.429256\n",
      "Epoch 1088 Loss 14.414762\n",
      "Epoch 1089 Loss 14.400277\n",
      "Epoch 1090 Loss 14.385802\n",
      "Epoch 1091 Loss 14.371342\n",
      "Epoch 1092 Loss 14.356897\n",
      "Epoch 1093 Loss 14.342456\n",
      "Epoch 1094 Loss 14.328037\n",
      "Epoch 1095 Loss 14.313622\n",
      "Epoch 1096 Loss 14.299225\n",
      "Epoch 1097 Loss 14.284840\n",
      "Epoch 1098 Loss 14.270465\n",
      "Epoch 1099 Loss 14.256100\n",
      "Epoch 1100 Loss 14.241754\n",
      "Epoch 1101 Loss 14.227414\n",
      "Epoch 1102 Loss 14.213083\n",
      "Epoch 1103 Loss 14.198775\n",
      "Epoch 1104 Loss 14.184470\n",
      "Epoch 1105 Loss 14.170182\n",
      "Epoch 1106 Loss 14.155902\n",
      "Epoch 1107 Loss 14.141639\n",
      "Epoch 1108 Loss 14.127384\n",
      "Epoch 1109 Loss 14.113145\n",
      "Epoch 1110 Loss 14.098922\n",
      "Epoch 1111 Loss 14.084698\n",
      "Epoch 1112 Loss 14.070495\n",
      "Epoch 1113 Loss 14.056304\n",
      "Epoch 1114 Loss 14.042123\n",
      "Epoch 1115 Loss 14.027954\n",
      "Epoch 1116 Loss 14.013798\n",
      "Epoch 1117 Loss 13.999655\n",
      "Epoch 1118 Loss 13.985520\n",
      "Epoch 1119 Loss 13.971404\n",
      "Epoch 1120 Loss 13.957296\n",
      "Epoch 1121 Loss 13.943200\n",
      "Epoch 1122 Loss 13.929116\n",
      "Epoch 1123 Loss 13.915048\n",
      "Epoch 1124 Loss 13.900986\n",
      "Epoch 1125 Loss 13.886942\n",
      "Epoch 1126 Loss 13.872906\n",
      "Epoch 1127 Loss 13.858883\n",
      "Epoch 1128 Loss 13.844876\n",
      "Epoch 1129 Loss 13.830877\n",
      "Epoch 1130 Loss 13.816889\n",
      "Epoch 1131 Loss 13.802915\n",
      "Epoch 1132 Loss 13.788952\n",
      "Epoch 1133 Loss 13.775006\n",
      "Epoch 1134 Loss 13.761068\n",
      "Epoch 1135 Loss 13.747140\n",
      "Epoch 1136 Loss 13.733228\n",
      "Epoch 1137 Loss 13.719327\n",
      "Epoch 1138 Loss 13.705438\n",
      "Epoch 1139 Loss 13.691560\n",
      "Epoch 1140 Loss 13.677692\n",
      "Epoch 1141 Loss 13.663842\n",
      "Epoch 1142 Loss 13.650002\n",
      "Epoch 1143 Loss 13.636175\n",
      "Epoch 1144 Loss 13.622356\n",
      "Epoch 1145 Loss 13.608554\n",
      "Epoch 1146 Loss 13.594760\n",
      "Epoch 1147 Loss 13.580978\n",
      "Epoch 1148 Loss 13.567211\n",
      "Epoch 1149 Loss 13.553455\n",
      "Epoch 1150 Loss 13.539712\n",
      "Epoch 1151 Loss 13.525977\n",
      "Epoch 1152 Loss 13.512259\n",
      "Epoch 1153 Loss 13.498549\n",
      "Epoch 1154 Loss 13.484855\n",
      "Epoch 1155 Loss 13.471170\n",
      "Epoch 1156 Loss 13.457499\n",
      "Epoch 1157 Loss 13.443838\n",
      "Epoch 1158 Loss 13.430190\n",
      "Epoch 1159 Loss 13.416558\n",
      "Epoch 1160 Loss 13.402933\n",
      "Epoch 1161 Loss 13.389322\n",
      "Epoch 1162 Loss 13.375721\n",
      "Epoch 1163 Loss 13.362134\n",
      "Epoch 1164 Loss 13.348557\n",
      "Epoch 1165 Loss 13.334997\n",
      "Epoch 1166 Loss 13.321445\n",
      "Epoch 1167 Loss 13.307904\n",
      "Epoch 1168 Loss 13.294379\n",
      "Epoch 1169 Loss 13.280862\n",
      "Epoch 1170 Loss 13.267359\n",
      "Epoch 1171 Loss 13.253869\n",
      "Epoch 1172 Loss 13.240387\n",
      "Epoch 1173 Loss 13.226920\n",
      "Epoch 1174 Loss 13.213467\n",
      "Epoch 1175 Loss 13.200022\n",
      "Epoch 1176 Loss 13.186591\n",
      "Epoch 1177 Loss 13.173172\n",
      "Epoch 1178 Loss 13.159762\n",
      "Epoch 1179 Loss 13.146371\n",
      "Epoch 1180 Loss 13.132984\n",
      "Epoch 1181 Loss 13.119617\n",
      "Epoch 1182 Loss 13.106256\n",
      "Epoch 1183 Loss 13.092907\n",
      "Epoch 1184 Loss 13.079575\n",
      "Epoch 1185 Loss 13.066248\n",
      "Epoch 1186 Loss 13.052938\n",
      "Epoch 1187 Loss 13.039639\n",
      "Epoch 1188 Loss 13.026353\n",
      "Epoch 1189 Loss 13.013076\n",
      "Epoch 1190 Loss 12.999811\n",
      "Epoch 1191 Loss 12.986563\n",
      "Epoch 1192 Loss 12.973322\n",
      "Epoch 1193 Loss 12.960097\n",
      "Epoch 1194 Loss 12.946877\n",
      "Epoch 1195 Loss 12.933674\n",
      "Epoch 1196 Loss 12.920486\n",
      "Epoch 1197 Loss 12.907306\n",
      "Epoch 1198 Loss 12.894137\n",
      "Epoch 1199 Loss 12.880980\n",
      "Epoch 1200 Loss 12.867841\n",
      "Epoch 1201 Loss 12.854705\n",
      "Epoch 1202 Loss 12.841584\n",
      "Epoch 1203 Loss 12.828476\n",
      "Epoch 1204 Loss 12.815381\n",
      "Epoch 1205 Loss 12.802296\n",
      "Epoch 1206 Loss 12.789225\n",
      "Epoch 1207 Loss 12.776166\n",
      "Epoch 1208 Loss 12.763116\n",
      "Epoch 1209 Loss 12.750079\n",
      "Epoch 1210 Loss 12.737054\n",
      "Epoch 1211 Loss 12.724041\n",
      "Epoch 1212 Loss 12.711043\n",
      "Epoch 1213 Loss 12.698052\n",
      "Epoch 1214 Loss 12.685071\n",
      "Epoch 1215 Loss 12.672108\n",
      "Epoch 1216 Loss 12.659155\n",
      "Epoch 1217 Loss 12.646215\n",
      "Epoch 1218 Loss 12.633286\n",
      "Epoch 1219 Loss 12.620367\n",
      "Epoch 1220 Loss 12.607461\n",
      "Epoch 1221 Loss 12.594567\n",
      "Epoch 1222 Loss 12.581684\n",
      "Epoch 1223 Loss 12.568811\n",
      "Epoch 1224 Loss 12.555955\n",
      "Epoch 1225 Loss 12.543109\n",
      "Epoch 1226 Loss 12.530274\n",
      "Epoch 1227 Loss 12.517450\n",
      "Epoch 1228 Loss 12.504640\n",
      "Epoch 1229 Loss 12.491845\n",
      "Epoch 1230 Loss 12.479053\n",
      "Epoch 1231 Loss 12.466279\n",
      "Epoch 1232 Loss 12.453518\n",
      "Epoch 1233 Loss 12.440763\n",
      "Epoch 1234 Loss 12.428023\n",
      "Epoch 1235 Loss 12.415295\n",
      "Epoch 1236 Loss 12.402578\n",
      "Epoch 1237 Loss 12.389872\n",
      "Epoch 1238 Loss 12.377181\n",
      "Epoch 1239 Loss 12.364498\n",
      "Epoch 1240 Loss 12.351830\n",
      "Epoch 1241 Loss 12.339171\n",
      "Epoch 1242 Loss 12.326527\n",
      "Epoch 1243 Loss 12.313892\n",
      "Epoch 1244 Loss 12.301272\n",
      "Epoch 1245 Loss 12.288660\n",
      "Epoch 1246 Loss 12.276059\n",
      "Epoch 1247 Loss 12.263474\n",
      "Epoch 1248 Loss 12.250899\n",
      "Epoch 1249 Loss 12.238337\n",
      "Epoch 1250 Loss 12.225785\n",
      "Epoch 1251 Loss 12.213247\n",
      "Epoch 1252 Loss 12.200718\n",
      "Epoch 1253 Loss 12.188199\n",
      "Epoch 1254 Loss 12.175695\n",
      "Epoch 1255 Loss 12.163204\n",
      "Epoch 1256 Loss 12.150723\n",
      "Epoch 1257 Loss 12.138253\n",
      "Epoch 1258 Loss 12.125794\n",
      "Epoch 1259 Loss 12.113349\n",
      "Epoch 1260 Loss 12.100918\n",
      "Epoch 1261 Loss 12.088492\n",
      "Epoch 1262 Loss 12.076081\n",
      "Epoch 1263 Loss 12.063684\n",
      "Epoch 1264 Loss 12.051294\n",
      "Epoch 1265 Loss 12.038922\n",
      "Epoch 1266 Loss 12.026560\n",
      "Epoch 1267 Loss 12.014206\n",
      "Epoch 1268 Loss 12.001865\n",
      "Epoch 1269 Loss 11.989535\n",
      "Epoch 1270 Loss 11.977218\n",
      "Epoch 1271 Loss 11.964914\n",
      "Epoch 1272 Loss 11.952621\n",
      "Epoch 1273 Loss 11.940339\n",
      "Epoch 1274 Loss 11.928067\n",
      "Epoch 1275 Loss 11.915809\n",
      "Epoch 1276 Loss 11.903562\n",
      "Epoch 1277 Loss 11.891327\n",
      "Epoch 1278 Loss 11.879102\n",
      "Epoch 1279 Loss 11.866889\n",
      "Epoch 1280 Loss 11.854692\n",
      "Epoch 1281 Loss 11.842502\n",
      "Epoch 1282 Loss 11.830322\n",
      "Epoch 1283 Loss 11.818160\n",
      "Epoch 1284 Loss 11.806004\n",
      "Epoch 1285 Loss 11.793863\n",
      "Epoch 1286 Loss 11.781730\n",
      "Epoch 1287 Loss 11.769614\n",
      "Epoch 1288 Loss 11.757506\n",
      "Epoch 1289 Loss 11.745410\n",
      "Epoch 1290 Loss 11.733326\n",
      "Epoch 1291 Loss 11.721253\n",
      "Epoch 1292 Loss 11.709194\n",
      "Epoch 1293 Loss 11.697141\n",
      "Epoch 1294 Loss 11.685104\n",
      "Epoch 1295 Loss 11.673077\n",
      "Epoch 1296 Loss 11.661063\n",
      "Epoch 1297 Loss 11.649056\n",
      "Epoch 1298 Loss 11.637067\n",
      "Epoch 1299 Loss 11.625085\n",
      "Epoch 1300 Loss 11.613116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1301 Loss 11.601162\n",
      "Epoch 1302 Loss 11.589216\n",
      "Epoch 1303 Loss 11.577283\n",
      "Epoch 1304 Loss 11.565360\n",
      "Epoch 1305 Loss 11.553451\n",
      "Epoch 1306 Loss 11.541551\n",
      "Epoch 1307 Loss 11.529666\n",
      "Epoch 1308 Loss 11.517788\n",
      "Epoch 1309 Loss 11.505924\n",
      "Epoch 1310 Loss 11.494068\n",
      "Epoch 1311 Loss 11.482227\n",
      "Epoch 1312 Loss 11.470398\n",
      "Epoch 1313 Loss 11.458580\n",
      "Epoch 1314 Loss 11.446775\n",
      "Epoch 1315 Loss 11.434976\n",
      "Epoch 1316 Loss 11.423194\n",
      "Epoch 1317 Loss 11.411420\n",
      "Epoch 1318 Loss 11.399660\n",
      "Epoch 1319 Loss 11.387910\n",
      "Epoch 1320 Loss 11.376173\n",
      "Epoch 1321 Loss 11.364447\n",
      "Epoch 1322 Loss 11.352732\n",
      "Epoch 1323 Loss 11.341026\n",
      "Epoch 1324 Loss 11.329335\n",
      "Epoch 1325 Loss 11.317651\n",
      "Epoch 1326 Loss 11.305984\n",
      "Epoch 1327 Loss 11.294329\n",
      "Epoch 1328 Loss 11.282678\n",
      "Epoch 1329 Loss 11.271046\n",
      "Epoch 1330 Loss 11.259421\n",
      "Epoch 1331 Loss 11.247808\n",
      "Epoch 1332 Loss 11.236207\n",
      "Epoch 1333 Loss 11.224615\n",
      "Epoch 1334 Loss 11.213038\n",
      "Epoch 1335 Loss 11.201472\n",
      "Epoch 1336 Loss 11.189917\n",
      "Epoch 1337 Loss 11.178374\n",
      "Epoch 1338 Loss 11.166842\n",
      "Epoch 1339 Loss 11.155322\n",
      "Epoch 1340 Loss 11.143813\n",
      "Epoch 1341 Loss 11.132315\n",
      "Epoch 1342 Loss 11.120825\n",
      "Epoch 1343 Loss 11.109351\n",
      "Epoch 1344 Loss 11.097886\n",
      "Epoch 1345 Loss 11.086434\n",
      "Epoch 1346 Loss 11.074993\n",
      "Epoch 1347 Loss 11.063560\n",
      "Epoch 1348 Loss 11.052143\n",
      "Epoch 1349 Loss 11.040735\n",
      "Epoch 1350 Loss 11.029341\n",
      "Epoch 1351 Loss 11.017956\n",
      "Epoch 1352 Loss 11.006582\n",
      "Epoch 1353 Loss 10.995217\n",
      "Epoch 1354 Loss 10.983870\n",
      "Epoch 1355 Loss 10.972530\n",
      "Epoch 1356 Loss 10.961201\n",
      "Epoch 1357 Loss 10.949883\n",
      "Epoch 1358 Loss 10.938577\n",
      "Epoch 1359 Loss 10.927285\n",
      "Epoch 1360 Loss 10.916000\n",
      "Epoch 1361 Loss 10.904731\n",
      "Epoch 1362 Loss 10.893467\n",
      "Epoch 1363 Loss 10.882221\n",
      "Epoch 1364 Loss 10.870982\n",
      "Epoch 1365 Loss 10.859756\n",
      "Epoch 1366 Loss 10.848537\n",
      "Epoch 1367 Loss 10.837335\n",
      "Epoch 1368 Loss 10.826140\n",
      "Epoch 1369 Loss 10.814960\n",
      "Epoch 1370 Loss 10.803793\n",
      "Epoch 1371 Loss 10.792629\n",
      "Epoch 1372 Loss 10.781480\n",
      "Epoch 1373 Loss 10.770347\n",
      "Epoch 1374 Loss 10.759217\n",
      "Epoch 1375 Loss 10.748107\n",
      "Epoch 1376 Loss 10.737004\n",
      "Epoch 1377 Loss 10.725913\n",
      "Epoch 1378 Loss 10.714829\n",
      "Epoch 1379 Loss 10.703761\n",
      "Epoch 1380 Loss 10.692701\n",
      "Epoch 1381 Loss 10.681653\n",
      "Epoch 1382 Loss 10.670620\n",
      "Epoch 1383 Loss 10.659595\n",
      "Epoch 1384 Loss 10.648580\n",
      "Epoch 1385 Loss 10.637577\n",
      "Epoch 1386 Loss 10.626587\n",
      "Epoch 1387 Loss 10.615607\n",
      "Epoch 1388 Loss 10.604638\n",
      "Epoch 1389 Loss 10.593680\n",
      "Epoch 1390 Loss 10.582731\n",
      "Epoch 1391 Loss 10.571797\n",
      "Epoch 1392 Loss 10.560873\n",
      "Epoch 1393 Loss 10.549958\n",
      "Epoch 1394 Loss 10.539060\n",
      "Epoch 1395 Loss 10.528165\n",
      "Epoch 1396 Loss 10.517286\n",
      "Epoch 1397 Loss 10.506420\n",
      "Epoch 1398 Loss 10.495557\n",
      "Epoch 1399 Loss 10.484710\n",
      "Epoch 1400 Loss 10.473876\n",
      "Epoch 1401 Loss 10.463051\n",
      "Epoch 1402 Loss 10.452240\n",
      "Epoch 1403 Loss 10.441435\n",
      "Epoch 1404 Loss 10.430642\n",
      "Epoch 1405 Loss 10.419867\n",
      "Epoch 1406 Loss 10.409095\n",
      "Epoch 1407 Loss 10.398335\n",
      "Epoch 1408 Loss 10.387589\n",
      "Epoch 1409 Loss 10.376852\n",
      "Epoch 1410 Loss 10.366126\n",
      "Epoch 1411 Loss 10.355414\n",
      "Epoch 1412 Loss 10.344710\n",
      "Epoch 1413 Loss 10.334017\n",
      "Epoch 1414 Loss 10.323338\n",
      "Epoch 1415 Loss 10.312666\n",
      "Epoch 1416 Loss 10.302007\n",
      "Epoch 1417 Loss 10.291360\n",
      "Epoch 1418 Loss 10.280724\n",
      "Epoch 1419 Loss 10.270098\n",
      "Epoch 1420 Loss 10.259482\n",
      "Epoch 1421 Loss 10.248878\n",
      "Epoch 1422 Loss 10.238283\n",
      "Epoch 1423 Loss 10.227700\n",
      "Epoch 1424 Loss 10.217132\n",
      "Epoch 1425 Loss 10.206573\n",
      "Epoch 1426 Loss 10.196024\n",
      "Epoch 1427 Loss 10.185485\n",
      "Epoch 1428 Loss 10.174956\n",
      "Epoch 1429 Loss 10.164440\n",
      "Epoch 1430 Loss 10.153934\n",
      "Epoch 1431 Loss 10.143441\n",
      "Epoch 1432 Loss 10.132957\n",
      "Epoch 1433 Loss 10.122485\n",
      "Epoch 1434 Loss 10.112021\n",
      "Epoch 1435 Loss 10.101571\n",
      "Epoch 1436 Loss 10.091132\n",
      "Epoch 1437 Loss 10.080704\n",
      "Epoch 1438 Loss 10.070285\n",
      "Epoch 1439 Loss 10.059875\n",
      "Epoch 1440 Loss 10.049480\n",
      "Epoch 1441 Loss 10.039094\n",
      "Epoch 1442 Loss 10.028720\n",
      "Epoch 1443 Loss 10.018357\n",
      "Epoch 1444 Loss 10.008002\n",
      "Epoch 1445 Loss 9.997662\n",
      "Epoch 1446 Loss 9.987329\n",
      "Epoch 1447 Loss 9.977010\n",
      "Epoch 1448 Loss 9.966702\n",
      "Epoch 1449 Loss 9.956405\n",
      "Epoch 1450 Loss 9.946116\n",
      "Epoch 1451 Loss 9.935840\n",
      "Epoch 1452 Loss 9.925570\n",
      "Epoch 1453 Loss 9.915315\n",
      "Epoch 1454 Loss 9.905069\n",
      "Epoch 1455 Loss 9.894837\n",
      "Epoch 1456 Loss 9.884613\n",
      "Epoch 1457 Loss 9.874400\n",
      "Epoch 1458 Loss 9.864204\n",
      "Epoch 1459 Loss 9.854011\n",
      "Epoch 1460 Loss 9.843829\n",
      "Epoch 1461 Loss 9.833659\n",
      "Epoch 1462 Loss 9.823502\n",
      "Epoch 1463 Loss 9.813354\n",
      "Epoch 1464 Loss 9.803217\n",
      "Epoch 1465 Loss 9.793094\n",
      "Epoch 1466 Loss 9.782977\n",
      "Epoch 1467 Loss 9.772871\n",
      "Epoch 1468 Loss 9.762776\n",
      "Epoch 1469 Loss 9.752693\n",
      "Epoch 1470 Loss 9.742620\n",
      "Epoch 1471 Loss 9.732559\n",
      "Epoch 1472 Loss 9.722507\n",
      "Epoch 1473 Loss 9.712467\n",
      "Epoch 1474 Loss 9.702437\n",
      "Epoch 1475 Loss 9.692417\n",
      "Epoch 1476 Loss 9.682410\n",
      "Epoch 1477 Loss 9.672410\n",
      "Epoch 1478 Loss 9.662425\n",
      "Epoch 1479 Loss 9.652449\n",
      "Epoch 1480 Loss 9.642486\n",
      "Epoch 1481 Loss 9.632528\n",
      "Epoch 1482 Loss 9.622587\n",
      "Epoch 1483 Loss 9.612652\n",
      "Epoch 1484 Loss 9.602728\n",
      "Epoch 1485 Loss 9.592815\n",
      "Epoch 1486 Loss 9.582914\n",
      "Epoch 1487 Loss 9.573025\n",
      "Epoch 1488 Loss 9.563142\n",
      "Epoch 1489 Loss 9.553270\n",
      "Epoch 1490 Loss 9.543410\n",
      "Epoch 1491 Loss 9.533564\n",
      "Epoch 1492 Loss 9.523726\n",
      "Epoch 1493 Loss 9.513895\n",
      "Epoch 1494 Loss 9.504078\n",
      "Epoch 1495 Loss 9.494271\n",
      "Epoch 1496 Loss 9.484477\n",
      "Epoch 1497 Loss 9.474692\n",
      "Epoch 1498 Loss 9.464917\n",
      "Epoch 1499 Loss 9.455153\n",
      "Epoch 1500 Loss 9.445397\n",
      "Epoch 1501 Loss 9.435655\n",
      "Epoch 1502 Loss 9.425922\n",
      "Epoch 1503 Loss 9.416203\n",
      "Epoch 1504 Loss 9.406488\n",
      "Epoch 1505 Loss 9.396790\n",
      "Epoch 1506 Loss 9.387098\n",
      "Epoch 1507 Loss 9.377418\n",
      "Epoch 1508 Loss 9.367748\n",
      "Epoch 1509 Loss 9.358089\n",
      "Epoch 1510 Loss 9.348439\n",
      "Epoch 1511 Loss 9.338804\n",
      "Epoch 1512 Loss 9.329175\n",
      "Epoch 1513 Loss 9.319559\n",
      "Epoch 1514 Loss 9.309950\n",
      "Epoch 1515 Loss 9.300355\n",
      "Epoch 1516 Loss 9.290770\n",
      "Epoch 1517 Loss 9.281193\n",
      "Epoch 1518 Loss 9.271629\n",
      "Epoch 1519 Loss 9.262074\n",
      "Epoch 1520 Loss 9.252530\n",
      "Epoch 1521 Loss 9.242996\n",
      "Epoch 1522 Loss 9.233474\n",
      "Epoch 1523 Loss 9.223963\n",
      "Epoch 1524 Loss 9.214458\n",
      "Epoch 1525 Loss 9.204965\n",
      "Epoch 1526 Loss 9.195487\n",
      "Epoch 1527 Loss 9.186015\n",
      "Epoch 1528 Loss 9.176554\n",
      "Epoch 1529 Loss 9.167101\n",
      "Epoch 1530 Loss 9.157665\n",
      "Epoch 1531 Loss 9.148232\n",
      "Epoch 1532 Loss 9.138816\n",
      "Epoch 1533 Loss 9.129404\n",
      "Epoch 1534 Loss 9.120006\n",
      "Epoch 1535 Loss 9.110620\n",
      "Epoch 1536 Loss 9.101242\n",
      "Epoch 1537 Loss 9.091873\n",
      "Epoch 1538 Loss 9.082517\n",
      "Epoch 1539 Loss 9.073167\n",
      "Epoch 1540 Loss 9.063834\n",
      "Epoch 1541 Loss 9.054505\n",
      "Epoch 1542 Loss 9.045190\n",
      "Epoch 1543 Loss 9.035882\n",
      "Epoch 1544 Loss 9.026592\n",
      "Epoch 1545 Loss 9.017304\n",
      "Epoch 1546 Loss 9.008032\n",
      "Epoch 1547 Loss 8.998765\n",
      "Epoch 1548 Loss 8.989512\n",
      "Epoch 1549 Loss 8.980265\n",
      "Epoch 1550 Loss 8.971036\n",
      "Epoch 1551 Loss 8.961811\n",
      "Epoch 1552 Loss 8.952598\n",
      "Epoch 1553 Loss 8.943394\n",
      "Epoch 1554 Loss 8.934203\n",
      "Epoch 1555 Loss 8.925019\n",
      "Epoch 1556 Loss 8.915847\n",
      "Epoch 1557 Loss 8.906684\n",
      "Epoch 1558 Loss 8.897530\n",
      "Epoch 1559 Loss 8.888391\n",
      "Epoch 1560 Loss 8.879259\n",
      "Epoch 1561 Loss 8.870140\n",
      "Epoch 1562 Loss 8.861028\n",
      "Epoch 1563 Loss 8.851928\n",
      "Epoch 1564 Loss 8.842836\n",
      "Epoch 1565 Loss 8.833756\n",
      "Epoch 1566 Loss 8.824688\n",
      "Epoch 1567 Loss 8.815628\n",
      "Epoch 1568 Loss 8.806576\n",
      "Epoch 1569 Loss 8.797536\n",
      "Epoch 1570 Loss 8.788507\n",
      "Epoch 1571 Loss 8.779489\n",
      "Epoch 1572 Loss 8.770478\n",
      "Epoch 1573 Loss 8.761479\n",
      "Epoch 1574 Loss 8.752488\n",
      "Epoch 1575 Loss 8.743506\n",
      "Epoch 1576 Loss 8.734540\n",
      "Epoch 1577 Loss 8.725582\n",
      "Epoch 1578 Loss 8.716634\n",
      "Epoch 1579 Loss 8.707692\n",
      "Epoch 1580 Loss 8.698765\n",
      "Epoch 1581 Loss 8.689846\n",
      "Epoch 1582 Loss 8.680935\n",
      "Epoch 1583 Loss 8.672038\n",
      "Epoch 1584 Loss 8.663151\n",
      "Epoch 1585 Loss 8.654273\n",
      "Epoch 1586 Loss 8.645406\n",
      "Epoch 1587 Loss 8.636547\n",
      "Epoch 1588 Loss 8.627698\n",
      "Epoch 1589 Loss 8.618862\n",
      "Epoch 1590 Loss 8.610028\n",
      "Epoch 1591 Loss 8.601210\n",
      "Epoch 1592 Loss 8.592406\n",
      "Epoch 1593 Loss 8.583605\n",
      "Epoch 1594 Loss 8.574818\n",
      "Epoch 1595 Loss 8.566040\n",
      "Epoch 1596 Loss 8.557273\n",
      "Epoch 1597 Loss 8.548515\n",
      "Epoch 1598 Loss 8.539762\n",
      "Epoch 1599 Loss 8.531028\n",
      "Epoch 1600 Loss 8.522300\n",
      "Epoch 1601 Loss 8.513579\n",
      "Epoch 1602 Loss 8.504871\n",
      "Epoch 1603 Loss 8.496171\n",
      "Epoch 1604 Loss 8.487485\n",
      "Epoch 1605 Loss 8.478806\n",
      "Epoch 1606 Loss 8.470138\n",
      "Epoch 1607 Loss 8.461480\n",
      "Epoch 1608 Loss 8.452829\n",
      "Epoch 1609 Loss 8.444192\n",
      "Epoch 1610 Loss 8.435563\n",
      "Epoch 1611 Loss 8.426942\n",
      "Epoch 1612 Loss 8.418336\n",
      "Epoch 1613 Loss 8.409733\n",
      "Epoch 1614 Loss 8.401145\n",
      "Epoch 1615 Loss 8.392564\n",
      "Epoch 1616 Loss 8.383999\n",
      "Epoch 1617 Loss 8.375438\n",
      "Epoch 1618 Loss 8.366886\n",
      "Epoch 1619 Loss 8.358346\n",
      "Epoch 1620 Loss 8.349814\n",
      "Epoch 1621 Loss 8.341294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1622 Loss 8.332785\n",
      "Epoch 1623 Loss 8.324283\n",
      "Epoch 1624 Loss 8.315795\n",
      "Epoch 1625 Loss 8.307309\n",
      "Epoch 1626 Loss 8.298840\n",
      "Epoch 1627 Loss 8.290379\n",
      "Epoch 1628 Loss 8.281925\n",
      "Epoch 1629 Loss 8.273484\n",
      "Epoch 1630 Loss 8.265052\n",
      "Epoch 1631 Loss 8.256628\n",
      "Epoch 1632 Loss 8.248217\n",
      "Epoch 1633 Loss 8.239814\n",
      "Epoch 1634 Loss 8.231423\n",
      "Epoch 1635 Loss 8.223037\n",
      "Epoch 1636 Loss 8.214664\n",
      "Epoch 1637 Loss 8.206300\n",
      "Epoch 1638 Loss 8.197946\n",
      "Epoch 1639 Loss 8.189597\n",
      "Epoch 1640 Loss 8.181265\n",
      "Epoch 1641 Loss 8.172937\n",
      "Epoch 1642 Loss 8.164623\n",
      "Epoch 1643 Loss 8.156320\n",
      "Epoch 1644 Loss 8.148023\n",
      "Epoch 1645 Loss 8.139738\n",
      "Epoch 1646 Loss 8.131463\n",
      "Epoch 1647 Loss 8.123196\n",
      "Epoch 1648 Loss 8.114936\n",
      "Epoch 1649 Loss 8.106686\n",
      "Epoch 1650 Loss 8.098451\n",
      "Epoch 1651 Loss 8.090221\n",
      "Epoch 1652 Loss 8.082002\n",
      "Epoch 1653 Loss 8.073793\n",
      "Epoch 1654 Loss 8.065595\n",
      "Epoch 1655 Loss 8.057407\n",
      "Epoch 1656 Loss 8.049225\n",
      "Epoch 1657 Loss 8.041054\n",
      "Epoch 1658 Loss 8.032894\n",
      "Epoch 1659 Loss 8.024739\n",
      "Epoch 1660 Loss 8.016598\n",
      "Epoch 1661 Loss 8.008465\n",
      "Epoch 1662 Loss 8.000341\n",
      "Epoch 1663 Loss 7.992230\n",
      "Epoch 1664 Loss 7.984125\n",
      "Epoch 1665 Loss 7.976032\n",
      "Epoch 1666 Loss 7.967949\n",
      "Epoch 1667 Loss 7.959873\n",
      "Epoch 1668 Loss 7.951808\n",
      "Epoch 1669 Loss 7.943753\n",
      "Epoch 1670 Loss 7.935707\n",
      "Epoch 1671 Loss 7.927669\n",
      "Epoch 1672 Loss 7.919642\n",
      "Epoch 1673 Loss 7.911622\n",
      "Epoch 1674 Loss 7.903615\n",
      "Epoch 1675 Loss 7.895614\n",
      "Epoch 1676 Loss 7.887626\n",
      "Epoch 1677 Loss 7.879648\n",
      "Epoch 1678 Loss 7.871677\n",
      "Epoch 1679 Loss 7.863716\n",
      "Epoch 1680 Loss 7.855764\n",
      "Epoch 1681 Loss 7.847826\n",
      "Epoch 1682 Loss 7.839891\n",
      "Epoch 1683 Loss 7.831967\n",
      "Epoch 1684 Loss 7.824057\n",
      "Epoch 1685 Loss 7.816151\n",
      "Epoch 1686 Loss 7.808255\n",
      "Epoch 1687 Loss 7.800374\n",
      "Epoch 1688 Loss 7.792494\n",
      "Epoch 1689 Loss 7.784629\n",
      "Epoch 1690 Loss 7.776775\n",
      "Epoch 1691 Loss 7.768922\n",
      "Epoch 1692 Loss 7.761087\n",
      "Epoch 1693 Loss 7.753256\n",
      "Epoch 1694 Loss 7.745438\n",
      "Epoch 1695 Loss 7.737628\n",
      "Epoch 1696 Loss 7.729828\n",
      "Epoch 1697 Loss 7.722038\n",
      "Epoch 1698 Loss 7.714254\n",
      "Epoch 1699 Loss 7.706481\n",
      "Epoch 1700 Loss 7.698718\n",
      "Epoch 1701 Loss 7.690962\n",
      "Epoch 1702 Loss 7.683218\n",
      "Epoch 1703 Loss 7.675484\n",
      "Epoch 1704 Loss 7.667757\n",
      "Epoch 1705 Loss 7.660040\n",
      "Epoch 1706 Loss 7.652333\n",
      "Epoch 1707 Loss 7.644635\n",
      "Epoch 1708 Loss 7.636946\n",
      "Epoch 1709 Loss 7.629270\n",
      "Epoch 1710 Loss 7.621597\n",
      "Epoch 1711 Loss 7.613935\n",
      "Epoch 1712 Loss 7.606284\n",
      "Epoch 1713 Loss 7.598640\n",
      "Epoch 1714 Loss 7.591006\n",
      "Epoch 1715 Loss 7.583384\n",
      "Epoch 1716 Loss 7.575768\n",
      "Epoch 1717 Loss 7.568164\n",
      "Epoch 1718 Loss 7.560565\n",
      "Epoch 1719 Loss 7.552979\n",
      "Epoch 1720 Loss 7.545402\n",
      "Epoch 1721 Loss 7.537834\n",
      "Epoch 1722 Loss 7.530275\n",
      "Epoch 1723 Loss 7.522724\n",
      "Epoch 1724 Loss 7.515182\n",
      "Epoch 1725 Loss 7.507649\n",
      "Epoch 1726 Loss 7.500125\n",
      "Epoch 1727 Loss 7.492617\n",
      "Epoch 1728 Loss 7.485111\n",
      "Epoch 1729 Loss 7.477615\n",
      "Epoch 1730 Loss 7.470125\n",
      "Epoch 1731 Loss 7.462651\n",
      "Epoch 1732 Loss 7.455185\n",
      "Epoch 1733 Loss 7.447724\n",
      "Epoch 1734 Loss 7.440276\n",
      "Epoch 1735 Loss 7.432837\n",
      "Epoch 1736 Loss 7.425407\n",
      "Epoch 1737 Loss 7.417984\n",
      "Epoch 1738 Loss 7.410571\n",
      "Epoch 1739 Loss 7.403163\n",
      "Epoch 1740 Loss 7.395772\n",
      "Epoch 1741 Loss 7.388384\n",
      "Epoch 1742 Loss 7.381009\n",
      "Epoch 1743 Loss 7.373639\n",
      "Epoch 1744 Loss 7.366280\n",
      "Epoch 1745 Loss 7.358931\n",
      "Epoch 1746 Loss 7.351590\n",
      "Epoch 1747 Loss 7.344258\n",
      "Epoch 1748 Loss 7.336935\n",
      "Epoch 1749 Loss 7.329621\n",
      "Epoch 1750 Loss 7.322320\n",
      "Epoch 1751 Loss 7.315023\n",
      "Epoch 1752 Loss 7.307733\n",
      "Epoch 1753 Loss 7.300459\n",
      "Epoch 1754 Loss 7.293188\n",
      "Epoch 1755 Loss 7.285933\n",
      "Epoch 1756 Loss 7.278682\n",
      "Epoch 1757 Loss 7.271438\n",
      "Epoch 1758 Loss 7.264205\n",
      "Epoch 1759 Loss 7.256981\n",
      "Epoch 1760 Loss 7.249764\n",
      "Epoch 1761 Loss 7.242561\n",
      "Epoch 1762 Loss 7.235364\n",
      "Epoch 1763 Loss 7.228177\n",
      "Epoch 1764 Loss 7.220999\n",
      "Epoch 1765 Loss 7.213829\n",
      "Epoch 1766 Loss 7.206668\n",
      "Epoch 1767 Loss 7.199513\n",
      "Epoch 1768 Loss 7.192372\n",
      "Epoch 1769 Loss 7.185241\n",
      "Epoch 1770 Loss 7.178113\n",
      "Epoch 1771 Loss 7.170995\n",
      "Epoch 1772 Loss 7.163889\n",
      "Epoch 1773 Loss 7.156788\n",
      "Epoch 1774 Loss 7.149703\n",
      "Epoch 1775 Loss 7.142620\n",
      "Epoch 1776 Loss 7.135546\n",
      "Epoch 1777 Loss 7.128487\n",
      "Epoch 1778 Loss 7.121428\n",
      "Epoch 1779 Loss 7.114382\n",
      "Epoch 1780 Loss 7.107350\n",
      "Epoch 1781 Loss 7.100319\n",
      "Epoch 1782 Loss 7.093298\n",
      "Epoch 1783 Loss 7.086288\n",
      "Epoch 1784 Loss 7.079285\n",
      "Epoch 1785 Loss 7.072297\n",
      "Epoch 1786 Loss 7.065310\n",
      "Epoch 1787 Loss 7.058336\n",
      "Epoch 1788 Loss 7.051369\n",
      "Epoch 1789 Loss 7.044414\n",
      "Epoch 1790 Loss 7.037462\n",
      "Epoch 1791 Loss 7.030525\n",
      "Epoch 1792 Loss 7.023593\n",
      "Epoch 1793 Loss 7.016669\n",
      "Epoch 1794 Loss 7.009757\n",
      "Epoch 1795 Loss 7.002852\n",
      "Epoch 1796 Loss 6.995955\n",
      "Epoch 1797 Loss 6.989067\n",
      "Epoch 1798 Loss 6.982190\n",
      "Epoch 1799 Loss 6.975320\n",
      "Epoch 1800 Loss 6.968460\n",
      "Epoch 1801 Loss 6.961605\n",
      "Epoch 1802 Loss 6.954761\n",
      "Epoch 1803 Loss 6.947925\n",
      "Epoch 1804 Loss 6.941099\n",
      "Epoch 1805 Loss 6.934278\n",
      "Epoch 1806 Loss 6.927470\n",
      "Epoch 1807 Loss 6.920670\n",
      "Epoch 1808 Loss 6.913877\n",
      "Epoch 1809 Loss 6.907095\n",
      "Epoch 1810 Loss 6.900321\n",
      "Epoch 1811 Loss 6.893555\n",
      "Epoch 1812 Loss 6.886795\n",
      "Epoch 1813 Loss 6.880045\n",
      "Epoch 1814 Loss 6.873305\n",
      "Epoch 1815 Loss 6.866574\n",
      "Epoch 1816 Loss 6.859851\n",
      "Epoch 1817 Loss 6.853134\n",
      "Epoch 1818 Loss 6.846430\n",
      "Epoch 1819 Loss 6.839734\n",
      "Epoch 1820 Loss 6.833046\n",
      "Epoch 1821 Loss 6.826363\n",
      "Epoch 1822 Loss 6.819695\n",
      "Epoch 1823 Loss 6.813028\n",
      "Epoch 1824 Loss 6.806374\n",
      "Epoch 1825 Loss 6.799731\n",
      "Epoch 1826 Loss 6.793092\n",
      "Epoch 1827 Loss 6.786464\n",
      "Epoch 1828 Loss 6.779845\n",
      "Epoch 1829 Loss 6.773228\n",
      "Epoch 1830 Loss 6.766628\n",
      "Epoch 1831 Loss 6.760031\n",
      "Epoch 1832 Loss 6.753447\n",
      "Epoch 1833 Loss 6.746866\n",
      "Epoch 1834 Loss 6.740298\n",
      "Epoch 1835 Loss 6.733736\n",
      "Epoch 1836 Loss 6.727185\n",
      "Epoch 1837 Loss 6.720640\n",
      "Epoch 1838 Loss 6.714104\n",
      "Epoch 1839 Loss 6.707576\n",
      "Epoch 1840 Loss 6.701061\n",
      "Epoch 1841 Loss 6.694548\n",
      "Epoch 1842 Loss 6.688047\n",
      "Epoch 1843 Loss 6.681555\n",
      "Epoch 1844 Loss 6.675065\n",
      "Epoch 1845 Loss 6.668591\n",
      "Epoch 1846 Loss 6.662125\n",
      "Epoch 1847 Loss 6.655663\n",
      "Epoch 1848 Loss 6.649213\n",
      "Epoch 1849 Loss 6.642772\n",
      "Epoch 1850 Loss 6.636338\n",
      "Epoch 1851 Loss 6.629908\n",
      "Epoch 1852 Loss 6.623490\n",
      "Epoch 1853 Loss 6.617080\n",
      "Epoch 1854 Loss 6.610678\n",
      "Epoch 1855 Loss 6.604284\n",
      "Epoch 1856 Loss 6.597902\n",
      "Epoch 1857 Loss 6.591527\n",
      "Epoch 1858 Loss 6.585156\n",
      "Epoch 1859 Loss 6.578800\n",
      "Epoch 1860 Loss 6.572449\n",
      "Epoch 1861 Loss 6.566104\n",
      "Epoch 1862 Loss 6.559770\n",
      "Epoch 1863 Loss 6.553442\n",
      "Epoch 1864 Loss 6.547125\n",
      "Epoch 1865 Loss 6.540818\n",
      "Epoch 1866 Loss 6.534516\n",
      "Epoch 1867 Loss 6.528223\n",
      "Epoch 1868 Loss 6.521934\n",
      "Epoch 1869 Loss 6.515659\n",
      "Epoch 1870 Loss 6.509390\n",
      "Epoch 1871 Loss 6.503129\n",
      "Epoch 1872 Loss 6.496878\n",
      "Epoch 1873 Loss 6.490634\n",
      "Epoch 1874 Loss 6.484401\n",
      "Epoch 1875 Loss 6.478174\n",
      "Epoch 1876 Loss 6.471954\n",
      "Epoch 1877 Loss 6.465743\n",
      "Epoch 1878 Loss 6.459541\n",
      "Epoch 1879 Loss 6.453345\n",
      "Epoch 1880 Loss 6.447160\n",
      "Epoch 1881 Loss 6.440979\n",
      "Epoch 1882 Loss 6.434811\n",
      "Epoch 1883 Loss 6.428644\n",
      "Epoch 1884 Loss 6.422493\n",
      "Epoch 1885 Loss 6.416351\n",
      "Epoch 1886 Loss 6.410210\n",
      "Epoch 1887 Loss 6.404082\n",
      "Epoch 1888 Loss 6.397963\n",
      "Epoch 1889 Loss 6.391851\n",
      "Epoch 1890 Loss 6.385745\n",
      "Epoch 1891 Loss 6.379646\n",
      "Epoch 1892 Loss 6.373558\n",
      "Epoch 1893 Loss 6.367479\n",
      "Epoch 1894 Loss 6.361406\n",
      "Epoch 1895 Loss 6.355343\n",
      "Epoch 1896 Loss 6.349286\n",
      "Epoch 1897 Loss 6.343239\n",
      "Epoch 1898 Loss 6.337198\n",
      "Epoch 1899 Loss 6.331166\n",
      "Epoch 1900 Loss 6.325144\n",
      "Epoch 1901 Loss 6.319128\n",
      "Epoch 1902 Loss 6.313118\n",
      "Epoch 1903 Loss 6.307119\n",
      "Epoch 1904 Loss 6.301126\n",
      "Epoch 1905 Loss 6.295143\n",
      "Epoch 1906 Loss 6.289165\n",
      "Epoch 1907 Loss 6.283197\n",
      "Epoch 1908 Loss 6.277237\n",
      "Epoch 1909 Loss 6.271286\n",
      "Epoch 1910 Loss 6.265343\n",
      "Epoch 1911 Loss 6.259407\n",
      "Epoch 1912 Loss 6.253479\n",
      "Epoch 1913 Loss 6.247559\n",
      "Epoch 1914 Loss 6.241646\n",
      "Epoch 1915 Loss 6.235741\n",
      "Epoch 1916 Loss 6.229846\n",
      "Epoch 1917 Loss 6.223958\n",
      "Epoch 1918 Loss 6.218076\n",
      "Epoch 1919 Loss 6.212205\n",
      "Epoch 1920 Loss 6.206343\n",
      "Epoch 1921 Loss 6.200487\n",
      "Epoch 1922 Loss 6.194636\n",
      "Epoch 1923 Loss 6.188797\n",
      "Epoch 1924 Loss 6.182965\n",
      "Epoch 1925 Loss 6.177140\n",
      "Epoch 1926 Loss 6.171321\n",
      "Epoch 1927 Loss 6.165515\n",
      "Epoch 1928 Loss 6.159712\n",
      "Epoch 1929 Loss 6.153917\n",
      "Epoch 1930 Loss 6.148134\n",
      "Epoch 1931 Loss 6.142356\n",
      "Epoch 1932 Loss 6.136584\n",
      "Epoch 1933 Loss 6.130823\n",
      "Epoch 1934 Loss 6.125070\n",
      "Epoch 1935 Loss 6.119323\n",
      "Epoch 1936 Loss 6.113586\n",
      "Epoch 1937 Loss 6.107855\n",
      "Epoch 1938 Loss 6.102132\n",
      "Epoch 1939 Loss 6.096417\n",
      "Epoch 1940 Loss 6.090709\n",
      "Epoch 1941 Loss 6.085011\n",
      "Epoch 1942 Loss 6.079317\n",
      "Epoch 1943 Loss 6.073636\n",
      "Epoch 1944 Loss 6.067961\n",
      "Epoch 1945 Loss 6.062292\n",
      "Epoch 1946 Loss 6.056631\n",
      "Epoch 1947 Loss 6.050976\n",
      "Epoch 1948 Loss 6.045331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1949 Loss 6.039694\n",
      "Epoch 1950 Loss 6.034064\n",
      "Epoch 1951 Loss 6.028440\n",
      "Epoch 1952 Loss 6.022829\n",
      "Epoch 1953 Loss 6.017223\n",
      "Epoch 1954 Loss 6.011623\n",
      "Epoch 1955 Loss 6.006033\n",
      "Epoch 1956 Loss 6.000447\n",
      "Epoch 1957 Loss 5.994872\n",
      "Epoch 1958 Loss 5.989305\n",
      "Epoch 1959 Loss 5.983744\n",
      "Epoch 1960 Loss 5.978192\n",
      "Epoch 1961 Loss 5.972646\n",
      "Epoch 1962 Loss 5.967111\n",
      "Epoch 1963 Loss 5.961580\n",
      "Epoch 1964 Loss 5.956057\n",
      "Epoch 1965 Loss 5.950543\n",
      "Epoch 1966 Loss 5.945036\n",
      "Epoch 1967 Loss 5.939539\n",
      "Epoch 1968 Loss 5.934044\n",
      "Epoch 1969 Loss 5.928561\n",
      "Epoch 1970 Loss 5.923082\n",
      "Epoch 1971 Loss 5.917616\n",
      "Epoch 1972 Loss 5.912152\n",
      "Epoch 1973 Loss 5.906697\n",
      "Epoch 1974 Loss 5.901255\n",
      "Epoch 1975 Loss 5.895815\n",
      "Epoch 1976 Loss 5.890382\n",
      "Epoch 1977 Loss 5.884960\n",
      "Epoch 1978 Loss 5.879544\n",
      "Epoch 1979 Loss 5.874136\n",
      "Epoch 1980 Loss 5.868733\n",
      "Epoch 1981 Loss 5.863342\n",
      "Epoch 1982 Loss 5.857956\n",
      "Epoch 1983 Loss 5.852579\n",
      "Epoch 1984 Loss 5.847207\n",
      "Epoch 1985 Loss 5.841846\n",
      "Epoch 1986 Loss 5.836490\n",
      "Epoch 1987 Loss 5.831139\n",
      "Epoch 1988 Loss 5.825799\n",
      "Epoch 1989 Loss 5.820465\n",
      "Epoch 1990 Loss 5.815142\n",
      "Epoch 1991 Loss 5.809822\n",
      "Epoch 1992 Loss 5.804511\n",
      "Epoch 1993 Loss 5.799207\n",
      "Epoch 1994 Loss 5.793911\n",
      "Epoch 1995 Loss 5.788624\n",
      "Epoch 1996 Loss 5.783340\n",
      "Epoch 1997 Loss 5.778068\n",
      "Epoch 1998 Loss 5.772801\n",
      "Epoch 1999 Loss 5.767542\n",
      "Epoch 2000 Loss 5.762289\n",
      "Epoch 2001 Loss 5.757049\n",
      "Epoch 2002 Loss 5.751811\n",
      "Epoch 2003 Loss 5.746581\n",
      "Epoch 2004 Loss 5.741362\n",
      "Epoch 2005 Loss 5.736144\n",
      "Epoch 2006 Loss 5.730938\n",
      "Epoch 2007 Loss 5.725739\n",
      "Epoch 2008 Loss 5.720544\n",
      "Epoch 2009 Loss 5.715363\n",
      "Epoch 2010 Loss 5.710182\n",
      "Epoch 2011 Loss 5.705013\n",
      "Epoch 2012 Loss 5.699848\n",
      "Epoch 2013 Loss 5.694691\n",
      "Epoch 2014 Loss 5.689544\n",
      "Epoch 2015 Loss 5.684402\n",
      "Epoch 2016 Loss 5.679270\n",
      "Epoch 2017 Loss 5.674143\n",
      "Epoch 2018 Loss 5.669025\n",
      "Epoch 2019 Loss 5.663910\n",
      "Epoch 2020 Loss 5.658805\n",
      "Epoch 2021 Loss 5.653709\n",
      "Epoch 2022 Loss 5.648619\n",
      "Epoch 2023 Loss 5.643533\n",
      "Epoch 2024 Loss 5.638457\n",
      "Epoch 2025 Loss 5.633389\n",
      "Epoch 2026 Loss 5.628330\n",
      "Epoch 2027 Loss 5.623274\n",
      "Epoch 2028 Loss 5.618228\n",
      "Epoch 2029 Loss 5.613189\n",
      "Epoch 2030 Loss 5.608156\n",
      "Epoch 2031 Loss 5.603131\n",
      "Epoch 2032 Loss 5.598114\n",
      "Epoch 2033 Loss 5.593102\n",
      "Epoch 2034 Loss 5.588099\n",
      "Epoch 2035 Loss 5.583101\n",
      "Epoch 2036 Loss 5.578113\n",
      "Epoch 2037 Loss 5.573131\n",
      "Epoch 2038 Loss 5.568155\n",
      "Epoch 2039 Loss 5.563185\n",
      "Epoch 2040 Loss 5.558226\n",
      "Epoch 2041 Loss 5.553273\n",
      "Epoch 2042 Loss 5.548326\n",
      "Epoch 2043 Loss 5.543387\n",
      "Epoch 2044 Loss 5.538453\n",
      "Epoch 2045 Loss 5.533530\n",
      "Epoch 2046 Loss 5.528610\n",
      "Epoch 2047 Loss 5.523699\n",
      "Epoch 2048 Loss 5.518796\n",
      "Epoch 2049 Loss 5.513896\n",
      "Epoch 2050 Loss 5.509007\n",
      "Epoch 2051 Loss 5.504127\n",
      "Epoch 2052 Loss 5.499250\n",
      "Epoch 2053 Loss 5.494383\n",
      "Epoch 2054 Loss 5.489520\n",
      "Epoch 2055 Loss 5.484667\n",
      "Epoch 2056 Loss 5.479818\n",
      "Epoch 2057 Loss 5.474979\n",
      "Epoch 2058 Loss 5.470143\n",
      "Epoch 2059 Loss 5.465319\n",
      "Epoch 2060 Loss 5.460501\n",
      "Epoch 2061 Loss 5.455688\n",
      "Epoch 2062 Loss 5.450883\n",
      "Epoch 2063 Loss 5.446085\n",
      "Epoch 2064 Loss 5.441292\n",
      "Epoch 2065 Loss 5.436508\n",
      "Epoch 2066 Loss 5.431732\n",
      "Epoch 2067 Loss 5.426962\n",
      "Epoch 2068 Loss 5.422199\n",
      "Epoch 2069 Loss 5.417442\n",
      "Epoch 2070 Loss 5.412691\n",
      "Epoch 2071 Loss 5.407949\n",
      "Epoch 2072 Loss 5.403214\n",
      "Epoch 2073 Loss 5.398484\n",
      "Epoch 2074 Loss 5.393762\n",
      "Epoch 2075 Loss 5.389050\n",
      "Epoch 2076 Loss 5.384342\n",
      "Epoch 2077 Loss 5.379639\n",
      "Epoch 2078 Loss 5.374947\n",
      "Epoch 2079 Loss 5.370258\n",
      "Epoch 2080 Loss 5.365578\n",
      "Epoch 2081 Loss 5.360903\n",
      "Epoch 2082 Loss 5.356236\n",
      "Epoch 2083 Loss 5.351576\n",
      "Epoch 2084 Loss 5.346928\n",
      "Epoch 2085 Loss 5.342279\n",
      "Epoch 2086 Loss 5.337639\n",
      "Epoch 2087 Loss 5.333006\n",
      "Epoch 2088 Loss 5.328384\n",
      "Epoch 2089 Loss 5.323764\n",
      "Epoch 2090 Loss 5.319149\n",
      "Epoch 2091 Loss 5.314548\n",
      "Epoch 2092 Loss 5.309949\n",
      "Epoch 2093 Loss 5.305356\n",
      "Epoch 2094 Loss 5.300772\n",
      "Epoch 2095 Loss 5.296193\n",
      "Epoch 2096 Loss 5.291625\n",
      "Epoch 2097 Loss 5.287056\n",
      "Epoch 2098 Loss 5.282502\n",
      "Epoch 2099 Loss 5.277950\n",
      "Epoch 2100 Loss 5.273407\n",
      "Epoch 2101 Loss 5.268868\n",
      "Epoch 2102 Loss 5.264339\n",
      "Epoch 2103 Loss 5.259817\n",
      "Epoch 2104 Loss 5.255297\n",
      "Epoch 2105 Loss 5.250787\n",
      "Epoch 2106 Loss 5.246286\n",
      "Epoch 2107 Loss 5.241785\n",
      "Epoch 2108 Loss 5.237297\n",
      "Epoch 2109 Loss 5.232814\n",
      "Epoch 2110 Loss 5.228340\n",
      "Epoch 2111 Loss 5.223866\n",
      "Epoch 2112 Loss 5.219402\n",
      "Epoch 2113 Loss 5.214948\n",
      "Epoch 2114 Loss 5.210499\n",
      "Epoch 2115 Loss 5.206054\n",
      "Epoch 2116 Loss 5.201618\n",
      "Epoch 2117 Loss 5.197186\n",
      "Epoch 2118 Loss 5.192762\n",
      "Epoch 2119 Loss 5.188346\n",
      "Epoch 2120 Loss 5.183938\n",
      "Epoch 2121 Loss 5.179533\n",
      "Epoch 2122 Loss 5.175135\n",
      "Epoch 2123 Loss 5.170744\n",
      "Epoch 2124 Loss 5.166361\n",
      "Epoch 2125 Loss 5.161983\n",
      "Epoch 2126 Loss 5.157612\n",
      "Epoch 2127 Loss 5.153247\n",
      "Epoch 2128 Loss 5.148893\n",
      "Epoch 2129 Loss 5.144541\n",
      "Epoch 2130 Loss 5.140196\n",
      "Epoch 2131 Loss 5.135858\n",
      "Epoch 2132 Loss 5.131529\n",
      "Epoch 2133 Loss 5.127202\n",
      "Epoch 2134 Loss 5.122883\n",
      "Epoch 2135 Loss 5.118576\n",
      "Epoch 2136 Loss 5.114269\n",
      "Epoch 2137 Loss 5.109973\n",
      "Epoch 2138 Loss 5.105678\n",
      "Epoch 2139 Loss 5.101395\n",
      "Epoch 2140 Loss 5.097114\n",
      "Epoch 2141 Loss 5.092843\n",
      "Epoch 2142 Loss 5.088576\n",
      "Epoch 2143 Loss 5.084318\n",
      "Epoch 2144 Loss 5.080061\n",
      "Epoch 2145 Loss 5.075817\n",
      "Epoch 2146 Loss 5.071577\n",
      "Epoch 2147 Loss 5.067343\n",
      "Epoch 2148 Loss 5.063115\n",
      "Epoch 2149 Loss 5.058895\n",
      "Epoch 2150 Loss 5.054679\n",
      "Epoch 2151 Loss 5.050473\n",
      "Epoch 2152 Loss 5.046271\n",
      "Epoch 2153 Loss 5.042078\n",
      "Epoch 2154 Loss 5.037887\n",
      "Epoch 2155 Loss 5.033707\n",
      "Epoch 2156 Loss 5.029530\n",
      "Epoch 2157 Loss 5.025362\n",
      "Epoch 2158 Loss 5.021199\n",
      "Epoch 2159 Loss 5.017042\n",
      "Epoch 2160 Loss 5.012892\n",
      "Epoch 2161 Loss 5.008749\n",
      "Epoch 2162 Loss 5.004610\n",
      "Epoch 2163 Loss 5.000479\n",
      "Epoch 2164 Loss 4.996354\n",
      "Epoch 2165 Loss 4.992235\n",
      "Epoch 2166 Loss 4.988123\n",
      "Epoch 2167 Loss 4.984016\n",
      "Epoch 2168 Loss 4.979918\n",
      "Epoch 2169 Loss 4.975823\n",
      "Epoch 2170 Loss 4.971741\n",
      "Epoch 2171 Loss 4.967657\n",
      "Epoch 2172 Loss 4.963584\n",
      "Epoch 2173 Loss 4.959513\n",
      "Epoch 2174 Loss 4.955451\n",
      "Epoch 2175 Loss 4.951399\n",
      "Epoch 2176 Loss 4.947348\n",
      "Epoch 2177 Loss 4.943302\n",
      "Epoch 2178 Loss 4.939268\n",
      "Epoch 2179 Loss 4.935239\n",
      "Epoch 2180 Loss 4.931215\n",
      "Epoch 2181 Loss 4.927195\n",
      "Epoch 2182 Loss 4.923185\n",
      "Epoch 2183 Loss 4.919178\n",
      "Epoch 2184 Loss 4.915181\n",
      "Epoch 2185 Loss 4.911187\n",
      "Epoch 2186 Loss 4.907201\n",
      "Epoch 2187 Loss 4.903220\n",
      "Epoch 2188 Loss 4.899248\n",
      "Epoch 2189 Loss 4.895277\n",
      "Epoch 2190 Loss 4.891315\n",
      "Epoch 2191 Loss 4.887360\n",
      "Epoch 2192 Loss 4.883412\n",
      "Epoch 2193 Loss 4.879465\n",
      "Epoch 2194 Loss 4.875530\n",
      "Epoch 2195 Loss 4.871601\n",
      "Epoch 2196 Loss 4.867674\n",
      "Epoch 2197 Loss 4.863752\n",
      "Epoch 2198 Loss 4.859841\n",
      "Epoch 2199 Loss 4.855935\n",
      "Epoch 2200 Loss 4.852034\n",
      "Epoch 2201 Loss 4.848139\n",
      "Epoch 2202 Loss 4.844252\n",
      "Epoch 2203 Loss 4.840369\n",
      "Epoch 2204 Loss 4.836493\n",
      "Epoch 2205 Loss 4.832624\n",
      "Epoch 2206 Loss 4.828759\n",
      "Epoch 2207 Loss 4.824902\n",
      "Epoch 2208 Loss 4.821051\n",
      "Epoch 2209 Loss 4.817204\n",
      "Epoch 2210 Loss 4.813365\n",
      "Epoch 2211 Loss 4.809530\n",
      "Epoch 2212 Loss 4.805706\n",
      "Epoch 2213 Loss 4.801883\n",
      "Epoch 2214 Loss 4.798068\n",
      "Epoch 2215 Loss 4.794257\n",
      "Epoch 2216 Loss 4.790456\n",
      "Epoch 2217 Loss 4.786657\n",
      "Epoch 2218 Loss 4.782866\n",
      "Epoch 2219 Loss 4.779079\n",
      "Epoch 2220 Loss 4.775301\n",
      "Epoch 2221 Loss 4.771526\n",
      "Epoch 2222 Loss 4.767762\n",
      "Epoch 2223 Loss 4.763999\n",
      "Epoch 2224 Loss 4.760246\n",
      "Epoch 2225 Loss 4.756493\n",
      "Epoch 2226 Loss 4.752751\n",
      "Epoch 2227 Loss 4.749012\n",
      "Epoch 2228 Loss 4.745283\n",
      "Epoch 2229 Loss 4.741554\n",
      "Epoch 2230 Loss 4.737837\n",
      "Epoch 2231 Loss 4.734121\n",
      "Epoch 2232 Loss 4.730413\n",
      "Epoch 2233 Loss 4.726712\n",
      "Epoch 2234 Loss 4.723015\n",
      "Epoch 2235 Loss 4.719326\n",
      "Epoch 2236 Loss 4.715642\n",
      "Epoch 2237 Loss 4.711963\n",
      "Epoch 2238 Loss 4.708291\n",
      "Epoch 2239 Loss 4.704624\n",
      "Epoch 2240 Loss 4.700963\n",
      "Epoch 2241 Loss 4.697311\n",
      "Epoch 2242 Loss 4.693662\n",
      "Epoch 2243 Loss 4.690018\n",
      "Epoch 2244 Loss 4.686383\n",
      "Epoch 2245 Loss 4.682751\n",
      "Epoch 2246 Loss 4.679123\n",
      "Epoch 2247 Loss 4.675505\n",
      "Epoch 2248 Loss 4.671891\n",
      "Epoch 2249 Loss 4.668282\n",
      "Epoch 2250 Loss 4.664682\n",
      "Epoch 2251 Loss 4.661083\n",
      "Epoch 2252 Loss 4.657493\n",
      "Epoch 2253 Loss 4.653907\n",
      "Epoch 2254 Loss 4.650327\n",
      "Epoch 2255 Loss 4.646758\n",
      "Epoch 2256 Loss 4.643189\n",
      "Epoch 2257 Loss 4.639626\n",
      "Epoch 2258 Loss 4.636071\n",
      "Epoch 2259 Loss 4.632520\n",
      "Epoch 2260 Loss 4.628977\n",
      "Epoch 2261 Loss 4.625437\n",
      "Epoch 2262 Loss 4.621905\n",
      "Epoch 2263 Loss 4.618377\n",
      "Epoch 2264 Loss 4.614855\n",
      "Epoch 2265 Loss 4.611340\n",
      "Epoch 2266 Loss 4.607830\n",
      "Epoch 2267 Loss 4.604326\n",
      "Epoch 2268 Loss 4.600827\n",
      "Epoch 2269 Loss 4.597336\n",
      "Epoch 2270 Loss 4.593846\n",
      "Epoch 2271 Loss 4.590365\n",
      "Epoch 2272 Loss 4.586891\n",
      "Epoch 2273 Loss 4.583419\n",
      "Epoch 2274 Loss 4.579956\n",
      "Epoch 2275 Loss 4.576499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2276 Loss 4.573044\n",
      "Epoch 2277 Loss 4.569596\n",
      "Epoch 2278 Loss 4.566155\n",
      "Epoch 2279 Loss 4.562717\n",
      "Epoch 2280 Loss 4.559287\n",
      "Epoch 2281 Loss 4.555862\n",
      "Epoch 2282 Loss 4.552443\n",
      "Epoch 2283 Loss 4.549029\n",
      "Epoch 2284 Loss 4.545622\n",
      "Epoch 2285 Loss 4.542220\n",
      "Epoch 2286 Loss 4.538823\n",
      "Epoch 2287 Loss 4.535429\n",
      "Epoch 2288 Loss 4.532043\n",
      "Epoch 2289 Loss 4.528664\n",
      "Epoch 2290 Loss 4.525288\n",
      "Epoch 2291 Loss 4.521920\n",
      "Epoch 2292 Loss 4.518557\n",
      "Epoch 2293 Loss 4.515199\n",
      "Epoch 2294 Loss 4.511847\n",
      "Epoch 2295 Loss 4.508500\n",
      "Epoch 2296 Loss 4.505159\n",
      "Epoch 2297 Loss 4.501822\n",
      "Epoch 2298 Loss 4.498493\n",
      "Epoch 2299 Loss 4.495167\n",
      "Epoch 2300 Loss 4.491848\n",
      "Epoch 2301 Loss 4.488533\n",
      "Epoch 2302 Loss 4.485225\n",
      "Epoch 2303 Loss 4.481922\n",
      "Epoch 2304 Loss 4.478624\n",
      "Epoch 2305 Loss 4.475332\n",
      "Epoch 2306 Loss 4.472044\n",
      "Epoch 2307 Loss 4.468766\n",
      "Epoch 2308 Loss 4.465490\n",
      "Epoch 2309 Loss 4.462221\n",
      "Epoch 2310 Loss 4.458956\n",
      "Epoch 2311 Loss 4.455695\n",
      "Epoch 2312 Loss 4.452441\n",
      "Epoch 2313 Loss 4.449193\n",
      "Epoch 2314 Loss 4.445948\n",
      "Epoch 2315 Loss 4.442713\n",
      "Epoch 2316 Loss 4.439480\n",
      "Epoch 2317 Loss 4.436253\n",
      "Epoch 2318 Loss 4.433033\n",
      "Epoch 2319 Loss 4.429815\n",
      "Epoch 2320 Loss 4.426603\n",
      "Epoch 2321 Loss 4.423399\n",
      "Epoch 2322 Loss 4.420201\n",
      "Epoch 2323 Loss 4.417006\n",
      "Epoch 2324 Loss 4.413818\n",
      "Epoch 2325 Loss 4.410632\n",
      "Epoch 2326 Loss 4.407454\n",
      "Epoch 2327 Loss 4.404280\n",
      "Epoch 2328 Loss 4.401113\n",
      "Epoch 2329 Loss 4.397950\n",
      "Epoch 2330 Loss 4.394793\n",
      "Epoch 2331 Loss 4.391643\n",
      "Epoch 2332 Loss 4.388494\n",
      "Epoch 2333 Loss 4.385353\n",
      "Epoch 2334 Loss 4.382218\n",
      "Epoch 2335 Loss 4.379086\n",
      "Epoch 2336 Loss 4.375961\n",
      "Epoch 2337 Loss 4.372843\n",
      "Epoch 2338 Loss 4.369728\n",
      "Epoch 2339 Loss 4.366617\n",
      "Epoch 2340 Loss 4.363513\n",
      "Epoch 2341 Loss 4.360413\n",
      "Epoch 2342 Loss 4.357321\n",
      "Epoch 2343 Loss 4.354232\n",
      "Epoch 2344 Loss 4.351148\n",
      "Epoch 2345 Loss 4.348072\n",
      "Epoch 2346 Loss 4.344998\n",
      "Epoch 2347 Loss 4.341929\n",
      "Epoch 2348 Loss 4.338867\n",
      "Epoch 2349 Loss 4.335811\n",
      "Epoch 2350 Loss 4.332758\n",
      "Epoch 2351 Loss 4.329711\n",
      "Epoch 2352 Loss 4.326671\n",
      "Epoch 2353 Loss 4.323633\n",
      "Epoch 2354 Loss 4.320603\n",
      "Epoch 2355 Loss 4.317575\n",
      "Epoch 2356 Loss 4.314555\n",
      "Epoch 2357 Loss 4.311538\n",
      "Epoch 2358 Loss 4.308527\n",
      "Epoch 2359 Loss 4.305523\n",
      "Epoch 2360 Loss 4.302522\n",
      "Epoch 2361 Loss 4.299527\n",
      "Epoch 2362 Loss 4.296537\n",
      "Epoch 2363 Loss 4.293553\n",
      "Epoch 2364 Loss 4.290573\n",
      "Epoch 2365 Loss 4.287596\n",
      "Epoch 2366 Loss 4.284626\n",
      "Epoch 2367 Loss 4.281663\n",
      "Epoch 2368 Loss 4.278704\n",
      "Epoch 2369 Loss 4.275751\n",
      "Epoch 2370 Loss 4.272800\n",
      "Epoch 2371 Loss 4.269856\n",
      "Epoch 2372 Loss 4.266918\n",
      "Epoch 2373 Loss 4.263983\n",
      "Epoch 2374 Loss 4.261056\n",
      "Epoch 2375 Loss 4.258130\n",
      "Epoch 2376 Loss 4.255212\n",
      "Epoch 2377 Loss 4.252297\n",
      "Epoch 2378 Loss 4.249389\n",
      "Epoch 2379 Loss 4.246484\n",
      "Epoch 2380 Loss 4.243584\n",
      "Epoch 2381 Loss 4.240694\n",
      "Epoch 2382 Loss 4.237803\n",
      "Epoch 2383 Loss 4.234921\n",
      "Epoch 2384 Loss 4.232041\n",
      "Epoch 2385 Loss 4.229167\n",
      "Epoch 2386 Loss 4.226298\n",
      "Epoch 2387 Loss 4.223435\n",
      "Epoch 2388 Loss 4.220578\n",
      "Epoch 2389 Loss 4.217721\n",
      "Epoch 2390 Loss 4.214874\n",
      "Epoch 2391 Loss 4.212031\n",
      "Epoch 2392 Loss 4.209191\n",
      "Epoch 2393 Loss 4.206356\n",
      "Epoch 2394 Loss 4.203529\n",
      "Epoch 2395 Loss 4.200703\n",
      "Epoch 2396 Loss 4.197887\n",
      "Epoch 2397 Loss 4.195072\n",
      "Epoch 2398 Loss 4.192263\n",
      "Epoch 2399 Loss 4.189457\n",
      "Epoch 2400 Loss 4.186657\n",
      "Epoch 2401 Loss 4.183865\n",
      "Epoch 2402 Loss 4.181076\n",
      "Epoch 2403 Loss 4.178288\n",
      "Epoch 2404 Loss 4.175510\n",
      "Epoch 2405 Loss 4.172735\n",
      "Epoch 2406 Loss 4.169965\n",
      "Epoch 2407 Loss 4.167200\n",
      "Epoch 2408 Loss 4.164441\n",
      "Epoch 2409 Loss 4.161684\n",
      "Epoch 2410 Loss 4.158934\n",
      "Epoch 2411 Loss 4.156187\n",
      "Epoch 2412 Loss 4.153448\n",
      "Epoch 2413 Loss 4.150709\n",
      "Epoch 2414 Loss 4.147979\n",
      "Epoch 2415 Loss 4.145252\n",
      "Epoch 2416 Loss 4.142530\n",
      "Epoch 2417 Loss 4.139814\n",
      "Epoch 2418 Loss 4.137102\n",
      "Epoch 2419 Loss 4.134394\n",
      "Epoch 2420 Loss 4.131693\n",
      "Epoch 2421 Loss 4.128996\n",
      "Epoch 2422 Loss 4.126302\n",
      "Epoch 2423 Loss 4.123614\n",
      "Epoch 2424 Loss 4.120931\n",
      "Epoch 2425 Loss 4.118252\n",
      "Epoch 2426 Loss 4.115578\n",
      "Epoch 2427 Loss 4.112909\n",
      "Epoch 2428 Loss 4.110243\n",
      "Epoch 2429 Loss 4.107584\n",
      "Epoch 2430 Loss 4.104928\n",
      "Epoch 2431 Loss 4.102279\n",
      "Epoch 2432 Loss 4.099636\n",
      "Epoch 2433 Loss 4.096993\n",
      "Epoch 2434 Loss 4.094357\n",
      "Epoch 2435 Loss 4.091725\n",
      "Epoch 2436 Loss 4.089100\n",
      "Epoch 2437 Loss 4.086475\n",
      "Epoch 2438 Loss 4.083860\n",
      "Epoch 2439 Loss 4.081248\n",
      "Epoch 2440 Loss 4.078637\n",
      "Epoch 2441 Loss 4.076036\n",
      "Epoch 2442 Loss 4.073436\n",
      "Epoch 2443 Loss 4.070841\n",
      "Epoch 2444 Loss 4.068254\n",
      "Epoch 2445 Loss 4.065671\n",
      "Epoch 2446 Loss 4.063089\n",
      "Epoch 2447 Loss 4.060513\n",
      "Epoch 2448 Loss 4.057945\n",
      "Epoch 2449 Loss 4.055378\n",
      "Epoch 2450 Loss 4.052816\n",
      "Epoch 2451 Loss 4.050259\n",
      "Epoch 2452 Loss 4.047705\n",
      "Epoch 2453 Loss 4.045157\n",
      "Epoch 2454 Loss 4.042614\n",
      "Epoch 2455 Loss 4.040077\n",
      "Epoch 2456 Loss 4.037542\n",
      "Epoch 2457 Loss 4.035012\n",
      "Epoch 2458 Loss 4.032489\n",
      "Epoch 2459 Loss 4.029966\n",
      "Epoch 2460 Loss 4.027452\n",
      "Epoch 2461 Loss 4.024939\n",
      "Epoch 2462 Loss 4.022434\n",
      "Epoch 2463 Loss 4.019933\n",
      "Epoch 2464 Loss 4.017434\n",
      "Epoch 2465 Loss 4.014942\n",
      "Epoch 2466 Loss 4.012454\n",
      "Epoch 2467 Loss 4.009967\n",
      "Epoch 2468 Loss 4.007490\n",
      "Epoch 2469 Loss 4.005015\n",
      "Epoch 2470 Loss 4.002545\n",
      "Epoch 2471 Loss 4.000079\n",
      "Epoch 2472 Loss 3.997618\n",
      "Epoch 2473 Loss 3.995161\n",
      "Epoch 2474 Loss 3.992709\n",
      "Epoch 2475 Loss 3.990260\n",
      "Epoch 2476 Loss 3.987814\n",
      "Epoch 2477 Loss 3.985377\n",
      "Epoch 2478 Loss 3.982945\n",
      "Epoch 2479 Loss 3.980513\n",
      "Epoch 2480 Loss 3.978088\n",
      "Epoch 2481 Loss 3.975666\n",
      "Epoch 2482 Loss 3.973248\n",
      "Epoch 2483 Loss 3.970839\n",
      "Epoch 2484 Loss 3.968428\n",
      "Epoch 2485 Loss 3.966026\n",
      "Epoch 2486 Loss 3.963627\n",
      "Epoch 2487 Loss 3.961232\n",
      "Epoch 2488 Loss 3.958843\n",
      "Epoch 2489 Loss 3.956455\n",
      "Epoch 2490 Loss 3.954074\n",
      "Epoch 2491 Loss 3.951697\n",
      "Epoch 2492 Loss 3.949327\n",
      "Epoch 2493 Loss 3.946955\n",
      "Epoch 2494 Loss 3.944591\n",
      "Epoch 2495 Loss 3.942233\n",
      "Epoch 2496 Loss 3.939876\n",
      "Epoch 2497 Loss 3.937526\n",
      "Epoch 2498 Loss 3.935179\n",
      "Epoch 2499 Loss 3.932838\n",
      "Epoch 2500 Loss 3.930500\n",
      "Epoch 2501 Loss 3.928167\n",
      "Epoch 2502 Loss 3.925838\n",
      "Epoch 2503 Loss 3.923513\n",
      "Epoch 2504 Loss 3.921192\n",
      "Epoch 2505 Loss 3.918873\n",
      "Epoch 2506 Loss 3.916563\n",
      "Epoch 2507 Loss 3.914253\n",
      "Epoch 2508 Loss 3.911952\n",
      "Epoch 2509 Loss 3.909652\n",
      "Epoch 2510 Loss 3.907355\n",
      "Epoch 2511 Loss 3.905066\n",
      "Epoch 2512 Loss 3.902780\n",
      "Epoch 2513 Loss 3.900497\n",
      "Epoch 2514 Loss 3.898222\n",
      "Epoch 2515 Loss 3.895948\n",
      "Epoch 2516 Loss 3.893677\n",
      "Epoch 2517 Loss 3.891412\n",
      "Epoch 2518 Loss 3.889151\n",
      "Epoch 2519 Loss 3.886894\n",
      "Epoch 2520 Loss 3.884643\n",
      "Epoch 2521 Loss 3.882394\n",
      "Epoch 2522 Loss 3.880152\n",
      "Epoch 2523 Loss 3.877908\n",
      "Epoch 2524 Loss 3.875674\n",
      "Epoch 2525 Loss 3.873443\n",
      "Epoch 2526 Loss 3.871215\n",
      "Epoch 2527 Loss 3.868993\n",
      "Epoch 2528 Loss 3.866775\n",
      "Epoch 2529 Loss 3.864559\n",
      "Epoch 2530 Loss 3.862348\n",
      "Epoch 2531 Loss 3.860139\n",
      "Epoch 2532 Loss 3.857939\n",
      "Epoch 2533 Loss 3.855742\n",
      "Epoch 2534 Loss 3.853547\n",
      "Epoch 2535 Loss 3.851359\n",
      "Epoch 2536 Loss 3.849174\n",
      "Epoch 2537 Loss 3.846992\n",
      "Epoch 2538 Loss 3.844815\n",
      "Epoch 2539 Loss 3.842642\n",
      "Epoch 2540 Loss 3.840472\n",
      "Epoch 2541 Loss 3.838307\n",
      "Epoch 2542 Loss 3.836146\n",
      "Epoch 2543 Loss 3.833989\n",
      "Epoch 2544 Loss 3.831835\n",
      "Epoch 2545 Loss 3.829686\n",
      "Epoch 2546 Loss 3.827541\n",
      "Epoch 2547 Loss 3.825403\n",
      "Epoch 2548 Loss 3.823266\n",
      "Epoch 2549 Loss 3.821131\n",
      "Epoch 2550 Loss 3.819004\n",
      "Epoch 2551 Loss 3.816882\n",
      "Epoch 2552 Loss 3.814760\n",
      "Epoch 2553 Loss 3.812645\n",
      "Epoch 2554 Loss 3.810532\n",
      "Epoch 2555 Loss 3.808423\n",
      "Epoch 2556 Loss 3.806319\n",
      "Epoch 2557 Loss 3.804220\n",
      "Epoch 2558 Loss 3.802125\n",
      "Epoch 2559 Loss 3.800033\n",
      "Epoch 2560 Loss 3.797944\n",
      "Epoch 2561 Loss 3.795860\n",
      "Epoch 2562 Loss 3.793780\n",
      "Epoch 2563 Loss 3.791705\n",
      "Epoch 2564 Loss 3.789632\n",
      "Epoch 2565 Loss 3.787563\n",
      "Epoch 2566 Loss 3.785500\n",
      "Epoch 2567 Loss 3.783441\n",
      "Epoch 2568 Loss 3.781382\n",
      "Epoch 2569 Loss 3.779332\n",
      "Epoch 2570 Loss 3.777285\n",
      "Epoch 2571 Loss 3.775239\n",
      "Epoch 2572 Loss 3.773198\n",
      "Epoch 2573 Loss 3.771161\n",
      "Epoch 2574 Loss 3.769129\n",
      "Epoch 2575 Loss 3.767101\n",
      "Epoch 2576 Loss 3.765077\n",
      "Epoch 2577 Loss 3.763056\n",
      "Epoch 2578 Loss 3.761038\n",
      "Epoch 2579 Loss 3.759026\n",
      "Epoch 2580 Loss 3.757018\n",
      "Epoch 2581 Loss 3.755013\n",
      "Epoch 2582 Loss 3.753010\n",
      "Epoch 2583 Loss 3.751013\n",
      "Epoch 2584 Loss 3.749018\n",
      "Epoch 2585 Loss 3.747028\n",
      "Epoch 2586 Loss 3.745045\n",
      "Epoch 2587 Loss 3.743060\n",
      "Epoch 2588 Loss 3.741082\n",
      "Epoch 2589 Loss 3.739112\n",
      "Epoch 2590 Loss 3.737139\n",
      "Epoch 2591 Loss 3.735172\n",
      "Epoch 2592 Loss 3.733208\n",
      "Epoch 2593 Loss 3.731252\n",
      "Epoch 2594 Loss 3.729296\n",
      "Epoch 2595 Loss 3.727342\n",
      "Epoch 2596 Loss 3.725396\n",
      "Epoch 2597 Loss 3.723451\n",
      "Epoch 2598 Loss 3.721513\n",
      "Epoch 2599 Loss 3.719578\n",
      "Epoch 2600 Loss 3.717645\n",
      "Epoch 2601 Loss 3.715718\n",
      "Epoch 2602 Loss 3.713794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2603 Loss 3.711871\n",
      "Epoch 2604 Loss 3.709954\n",
      "Epoch 2605 Loss 3.708040\n",
      "Epoch 2606 Loss 3.706131\n",
      "Epoch 2607 Loss 3.704226\n",
      "Epoch 2608 Loss 3.702322\n",
      "Epoch 2609 Loss 3.700425\n",
      "Epoch 2610 Loss 3.698530\n",
      "Epoch 2611 Loss 3.696638\n",
      "Epoch 2612 Loss 3.694752\n",
      "Epoch 2613 Loss 3.692869\n",
      "Epoch 2614 Loss 3.690989\n",
      "Epoch 2615 Loss 3.689115\n",
      "Epoch 2616 Loss 3.687240\n",
      "Epoch 2617 Loss 3.685372\n",
      "Epoch 2618 Loss 3.683506\n",
      "Epoch 2619 Loss 3.681647\n",
      "Epoch 2620 Loss 3.679787\n",
      "Epoch 2621 Loss 3.677934\n",
      "Epoch 2622 Loss 3.676085\n",
      "Epoch 2623 Loss 3.674237\n",
      "Epoch 2624 Loss 3.672395\n",
      "Epoch 2625 Loss 3.670556\n",
      "Epoch 2626 Loss 3.668719\n",
      "Epoch 2627 Loss 3.666889\n",
      "Epoch 2628 Loss 3.665062\n",
      "Epoch 2629 Loss 3.663235\n",
      "Epoch 2630 Loss 3.661416\n",
      "Epoch 2631 Loss 3.659597\n",
      "Epoch 2632 Loss 3.657783\n",
      "Epoch 2633 Loss 3.655976\n",
      "Epoch 2634 Loss 3.654168\n",
      "Epoch 2635 Loss 3.652365\n",
      "Epoch 2636 Loss 3.650568\n",
      "Epoch 2637 Loss 3.648770\n",
      "Epoch 2638 Loss 3.646977\n",
      "Epoch 2639 Loss 3.645190\n",
      "Epoch 2640 Loss 3.643404\n",
      "Epoch 2641 Loss 3.641627\n",
      "Epoch 2642 Loss 3.639845\n",
      "Epoch 2643 Loss 3.638074\n",
      "Epoch 2644 Loss 3.636301\n",
      "Epoch 2645 Loss 3.634535\n",
      "Epoch 2646 Loss 3.632771\n",
      "Epoch 2647 Loss 3.631011\n",
      "Epoch 2648 Loss 3.629256\n",
      "Epoch 2649 Loss 3.627501\n",
      "Epoch 2650 Loss 3.625753\n",
      "Epoch 2651 Loss 3.624007\n",
      "Epoch 2652 Loss 3.622265\n",
      "Epoch 2653 Loss 3.620527\n",
      "Epoch 2654 Loss 3.618792\n",
      "Epoch 2655 Loss 3.617059\n",
      "Epoch 2656 Loss 3.615333\n",
      "Epoch 2657 Loss 3.613606\n",
      "Epoch 2658 Loss 3.611884\n",
      "Epoch 2659 Loss 3.610168\n",
      "Epoch 2660 Loss 3.608454\n",
      "Epoch 2661 Loss 3.606741\n",
      "Epoch 2662 Loss 3.605035\n",
      "Epoch 2663 Loss 3.603329\n",
      "Epoch 2664 Loss 3.601631\n",
      "Epoch 2665 Loss 3.599933\n",
      "Epoch 2666 Loss 3.598240\n",
      "Epoch 2667 Loss 3.596549\n",
      "Epoch 2668 Loss 3.594862\n",
      "Epoch 2669 Loss 3.593179\n",
      "Epoch 2670 Loss 3.591500\n",
      "Epoch 2671 Loss 3.589825\n",
      "Epoch 2672 Loss 3.588150\n",
      "Epoch 2673 Loss 3.586482\n",
      "Epoch 2674 Loss 3.584812\n",
      "Epoch 2675 Loss 3.583154\n",
      "Epoch 2676 Loss 3.581492\n",
      "Epoch 2677 Loss 3.579836\n",
      "Epoch 2678 Loss 3.578187\n",
      "Epoch 2679 Loss 3.576535\n",
      "Epoch 2680 Loss 3.574888\n",
      "Epoch 2681 Loss 3.573247\n",
      "Epoch 2682 Loss 3.571607\n",
      "Epoch 2683 Loss 3.569973\n",
      "Epoch 2684 Loss 3.568342\n",
      "Epoch 2685 Loss 3.566713\n",
      "Epoch 2686 Loss 3.565087\n",
      "Epoch 2687 Loss 3.563465\n",
      "Epoch 2688 Loss 3.561846\n",
      "Epoch 2689 Loss 3.560230\n",
      "Epoch 2690 Loss 3.558618\n",
      "Epoch 2691 Loss 3.557008\n",
      "Epoch 2692 Loss 3.555404\n",
      "Epoch 2693 Loss 3.553803\n",
      "Epoch 2694 Loss 3.552204\n",
      "Epoch 2695 Loss 3.550607\n",
      "Epoch 2696 Loss 3.549013\n",
      "Epoch 2697 Loss 3.547426\n",
      "Epoch 2698 Loss 3.545839\n",
      "Epoch 2699 Loss 3.544258\n",
      "Epoch 2700 Loss 3.542679\n",
      "Epoch 2701 Loss 3.541102\n",
      "Epoch 2702 Loss 3.539531\n",
      "Epoch 2703 Loss 3.537960\n",
      "Epoch 2704 Loss 3.536394\n",
      "Epoch 2705 Loss 3.534833\n",
      "Epoch 2706 Loss 3.533273\n",
      "Epoch 2707 Loss 3.531716\n",
      "Epoch 2708 Loss 3.530163\n",
      "Epoch 2709 Loss 3.528615\n",
      "Epoch 2710 Loss 3.527069\n",
      "Epoch 2711 Loss 3.525524\n",
      "Epoch 2712 Loss 3.523984\n",
      "Epoch 2713 Loss 3.522447\n",
      "Epoch 2714 Loss 3.520912\n",
      "Epoch 2715 Loss 3.519383\n",
      "Epoch 2716 Loss 3.517855\n",
      "Epoch 2717 Loss 3.516331\n",
      "Epoch 2718 Loss 3.514812\n",
      "Epoch 2719 Loss 3.513293\n",
      "Epoch 2720 Loss 3.511778\n",
      "Epoch 2721 Loss 3.510267\n",
      "Epoch 2722 Loss 3.508759\n",
      "Epoch 2723 Loss 3.507252\n",
      "Epoch 2724 Loss 3.505752\n",
      "Epoch 2725 Loss 3.504252\n",
      "Epoch 2726 Loss 3.502757\n",
      "Epoch 2727 Loss 3.501264\n",
      "Epoch 2728 Loss 3.499777\n",
      "Epoch 2729 Loss 3.498289\n",
      "Epoch 2730 Loss 3.496806\n",
      "Epoch 2731 Loss 3.495326\n",
      "Epoch 2732 Loss 3.493851\n",
      "Epoch 2733 Loss 3.492377\n",
      "Epoch 2734 Loss 3.490909\n",
      "Epoch 2735 Loss 3.489439\n",
      "Epoch 2736 Loss 3.487973\n",
      "Epoch 2737 Loss 3.486515\n",
      "Epoch 2738 Loss 3.485055\n",
      "Epoch 2739 Loss 3.483602\n",
      "Epoch 2740 Loss 3.482147\n",
      "Epoch 2741 Loss 3.480700\n",
      "Epoch 2742 Loss 3.479254\n",
      "Epoch 2743 Loss 3.477811\n",
      "Epoch 2744 Loss 3.476372\n",
      "Epoch 2745 Loss 3.474935\n",
      "Epoch 2746 Loss 3.473502\n",
      "Epoch 2747 Loss 3.472073\n",
      "Epoch 2748 Loss 3.470644\n",
      "Epoch 2749 Loss 3.469222\n",
      "Epoch 2750 Loss 3.467797\n",
      "Epoch 2751 Loss 3.466380\n",
      "Epoch 2752 Loss 3.464966\n",
      "Epoch 2753 Loss 3.463551\n",
      "Epoch 2754 Loss 3.462147\n",
      "Epoch 2755 Loss 3.460739\n",
      "Epoch 2756 Loss 3.459334\n",
      "Epoch 2757 Loss 3.457935\n",
      "Epoch 2758 Loss 3.456536\n",
      "Epoch 2759 Loss 3.455145\n",
      "Epoch 2760 Loss 3.453753\n",
      "Epoch 2761 Loss 3.452364\n",
      "Epoch 2762 Loss 3.450980\n",
      "Epoch 2763 Loss 3.449597\n",
      "Epoch 2764 Loss 3.448219\n",
      "Epoch 2765 Loss 3.446842\n",
      "Epoch 2766 Loss 3.445468\n",
      "Epoch 2767 Loss 3.444097\n",
      "Epoch 2768 Loss 3.442730\n",
      "Epoch 2769 Loss 3.441366\n",
      "Epoch 2770 Loss 3.440004\n",
      "Epoch 2771 Loss 3.438644\n",
      "Epoch 2772 Loss 3.437291\n",
      "Epoch 2773 Loss 3.435937\n",
      "Epoch 2774 Loss 3.434588\n",
      "Epoch 2775 Loss 3.433243\n",
      "Epoch 2776 Loss 3.431896\n",
      "Epoch 2777 Loss 3.430556\n",
      "Epoch 2778 Loss 3.429218\n",
      "Epoch 2779 Loss 3.427882\n",
      "Epoch 2780 Loss 3.426550\n",
      "Epoch 2781 Loss 3.425223\n",
      "Epoch 2782 Loss 3.423895\n",
      "Epoch 2783 Loss 3.422572\n",
      "Epoch 2784 Loss 3.421251\n",
      "Epoch 2785 Loss 3.419932\n",
      "Epoch 2786 Loss 3.418617\n",
      "Epoch 2787 Loss 3.417303\n",
      "Epoch 2788 Loss 3.415995\n",
      "Epoch 2789 Loss 3.414689\n",
      "Epoch 2790 Loss 3.413384\n",
      "Epoch 2791 Loss 3.412083\n",
      "Epoch 2792 Loss 3.410785\n",
      "Epoch 2793 Loss 3.409492\n",
      "Epoch 2794 Loss 3.408201\n",
      "Epoch 2795 Loss 3.406911\n",
      "Epoch 2796 Loss 3.405623\n",
      "Epoch 2797 Loss 3.404341\n",
      "Epoch 2798 Loss 3.403057\n",
      "Epoch 2799 Loss 3.401782\n",
      "Epoch 2800 Loss 3.400506\n",
      "Epoch 2801 Loss 3.399232\n",
      "Epoch 2802 Loss 3.397964\n",
      "Epoch 2803 Loss 3.396696\n",
      "Epoch 2804 Loss 3.395431\n",
      "Epoch 2805 Loss 3.394171\n",
      "Epoch 2806 Loss 3.392912\n",
      "Epoch 2807 Loss 3.391655\n",
      "Epoch 2808 Loss 3.390404\n",
      "Epoch 2809 Loss 3.389153\n",
      "Epoch 2810 Loss 3.387905\n",
      "Epoch 2811 Loss 3.386659\n",
      "Epoch 2812 Loss 3.385418\n",
      "Epoch 2813 Loss 3.384180\n",
      "Epoch 2814 Loss 3.382943\n",
      "Epoch 2815 Loss 3.381709\n",
      "Epoch 2816 Loss 3.380477\n",
      "Epoch 2817 Loss 3.379252\n",
      "Epoch 2818 Loss 3.378025\n",
      "Epoch 2819 Loss 3.376801\n",
      "Epoch 2820 Loss 3.375582\n",
      "Epoch 2821 Loss 3.374366\n",
      "Epoch 2822 Loss 3.373151\n",
      "Epoch 2823 Loss 3.371938\n",
      "Epoch 2824 Loss 3.370728\n",
      "Epoch 2825 Loss 3.369522\n",
      "Epoch 2826 Loss 3.368320\n",
      "Epoch 2827 Loss 3.367116\n",
      "Epoch 2828 Loss 3.365919\n",
      "Epoch 2829 Loss 3.364723\n",
      "Epoch 2830 Loss 3.363532\n",
      "Epoch 2831 Loss 3.362340\n",
      "Epoch 2832 Loss 3.361151\n",
      "Epoch 2833 Loss 3.359969\n",
      "Epoch 2834 Loss 3.358783\n",
      "Epoch 2835 Loss 3.357604\n",
      "Epoch 2836 Loss 3.356427\n",
      "Epoch 2837 Loss 3.355252\n",
      "Epoch 2838 Loss 3.354079\n",
      "Epoch 2839 Loss 3.352910\n",
      "Epoch 2840 Loss 3.351747\n",
      "Epoch 2841 Loss 3.350579\n",
      "Epoch 2842 Loss 3.349417\n",
      "Epoch 2843 Loss 3.348260\n",
      "Epoch 2844 Loss 3.347103\n",
      "Epoch 2845 Loss 3.345949\n",
      "Epoch 2846 Loss 3.344800\n",
      "Epoch 2847 Loss 3.343650\n",
      "Epoch 2848 Loss 3.342505\n",
      "Epoch 2849 Loss 3.341361\n",
      "Epoch 2850 Loss 3.340220\n",
      "Epoch 2851 Loss 3.339085\n",
      "Epoch 2852 Loss 3.337949\n",
      "Epoch 2853 Loss 3.336816\n",
      "Epoch 2854 Loss 3.335683\n",
      "Epoch 2855 Loss 3.334559\n",
      "Epoch 2856 Loss 3.333432\n",
      "Epoch 2857 Loss 3.332310\n",
      "Epoch 2858 Loss 3.331189\n",
      "Epoch 2859 Loss 3.330071\n",
      "Epoch 2860 Loss 3.328958\n",
      "Epoch 2861 Loss 3.327846\n",
      "Epoch 2862 Loss 3.326734\n",
      "Epoch 2863 Loss 3.325627\n",
      "Epoch 2864 Loss 3.324523\n",
      "Epoch 2865 Loss 3.323421\n",
      "Epoch 2866 Loss 3.322323\n",
      "Epoch 2867 Loss 3.321223\n",
      "Epoch 2868 Loss 3.320128\n",
      "Epoch 2869 Loss 3.319036\n",
      "Epoch 2870 Loss 3.317948\n",
      "Epoch 2871 Loss 3.316860\n",
      "Epoch 2872 Loss 3.315774\n",
      "Epoch 2873 Loss 3.314692\n",
      "Epoch 2874 Loss 3.313613\n",
      "Epoch 2875 Loss 3.312535\n",
      "Epoch 2876 Loss 3.311461\n",
      "Epoch 2877 Loss 3.310388\n",
      "Epoch 2878 Loss 3.309318\n",
      "Epoch 2879 Loss 3.308250\n",
      "Epoch 2880 Loss 3.307185\n",
      "Epoch 2881 Loss 3.306124\n",
      "Epoch 2882 Loss 3.305063\n",
      "Epoch 2883 Loss 3.304006\n",
      "Epoch 2884 Loss 3.302952\n",
      "Epoch 2885 Loss 3.301899\n",
      "Epoch 2886 Loss 3.300848\n",
      "Epoch 2887 Loss 3.299799\n",
      "Epoch 2888 Loss 3.298752\n",
      "Epoch 2889 Loss 3.297711\n",
      "Epoch 2890 Loss 3.296670\n",
      "Epoch 2891 Loss 3.295634\n",
      "Epoch 2892 Loss 3.294595\n",
      "Epoch 2893 Loss 3.293561\n",
      "Epoch 2894 Loss 3.292532\n",
      "Epoch 2895 Loss 3.291501\n",
      "Epoch 2896 Loss 3.290476\n",
      "Epoch 2897 Loss 3.289451\n",
      "Epoch 2898 Loss 3.288432\n",
      "Epoch 2899 Loss 3.287414\n",
      "Epoch 2900 Loss 3.286396\n",
      "Epoch 2901 Loss 3.285381\n",
      "Epoch 2902 Loss 3.284370\n",
      "Epoch 2903 Loss 3.283359\n",
      "Epoch 2904 Loss 3.282352\n",
      "Epoch 2905 Loss 3.281348\n",
      "Epoch 2906 Loss 3.280344\n",
      "Epoch 2907 Loss 3.279343\n",
      "Epoch 2908 Loss 3.278345\n",
      "Epoch 2909 Loss 3.277349\n",
      "Epoch 2910 Loss 3.276357\n",
      "Epoch 2911 Loss 3.275366\n",
      "Epoch 2912 Loss 3.274377\n",
      "Epoch 2913 Loss 3.273391\n",
      "Epoch 2914 Loss 3.272406\n",
      "Epoch 2915 Loss 3.271426\n",
      "Epoch 2916 Loss 3.270446\n",
      "Epoch 2917 Loss 3.269469\n",
      "Epoch 2918 Loss 3.268494\n",
      "Epoch 2919 Loss 3.267522\n",
      "Epoch 2920 Loss 3.266551\n",
      "Epoch 2921 Loss 3.265585\n",
      "Epoch 2922 Loss 3.264618\n",
      "Epoch 2923 Loss 3.263654\n",
      "Epoch 2924 Loss 3.262695\n",
      "Epoch 2925 Loss 3.261735\n",
      "Epoch 2926 Loss 3.260778\n",
      "Epoch 2927 Loss 3.259826\n",
      "Epoch 2928 Loss 3.258873\n",
      "Epoch 2929 Loss 3.257922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2930 Loss 3.256977\n",
      "Epoch 2931 Loss 3.256029\n",
      "Epoch 2932 Loss 3.255087\n",
      "Epoch 2933 Loss 3.254148\n",
      "Epoch 2934 Loss 3.253208\n",
      "Epoch 2935 Loss 3.252273\n",
      "Epoch 2936 Loss 3.251338\n",
      "Epoch 2937 Loss 3.250405\n",
      "Epoch 2938 Loss 3.249475\n",
      "Epoch 2939 Loss 3.248549\n",
      "Epoch 2940 Loss 3.247622\n",
      "Epoch 2941 Loss 3.246699\n",
      "Epoch 2942 Loss 3.245779\n",
      "Epoch 2943 Loss 3.244862\n",
      "Epoch 2944 Loss 3.243944\n",
      "Epoch 2945 Loss 3.243030\n",
      "Epoch 2946 Loss 3.242121\n",
      "Epoch 2947 Loss 3.241209\n",
      "Epoch 2948 Loss 3.240300\n",
      "Epoch 2949 Loss 3.239395\n",
      "Epoch 2950 Loss 3.238493\n",
      "Epoch 2951 Loss 3.237591\n",
      "Epoch 2952 Loss 3.236693\n",
      "Epoch 2953 Loss 3.235795\n",
      "Epoch 2954 Loss 3.234901\n",
      "Epoch 2955 Loss 3.234010\n",
      "Epoch 2956 Loss 3.233119\n",
      "Epoch 2957 Loss 3.232231\n",
      "Epoch 2958 Loss 3.231344\n",
      "Epoch 2959 Loss 3.230462\n",
      "Epoch 2960 Loss 3.229578\n",
      "Epoch 2961 Loss 3.228700\n",
      "Epoch 2962 Loss 3.227821\n",
      "Epoch 2963 Loss 3.226946\n",
      "Epoch 2964 Loss 3.226073\n",
      "Epoch 2965 Loss 3.225201\n",
      "Epoch 2966 Loss 3.224332\n",
      "Epoch 2967 Loss 3.223464\n",
      "Epoch 2968 Loss 3.222601\n",
      "Epoch 2969 Loss 3.221739\n",
      "Epoch 2970 Loss 3.220877\n",
      "Epoch 2971 Loss 3.220016\n",
      "Epoch 2972 Loss 3.219160\n",
      "Epoch 2973 Loss 3.218308\n",
      "Epoch 2974 Loss 3.217453\n",
      "Epoch 2975 Loss 3.216603\n",
      "Epoch 2976 Loss 3.215758\n",
      "Epoch 2977 Loss 3.214910\n",
      "Epoch 2978 Loss 3.214066\n",
      "Epoch 2979 Loss 3.213225\n",
      "Epoch 2980 Loss 3.212383\n",
      "Epoch 2981 Loss 3.211546\n",
      "Epoch 2982 Loss 3.210710\n",
      "Epoch 2983 Loss 3.209876\n",
      "Epoch 2984 Loss 3.209046\n",
      "Epoch 2985 Loss 3.208215\n",
      "Epoch 2986 Loss 3.207388\n",
      "Epoch 2987 Loss 3.206560\n",
      "Epoch 2988 Loss 3.205737\n",
      "Epoch 2989 Loss 3.204916\n",
      "Epoch 2990 Loss 3.204095\n",
      "Epoch 2991 Loss 3.203279\n",
      "Epoch 2992 Loss 3.202463\n",
      "Epoch 2993 Loss 3.201652\n",
      "Epoch 2994 Loss 3.200838\n",
      "Epoch 2995 Loss 3.200029\n",
      "Epoch 2996 Loss 3.199220\n",
      "Epoch 2997 Loss 3.198416\n",
      "Epoch 2998 Loss 3.197610\n",
      "Epoch 2999 Loss 3.196811\n",
      "Epoch 3000 Loss 3.196009\n",
      "Epoch 3001 Loss 3.195212\n",
      "Epoch 3002 Loss 3.194415\n",
      "Epoch 3003 Loss 3.193622\n",
      "Epoch 3004 Loss 3.192831\n",
      "Epoch 3005 Loss 3.192040\n",
      "Epoch 3006 Loss 3.191252\n",
      "Epoch 3007 Loss 3.190468\n",
      "Epoch 3008 Loss 3.189686\n",
      "Epoch 3009 Loss 3.188902\n",
      "Epoch 3010 Loss 3.188122\n",
      "Epoch 3011 Loss 3.187343\n",
      "Epoch 3012 Loss 3.186567\n",
      "Epoch 3013 Loss 3.185794\n",
      "Epoch 3014 Loss 3.185020\n",
      "Epoch 3015 Loss 3.184250\n",
      "Epoch 3016 Loss 3.183482\n",
      "Epoch 3017 Loss 3.182717\n",
      "Epoch 3018 Loss 3.181950\n",
      "Epoch 3019 Loss 3.181185\n",
      "Epoch 3020 Loss 3.180428\n",
      "Epoch 3021 Loss 3.179668\n",
      "Epoch 3022 Loss 3.178910\n",
      "Epoch 3023 Loss 3.178158\n",
      "Epoch 3024 Loss 3.177402\n",
      "Epoch 3025 Loss 3.176651\n",
      "Epoch 3026 Loss 3.175903\n",
      "Epoch 3027 Loss 3.175154\n",
      "Epoch 3028 Loss 3.174409\n",
      "Epoch 3029 Loss 3.173666\n",
      "Epoch 3030 Loss 3.172924\n",
      "Epoch 3031 Loss 3.172182\n",
      "Epoch 3032 Loss 3.171445\n",
      "Epoch 3033 Loss 3.170708\n",
      "Epoch 3034 Loss 3.169973\n",
      "Epoch 3035 Loss 3.169241\n",
      "Epoch 3036 Loss 3.168510\n",
      "Epoch 3037 Loss 3.167781\n",
      "Epoch 3038 Loss 3.167055\n",
      "Epoch 3039 Loss 3.166329\n",
      "Epoch 3040 Loss 3.165606\n",
      "Epoch 3041 Loss 3.164883\n",
      "Epoch 3042 Loss 3.164166\n",
      "Epoch 3043 Loss 3.163445\n",
      "Epoch 3044 Loss 3.162729\n",
      "Epoch 3045 Loss 3.162015\n",
      "Epoch 3046 Loss 3.161301\n",
      "Epoch 3047 Loss 3.160591\n",
      "Epoch 3048 Loss 3.159882\n",
      "Epoch 3049 Loss 3.159175\n",
      "Epoch 3050 Loss 3.158471\n",
      "Epoch 3051 Loss 3.157767\n",
      "Epoch 3052 Loss 3.157064\n",
      "Epoch 3053 Loss 3.156366\n",
      "Epoch 3054 Loss 3.155668\n",
      "Epoch 3055 Loss 3.154971\n",
      "Epoch 3056 Loss 3.154274\n",
      "Epoch 3057 Loss 3.153582\n",
      "Epoch 3058 Loss 3.152889\n",
      "Epoch 3059 Loss 3.152203\n",
      "Epoch 3060 Loss 3.151515\n",
      "Epoch 3061 Loss 3.150828\n",
      "Epoch 3062 Loss 3.150146\n",
      "Epoch 3063 Loss 3.149463\n",
      "Epoch 3064 Loss 3.148782\n",
      "Epoch 3065 Loss 3.148103\n",
      "Epoch 3066 Loss 3.147426\n",
      "Epoch 3067 Loss 3.146751\n",
      "Epoch 3068 Loss 3.146078\n",
      "Epoch 3069 Loss 3.145405\n",
      "Epoch 3070 Loss 3.144736\n",
      "Epoch 3071 Loss 3.144068\n",
      "Epoch 3072 Loss 3.143401\n",
      "Epoch 3073 Loss 3.142736\n",
      "Epoch 3074 Loss 3.142072\n",
      "Epoch 3075 Loss 3.141411\n",
      "Epoch 3076 Loss 3.140749\n",
      "Epoch 3077 Loss 3.140093\n",
      "Epoch 3078 Loss 3.139436\n",
      "Epoch 3079 Loss 3.138781\n",
      "Epoch 3080 Loss 3.138127\n",
      "Epoch 3081 Loss 3.137477\n",
      "Epoch 3082 Loss 3.136828\n",
      "Epoch 3083 Loss 3.136178\n",
      "Epoch 3084 Loss 3.135531\n",
      "Epoch 3085 Loss 3.134887\n",
      "Epoch 3086 Loss 3.134244\n",
      "Epoch 3087 Loss 3.133604\n",
      "Epoch 3088 Loss 3.132965\n",
      "Epoch 3089 Loss 3.132327\n",
      "Epoch 3090 Loss 3.131691\n",
      "Epoch 3091 Loss 3.131055\n",
      "Epoch 3092 Loss 3.130422\n",
      "Epoch 3093 Loss 3.129790\n",
      "Epoch 3094 Loss 3.129162\n",
      "Epoch 3095 Loss 3.128536\n",
      "Epoch 3096 Loss 3.127909\n",
      "Epoch 3097 Loss 3.127282\n",
      "Epoch 3098 Loss 3.126660\n",
      "Epoch 3099 Loss 3.126037\n",
      "Epoch 3100 Loss 3.125419\n",
      "Epoch 3101 Loss 3.124799\n",
      "Epoch 3102 Loss 3.124185\n",
      "Epoch 3103 Loss 3.123568\n",
      "Epoch 3104 Loss 3.122956\n",
      "Epoch 3105 Loss 3.122343\n",
      "Epoch 3106 Loss 3.121735\n",
      "Epoch 3107 Loss 3.121125\n",
      "Epoch 3108 Loss 3.120516\n",
      "Epoch 3109 Loss 3.119913\n",
      "Epoch 3110 Loss 3.119308\n",
      "Epoch 3111 Loss 3.118705\n",
      "Epoch 3112 Loss 3.118106\n",
      "Epoch 3113 Loss 3.117507\n",
      "Epoch 3114 Loss 3.116909\n",
      "Epoch 3115 Loss 3.116313\n",
      "Epoch 3116 Loss 3.115720\n",
      "Epoch 3117 Loss 3.115128\n",
      "Epoch 3118 Loss 3.114536\n",
      "Epoch 3119 Loss 3.113946\n",
      "Epoch 3120 Loss 3.113358\n",
      "Epoch 3121 Loss 3.112773\n",
      "Epoch 3122 Loss 3.112186\n",
      "Epoch 3123 Loss 3.111604\n",
      "Epoch 3124 Loss 3.111022\n",
      "Epoch 3125 Loss 3.110441\n",
      "Epoch 3126 Loss 3.109863\n",
      "Epoch 3127 Loss 3.109286\n",
      "Epoch 3128 Loss 3.108711\n",
      "Epoch 3129 Loss 3.108135\n",
      "Epoch 3130 Loss 3.107563\n",
      "Epoch 3131 Loss 3.106992\n",
      "Epoch 3132 Loss 3.106424\n",
      "Epoch 3133 Loss 3.105857\n",
      "Epoch 3134 Loss 3.105289\n",
      "Epoch 3135 Loss 3.104724\n",
      "Epoch 3136 Loss 3.104159\n",
      "Epoch 3137 Loss 3.103599\n",
      "Epoch 3138 Loss 3.103038\n",
      "Epoch 3139 Loss 3.102480\n",
      "Epoch 3140 Loss 3.101921\n",
      "Epoch 3141 Loss 3.101367\n",
      "Epoch 3142 Loss 3.100813\n",
      "Epoch 3143 Loss 3.100258\n",
      "Epoch 3144 Loss 3.099706\n",
      "Epoch 3145 Loss 3.099156\n",
      "Epoch 3146 Loss 3.098608\n",
      "Epoch 3147 Loss 3.098062\n",
      "Epoch 3148 Loss 3.097518\n",
      "Epoch 3149 Loss 3.096972\n",
      "Epoch 3150 Loss 3.096430\n",
      "Epoch 3151 Loss 3.095888\n",
      "Epoch 3152 Loss 3.095348\n",
      "Epoch 3153 Loss 3.094811\n",
      "Epoch 3154 Loss 3.094273\n",
      "Epoch 3155 Loss 3.093740\n",
      "Epoch 3156 Loss 3.093203\n",
      "Epoch 3157 Loss 3.092674\n",
      "Epoch 3158 Loss 3.092142\n",
      "Epoch 3159 Loss 3.091612\n",
      "Epoch 3160 Loss 3.091083\n",
      "Epoch 3161 Loss 3.090557\n",
      "Epoch 3162 Loss 3.090033\n",
      "Epoch 3163 Loss 3.089509\n",
      "Epoch 3164 Loss 3.088986\n",
      "Epoch 3165 Loss 3.088466\n",
      "Epoch 3166 Loss 3.087945\n",
      "Epoch 3167 Loss 3.087429\n",
      "Epoch 3168 Loss 3.086910\n",
      "Epoch 3169 Loss 3.086395\n",
      "Epoch 3170 Loss 3.085881\n",
      "Epoch 3171 Loss 3.085370\n",
      "Epoch 3172 Loss 3.084858\n",
      "Epoch 3173 Loss 3.084348\n",
      "Epoch 3174 Loss 3.083841\n",
      "Epoch 3175 Loss 3.083333\n",
      "Epoch 3176 Loss 3.082827\n",
      "Epoch 3177 Loss 3.082324\n",
      "Epoch 3178 Loss 3.081821\n",
      "Epoch 3179 Loss 3.081320\n",
      "Epoch 3180 Loss 3.080821\n",
      "Epoch 3181 Loss 3.080319\n",
      "Epoch 3182 Loss 3.079824\n",
      "Epoch 3183 Loss 3.079328\n",
      "Epoch 3184 Loss 3.078834\n",
      "Epoch 3185 Loss 3.078340\n",
      "Epoch 3186 Loss 3.077849\n",
      "Epoch 3187 Loss 3.077360\n",
      "Epoch 3188 Loss 3.076868\n",
      "Epoch 3189 Loss 3.076381\n",
      "Epoch 3190 Loss 3.075894\n",
      "Epoch 3191 Loss 3.075410\n",
      "Epoch 3192 Loss 3.074927\n",
      "Epoch 3193 Loss 3.074445\n",
      "Epoch 3194 Loss 3.073963\n",
      "Epoch 3195 Loss 3.073482\n",
      "Epoch 3196 Loss 3.073004\n",
      "Epoch 3197 Loss 3.072527\n",
      "Epoch 3198 Loss 3.072053\n",
      "Epoch 3199 Loss 3.071579\n",
      "Epoch 3200 Loss 3.071105\n",
      "Epoch 3201 Loss 3.070633\n",
      "Epoch 3202 Loss 3.070162\n",
      "Epoch 3203 Loss 3.069694\n",
      "Epoch 3204 Loss 3.069226\n",
      "Epoch 3205 Loss 3.068758\n",
      "Epoch 3206 Loss 3.068294\n",
      "Epoch 3207 Loss 3.067829\n",
      "Epoch 3208 Loss 3.067368\n",
      "Epoch 3209 Loss 3.066906\n",
      "Epoch 3210 Loss 3.066445\n",
      "Epoch 3211 Loss 3.065987\n",
      "Epoch 3212 Loss 3.065528\n",
      "Epoch 3213 Loss 3.065075\n",
      "Epoch 3214 Loss 3.064617\n",
      "Epoch 3215 Loss 3.064165\n",
      "Epoch 3216 Loss 3.063712\n",
      "Epoch 3217 Loss 3.063261\n",
      "Epoch 3218 Loss 3.062812\n",
      "Epoch 3219 Loss 3.062363\n",
      "Epoch 3220 Loss 3.061915\n",
      "Epoch 3221 Loss 3.061469\n",
      "Epoch 3222 Loss 3.061025\n",
      "Epoch 3223 Loss 3.060581\n",
      "Epoch 3224 Loss 3.060139\n",
      "Epoch 3225 Loss 3.059698\n",
      "Epoch 3226 Loss 3.059257\n",
      "Epoch 3227 Loss 3.058820\n",
      "Epoch 3228 Loss 3.058382\n",
      "Epoch 3229 Loss 3.057947\n",
      "Epoch 3230 Loss 3.057511\n",
      "Epoch 3231 Loss 3.057077\n",
      "Epoch 3232 Loss 3.056645\n",
      "Epoch 3233 Loss 3.056213\n",
      "Epoch 3234 Loss 3.055783\n",
      "Epoch 3235 Loss 3.055356\n",
      "Epoch 3236 Loss 3.054928\n",
      "Epoch 3237 Loss 3.054501\n",
      "Epoch 3238 Loss 3.054076\n",
      "Epoch 3239 Loss 3.053653\n",
      "Epoch 3240 Loss 3.053231\n",
      "Epoch 3241 Loss 3.052809\n",
      "Epoch 3242 Loss 3.052387\n",
      "Epoch 3243 Loss 3.051969\n",
      "Epoch 3244 Loss 3.051552\n",
      "Epoch 3245 Loss 3.051135\n",
      "Epoch 3246 Loss 3.050719\n",
      "Epoch 3247 Loss 3.050305\n",
      "Epoch 3248 Loss 3.049891\n",
      "Epoch 3249 Loss 3.049480\n",
      "Epoch 3250 Loss 3.049069\n",
      "Epoch 3251 Loss 3.048659\n",
      "Epoch 3252 Loss 3.048250\n",
      "Epoch 3253 Loss 3.047843\n",
      "Epoch 3254 Loss 3.047437\n",
      "Epoch 3255 Loss 3.047033\n",
      "Epoch 3256 Loss 3.046628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3257 Loss 3.046227\n",
      "Epoch 3258 Loss 3.045824\n",
      "Epoch 3259 Loss 3.045424\n",
      "Epoch 3260 Loss 3.045025\n",
      "Epoch 3261 Loss 3.044628\n",
      "Epoch 3262 Loss 3.044232\n",
      "Epoch 3263 Loss 3.043835\n",
      "Epoch 3264 Loss 3.043441\n",
      "Epoch 3265 Loss 3.043047\n",
      "Epoch 3266 Loss 3.042654\n",
      "Epoch 3267 Loss 3.042263\n",
      "Epoch 3268 Loss 3.041873\n",
      "Epoch 3269 Loss 3.041486\n",
      "Epoch 3270 Loss 3.041097\n",
      "Epoch 3271 Loss 3.040710\n",
      "Epoch 3272 Loss 3.040325\n",
      "Epoch 3273 Loss 3.039939\n",
      "Epoch 3274 Loss 3.039558\n",
      "Epoch 3275 Loss 3.039175\n",
      "Epoch 3276 Loss 3.038793\n",
      "Epoch 3277 Loss 3.038415\n",
      "Epoch 3278 Loss 3.038036\n",
      "Epoch 3279 Loss 3.037658\n",
      "Epoch 3280 Loss 3.037283\n",
      "Epoch 3281 Loss 3.036906\n",
      "Epoch 3282 Loss 3.036532\n",
      "Epoch 3283 Loss 3.036158\n",
      "Epoch 3284 Loss 3.035786\n",
      "Epoch 3285 Loss 3.035415\n",
      "Epoch 3286 Loss 3.035043\n",
      "Epoch 3287 Loss 3.034675\n",
      "Epoch 3288 Loss 3.034306\n",
      "Epoch 3289 Loss 3.033940\n",
      "Epoch 3290 Loss 3.033575\n",
      "Epoch 3291 Loss 3.033210\n",
      "Epoch 3292 Loss 3.032846\n",
      "Epoch 3293 Loss 3.032483\n",
      "Epoch 3294 Loss 3.032124\n",
      "Epoch 3295 Loss 3.031761\n",
      "Epoch 3296 Loss 3.031402\n",
      "Epoch 3297 Loss 3.031044\n",
      "Epoch 3298 Loss 3.030687\n",
      "Epoch 3299 Loss 3.030329\n",
      "Epoch 3300 Loss 3.029976\n",
      "Epoch 3301 Loss 3.029622\n",
      "Epoch 3302 Loss 3.029269\n",
      "Epoch 3303 Loss 3.028916\n",
      "Epoch 3304 Loss 3.028567\n",
      "Epoch 3305 Loss 3.028216\n",
      "Epoch 3306 Loss 3.027868\n",
      "Epoch 3307 Loss 3.027517\n",
      "Epoch 3308 Loss 3.027172\n",
      "Epoch 3309 Loss 3.026825\n",
      "Epoch 3310 Loss 3.026481\n",
      "Epoch 3311 Loss 3.026138\n",
      "Epoch 3312 Loss 3.025795\n",
      "Epoch 3313 Loss 3.025452\n",
      "Epoch 3314 Loss 3.025112\n",
      "Epoch 3315 Loss 3.024773\n",
      "Epoch 3316 Loss 3.024433\n",
      "Epoch 3317 Loss 3.024096\n",
      "Epoch 3318 Loss 3.023759\n",
      "Epoch 3319 Loss 3.023423\n",
      "Epoch 3320 Loss 3.023088\n",
      "Epoch 3321 Loss 3.022755\n",
      "Epoch 3322 Loss 3.022423\n",
      "Epoch 3323 Loss 3.022091\n",
      "Epoch 3324 Loss 3.021760\n",
      "Epoch 3325 Loss 3.021430\n",
      "Epoch 3326 Loss 3.021101\n",
      "Epoch 3327 Loss 3.020773\n",
      "Epoch 3328 Loss 3.020447\n",
      "Epoch 3329 Loss 3.020122\n",
      "Epoch 3330 Loss 3.019795\n",
      "Epoch 3331 Loss 3.019473\n",
      "Epoch 3332 Loss 3.019150\n",
      "Epoch 3333 Loss 3.018829\n",
      "Epoch 3334 Loss 3.018507\n",
      "Epoch 3335 Loss 3.018189\n",
      "Epoch 3336 Loss 3.017869\n",
      "Epoch 3337 Loss 3.017550\n",
      "Epoch 3338 Loss 3.017233\n",
      "Epoch 3339 Loss 3.016917\n",
      "Epoch 3340 Loss 3.016602\n",
      "Epoch 3341 Loss 3.016288\n",
      "Epoch 3342 Loss 3.015975\n",
      "Epoch 3343 Loss 3.015661\n",
      "Epoch 3344 Loss 3.015350\n",
      "Epoch 3345 Loss 3.015040\n",
      "Epoch 3346 Loss 3.014731\n",
      "Epoch 3347 Loss 3.014422\n",
      "Epoch 3348 Loss 3.014116\n",
      "Epoch 3349 Loss 3.013808\n",
      "Epoch 3350 Loss 3.013505\n",
      "Epoch 3351 Loss 3.013199\n",
      "Epoch 3352 Loss 3.012894\n",
      "Epoch 3353 Loss 3.012592\n",
      "Epoch 3354 Loss 3.012289\n",
      "Epoch 3355 Loss 3.011989\n",
      "Epoch 3356 Loss 3.011687\n",
      "Epoch 3357 Loss 3.011389\n",
      "Epoch 3358 Loss 3.011090\n",
      "Epoch 3359 Loss 3.010792\n",
      "Epoch 3360 Loss 3.010496\n",
      "Epoch 3361 Loss 3.010200\n",
      "Epoch 3362 Loss 3.009907\n",
      "Epoch 3363 Loss 3.009613\n",
      "Epoch 3364 Loss 3.009319\n",
      "Epoch 3365 Loss 3.009027\n",
      "Epoch 3366 Loss 3.008737\n",
      "Epoch 3367 Loss 3.008446\n",
      "Epoch 3368 Loss 3.008156\n",
      "Epoch 3369 Loss 3.007868\n",
      "Epoch 3370 Loss 3.007581\n",
      "Epoch 3371 Loss 3.007293\n",
      "Epoch 3372 Loss 3.007007\n",
      "Epoch 3373 Loss 3.006723\n",
      "Epoch 3374 Loss 3.006439\n",
      "Epoch 3375 Loss 3.006156\n",
      "Epoch 3376 Loss 3.005872\n",
      "Epoch 3377 Loss 3.005592\n",
      "Epoch 3378 Loss 3.005311\n",
      "Epoch 3379 Loss 3.005032\n",
      "Epoch 3380 Loss 3.004754\n",
      "Epoch 3381 Loss 3.004475\n",
      "Epoch 3382 Loss 3.004197\n",
      "Epoch 3383 Loss 3.003921\n",
      "Epoch 3384 Loss 3.003647\n",
      "Epoch 3385 Loss 3.003371\n",
      "Epoch 3386 Loss 3.003098\n",
      "Epoch 3387 Loss 3.002826\n",
      "Epoch 3388 Loss 3.002551\n",
      "Epoch 3389 Loss 3.002282\n",
      "Epoch 3390 Loss 3.002012\n",
      "Epoch 3391 Loss 3.001742\n",
      "Epoch 3392 Loss 3.001472\n",
      "Epoch 3393 Loss 3.001204\n",
      "Epoch 3394 Loss 3.000939\n",
      "Epoch 3395 Loss 3.000673\n",
      "Epoch 3396 Loss 3.000407\n",
      "Epoch 3397 Loss 3.000143\n",
      "Epoch 3398 Loss 2.999879\n",
      "Epoch 3399 Loss 2.999614\n",
      "Epoch 3400 Loss 2.999354\n",
      "Epoch 3401 Loss 2.999093\n",
      "Epoch 3402 Loss 2.998832\n",
      "Epoch 3403 Loss 2.998573\n",
      "Epoch 3404 Loss 2.998314\n",
      "Epoch 3405 Loss 2.998057\n",
      "Epoch 3406 Loss 2.997798\n",
      "Epoch 3407 Loss 2.997542\n",
      "Epoch 3408 Loss 2.997287\n",
      "Epoch 3409 Loss 2.997032\n",
      "Epoch 3410 Loss 2.996778\n",
      "Epoch 3411 Loss 2.996525\n",
      "Epoch 3412 Loss 2.996274\n",
      "Epoch 3413 Loss 2.996022\n",
      "Epoch 3414 Loss 2.995772\n",
      "Epoch 3415 Loss 2.995521\n",
      "Epoch 3416 Loss 2.995272\n",
      "Epoch 3417 Loss 2.995024\n",
      "Epoch 3418 Loss 2.994776\n",
      "Epoch 3419 Loss 2.994528\n",
      "Epoch 3420 Loss 2.994282\n",
      "Epoch 3421 Loss 2.994038\n",
      "Epoch 3422 Loss 2.993794\n",
      "Epoch 3423 Loss 2.993549\n",
      "Epoch 3424 Loss 2.993306\n",
      "Epoch 3425 Loss 2.993063\n",
      "Epoch 3426 Loss 2.992824\n",
      "Epoch 3427 Loss 2.992584\n",
      "Epoch 3428 Loss 2.992344\n",
      "Epoch 3429 Loss 2.992104\n",
      "Epoch 3430 Loss 2.991866\n",
      "Epoch 3431 Loss 2.991628\n",
      "Epoch 3432 Loss 2.991392\n",
      "Epoch 3433 Loss 2.991155\n",
      "Epoch 3434 Loss 2.990921\n",
      "Epoch 3435 Loss 2.990685\n",
      "Epoch 3436 Loss 2.990453\n",
      "Epoch 3437 Loss 2.990219\n",
      "Epoch 3438 Loss 2.989987\n",
      "Epoch 3439 Loss 2.989755\n",
      "Epoch 3440 Loss 2.989523\n",
      "Epoch 3441 Loss 2.989293\n",
      "Epoch 3442 Loss 2.989065\n",
      "Epoch 3443 Loss 2.988837\n",
      "Epoch 3444 Loss 2.988609\n",
      "Epoch 3445 Loss 2.988381\n",
      "Epoch 3446 Loss 2.988154\n",
      "Epoch 3447 Loss 2.987931\n",
      "Epoch 3448 Loss 2.987704\n",
      "Epoch 3449 Loss 2.987479\n",
      "Epoch 3450 Loss 2.987257\n",
      "Epoch 3451 Loss 2.987034\n",
      "Epoch 3452 Loss 2.986812\n",
      "Epoch 3453 Loss 2.986591\n",
      "Epoch 3454 Loss 2.986372\n",
      "Epoch 3455 Loss 2.986150\n",
      "Epoch 3456 Loss 2.985931\n",
      "Epoch 3457 Loss 2.985712\n",
      "Epoch 3458 Loss 2.985494\n",
      "Epoch 3459 Loss 2.985277\n",
      "Epoch 3460 Loss 2.985062\n",
      "Epoch 3461 Loss 2.984845\n",
      "Epoch 3462 Loss 2.984630\n",
      "Epoch 3463 Loss 2.984415\n",
      "Epoch 3464 Loss 2.984203\n",
      "Epoch 3465 Loss 2.983990\n",
      "Epoch 3466 Loss 2.983778\n",
      "Epoch 3467 Loss 2.983567\n",
      "Epoch 3468 Loss 2.983355\n",
      "Epoch 3469 Loss 2.983146\n",
      "Epoch 3470 Loss 2.982937\n",
      "Epoch 3471 Loss 2.982727\n",
      "Epoch 3472 Loss 2.982518\n",
      "Epoch 3473 Loss 2.982312\n",
      "Epoch 3474 Loss 2.982105\n",
      "Epoch 3475 Loss 2.981897\n",
      "Epoch 3476 Loss 2.981693\n",
      "Epoch 3477 Loss 2.981489\n",
      "Epoch 3478 Loss 2.981284\n",
      "Epoch 3479 Loss 2.981081\n",
      "Epoch 3480 Loss 2.980879\n",
      "Epoch 3481 Loss 2.980676\n",
      "Epoch 3482 Loss 2.980474\n",
      "Epoch 3483 Loss 2.980273\n",
      "Epoch 3484 Loss 2.980074\n",
      "Epoch 3485 Loss 2.979875\n",
      "Epoch 3486 Loss 2.979676\n",
      "Epoch 3487 Loss 2.979478\n",
      "Epoch 3488 Loss 2.979281\n",
      "Epoch 3489 Loss 2.979083\n",
      "Epoch 3490 Loss 2.978887\n",
      "Epoch 3491 Loss 2.978691\n",
      "Epoch 3492 Loss 2.978498\n",
      "Epoch 3493 Loss 2.978303\n",
      "Epoch 3494 Loss 2.978109\n",
      "Epoch 3495 Loss 2.977917\n",
      "Epoch 3496 Loss 2.977725\n",
      "Epoch 3497 Loss 2.977532\n",
      "Epoch 3498 Loss 2.977341\n",
      "Epoch 3499 Loss 2.977151\n",
      "Epoch 3500 Loss 2.976961\n",
      "Epoch 3501 Loss 2.976772\n",
      "Epoch 3502 Loss 2.976585\n",
      "Epoch 3503 Loss 2.976398\n",
      "Epoch 3504 Loss 2.976209\n",
      "Epoch 3505 Loss 2.976022\n",
      "Epoch 3506 Loss 2.975837\n",
      "Epoch 3507 Loss 2.975652\n",
      "Epoch 3508 Loss 2.975467\n",
      "Epoch 3509 Loss 2.975282\n",
      "Epoch 3510 Loss 2.975099\n",
      "Epoch 3511 Loss 2.974917\n",
      "Epoch 3512 Loss 2.974734\n",
      "Epoch 3513 Loss 2.974552\n",
      "Epoch 3514 Loss 2.974370\n",
      "Epoch 3515 Loss 2.974191\n",
      "Epoch 3516 Loss 2.974010\n",
      "Epoch 3517 Loss 2.973830\n",
      "Epoch 3518 Loss 2.973653\n",
      "Epoch 3519 Loss 2.973474\n",
      "Epoch 3520 Loss 2.973297\n",
      "Epoch 3521 Loss 2.973119\n",
      "Epoch 3522 Loss 2.972943\n",
      "Epoch 3523 Loss 2.972768\n",
      "Epoch 3524 Loss 2.972594\n",
      "Epoch 3525 Loss 2.972419\n",
      "Epoch 3526 Loss 2.972246\n",
      "Epoch 3527 Loss 2.972072\n",
      "Epoch 3528 Loss 2.971898\n",
      "Epoch 3529 Loss 2.971726\n",
      "Epoch 3530 Loss 2.971557\n",
      "Epoch 3531 Loss 2.971384\n",
      "Epoch 3532 Loss 2.971214\n",
      "Epoch 3533 Loss 2.971043\n",
      "Epoch 3534 Loss 2.970876\n",
      "Epoch 3535 Loss 2.970707\n",
      "Epoch 3536 Loss 2.970538\n",
      "Epoch 3537 Loss 2.970370\n",
      "Epoch 3538 Loss 2.970204\n",
      "Epoch 3539 Loss 2.970038\n",
      "Epoch 3540 Loss 2.969872\n",
      "Epoch 3541 Loss 2.969707\n",
      "Epoch 3542 Loss 2.969541\n",
      "Epoch 3543 Loss 2.969377\n",
      "Epoch 3544 Loss 2.969214\n",
      "Epoch 3545 Loss 2.969052\n",
      "Epoch 3546 Loss 2.968889\n",
      "Epoch 3547 Loss 2.968725\n",
      "Epoch 3548 Loss 2.968563\n",
      "Epoch 3549 Loss 2.968404\n",
      "Epoch 3550 Loss 2.968244\n",
      "Epoch 3551 Loss 2.968083\n",
      "Epoch 3552 Loss 2.967924\n",
      "Epoch 3553 Loss 2.967764\n",
      "Epoch 3554 Loss 2.967607\n",
      "Epoch 3555 Loss 2.967450\n",
      "Epoch 3556 Loss 2.967293\n",
      "Epoch 3557 Loss 2.967136\n",
      "Epoch 3558 Loss 2.966982\n",
      "Epoch 3559 Loss 2.966825\n",
      "Epoch 3560 Loss 2.966670\n",
      "Epoch 3561 Loss 2.966515\n",
      "Epoch 3562 Loss 2.966364\n",
      "Epoch 3563 Loss 2.966210\n",
      "Epoch 3564 Loss 2.966056\n",
      "Epoch 3565 Loss 2.965904\n",
      "Epoch 3566 Loss 2.965753\n",
      "Epoch 3567 Loss 2.965601\n",
      "Epoch 3568 Loss 2.965451\n",
      "Epoch 3569 Loss 2.965301\n",
      "Epoch 3570 Loss 2.965151\n",
      "Epoch 3571 Loss 2.965003\n",
      "Epoch 3572 Loss 2.964854\n",
      "Epoch 3573 Loss 2.964705\n",
      "Epoch 3574 Loss 2.964559\n",
      "Epoch 3575 Loss 2.964410\n",
      "Epoch 3576 Loss 2.964266\n",
      "Epoch 3577 Loss 2.964119\n",
      "Epoch 3578 Loss 2.963973\n",
      "Epoch 3579 Loss 2.963830\n",
      "Epoch 3580 Loss 2.963684\n",
      "Epoch 3581 Loss 2.963540\n",
      "Epoch 3582 Loss 2.963398\n",
      "Epoch 3583 Loss 2.963253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3584 Loss 2.963110\n",
      "Epoch 3585 Loss 2.962970\n",
      "Epoch 3586 Loss 2.962827\n",
      "Epoch 3587 Loss 2.962686\n",
      "Epoch 3588 Loss 2.962547\n",
      "Epoch 3589 Loss 2.962407\n",
      "Epoch 3590 Loss 2.962268\n",
      "Epoch 3591 Loss 2.962128\n",
      "Epoch 3592 Loss 2.961989\n",
      "Epoch 3593 Loss 2.961852\n",
      "Epoch 3594 Loss 2.961714\n",
      "Epoch 3595 Loss 2.961577\n",
      "Epoch 3596 Loss 2.961441\n",
      "Epoch 3597 Loss 2.961305\n",
      "Epoch 3598 Loss 2.961167\n",
      "Epoch 3599 Loss 2.961034\n",
      "Epoch 3600 Loss 2.960898\n",
      "Epoch 3601 Loss 2.960764\n",
      "Epoch 3602 Loss 2.960629\n",
      "Epoch 3603 Loss 2.960497\n",
      "Epoch 3604 Loss 2.960363\n",
      "Epoch 3605 Loss 2.960230\n",
      "Epoch 3606 Loss 2.960099\n",
      "Epoch 3607 Loss 2.959969\n",
      "Epoch 3608 Loss 2.959838\n",
      "Epoch 3609 Loss 2.959706\n",
      "Epoch 3610 Loss 2.959577\n",
      "Epoch 3611 Loss 2.959447\n",
      "Epoch 3612 Loss 2.959318\n",
      "Epoch 3613 Loss 2.959188\n",
      "Epoch 3614 Loss 2.959061\n",
      "Epoch 3615 Loss 2.958933\n",
      "Epoch 3616 Loss 2.958805\n",
      "Epoch 3617 Loss 2.958678\n",
      "Epoch 3618 Loss 2.958552\n",
      "Epoch 3619 Loss 2.958426\n",
      "Epoch 3620 Loss 2.958302\n",
      "Epoch 3621 Loss 2.958176\n",
      "Epoch 3622 Loss 2.958050\n",
      "Epoch 3623 Loss 2.957927\n",
      "Epoch 3624 Loss 2.957802\n",
      "Epoch 3625 Loss 2.957681\n",
      "Epoch 3626 Loss 2.957558\n",
      "Epoch 3627 Loss 2.957434\n",
      "Epoch 3628 Loss 2.957314\n",
      "Epoch 3629 Loss 2.957192\n",
      "Epoch 3630 Loss 2.957072\n",
      "Epoch 3631 Loss 2.956951\n",
      "Epoch 3632 Loss 2.956831\n",
      "Epoch 3633 Loss 2.956711\n",
      "Epoch 3634 Loss 2.956590\n",
      "Epoch 3635 Loss 2.956472\n",
      "Epoch 3636 Loss 2.956353\n",
      "Epoch 3637 Loss 2.956234\n",
      "Epoch 3638 Loss 2.956118\n",
      "Epoch 3639 Loss 2.955999\n",
      "Epoch 3640 Loss 2.955882\n",
      "Epoch 3641 Loss 2.955766\n",
      "Epoch 3642 Loss 2.955650\n",
      "Epoch 3643 Loss 2.955536\n",
      "Epoch 3644 Loss 2.955420\n",
      "Epoch 3645 Loss 2.955305\n",
      "Epoch 3646 Loss 2.955190\n",
      "Epoch 3647 Loss 2.955076\n",
      "Epoch 3648 Loss 2.954965\n",
      "Epoch 3649 Loss 2.954851\n",
      "Epoch 3650 Loss 2.954737\n",
      "Epoch 3651 Loss 2.954626\n",
      "Epoch 3652 Loss 2.954514\n",
      "Epoch 3653 Loss 2.954404\n",
      "Epoch 3654 Loss 2.954291\n",
      "Epoch 3655 Loss 2.954180\n",
      "Epoch 3656 Loss 2.954070\n",
      "Epoch 3657 Loss 2.953960\n",
      "Epoch 3658 Loss 2.953851\n",
      "Epoch 3659 Loss 2.953743\n",
      "Epoch 3660 Loss 2.953635\n",
      "Epoch 3661 Loss 2.953526\n",
      "Epoch 3662 Loss 2.953419\n",
      "Epoch 3663 Loss 2.953311\n",
      "Epoch 3664 Loss 2.953203\n",
      "Epoch 3665 Loss 2.953097\n",
      "Epoch 3666 Loss 2.952990\n",
      "Epoch 3667 Loss 2.952884\n",
      "Epoch 3668 Loss 2.952779\n",
      "Epoch 3669 Loss 2.952675\n",
      "Epoch 3670 Loss 2.952569\n",
      "Epoch 3671 Loss 2.952466\n",
      "Epoch 3672 Loss 2.952360\n",
      "Epoch 3673 Loss 2.952257\n",
      "Epoch 3674 Loss 2.952154\n",
      "Epoch 3675 Loss 2.952052\n",
      "Epoch 3676 Loss 2.951950\n",
      "Epoch 3677 Loss 2.951847\n",
      "Epoch 3678 Loss 2.951744\n",
      "Epoch 3679 Loss 2.951644\n",
      "Epoch 3680 Loss 2.951542\n",
      "Epoch 3681 Loss 2.951443\n",
      "Epoch 3682 Loss 2.951342\n",
      "Epoch 3683 Loss 2.951242\n",
      "Epoch 3684 Loss 2.951144\n",
      "Epoch 3685 Loss 2.951044\n",
      "Epoch 3686 Loss 2.950944\n",
      "Epoch 3687 Loss 2.950848\n",
      "Epoch 3688 Loss 2.950749\n",
      "Epoch 3689 Loss 2.950650\n",
      "Epoch 3690 Loss 2.950555\n",
      "Epoch 3691 Loss 2.950456\n",
      "Epoch 3692 Loss 2.950362\n",
      "Epoch 3693 Loss 2.950264\n",
      "Epoch 3694 Loss 2.950168\n",
      "Epoch 3695 Loss 2.950073\n",
      "Epoch 3696 Loss 2.949979\n",
      "Epoch 3697 Loss 2.949883\n",
      "Epoch 3698 Loss 2.949788\n",
      "Epoch 3699 Loss 2.949695\n",
      "Epoch 3700 Loss 2.949601\n",
      "Epoch 3701 Loss 2.949508\n",
      "Epoch 3702 Loss 2.949415\n",
      "Epoch 3703 Loss 2.949322\n",
      "Epoch 3704 Loss 2.949229\n",
      "Epoch 3705 Loss 2.949138\n",
      "Epoch 3706 Loss 2.949045\n",
      "Epoch 3707 Loss 2.948955\n",
      "Epoch 3708 Loss 2.948864\n",
      "Epoch 3709 Loss 2.948773\n",
      "Epoch 3710 Loss 2.948683\n",
      "Epoch 3711 Loss 2.948594\n",
      "Epoch 3712 Loss 2.948503\n",
      "Epoch 3713 Loss 2.948415\n",
      "Epoch 3714 Loss 2.948326\n",
      "Epoch 3715 Loss 2.948238\n",
      "Epoch 3716 Loss 2.948150\n",
      "Epoch 3717 Loss 2.948061\n",
      "Epoch 3718 Loss 2.947973\n",
      "Epoch 3719 Loss 2.947886\n",
      "Epoch 3720 Loss 2.947800\n",
      "Epoch 3721 Loss 2.947712\n",
      "Epoch 3722 Loss 2.947627\n",
      "Epoch 3723 Loss 2.947540\n",
      "Epoch 3724 Loss 2.947454\n",
      "Epoch 3725 Loss 2.947369\n",
      "Epoch 3726 Loss 2.947285\n",
      "Epoch 3727 Loss 2.947200\n",
      "Epoch 3728 Loss 2.947116\n",
      "Epoch 3729 Loss 2.947032\n",
      "Epoch 3730 Loss 2.946947\n",
      "Epoch 3731 Loss 2.946864\n",
      "Epoch 3732 Loss 2.946781\n",
      "Epoch 3733 Loss 2.946698\n",
      "Epoch 3734 Loss 2.946615\n",
      "Epoch 3735 Loss 2.946533\n",
      "Epoch 3736 Loss 2.946451\n",
      "Epoch 3737 Loss 2.946372\n",
      "Epoch 3738 Loss 2.946289\n",
      "Epoch 3739 Loss 2.946209\n",
      "Epoch 3740 Loss 2.946129\n",
      "Epoch 3741 Loss 2.946049\n",
      "Epoch 3742 Loss 2.945967\n",
      "Epoch 3743 Loss 2.945888\n",
      "Epoch 3744 Loss 2.945809\n",
      "Epoch 3745 Loss 2.945731\n",
      "Epoch 3746 Loss 2.945652\n",
      "Epoch 3747 Loss 2.945573\n",
      "Epoch 3748 Loss 2.945495\n",
      "Epoch 3749 Loss 2.945417\n",
      "Epoch 3750 Loss 2.945340\n",
      "Epoch 3751 Loss 2.945262\n",
      "Epoch 3752 Loss 2.945187\n",
      "Epoch 3753 Loss 2.945109\n",
      "Epoch 3754 Loss 2.945034\n",
      "Epoch 3755 Loss 2.944957\n",
      "Epoch 3756 Loss 2.944880\n",
      "Epoch 3757 Loss 2.944807\n",
      "Epoch 3758 Loss 2.944731\n",
      "Epoch 3759 Loss 2.944656\n",
      "Epoch 3760 Loss 2.944581\n",
      "Epoch 3761 Loss 2.944506\n",
      "Epoch 3762 Loss 2.944433\n",
      "Epoch 3763 Loss 2.944360\n",
      "Epoch 3764 Loss 2.944286\n",
      "Epoch 3765 Loss 2.944212\n",
      "Epoch 3766 Loss 2.944141\n",
      "Epoch 3767 Loss 2.944066\n",
      "Epoch 3768 Loss 2.943995\n",
      "Epoch 3769 Loss 2.943924\n",
      "Epoch 3770 Loss 2.943851\n",
      "Epoch 3771 Loss 2.943780\n",
      "Epoch 3772 Loss 2.943707\n",
      "Epoch 3773 Loss 2.943639\n",
      "Epoch 3774 Loss 2.943568\n",
      "Epoch 3775 Loss 2.943497\n",
      "Epoch 3776 Loss 2.943426\n",
      "Epoch 3777 Loss 2.943357\n",
      "Epoch 3778 Loss 2.943286\n",
      "Epoch 3779 Loss 2.943218\n",
      "Epoch 3780 Loss 2.943149\n",
      "Epoch 3781 Loss 2.943081\n",
      "Epoch 3782 Loss 2.943012\n",
      "Epoch 3783 Loss 2.942944\n",
      "Epoch 3784 Loss 2.942875\n",
      "Epoch 3785 Loss 2.942809\n",
      "Epoch 3786 Loss 2.942742\n",
      "Epoch 3787 Loss 2.942674\n",
      "Epoch 3788 Loss 2.942607\n",
      "Epoch 3789 Loss 2.942540\n",
      "Epoch 3790 Loss 2.942474\n",
      "Epoch 3791 Loss 2.942408\n",
      "Epoch 3792 Loss 2.942343\n",
      "Epoch 3793 Loss 2.942276\n",
      "Epoch 3794 Loss 2.942212\n",
      "Epoch 3795 Loss 2.942146\n",
      "Epoch 3796 Loss 2.942081\n",
      "Epoch 3797 Loss 2.942016\n",
      "Epoch 3798 Loss 2.941953\n",
      "Epoch 3799 Loss 2.941889\n",
      "Epoch 3800 Loss 2.941826\n",
      "Epoch 3801 Loss 2.941761\n",
      "Epoch 3802 Loss 2.941698\n",
      "Epoch 3803 Loss 2.941635\n",
      "Epoch 3804 Loss 2.941571\n",
      "Epoch 3805 Loss 2.941511\n",
      "Epoch 3806 Loss 2.941447\n",
      "Epoch 3807 Loss 2.941386\n",
      "Epoch 3808 Loss 2.941325\n",
      "Epoch 3809 Loss 2.941263\n",
      "Epoch 3810 Loss 2.941203\n",
      "Epoch 3811 Loss 2.941141\n",
      "Epoch 3812 Loss 2.941080\n",
      "Epoch 3813 Loss 2.941017\n",
      "Epoch 3814 Loss 2.940960\n",
      "Epoch 3815 Loss 2.940899\n",
      "Epoch 3816 Loss 2.940840\n",
      "Epoch 3817 Loss 2.940780\n",
      "Epoch 3818 Loss 2.940719\n",
      "Epoch 3819 Loss 2.940661\n",
      "Epoch 3820 Loss 2.940602\n",
      "Epoch 3821 Loss 2.940543\n",
      "Epoch 3822 Loss 2.940485\n",
      "Epoch 3823 Loss 2.940427\n",
      "Epoch 3824 Loss 2.940369\n",
      "Epoch 3825 Loss 2.940312\n",
      "Epoch 3826 Loss 2.940253\n",
      "Epoch 3827 Loss 2.940197\n",
      "Epoch 3828 Loss 2.940140\n",
      "Epoch 3829 Loss 2.940084\n",
      "Epoch 3830 Loss 2.940027\n",
      "Epoch 3831 Loss 2.939973\n",
      "Epoch 3832 Loss 2.939913\n",
      "Epoch 3833 Loss 2.939858\n",
      "Epoch 3834 Loss 2.939802\n",
      "Epoch 3835 Loss 2.939748\n",
      "Epoch 3836 Loss 2.939692\n",
      "Epoch 3837 Loss 2.939637\n",
      "Epoch 3838 Loss 2.939583\n",
      "Epoch 3839 Loss 2.939529\n",
      "Epoch 3840 Loss 2.939474\n",
      "Epoch 3841 Loss 2.939420\n",
      "Epoch 3842 Loss 2.939365\n",
      "Epoch 3843 Loss 2.939313\n",
      "Epoch 3844 Loss 2.939258\n",
      "Epoch 3845 Loss 2.939205\n",
      "Epoch 3846 Loss 2.939153\n",
      "Epoch 3847 Loss 2.939101\n",
      "Epoch 3848 Loss 2.939048\n",
      "Epoch 3849 Loss 2.938996\n",
      "Epoch 3850 Loss 2.938941\n",
      "Epoch 3851 Loss 2.938891\n",
      "Epoch 3852 Loss 2.938841\n",
      "Epoch 3853 Loss 2.938789\n",
      "Epoch 3854 Loss 2.938736\n",
      "Epoch 3855 Loss 2.938686\n",
      "Epoch 3856 Loss 2.938634\n",
      "Epoch 3857 Loss 2.938583\n",
      "Epoch 3858 Loss 2.938534\n",
      "Epoch 3859 Loss 2.938482\n",
      "Epoch 3860 Loss 2.938433\n",
      "Epoch 3861 Loss 2.938383\n",
      "Epoch 3862 Loss 2.938333\n",
      "Epoch 3863 Loss 2.938283\n",
      "Epoch 3864 Loss 2.938235\n",
      "Epoch 3865 Loss 2.938187\n",
      "Epoch 3866 Loss 2.938136\n",
      "Epoch 3867 Loss 2.938088\n",
      "Epoch 3868 Loss 2.938041\n",
      "Epoch 3869 Loss 2.937991\n",
      "Epoch 3870 Loss 2.937945\n",
      "Epoch 3871 Loss 2.937898\n",
      "Epoch 3872 Loss 2.937848\n",
      "Epoch 3873 Loss 2.937802\n",
      "Epoch 3874 Loss 2.937753\n",
      "Epoch 3875 Loss 2.937705\n",
      "Epoch 3876 Loss 2.937659\n",
      "Epoch 3877 Loss 2.937613\n",
      "Epoch 3878 Loss 2.937566\n",
      "Epoch 3879 Loss 2.937521\n",
      "Epoch 3880 Loss 2.937475\n",
      "Epoch 3881 Loss 2.937429\n",
      "Epoch 3882 Loss 2.937384\n",
      "Epoch 3883 Loss 2.937339\n",
      "Epoch 3884 Loss 2.937293\n",
      "Epoch 3885 Loss 2.937248\n",
      "Epoch 3886 Loss 2.937203\n",
      "Epoch 3887 Loss 2.937157\n",
      "Epoch 3888 Loss 2.937112\n",
      "Epoch 3889 Loss 2.937069\n",
      "Epoch 3890 Loss 2.937024\n",
      "Epoch 3891 Loss 2.936980\n",
      "Epoch 3892 Loss 2.936936\n",
      "Epoch 3893 Loss 2.936893\n",
      "Epoch 3894 Loss 2.936850\n",
      "Epoch 3895 Loss 2.936807\n",
      "Epoch 3896 Loss 2.936764\n",
      "Epoch 3897 Loss 2.936721\n",
      "Epoch 3898 Loss 2.936679\n",
      "Epoch 3899 Loss 2.936636\n",
      "Epoch 3900 Loss 2.936593\n",
      "Epoch 3901 Loss 2.936551\n",
      "Epoch 3902 Loss 2.936509\n",
      "Epoch 3903 Loss 2.936466\n",
      "Epoch 3904 Loss 2.936425\n",
      "Epoch 3905 Loss 2.936384\n",
      "Epoch 3906 Loss 2.936341\n",
      "Epoch 3907 Loss 2.936301\n",
      "Epoch 3908 Loss 2.936261\n",
      "Epoch 3909 Loss 2.936220\n",
      "Epoch 3910 Loss 2.936179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3911 Loss 2.936139\n",
      "Epoch 3912 Loss 2.936098\n",
      "Epoch 3913 Loss 2.936058\n",
      "Epoch 3914 Loss 2.936018\n",
      "Epoch 3915 Loss 2.935977\n",
      "Epoch 3916 Loss 2.935939\n",
      "Epoch 3917 Loss 2.935898\n",
      "Epoch 3918 Loss 2.935860\n",
      "Epoch 3919 Loss 2.935821\n",
      "Epoch 3920 Loss 2.935781\n",
      "Epoch 3921 Loss 2.935744\n",
      "Epoch 3922 Loss 2.935705\n",
      "Epoch 3923 Loss 2.935666\n",
      "Epoch 3924 Loss 2.935627\n",
      "Epoch 3925 Loss 2.935589\n",
      "Epoch 3926 Loss 2.935553\n",
      "Epoch 3927 Loss 2.935515\n",
      "Epoch 3928 Loss 2.935475\n",
      "Epoch 3929 Loss 2.935439\n",
      "Epoch 3930 Loss 2.935402\n",
      "Epoch 3931 Loss 2.935364\n",
      "Epoch 3932 Loss 2.935328\n",
      "Epoch 3933 Loss 2.935289\n",
      "Epoch 3934 Loss 2.935255\n",
      "Epoch 3935 Loss 2.935217\n",
      "Epoch 3936 Loss 2.935180\n",
      "Epoch 3937 Loss 2.935144\n",
      "Epoch 3938 Loss 2.935108\n",
      "Epoch 3939 Loss 2.935073\n",
      "Epoch 3940 Loss 2.935038\n",
      "Epoch 3941 Loss 2.935001\n",
      "Epoch 3942 Loss 2.934966\n",
      "Epoch 3943 Loss 2.934930\n",
      "Epoch 3944 Loss 2.934895\n",
      "Epoch 3945 Loss 2.934860\n",
      "Epoch 3946 Loss 2.934825\n",
      "Epoch 3947 Loss 2.934792\n",
      "Epoch 3948 Loss 2.934756\n",
      "Epoch 3949 Loss 2.934722\n",
      "Epoch 3950 Loss 2.934689\n",
      "Epoch 3951 Loss 2.934654\n",
      "Epoch 3952 Loss 2.934619\n",
      "Epoch 3953 Loss 2.934587\n",
      "Epoch 3954 Loss 2.934555\n",
      "Epoch 3955 Loss 2.934519\n",
      "Epoch 3956 Loss 2.934486\n",
      "Epoch 3957 Loss 2.934452\n",
      "Epoch 3958 Loss 2.934419\n",
      "Epoch 3959 Loss 2.934387\n",
      "Epoch 3960 Loss 2.934354\n",
      "Epoch 3961 Loss 2.934321\n",
      "Epoch 3962 Loss 2.934290\n",
      "Epoch 3963 Loss 2.934256\n",
      "Epoch 3964 Loss 2.934224\n",
      "Epoch 3965 Loss 2.934191\n",
      "Epoch 3966 Loss 2.934159\n",
      "Epoch 3967 Loss 2.934128\n",
      "Epoch 3968 Loss 2.934098\n",
      "Epoch 3969 Loss 2.934064\n",
      "Epoch 3970 Loss 2.934034\n",
      "Epoch 3971 Loss 2.934003\n",
      "Epoch 3972 Loss 2.933971\n",
      "Epoch 3973 Loss 2.933941\n",
      "Epoch 3974 Loss 2.933910\n",
      "Epoch 3975 Loss 2.933878\n",
      "Epoch 3976 Loss 2.933849\n",
      "Epoch 3977 Loss 2.933819\n",
      "Epoch 3978 Loss 2.933788\n",
      "Epoch 3979 Loss 2.933757\n",
      "Epoch 3980 Loss 2.933729\n",
      "Epoch 3981 Loss 2.933697\n",
      "Epoch 3982 Loss 2.933668\n",
      "Epoch 3983 Loss 2.933637\n",
      "Epoch 3984 Loss 2.933610\n",
      "Epoch 3985 Loss 2.933578\n",
      "Epoch 3986 Loss 2.933549\n",
      "Epoch 3987 Loss 2.933521\n",
      "Epoch 3988 Loss 2.933492\n",
      "Epoch 3989 Loss 2.933463\n",
      "Epoch 3990 Loss 2.933435\n",
      "Epoch 3991 Loss 2.933405\n",
      "Epoch 3992 Loss 2.933377\n",
      "Epoch 3993 Loss 2.933350\n",
      "Epoch 3994 Loss 2.933320\n",
      "Epoch 3995 Loss 2.933292\n",
      "Epoch 3996 Loss 2.933264\n",
      "Epoch 3997 Loss 2.933236\n",
      "Epoch 3998 Loss 2.933208\n",
      "Epoch 3999 Loss 2.933181\n",
      "Epoch 4000 Loss 2.933154\n",
      "Epoch 4001 Loss 2.933127\n",
      "Epoch 4002 Loss 2.933099\n",
      "Epoch 4003 Loss 2.933073\n",
      "Epoch 4004 Loss 2.933044\n",
      "Epoch 4005 Loss 2.933019\n",
      "Epoch 4006 Loss 2.932991\n",
      "Epoch 4007 Loss 2.932964\n",
      "Epoch 4008 Loss 2.932938\n",
      "Epoch 4009 Loss 2.932912\n",
      "Epoch 4010 Loss 2.932886\n",
      "Epoch 4011 Loss 2.932859\n",
      "Epoch 4012 Loss 2.932834\n",
      "Epoch 4013 Loss 2.932808\n",
      "Epoch 4014 Loss 2.932784\n",
      "Epoch 4015 Loss 2.932756\n",
      "Epoch 4016 Loss 2.932730\n",
      "Epoch 4017 Loss 2.932705\n",
      "Epoch 4018 Loss 2.932680\n",
      "Epoch 4019 Loss 2.932655\n",
      "Epoch 4020 Loss 2.932629\n",
      "Epoch 4021 Loss 2.932604\n",
      "Epoch 4022 Loss 2.932580\n",
      "Epoch 4023 Loss 2.932554\n",
      "Epoch 4024 Loss 2.932531\n",
      "Epoch 4025 Loss 2.932505\n",
      "Epoch 4026 Loss 2.932482\n",
      "Epoch 4027 Loss 2.932457\n",
      "Epoch 4028 Loss 2.932432\n",
      "Epoch 4029 Loss 2.932408\n",
      "Epoch 4030 Loss 2.932385\n",
      "Epoch 4031 Loss 2.932361\n",
      "Epoch 4032 Loss 2.932337\n",
      "Epoch 4033 Loss 2.932312\n",
      "Epoch 4034 Loss 2.932287\n",
      "Epoch 4035 Loss 2.932266\n",
      "Epoch 4036 Loss 2.932243\n",
      "Epoch 4037 Loss 2.932221\n",
      "Epoch 4038 Loss 2.932196\n",
      "Epoch 4039 Loss 2.932172\n",
      "Epoch 4040 Loss 2.932149\n",
      "Epoch 4041 Loss 2.932127\n",
      "Epoch 4042 Loss 2.932104\n",
      "Epoch 4043 Loss 2.932083\n",
      "Epoch 4044 Loss 2.932059\n",
      "Epoch 4045 Loss 2.932037\n",
      "Epoch 4046 Loss 2.932014\n",
      "Epoch 4047 Loss 2.931991\n",
      "Epoch 4048 Loss 2.931972\n",
      "Epoch 4049 Loss 2.931948\n",
      "Epoch 4050 Loss 2.931926\n",
      "Epoch 4051 Loss 2.931905\n",
      "Epoch 4052 Loss 2.931882\n",
      "Epoch 4053 Loss 2.931861\n",
      "Epoch 4054 Loss 2.931839\n",
      "Epoch 4055 Loss 2.931819\n",
      "Epoch 4056 Loss 2.931797\n",
      "Epoch 4057 Loss 2.931776\n",
      "Epoch 4058 Loss 2.931754\n",
      "Epoch 4059 Loss 2.931733\n",
      "Epoch 4060 Loss 2.931713\n",
      "Epoch 4061 Loss 2.931690\n",
      "Epoch 4062 Loss 2.931671\n",
      "Epoch 4063 Loss 2.931650\n",
      "Epoch 4064 Loss 2.931628\n",
      "Epoch 4065 Loss 2.931609\n",
      "Epoch 4066 Loss 2.931588\n",
      "Epoch 4067 Loss 2.931568\n",
      "Epoch 4068 Loss 2.931548\n",
      "Epoch 4069 Loss 2.931529\n",
      "Epoch 4070 Loss 2.931510\n",
      "Epoch 4071 Loss 2.931488\n",
      "Epoch 4072 Loss 2.931469\n",
      "Epoch 4073 Loss 2.931449\n",
      "Epoch 4074 Loss 2.931430\n",
      "Epoch 4075 Loss 2.931411\n",
      "Epoch 4076 Loss 2.931391\n",
      "Epoch 4077 Loss 2.931371\n",
      "Epoch 4078 Loss 2.931353\n",
      "Epoch 4079 Loss 2.931333\n",
      "Epoch 4080 Loss 2.931313\n",
      "Epoch 4081 Loss 2.931295\n",
      "Epoch 4082 Loss 2.931274\n",
      "Epoch 4083 Loss 2.931256\n",
      "Epoch 4084 Loss 2.931239\n",
      "Epoch 4085 Loss 2.931220\n",
      "Epoch 4086 Loss 2.931201\n",
      "Epoch 4087 Loss 2.931184\n",
      "Epoch 4088 Loss 2.931164\n",
      "Epoch 4089 Loss 2.931145\n",
      "Epoch 4090 Loss 2.931127\n",
      "Epoch 4091 Loss 2.931109\n",
      "Epoch 4092 Loss 2.931091\n",
      "Epoch 4093 Loss 2.931073\n",
      "Epoch 4094 Loss 2.931055\n",
      "Epoch 4095 Loss 2.931039\n",
      "Epoch 4096 Loss 2.931020\n",
      "Epoch 4097 Loss 2.931003\n",
      "Epoch 4098 Loss 2.930984\n",
      "Epoch 4099 Loss 2.930968\n",
      "Epoch 4100 Loss 2.930949\n",
      "Epoch 4101 Loss 2.930933\n",
      "Epoch 4102 Loss 2.930915\n",
      "Epoch 4103 Loss 2.930899\n",
      "Epoch 4104 Loss 2.930882\n",
      "Epoch 4105 Loss 2.930865\n",
      "Epoch 4106 Loss 2.930848\n",
      "Epoch 4107 Loss 2.930831\n",
      "Epoch 4108 Loss 2.930814\n",
      "Epoch 4109 Loss 2.930797\n",
      "Epoch 4110 Loss 2.930782\n",
      "Epoch 4111 Loss 2.930764\n",
      "Epoch 4112 Loss 2.930749\n",
      "Epoch 4113 Loss 2.930733\n",
      "Epoch 4114 Loss 2.930715\n",
      "Epoch 4115 Loss 2.930701\n",
      "Epoch 4116 Loss 2.930683\n",
      "Epoch 4117 Loss 2.930669\n",
      "Epoch 4118 Loss 2.930653\n",
      "Epoch 4119 Loss 2.930635\n",
      "Epoch 4120 Loss 2.930620\n",
      "Epoch 4121 Loss 2.930605\n",
      "Epoch 4122 Loss 2.930588\n",
      "Epoch 4123 Loss 2.930572\n",
      "Epoch 4124 Loss 2.930559\n",
      "Epoch 4125 Loss 2.930542\n",
      "Epoch 4126 Loss 2.930527\n",
      "Epoch 4127 Loss 2.930511\n",
      "Epoch 4128 Loss 2.930496\n",
      "Epoch 4129 Loss 2.930482\n",
      "Epoch 4130 Loss 2.930466\n",
      "Epoch 4131 Loss 2.930450\n",
      "Epoch 4132 Loss 2.930435\n",
      "Epoch 4133 Loss 2.930420\n",
      "Epoch 4134 Loss 2.930406\n",
      "Epoch 4135 Loss 2.930391\n",
      "Epoch 4136 Loss 2.930378\n",
      "Epoch 4137 Loss 2.930363\n",
      "Epoch 4138 Loss 2.930349\n",
      "Epoch 4139 Loss 2.930334\n",
      "Epoch 4140 Loss 2.930318\n",
      "Epoch 4141 Loss 2.930305\n",
      "Epoch 4142 Loss 2.930292\n",
      "Epoch 4143 Loss 2.930276\n",
      "Epoch 4144 Loss 2.930263\n",
      "Epoch 4145 Loss 2.930248\n",
      "Epoch 4146 Loss 2.930235\n",
      "Epoch 4147 Loss 2.930220\n",
      "Epoch 4148 Loss 2.930206\n",
      "Epoch 4149 Loss 2.930194\n",
      "Epoch 4150 Loss 2.930179\n",
      "Epoch 4151 Loss 2.930166\n",
      "Epoch 4152 Loss 2.930152\n",
      "Epoch 4153 Loss 2.930138\n",
      "Epoch 4154 Loss 2.930125\n",
      "Epoch 4155 Loss 2.930112\n",
      "Epoch 4156 Loss 2.930100\n",
      "Epoch 4157 Loss 2.930084\n",
      "Epoch 4158 Loss 2.930073\n",
      "Epoch 4159 Loss 2.930060\n",
      "Epoch 4160 Loss 2.930046\n",
      "Epoch 4161 Loss 2.930032\n",
      "Epoch 4162 Loss 2.930020\n",
      "Epoch 4163 Loss 2.930009\n",
      "Epoch 4164 Loss 2.929996\n",
      "Epoch 4165 Loss 2.929983\n",
      "Epoch 4166 Loss 2.929970\n",
      "Epoch 4167 Loss 2.929957\n",
      "Epoch 4168 Loss 2.929943\n",
      "Epoch 4169 Loss 2.929932\n",
      "Epoch 4170 Loss 2.929919\n",
      "Epoch 4171 Loss 2.929906\n",
      "Epoch 4172 Loss 2.929896\n",
      "Epoch 4173 Loss 2.929882\n",
      "Epoch 4174 Loss 2.929871\n",
      "Epoch 4175 Loss 2.929857\n",
      "Epoch 4176 Loss 2.929846\n",
      "Epoch 4177 Loss 2.929835\n",
      "Epoch 4178 Loss 2.929821\n",
      "Epoch 4179 Loss 2.929811\n",
      "Epoch 4180 Loss 2.929799\n",
      "Epoch 4181 Loss 2.929787\n",
      "Epoch 4182 Loss 2.929776\n",
      "Epoch 4183 Loss 2.929764\n",
      "Epoch 4184 Loss 2.929753\n",
      "Epoch 4185 Loss 2.929740\n",
      "Epoch 4186 Loss 2.929730\n",
      "Epoch 4187 Loss 2.929717\n",
      "Epoch 4188 Loss 2.929708\n",
      "Epoch 4189 Loss 2.929695\n",
      "Epoch 4190 Loss 2.929683\n",
      "Epoch 4191 Loss 2.929672\n",
      "Epoch 4192 Loss 2.929663\n",
      "Epoch 4193 Loss 2.929651\n",
      "Epoch 4194 Loss 2.929641\n",
      "Epoch 4195 Loss 2.929629\n",
      "Epoch 4196 Loss 2.929619\n",
      "Epoch 4197 Loss 2.929607\n",
      "Epoch 4198 Loss 2.929596\n",
      "Epoch 4199 Loss 2.929586\n",
      "Epoch 4200 Loss 2.929575\n",
      "Epoch 4201 Loss 2.929565\n",
      "Epoch 4202 Loss 2.929554\n",
      "Epoch 4203 Loss 2.929544\n",
      "Epoch 4204 Loss 2.929533\n",
      "Epoch 4205 Loss 2.929521\n",
      "Epoch 4206 Loss 2.929511\n",
      "Epoch 4207 Loss 2.929501\n",
      "Epoch 4208 Loss 2.929492\n",
      "Epoch 4209 Loss 2.929481\n",
      "Epoch 4210 Loss 2.929471\n",
      "Epoch 4211 Loss 2.929461\n",
      "Epoch 4212 Loss 2.929451\n",
      "Epoch 4213 Loss 2.929440\n",
      "Epoch 4214 Loss 2.929430\n",
      "Epoch 4215 Loss 2.929422\n",
      "Epoch 4216 Loss 2.929411\n",
      "Epoch 4217 Loss 2.929403\n",
      "Epoch 4218 Loss 2.929391\n",
      "Epoch 4219 Loss 2.929382\n",
      "Epoch 4220 Loss 2.929372\n",
      "Epoch 4221 Loss 2.929363\n",
      "Epoch 4222 Loss 2.929352\n",
      "Epoch 4223 Loss 2.929344\n",
      "Epoch 4224 Loss 2.929333\n",
      "Epoch 4225 Loss 2.929325\n",
      "Epoch 4226 Loss 2.929315\n",
      "Epoch 4227 Loss 2.929305\n",
      "Epoch 4228 Loss 2.929296\n",
      "Epoch 4229 Loss 2.929288\n",
      "Epoch 4230 Loss 2.929278\n",
      "Epoch 4231 Loss 2.929269\n",
      "Epoch 4232 Loss 2.929260\n",
      "Epoch 4233 Loss 2.929251\n",
      "Epoch 4234 Loss 2.929243\n",
      "Epoch 4235 Loss 2.929233\n",
      "Epoch 4236 Loss 2.929225\n",
      "Epoch 4237 Loss 2.929217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4238 Loss 2.929207\n",
      "Epoch 4239 Loss 2.929197\n",
      "Epoch 4240 Loss 2.929189\n",
      "Epoch 4241 Loss 2.929180\n",
      "Epoch 4242 Loss 2.929172\n",
      "Epoch 4243 Loss 2.929164\n",
      "Epoch 4244 Loss 2.929154\n",
      "Epoch 4245 Loss 2.929145\n",
      "Epoch 4246 Loss 2.929137\n",
      "Epoch 4247 Loss 2.929131\n",
      "Epoch 4248 Loss 2.929121\n",
      "Epoch 4249 Loss 2.929114\n",
      "Epoch 4250 Loss 2.929105\n",
      "Epoch 4251 Loss 2.929096\n",
      "Epoch 4252 Loss 2.929087\n",
      "Epoch 4253 Loss 2.929080\n",
      "Epoch 4254 Loss 2.929070\n",
      "Epoch 4255 Loss 2.929064\n",
      "Epoch 4256 Loss 2.929054\n",
      "Epoch 4257 Loss 2.929047\n",
      "Epoch 4258 Loss 2.929038\n",
      "Epoch 4259 Loss 2.929032\n",
      "Epoch 4260 Loss 2.929024\n",
      "Epoch 4261 Loss 2.929015\n",
      "Epoch 4262 Loss 2.929008\n",
      "Epoch 4263 Loss 2.929001\n",
      "Epoch 4264 Loss 2.928993\n",
      "Epoch 4265 Loss 2.928985\n",
      "Epoch 4266 Loss 2.928977\n",
      "Epoch 4267 Loss 2.928970\n",
      "Epoch 4268 Loss 2.928962\n",
      "Epoch 4269 Loss 2.928955\n",
      "Epoch 4270 Loss 2.928947\n",
      "Epoch 4271 Loss 2.928939\n",
      "Epoch 4272 Loss 2.928933\n",
      "Epoch 4273 Loss 2.928924\n",
      "Epoch 4274 Loss 2.928917\n",
      "Epoch 4275 Loss 2.928911\n",
      "Epoch 4276 Loss 2.928903\n",
      "Epoch 4277 Loss 2.928896\n",
      "Epoch 4278 Loss 2.928887\n",
      "Epoch 4279 Loss 2.928881\n",
      "Epoch 4280 Loss 2.928874\n",
      "Epoch 4281 Loss 2.928868\n",
      "Epoch 4282 Loss 2.928860\n",
      "Epoch 4283 Loss 2.928854\n",
      "Epoch 4284 Loss 2.928845\n",
      "Epoch 4285 Loss 2.928840\n",
      "Epoch 4286 Loss 2.928832\n",
      "Epoch 4287 Loss 2.928826\n",
      "Epoch 4288 Loss 2.928820\n",
      "Epoch 4289 Loss 2.928812\n",
      "Epoch 4290 Loss 2.928806\n",
      "Epoch 4291 Loss 2.928800\n",
      "Epoch 4292 Loss 2.928792\n",
      "Epoch 4293 Loss 2.928784\n",
      "Epoch 4294 Loss 2.928778\n",
      "Epoch 4295 Loss 2.928772\n",
      "Epoch 4296 Loss 2.928767\n",
      "Epoch 4297 Loss 2.928760\n",
      "Epoch 4298 Loss 2.928754\n",
      "Epoch 4299 Loss 2.928746\n",
      "Epoch 4300 Loss 2.928740\n",
      "Epoch 4301 Loss 2.928734\n",
      "Epoch 4302 Loss 2.928728\n",
      "Epoch 4303 Loss 2.928720\n",
      "Epoch 4304 Loss 2.928715\n",
      "Epoch 4305 Loss 2.928709\n",
      "Epoch 4306 Loss 2.928702\n",
      "Epoch 4307 Loss 2.928696\n",
      "Epoch 4308 Loss 2.928690\n",
      "Epoch 4309 Loss 2.928685\n",
      "Epoch 4310 Loss 2.928679\n",
      "Epoch 4311 Loss 2.928672\n",
      "Epoch 4312 Loss 2.928667\n",
      "Epoch 4313 Loss 2.928661\n",
      "Epoch 4314 Loss 2.928653\n",
      "Epoch 4315 Loss 2.928649\n",
      "Epoch 4316 Loss 2.928642\n",
      "Epoch 4317 Loss 2.928636\n",
      "Epoch 4318 Loss 2.928630\n",
      "Epoch 4319 Loss 2.928625\n",
      "Epoch 4320 Loss 2.928619\n",
      "Epoch 4321 Loss 2.928613\n",
      "Epoch 4322 Loss 2.928607\n",
      "Epoch 4323 Loss 2.928603\n",
      "Epoch 4324 Loss 2.928598\n",
      "Epoch 4325 Loss 2.928592\n",
      "Epoch 4326 Loss 2.928586\n",
      "Epoch 4327 Loss 2.928580\n",
      "Epoch 4328 Loss 2.928575\n",
      "Epoch 4329 Loss 2.928568\n",
      "Epoch 4330 Loss 2.928564\n",
      "Epoch 4331 Loss 2.928558\n",
      "Epoch 4332 Loss 2.928553\n",
      "Epoch 4333 Loss 2.928547\n",
      "Epoch 4334 Loss 2.928543\n",
      "Epoch 4335 Loss 2.928536\n",
      "Epoch 4336 Loss 2.928532\n",
      "Epoch 4337 Loss 2.928526\n",
      "Epoch 4338 Loss 2.928521\n",
      "Epoch 4339 Loss 2.928515\n",
      "Epoch 4340 Loss 2.928512\n",
      "Epoch 4341 Loss 2.928506\n",
      "Epoch 4342 Loss 2.928501\n",
      "Epoch 4343 Loss 2.928495\n",
      "Epoch 4344 Loss 2.928491\n",
      "Epoch 4345 Loss 2.928485\n",
      "Epoch 4346 Loss 2.928481\n",
      "Epoch 4347 Loss 2.928475\n",
      "Epoch 4348 Loss 2.928471\n",
      "Epoch 4349 Loss 2.928465\n",
      "Epoch 4350 Loss 2.928461\n",
      "Epoch 4351 Loss 2.928456\n",
      "Epoch 4352 Loss 2.928452\n",
      "Epoch 4353 Loss 2.928446\n",
      "Epoch 4354 Loss 2.928441\n",
      "Epoch 4355 Loss 2.928435\n",
      "Epoch 4356 Loss 2.928431\n",
      "Epoch 4357 Loss 2.928428\n",
      "Epoch 4358 Loss 2.928423\n",
      "Epoch 4359 Loss 2.928418\n",
      "Epoch 4360 Loss 2.928414\n",
      "Epoch 4361 Loss 2.928410\n",
      "Epoch 4362 Loss 2.928404\n",
      "Epoch 4363 Loss 2.928400\n",
      "Epoch 4364 Loss 2.928394\n",
      "Epoch 4365 Loss 2.928391\n",
      "Epoch 4366 Loss 2.928385\n",
      "Epoch 4367 Loss 2.928383\n",
      "Epoch 4368 Loss 2.928376\n",
      "Epoch 4369 Loss 2.928373\n",
      "Epoch 4370 Loss 2.928370\n",
      "Epoch 4371 Loss 2.928364\n",
      "Epoch 4372 Loss 2.928360\n",
      "Epoch 4373 Loss 2.928355\n",
      "Epoch 4374 Loss 2.928353\n",
      "Epoch 4375 Loss 2.928346\n",
      "Epoch 4376 Loss 2.928342\n",
      "Epoch 4377 Loss 2.928340\n",
      "Epoch 4378 Loss 2.928333\n",
      "Epoch 4379 Loss 2.928332\n",
      "Epoch 4380 Loss 2.928326\n",
      "Epoch 4381 Loss 2.928323\n",
      "Epoch 4382 Loss 2.928317\n",
      "Epoch 4383 Loss 2.928313\n",
      "Epoch 4384 Loss 2.928311\n",
      "Epoch 4385 Loss 2.928307\n",
      "Epoch 4386 Loss 2.928302\n",
      "Epoch 4387 Loss 2.928298\n",
      "Epoch 4388 Loss 2.928294\n",
      "Epoch 4389 Loss 2.928291\n",
      "Epoch 4390 Loss 2.928286\n",
      "Epoch 4391 Loss 2.928281\n",
      "Epoch 4392 Loss 2.928279\n",
      "Epoch 4393 Loss 2.928274\n",
      "Epoch 4394 Loss 2.928270\n",
      "Epoch 4395 Loss 2.928267\n",
      "Epoch 4396 Loss 2.928263\n",
      "Epoch 4397 Loss 2.928260\n",
      "Epoch 4398 Loss 2.928254\n",
      "Epoch 4399 Loss 2.928252\n",
      "Epoch 4400 Loss 2.928248\n",
      "Epoch 4401 Loss 2.928245\n",
      "Epoch 4402 Loss 2.928240\n",
      "Epoch 4403 Loss 2.928238\n",
      "Epoch 4404 Loss 2.928233\n",
      "Epoch 4405 Loss 2.928229\n",
      "Epoch 4406 Loss 2.928226\n",
      "Epoch 4407 Loss 2.928223\n",
      "Epoch 4408 Loss 2.928219\n",
      "Epoch 4409 Loss 2.928215\n",
      "Epoch 4410 Loss 2.928212\n",
      "Epoch 4411 Loss 2.928208\n",
      "Epoch 4412 Loss 2.928206\n",
      "Epoch 4413 Loss 2.928203\n",
      "Epoch 4414 Loss 2.928198\n",
      "Epoch 4415 Loss 2.928195\n",
      "Epoch 4416 Loss 2.928190\n",
      "Epoch 4417 Loss 2.928188\n",
      "Epoch 4418 Loss 2.928185\n",
      "Epoch 4419 Loss 2.928183\n",
      "Epoch 4420 Loss 2.928180\n",
      "Epoch 4421 Loss 2.928175\n",
      "Epoch 4422 Loss 2.928172\n",
      "Epoch 4423 Loss 2.928169\n",
      "Epoch 4424 Loss 2.928166\n",
      "Epoch 4425 Loss 2.928161\n",
      "Epoch 4426 Loss 2.928158\n",
      "Epoch 4427 Loss 2.928155\n",
      "Epoch 4428 Loss 2.928153\n",
      "Epoch 4429 Loss 2.928149\n",
      "Epoch 4430 Loss 2.928146\n",
      "Epoch 4431 Loss 2.928142\n",
      "Epoch 4432 Loss 2.928140\n",
      "Epoch 4433 Loss 2.928138\n",
      "Epoch 4434 Loss 2.928132\n",
      "Epoch 4435 Loss 2.928131\n",
      "Epoch 4436 Loss 2.928129\n",
      "Epoch 4437 Loss 2.928126\n",
      "Epoch 4438 Loss 2.928123\n",
      "Epoch 4439 Loss 2.928119\n",
      "Epoch 4440 Loss 2.928117\n",
      "Epoch 4441 Loss 2.928113\n",
      "Epoch 4442 Loss 2.928110\n",
      "Epoch 4443 Loss 2.928106\n",
      "Epoch 4444 Loss 2.928103\n",
      "Epoch 4445 Loss 2.928102\n",
      "Epoch 4446 Loss 2.928099\n",
      "Epoch 4447 Loss 2.928094\n",
      "Epoch 4448 Loss 2.928093\n",
      "Epoch 4449 Loss 2.928090\n",
      "Epoch 4450 Loss 2.928087\n",
      "Epoch 4451 Loss 2.928086\n",
      "Epoch 4452 Loss 2.928082\n",
      "Epoch 4453 Loss 2.928079\n",
      "Epoch 4454 Loss 2.928077\n",
      "Epoch 4455 Loss 2.928073\n",
      "Epoch 4456 Loss 2.928070\n",
      "Epoch 4457 Loss 2.928066\n",
      "Epoch 4458 Loss 2.928066\n",
      "Epoch 4459 Loss 2.928062\n",
      "Epoch 4460 Loss 2.928061\n",
      "Epoch 4461 Loss 2.928058\n",
      "Epoch 4462 Loss 2.928055\n",
      "Epoch 4463 Loss 2.928053\n",
      "Epoch 4464 Loss 2.928049\n",
      "Epoch 4465 Loss 2.928048\n",
      "Epoch 4466 Loss 2.928046\n",
      "Epoch 4467 Loss 2.928043\n",
      "Epoch 4468 Loss 2.928039\n",
      "Epoch 4469 Loss 2.928037\n",
      "Epoch 4470 Loss 2.928036\n",
      "Epoch 4471 Loss 2.928031\n",
      "Epoch 4472 Loss 2.928031\n",
      "Epoch 4473 Loss 2.928028\n",
      "Epoch 4474 Loss 2.928026\n",
      "Epoch 4475 Loss 2.928022\n",
      "Epoch 4476 Loss 2.928020\n",
      "Epoch 4477 Loss 2.928018\n",
      "Epoch 4478 Loss 2.928014\n",
      "Epoch 4479 Loss 2.928013\n",
      "Epoch 4480 Loss 2.928012\n",
      "Epoch 4481 Loss 2.928010\n",
      "Epoch 4482 Loss 2.928006\n",
      "Epoch 4483 Loss 2.928004\n",
      "Epoch 4484 Loss 2.928001\n",
      "Epoch 4485 Loss 2.927999\n",
      "Epoch 4486 Loss 2.927997\n",
      "Epoch 4487 Loss 2.927995\n",
      "Epoch 4488 Loss 2.927991\n",
      "Epoch 4489 Loss 2.927990\n",
      "Epoch 4490 Loss 2.927988\n",
      "Epoch 4491 Loss 2.927986\n",
      "Epoch 4492 Loss 2.927984\n",
      "Epoch 4493 Loss 2.927981\n",
      "Epoch 4494 Loss 2.927978\n",
      "Epoch 4495 Loss 2.927977\n",
      "Epoch 4496 Loss 2.927975\n",
      "Epoch 4497 Loss 2.927972\n",
      "Epoch 4498 Loss 2.927971\n",
      "Epoch 4499 Loss 2.927968\n",
      "Epoch 4500 Loss 2.927966\n",
      "Epoch 4501 Loss 2.927965\n",
      "Epoch 4502 Loss 2.927964\n",
      "Epoch 4503 Loss 2.927962\n",
      "Epoch 4504 Loss 2.927958\n",
      "Epoch 4505 Loss 2.927958\n",
      "Epoch 4506 Loss 2.927955\n",
      "Epoch 4507 Loss 2.927951\n",
      "Epoch 4508 Loss 2.927950\n",
      "Epoch 4509 Loss 2.927949\n",
      "Epoch 4510 Loss 2.927946\n",
      "Epoch 4511 Loss 2.927945\n",
      "Epoch 4512 Loss 2.927943\n",
      "Epoch 4513 Loss 2.927940\n",
      "Epoch 4514 Loss 2.927938\n",
      "Epoch 4515 Loss 2.927937\n",
      "Epoch 4516 Loss 2.927936\n",
      "Epoch 4517 Loss 2.927934\n",
      "Epoch 4518 Loss 2.927933\n",
      "Epoch 4519 Loss 2.927929\n",
      "Epoch 4520 Loss 2.927929\n",
      "Epoch 4521 Loss 2.927925\n",
      "Epoch 4522 Loss 2.927924\n",
      "Epoch 4523 Loss 2.927923\n",
      "Epoch 4524 Loss 2.927921\n",
      "Epoch 4525 Loss 2.927918\n",
      "Epoch 4526 Loss 2.927917\n",
      "Epoch 4527 Loss 2.927916\n",
      "Epoch 4528 Loss 2.927913\n",
      "Epoch 4529 Loss 2.927912\n",
      "Epoch 4530 Loss 2.927909\n",
      "Epoch 4531 Loss 2.927909\n",
      "Epoch 4532 Loss 2.927906\n",
      "Epoch 4533 Loss 2.927904\n",
      "Epoch 4534 Loss 2.927904\n",
      "Epoch 4535 Loss 2.927901\n",
      "Epoch 4536 Loss 2.927899\n",
      "Epoch 4537 Loss 2.927898\n",
      "Epoch 4538 Loss 2.927897\n",
      "Epoch 4539 Loss 2.927895\n",
      "Epoch 4540 Loss 2.927893\n",
      "Epoch 4541 Loss 2.927892\n",
      "Epoch 4542 Loss 2.927889\n",
      "Epoch 4543 Loss 2.927888\n",
      "Epoch 4544 Loss 2.927885\n",
      "Epoch 4545 Loss 2.927885\n",
      "Epoch 4546 Loss 2.927884\n",
      "Epoch 4547 Loss 2.927882\n",
      "Epoch 4548 Loss 2.927880\n",
      "Epoch 4549 Loss 2.927879\n",
      "Epoch 4550 Loss 2.927878\n",
      "Epoch 4551 Loss 2.927876\n",
      "Epoch 4552 Loss 2.927874\n",
      "Epoch 4553 Loss 2.927873\n",
      "Epoch 4554 Loss 2.927870\n",
      "Epoch 4555 Loss 2.927869\n",
      "Epoch 4556 Loss 2.927867\n",
      "Epoch 4557 Loss 2.927866\n",
      "Epoch 4558 Loss 2.927863\n",
      "Epoch 4559 Loss 2.927863\n",
      "Epoch 4560 Loss 2.927862\n",
      "Epoch 4561 Loss 2.927861\n",
      "Epoch 4562 Loss 2.927860\n",
      "Epoch 4563 Loss 2.927859\n",
      "Epoch 4564 Loss 2.927857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4565 Loss 2.927856\n",
      "Epoch 4566 Loss 2.927855\n",
      "Epoch 4567 Loss 2.927853\n",
      "Epoch 4568 Loss 2.927852\n",
      "Epoch 4569 Loss 2.927850\n",
      "Epoch 4570 Loss 2.927849\n",
      "Epoch 4571 Loss 2.927848\n",
      "Epoch 4572 Loss 2.927844\n",
      "Epoch 4573 Loss 2.927846\n",
      "Epoch 4574 Loss 2.927841\n",
      "Epoch 4575 Loss 2.927842\n",
      "Epoch 4576 Loss 2.927840\n",
      "Epoch 4577 Loss 2.927839\n",
      "Epoch 4578 Loss 2.927838\n",
      "Epoch 4579 Loss 2.927835\n",
      "Epoch 4580 Loss 2.927836\n",
      "Epoch 4581 Loss 2.927834\n",
      "Epoch 4582 Loss 2.927831\n",
      "Epoch 4583 Loss 2.927832\n",
      "Epoch 4584 Loss 2.927830\n",
      "Epoch 4585 Loss 2.927830\n",
      "Epoch 4586 Loss 2.927828\n",
      "Epoch 4587 Loss 2.927827\n",
      "Epoch 4588 Loss 2.927826\n",
      "Epoch 4589 Loss 2.927825\n",
      "Epoch 4590 Loss 2.927823\n",
      "Epoch 4591 Loss 2.927822\n",
      "Epoch 4592 Loss 2.927819\n",
      "Epoch 4593 Loss 2.927819\n",
      "Epoch 4594 Loss 2.927819\n",
      "Epoch 4595 Loss 2.927818\n",
      "Epoch 4596 Loss 2.927815\n",
      "Epoch 4597 Loss 2.927814\n",
      "Epoch 4598 Loss 2.927814\n",
      "Epoch 4599 Loss 2.927811\n",
      "Epoch 4600 Loss 2.927811\n",
      "Epoch 4601 Loss 2.927810\n",
      "Epoch 4602 Loss 2.927809\n",
      "Epoch 4603 Loss 2.927808\n",
      "Epoch 4604 Loss 2.927808\n",
      "Epoch 4605 Loss 2.927806\n",
      "Epoch 4606 Loss 2.927804\n",
      "Epoch 4607 Loss 2.927803\n",
      "Epoch 4608 Loss 2.927803\n",
      "Epoch 4609 Loss 2.927802\n",
      "Epoch 4610 Loss 2.927799\n",
      "Epoch 4611 Loss 2.927799\n",
      "Epoch 4612 Loss 2.927798\n",
      "Epoch 4613 Loss 2.927799\n",
      "Epoch 4614 Loss 2.927798\n",
      "Epoch 4615 Loss 2.927796\n",
      "Epoch 4616 Loss 2.927795\n",
      "Epoch 4617 Loss 2.927794\n",
      "Epoch 4618 Loss 2.927793\n",
      "Epoch 4619 Loss 2.927792\n",
      "Epoch 4620 Loss 2.927791\n",
      "Epoch 4621 Loss 2.927790\n",
      "Epoch 4622 Loss 2.927787\n",
      "Epoch 4623 Loss 2.927787\n",
      "Epoch 4624 Loss 2.927786\n",
      "Epoch 4625 Loss 2.927785\n",
      "Epoch 4626 Loss 2.927785\n",
      "Epoch 4627 Loss 2.927784\n",
      "Epoch 4628 Loss 2.927783\n",
      "Epoch 4629 Loss 2.927782\n",
      "Epoch 4630 Loss 2.927781\n",
      "Epoch 4631 Loss 2.927779\n",
      "Epoch 4632 Loss 2.927778\n",
      "Epoch 4633 Loss 2.927779\n",
      "Epoch 4634 Loss 2.927778\n",
      "Epoch 4635 Loss 2.927775\n",
      "Epoch 4636 Loss 2.927775\n",
      "Epoch 4637 Loss 2.927775\n",
      "Epoch 4638 Loss 2.927775\n",
      "Epoch 4639 Loss 2.927773\n",
      "Epoch 4640 Loss 2.927773\n",
      "Epoch 4641 Loss 2.927770\n",
      "Epoch 4642 Loss 2.927770\n",
      "Epoch 4643 Loss 2.927769\n",
      "Epoch 4644 Loss 2.927768\n",
      "Epoch 4645 Loss 2.927767\n",
      "Epoch 4646 Loss 2.927765\n",
      "Epoch 4647 Loss 2.927767\n",
      "Epoch 4648 Loss 2.927765\n",
      "Epoch 4649 Loss 2.927763\n",
      "Epoch 4650 Loss 2.927763\n",
      "Epoch 4651 Loss 2.927763\n",
      "Epoch 4652 Loss 2.927761\n",
      "Epoch 4653 Loss 2.927761\n",
      "Epoch 4654 Loss 2.927760\n",
      "Epoch 4655 Loss 2.927759\n",
      "Epoch 4656 Loss 2.927758\n",
      "Epoch 4657 Loss 2.927758\n",
      "Epoch 4658 Loss 2.927757\n",
      "Epoch 4659 Loss 2.927755\n",
      "Epoch 4660 Loss 2.927756\n",
      "Epoch 4661 Loss 2.927754\n",
      "Epoch 4662 Loss 2.927754\n",
      "Epoch 4663 Loss 2.927754\n",
      "Epoch 4664 Loss 2.927753\n",
      "Epoch 4665 Loss 2.927752\n",
      "Epoch 4666 Loss 2.927751\n",
      "Epoch 4667 Loss 2.927750\n",
      "Epoch 4668 Loss 2.927749\n",
      "Epoch 4669 Loss 2.927747\n",
      "Epoch 4670 Loss 2.927747\n",
      "Epoch 4671 Loss 2.927747\n",
      "Epoch 4672 Loss 2.927746\n",
      "Epoch 4673 Loss 2.927746\n",
      "Epoch 4674 Loss 2.927744\n",
      "Epoch 4675 Loss 2.927744\n",
      "Epoch 4676 Loss 2.927744\n",
      "Epoch 4677 Loss 2.927744\n",
      "Epoch 4678 Loss 2.927741\n",
      "Epoch 4679 Loss 2.927741\n",
      "Epoch 4680 Loss 2.927741\n",
      "Epoch 4681 Loss 2.927740\n",
      "Epoch 4682 Loss 2.927740\n",
      "Epoch 4683 Loss 2.927740\n",
      "Epoch 4684 Loss 2.927739\n",
      "Epoch 4685 Loss 2.927738\n",
      "Epoch 4686 Loss 2.927736\n",
      "Epoch 4687 Loss 2.927737\n",
      "Epoch 4688 Loss 2.927735\n",
      "Epoch 4689 Loss 2.927734\n",
      "Epoch 4690 Loss 2.927733\n",
      "Epoch 4691 Loss 2.927734\n",
      "Epoch 4692 Loss 2.927732\n",
      "Epoch 4693 Loss 2.927732\n",
      "Epoch 4694 Loss 2.927733\n",
      "Epoch 4695 Loss 2.927730\n",
      "Epoch 4696 Loss 2.927730\n",
      "Epoch 4697 Loss 2.927732\n",
      "Epoch 4698 Loss 2.927731\n",
      "Epoch 4699 Loss 2.927728\n",
      "Epoch 4700 Loss 2.927729\n",
      "Epoch 4701 Loss 2.927728\n",
      "Epoch 4702 Loss 2.927727\n",
      "Epoch 4703 Loss 2.927726\n",
      "Epoch 4704 Loss 2.927726\n",
      "Epoch 4705 Loss 2.927725\n",
      "Epoch 4706 Loss 2.927725\n",
      "Epoch 4707 Loss 2.927724\n",
      "Epoch 4708 Loss 2.927724\n",
      "Epoch 4709 Loss 2.927723\n",
      "Epoch 4710 Loss 2.927722\n",
      "Epoch 4711 Loss 2.927723\n",
      "Epoch 4712 Loss 2.927721\n",
      "Epoch 4713 Loss 2.927720\n",
      "Epoch 4714 Loss 2.927719\n",
      "Epoch 4715 Loss 2.927719\n",
      "Epoch 4716 Loss 2.927718\n",
      "Epoch 4717 Loss 2.927719\n",
      "Epoch 4718 Loss 2.927717\n",
      "Epoch 4719 Loss 2.927718\n",
      "Epoch 4720 Loss 2.927718\n",
      "Epoch 4721 Loss 2.927718\n",
      "Epoch 4722 Loss 2.927716\n",
      "Epoch 4723 Loss 2.927715\n",
      "Epoch 4724 Loss 2.927716\n",
      "Epoch 4725 Loss 2.927714\n",
      "Epoch 4726 Loss 2.927714\n",
      "Epoch 4727 Loss 2.927712\n",
      "Epoch 4728 Loss 2.927713\n",
      "Epoch 4729 Loss 2.927711\n",
      "Epoch 4730 Loss 2.927712\n",
      "Epoch 4731 Loss 2.927711\n",
      "Epoch 4732 Loss 2.927711\n",
      "Epoch 4733 Loss 2.927711\n",
      "Epoch 4734 Loss 2.927710\n",
      "Epoch 4735 Loss 2.927709\n",
      "Epoch 4736 Loss 2.927709\n",
      "Epoch 4737 Loss 2.927710\n",
      "Epoch 4738 Loss 2.927709\n",
      "Epoch 4739 Loss 2.927708\n",
      "Epoch 4740 Loss 2.927707\n",
      "Epoch 4741 Loss 2.927706\n",
      "Epoch 4742 Loss 2.927706\n",
      "Epoch 4743 Loss 2.927706\n",
      "Epoch 4744 Loss 2.927707\n",
      "Epoch 4745 Loss 2.927705\n",
      "Epoch 4746 Loss 2.927705\n",
      "Epoch 4747 Loss 2.927704\n",
      "Epoch 4748 Loss 2.927702\n",
      "Epoch 4749 Loss 2.927704\n",
      "Epoch 4750 Loss 2.927703\n",
      "Epoch 4751 Loss 2.927704\n",
      "Epoch 4752 Loss 2.927704\n",
      "Epoch 4753 Loss 2.927702\n",
      "Epoch 4754 Loss 2.927701\n",
      "Epoch 4755 Loss 2.927701\n",
      "Epoch 4756 Loss 2.927702\n",
      "Epoch 4757 Loss 2.927699\n",
      "Epoch 4758 Loss 2.927699\n",
      "Epoch 4759 Loss 2.927700\n",
      "Epoch 4760 Loss 2.927699\n",
      "Epoch 4761 Loss 2.927700\n",
      "Epoch 4762 Loss 2.927700\n",
      "Epoch 4763 Loss 2.927697\n",
      "Epoch 4764 Loss 2.927696\n",
      "Epoch 4765 Loss 2.927696\n",
      "Epoch 4766 Loss 2.927696\n",
      "Epoch 4767 Loss 2.927698\n",
      "Epoch 4768 Loss 2.927698\n",
      "Epoch 4769 Loss 2.927697\n",
      "Epoch 4770 Loss 2.927696\n",
      "Epoch 4771 Loss 2.927696\n",
      "Epoch 4772 Loss 2.927695\n",
      "Epoch 4773 Loss 2.927693\n",
      "Epoch 4774 Loss 2.927694\n",
      "Epoch 4775 Loss 2.927693\n",
      "Epoch 4776 Loss 2.927693\n",
      "Epoch 4777 Loss 2.927693\n",
      "Epoch 4778 Loss 2.927692\n",
      "Epoch 4779 Loss 2.927691\n",
      "Epoch 4780 Loss 2.927691\n",
      "Epoch 4781 Loss 2.927692\n",
      "Epoch 4782 Loss 2.927689\n",
      "Epoch 4783 Loss 2.927691\n",
      "Epoch 4784 Loss 2.927691\n",
      "Epoch 4785 Loss 2.927690\n",
      "Epoch 4786 Loss 2.927690\n",
      "Epoch 4787 Loss 2.927689\n",
      "Epoch 4788 Loss 2.927689\n",
      "Epoch 4789 Loss 2.927689\n",
      "Epoch 4790 Loss 2.927688\n",
      "Epoch 4791 Loss 2.927688\n",
      "Epoch 4792 Loss 2.927688\n",
      "Epoch 4793 Loss 2.927689\n",
      "Epoch 4794 Loss 2.927686\n",
      "Epoch 4795 Loss 2.927687\n",
      "Epoch 4796 Loss 2.927686\n",
      "Epoch 4797 Loss 2.927686\n",
      "Epoch 4798 Loss 2.927686\n",
      "Epoch 4799 Loss 2.927685\n",
      "Epoch 4800 Loss 2.927686\n",
      "Epoch 4801 Loss 2.927686\n",
      "Epoch 4802 Loss 2.927685\n",
      "Epoch 4803 Loss 2.927685\n",
      "Epoch 4804 Loss 2.927684\n",
      "Epoch 4805 Loss 2.927685\n",
      "Epoch 4806 Loss 2.927685\n",
      "Epoch 4807 Loss 2.927683\n",
      "Epoch 4808 Loss 2.927682\n",
      "Epoch 4809 Loss 2.927683\n",
      "Epoch 4810 Loss 2.927682\n",
      "Epoch 4811 Loss 2.927681\n",
      "Epoch 4812 Loss 2.927682\n",
      "Epoch 4813 Loss 2.927681\n",
      "Epoch 4814 Loss 2.927681\n",
      "Epoch 4815 Loss 2.927681\n",
      "Epoch 4816 Loss 2.927681\n",
      "Epoch 4817 Loss 2.927681\n",
      "Epoch 4818 Loss 2.927681\n",
      "Epoch 4819 Loss 2.927679\n",
      "Epoch 4820 Loss 2.927679\n",
      "Epoch 4821 Loss 2.927681\n",
      "Epoch 4822 Loss 2.927680\n",
      "Epoch 4823 Loss 2.927680\n",
      "Epoch 4824 Loss 2.927680\n",
      "Epoch 4825 Loss 2.927679\n",
      "Epoch 4826 Loss 2.927678\n",
      "Epoch 4827 Loss 2.927679\n",
      "Epoch 4828 Loss 2.927676\n",
      "Epoch 4829 Loss 2.927677\n",
      "Epoch 4830 Loss 2.927677\n",
      "Epoch 4831 Loss 2.927675\n",
      "Epoch 4832 Loss 2.927677\n",
      "Epoch 4833 Loss 2.927677\n",
      "Epoch 4834 Loss 2.927677\n",
      "Epoch 4835 Loss 2.927677\n",
      "Epoch 4836 Loss 2.927676\n",
      "Epoch 4837 Loss 2.927676\n",
      "Epoch 4838 Loss 2.927675\n",
      "Epoch 4839 Loss 2.927674\n",
      "Epoch 4840 Loss 2.927676\n",
      "Epoch 4841 Loss 2.927675\n",
      "Epoch 4842 Loss 2.927674\n",
      "Epoch 4843 Loss 2.927674\n",
      "Epoch 4844 Loss 2.927673\n",
      "Epoch 4845 Loss 2.927675\n",
      "Epoch 4846 Loss 2.927674\n",
      "Epoch 4847 Loss 2.927673\n",
      "Epoch 4848 Loss 2.927674\n",
      "Epoch 4849 Loss 2.927672\n",
      "Epoch 4850 Loss 2.927672\n",
      "Epoch 4851 Loss 2.927673\n",
      "Epoch 4852 Loss 2.927673\n",
      "Epoch 4853 Loss 2.927672\n",
      "Epoch 4854 Loss 2.927671\n",
      "Epoch 4855 Loss 2.927672\n",
      "Epoch 4856 Loss 2.927672\n",
      "Epoch 4857 Loss 2.927671\n",
      "Epoch 4858 Loss 2.927671\n",
      "Epoch 4859 Loss 2.927672\n",
      "Epoch 4860 Loss 2.927670\n",
      "Epoch 4861 Loss 2.927670\n",
      "Epoch 4862 Loss 2.927670\n",
      "Epoch 4863 Loss 2.927669\n",
      "Epoch 4864 Loss 2.927670\n",
      "Epoch 4865 Loss 2.927671\n",
      "Epoch 4866 Loss 2.927670\n",
      "Epoch 4867 Loss 2.927670\n",
      "Epoch 4868 Loss 2.927669\n",
      "Epoch 4869 Loss 2.927670\n",
      "Epoch 4870 Loss 2.927669\n",
      "Epoch 4871 Loss 2.927669\n",
      "Epoch 4872 Loss 2.927670\n",
      "Epoch 4873 Loss 2.927668\n",
      "Epoch 4874 Loss 2.927669\n",
      "Epoch 4875 Loss 2.927668\n",
      "Epoch 4876 Loss 2.927667\n",
      "Epoch 4877 Loss 2.927669\n",
      "Epoch 4878 Loss 2.927667\n",
      "Epoch 4879 Loss 2.927668\n",
      "Epoch 4880 Loss 2.927667\n",
      "Epoch 4881 Loss 2.927667\n",
      "Epoch 4882 Loss 2.927667\n",
      "Epoch 4883 Loss 2.927668\n",
      "Epoch 4884 Loss 2.927666\n",
      "Epoch 4885 Loss 2.927666\n",
      "Epoch 4886 Loss 2.927666\n",
      "Epoch 4887 Loss 2.927665\n",
      "Epoch 4888 Loss 2.927667\n",
      "Epoch 4889 Loss 2.927666\n",
      "Epoch 4890 Loss 2.927665\n",
      "Epoch 4891 Loss 2.927665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4892 Loss 2.927665\n",
      "Epoch 4893 Loss 2.927665\n",
      "Epoch 4894 Loss 2.927665\n",
      "Epoch 4895 Loss 2.927664\n",
      "Epoch 4896 Loss 2.927665\n",
      "Epoch 4897 Loss 2.927665\n",
      "Epoch 4898 Loss 2.927665\n",
      "Epoch 4899 Loss 2.927664\n",
      "Epoch 4900 Loss 2.927665\n",
      "Epoch 4901 Loss 2.927663\n",
      "Epoch 4902 Loss 2.927664\n",
      "Epoch 4903 Loss 2.927664\n",
      "Epoch 4904 Loss 2.927663\n",
      "Epoch 4905 Loss 2.927663\n",
      "Epoch 4906 Loss 2.927664\n",
      "Epoch 4907 Loss 2.927663\n",
      "Epoch 4908 Loss 2.927665\n",
      "Epoch 4909 Loss 2.927664\n",
      "Epoch 4910 Loss 2.927664\n",
      "Epoch 4911 Loss 2.927663\n",
      "Epoch 4912 Loss 2.927663\n",
      "Epoch 4913 Loss 2.927662\n",
      "Epoch 4914 Loss 2.927663\n",
      "Epoch 4915 Loss 2.927661\n",
      "Epoch 4916 Loss 2.927662\n",
      "Epoch 4917 Loss 2.927661\n",
      "Epoch 4918 Loss 2.927663\n",
      "Epoch 4919 Loss 2.927662\n",
      "Epoch 4920 Loss 2.927663\n",
      "Epoch 4921 Loss 2.927661\n",
      "Epoch 4922 Loss 2.927661\n",
      "Epoch 4923 Loss 2.927660\n",
      "Epoch 4924 Loss 2.927660\n",
      "Epoch 4925 Loss 2.927660\n",
      "Epoch 4926 Loss 2.927660\n",
      "Epoch 4927 Loss 2.927662\n",
      "Epoch 4928 Loss 2.927661\n",
      "Epoch 4929 Loss 2.927662\n",
      "Epoch 4930 Loss 2.927660\n",
      "Epoch 4931 Loss 2.927660\n",
      "Epoch 4932 Loss 2.927660\n",
      "Epoch 4933 Loss 2.927660\n",
      "Epoch 4934 Loss 2.927658\n",
      "Epoch 4935 Loss 2.927660\n",
      "Epoch 4936 Loss 2.927660\n",
      "Epoch 4937 Loss 2.927660\n",
      "Epoch 4938 Loss 2.927660\n",
      "Epoch 4939 Loss 2.927659\n",
      "Epoch 4940 Loss 2.927658\n",
      "Epoch 4941 Loss 2.927659\n",
      "Epoch 4942 Loss 2.927660\n",
      "Epoch 4943 Loss 2.927659\n",
      "Epoch 4944 Loss 2.927659\n",
      "Epoch 4945 Loss 2.927658\n",
      "Epoch 4946 Loss 2.927659\n",
      "Epoch 4947 Loss 2.927659\n",
      "Epoch 4948 Loss 2.927657\n",
      "Epoch 4949 Loss 2.927658\n",
      "Epoch 4950 Loss 2.927659\n",
      "Epoch 4951 Loss 2.927658\n",
      "Epoch 4952 Loss 2.927658\n",
      "Epoch 4953 Loss 2.927658\n",
      "Epoch 4954 Loss 2.927658\n",
      "Epoch 4955 Loss 2.927660\n",
      "Epoch 4956 Loss 2.927657\n",
      "Epoch 4957 Loss 2.927657\n",
      "Epoch 4958 Loss 2.927658\n",
      "Epoch 4959 Loss 2.927656\n",
      "Epoch 4960 Loss 2.927657\n",
      "Epoch 4961 Loss 2.927657\n",
      "Epoch 4962 Loss 2.927657\n",
      "Epoch 4963 Loss 2.927658\n",
      "Epoch 4964 Loss 2.927657\n",
      "Epoch 4965 Loss 2.927656\n",
      "Epoch 4966 Loss 2.927656\n",
      "Epoch 4967 Loss 2.927657\n",
      "Epoch 4968 Loss 2.927657\n",
      "Epoch 4969 Loss 2.927657\n",
      "Epoch 4970 Loss 2.927655\n",
      "Epoch 4971 Loss 2.927656\n",
      "Epoch 4972 Loss 2.927655\n",
      "Epoch 4973 Loss 2.927656\n",
      "Epoch 4974 Loss 2.927658\n",
      "Epoch 4975 Loss 2.927656\n",
      "Epoch 4976 Loss 2.927656\n",
      "Epoch 4977 Loss 2.927656\n",
      "Epoch 4978 Loss 2.927655\n",
      "Epoch 4979 Loss 2.927655\n",
      "Epoch 4980 Loss 2.927655\n",
      "Epoch 4981 Loss 2.927655\n",
      "Epoch 4982 Loss 2.927655\n",
      "Epoch 4983 Loss 2.927655\n",
      "Epoch 4984 Loss 2.927655\n",
      "Epoch 4985 Loss 2.927656\n",
      "Epoch 4986 Loss 2.927655\n",
      "Epoch 4987 Loss 2.927656\n",
      "Epoch 4988 Loss 2.927655\n",
      "Epoch 4989 Loss 2.927655\n",
      "Epoch 4990 Loss 2.927656\n",
      "Epoch 4991 Loss 2.927654\n",
      "Epoch 4992 Loss 2.927655\n",
      "Epoch 4993 Loss 2.927654\n",
      "Epoch 4994 Loss 2.927654\n",
      "Epoch 4995 Loss 2.927655\n",
      "Epoch 4996 Loss 2.927653\n",
      "Epoch 4997 Loss 2.927654\n",
      "Epoch 4998 Loss 2.927654\n",
      "Epoch 4999 Loss 2.927654\n"
     ]
    }
   ],
   "source": [
    "params = torch.tensor([1.0,0.0], requires_grad=True)\n",
    "lr = 1e-2\n",
    "epochs = 5000\n",
    "optimizer = optim.Adam([params], lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    t_p = model(t_u, *params)\n",
    "    loss = loss_fn(t_p, t_c)\n",
    "    print('Epoch %d Loss %f' % (epoch, float(loss)))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "t_p = model(t_u, *params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  5.3660, -17.2952], requires_grad=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2579710d908>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(0.1 * t_u.numpy(), t_p.detach().numpy())\n",
    "plt.plot(0.1 * t_u.numpy(), t_c.numpy(),'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 9,  7,  5,  6,  1,  2,  0,  3, 10]), tensor([4, 8]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "train_indices, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_u_train = t_u[train_indices] * 0.1\n",
    "t_c_train = t_c[train_indices]\n",
    "t_u_val = t_u[val_indices] * 0.1\n",
    "t_c_val = t_c[val_indices] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0,0.0], requires_grad=True)\n",
    "lr = 1e-2\n",
    "epochs = 5000\n",
    "optimizer = optim.SGD([params], lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training Loss 2.927654, Val Loss: 69.678619\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-27a8bdb919ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch %d training Loss %f, Val Loss: %f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mt_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_u\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    t_p_train = model(t_u_train, *params)\n",
    "    \n",
    "    loss_train = loss_fn(t_p_train, t_c_train)\n",
    "    t_p_val = model(t_u_val,*params)\n",
    "    loss_val = loss_fn(t_p_val, t_c_val)\n",
    "    \n",
    "    print('Epoch %d training Loss %f, Val Loss: %f' % (epoch, float(loss),float(loss_val)))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "t_p = model(t_u, *params)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
